#ignore
# draw = True
# mask = tf.expand_dims(look_ahead_mask, axis=0)
# output, attention_weights = scaled_dot_product_attention_demo(
#     temp_q, temp_k, temp_v, mask, draw=draw)
# print("output:", output)
# print("-" * 20)
# print("attention_weights:", attention_weights)