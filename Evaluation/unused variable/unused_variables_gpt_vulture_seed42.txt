[{'reason': 'stop', 'result': 'fig, axs = plt.subplots(1,2, figsize=(10,4))\naxs[0].imshow(img_advi)\naxs[0].set_title("segmented image (ADVI)")\naxs[1].hist(y, bins=K);\naxs[1].set_title("cluster assignments (ADVI)")\nplt.tight_layout()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '(score_vgg16_seq,) = VGG16Seq.evaluate(arr_X_test, arr_y_test, verbose=True)\n\nscore_vgg16_seq```'}, {'reason': 'stop', 'result': "#set up lists containing calculated rations for plotting \nearningscost10k = [earningscost for earningscost in df0_10k_grouped['earnings_cost_ratio']]\nearningsgrowth10k = [earnings for earnings in df0_10k_grouped['earnings_growth_y6_y10']]\nweighted_growth10k = [worthit for worthit in df0_10k_grouped['weighted_growth_to_tuition']]\n\nearningscost10k_18k = [earningscost for earningscost in df10_18k_grouped['earnings_cost_ratio']]\nearningsgrowth10k_18k = [earnings for earnings in df10_18k_grouped['earnings_growth_y6_y10']]\nweighted_growth10k_18k = [worthit for worthit in df10_18k_grouped['weighted_growth_to_tuition']]\n\nearningscost18k_32k = [earningscost for earningscost in df18_32_grouped['earnings_cost_ratio']]\nearningsgrowth18k_32k = [earnings for earnings in df18_32_grouped['earnings_growth_y6_y10']]\nweighted_growth18k_32k = [worthit for worthit in df18_32_grouped['weighted_growth_to_tuition']]\n\nearningscost32k = [earningscost for earningscost in df32_grouped['earnings_cost_ratio']]\nearningsgrowth32k = [earnings for earnings in df32_grouped['earnings_growth_y6_y10']]\nweighted_growth32k = [worthit for worthit in df32_grouped['weighted_growth_to_tuition']]\n\n```\n\nUnused variables:\n- earningscost10k_18k\n- earningsgrowth10k_18k\n- weighted_growth10k_18k\n- earningscost18k_32k\n- earningsgrowth18k_32k\n- weighted_growth18k_32k\n- earningsgrowth32kk\n- weighted_growth32k"}, {'reason': 'stop', 'result': "# Load the clip file\nclip2 = VideoFileClip('challenge.mp4')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'yhat = forecast_lstm(lstm_model, 1, X)'}, {'reason': 'stop', 'result': '# call pandas describe method on dataframe\ndescribe = data.describe(include="all")\n\n# transpose\ndescribe_transposed = describe.T\n\n# reset_index, moving the column names into a new series\ndescribe_final = describe_transposed.reset_index()```\n\nUnused variables:\n- describe_final'}, {'reason': 'stop', 'result': 'x_values = None\ny_values = None```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# Create the mfusg headfile object\nheadfile = os.path.join(modelpth, '{0}.hds'.format(modelname))\nheadobj = flopy.utils.HeadFile(headfile)\ntimes = headobj.get_times()\nmfusghead = headobj.get_data(totim=times[-1])```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'fig, ax = plt.subplots(figsize=(16,8))\nbp = dfBabies.boxplot(column="weight", by="smoke", ax=ax, return_type="dict")\n\nfor column in bp:\n    for box in column[\'boxes\']:\n        box.set(color=\'steelblue\', linewidth=2)\n    \n    for whisker in column[\'whiskers\']:\n        whisker.set(color=\'gray\', linewidth=2)\n\n    for cap in column[\'caps\']:\n        cap.set(color=\'gray\', linewidth=2)\n\n    for cap in column[\'medians\']:\n        cap.set(color=\'green\', linewidth=2, alpha=0.5)\n\n    for cap in column[\'fliers\']:\n        cap.set(markerfacecolor=\'steelblue\', linewidth=2, marker=\'s\', markersize=6, alpha=0.5)\n\nax.set_title(\'Weight of Smoker\\\'s Babies vs Non-Smoker\\\'s Babies\', fontsize=18)\nax.set_ylabel("Weight (in Ounces)", fontsize=16)\n\nshort_names = ["Non-Smoker", "Smoker"]\nplt.xticks(range(1,len(short_names)+1),short_names, rotation=90, fontsize=16)\n\nplt.suptitle("")\nax.set_xlabel("")\n\nax.grid(alpha=0.25)'}, {'reason': 'stop', 'result': 'cadencepath = "/Users/berto/projects/LSST/cadence/LSST_DDF_2786_cadence.dat"\n\ndata = Table.read(cadencepath, format="ascii.commented_header")\nw = np.logical_or(data["filter"] == "LSST_g", data["filter"] == "LSST_r")\n\nmjd_to_sim = data[w]["MJD"].data\nfilters_to_sim = convert_column_string_encoding(data[w]["filter"]).data\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'import math\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef filterFn(alpha, prev, new):\n    return alpha * new + (1 - alpha) * prev\n\ndef clearUnjitter():\n    global previous\n    previous = {\n        "right": [0, 0, 0, 0], # x1, y1, x2, y2\n        "left": [0, 0, 0, 0]\n    }\n\ndef unjitter(side, value):\n    global previous\n    alpha = 0.6\n    # destructure value\n    x1 = value[0]\n    y1 = value[1]\n    x2 = value[2]\n    y2 = value[3]\n    # set start value on first frame\n    if previous[side][0] == 0:\n        previous[side][0] = x1\n    if previous[side][1] == 0:\n        previous[side][1] = y1\n    if previous[side][2] == 0:\n        previous[side][2] = x2\n    if previous[side][3] == 0:\n        previous[side][3] = y2\n    # calculate filtered results\n    x1 = filterFn(alpha, previous[side][0], x1)\n    y1 = filterFn(alpha, previous[side][1], y1)\n    x2 = filterFn(alpha, previous[side][2], x2)\n    y2 = filterFn(alpha, previous[side][3], y2)\n    # set new values to previous\n    previous[side][0] = x1\n    previous[side][1] = y1\n    previous[side][2] = x2\n    previous[side][3] = y2\n    return [x1, y1, x2, y2]\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=5):\n    global previous\n    sides = {\n        "left": [],\n        "right": []\n    }\n    yMin = img.shape[0]\n    xHalf = img.shape[1] / 2\n    yMax = 315\n    drawn = []\n    if lines != None:\n        for line in lines:\n            for x1,y1,x2,y2 in line:\n                slope = ((y2 - y1) / (x2 - x1))\n                if slope > 0.50 and slope < 0.8: # Right line, because of positive slope (y-reversed)\n                    sides["right"].append([x1,y1,x2,y2]) \n                elif slope < -0.50 and slope > -0.8: # Left line because of negative slope\n                    sides["left"].append([x1,y1,x2,y2])\n    yHalf = None\n    for side in sides:\n        avgSlope = None\n        totalSlope = 0\n        totalWeight = 0\n        xAvg = None\n        yAvg = None\n        for x1,y1,x2,y2 in sides[side]:\n            slope = (y2 - y1) / (x2 - x1)\n            length = math.sqrt(abs(x2-x1)^2+abs(y2 - y1)^2)\n            if xAvg == None:\n                xAvg = (x1 + x2) / 2\n                yAvg = (y1 + y2) / 2\n            else:\n                xAvg = (xAvg + ((x1 + x2) / 2)) / 2\n                yAvg = (yAvg + ((y1 + y2) / 2)) / 2\n            totalSlope += slope * length\n            totalWeight += length\n        if totalWeight > 0:\n            avgSlope = totalSlope / totalWeight\n        if avgSlope != None and xAvg != None and yAvg != None:\n            yIntercept = -(avgSlope * xAvg) + yAvg\n            xMax = (yMax - yIntercept) / avgSlope\n            xMin = (yMin - yIntercept) / avgSlope\n            if side == "right":\n                offset = 20\n            else:\n                offset = -20\n            _yHalf = avgSlope * (xHalf + offset) + yIntercept \n            if yHalf == None:\n                yHalf = _yHalf\n            else:\n                xHalf = ((yHalf - yIntercept) / avgSlope) - offset\n            points = unjitter(side, [xMin, yMin, (xHalf + offset), yHalf])\n            cv2.line(img, (int(points[0]), int(points[1])), (int(points[2]), int(points[3])), color, thickness)\n        else:\n            points = unjitter(side, previous[side])\n            cv2.line(img, (int(points[0]), int(points[1])), (int(points[2]), int(points[3])), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((*img.shape, 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': 'def highlight_column_matches(data, column=\'\', color=\'yellow\'):\n    \'\'\'\n    highlight the maximum in a Series or DataFrame\n    \'\'\'\n    attr = \'background-color: {}\'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_mixed = data == data[column]\n        return [attr if v else \'\' for v in is_mixed]\n    else:  # from .apply(axis=None)\n        is_mixed = data == data[column]\n        return pd.DataFrame(np.where(is_mixed, attr, \'\'), index=data.index, columns=data.columns)\n\ndef plot_stats(csv_filename, columns=[\'total_reward\'], **kwargs):\n    """Plot specified columns from CSV file."""\n    df_stats = pd.read_csv(csv_filename)\n    df_stats[columns].plot(**kwargs)\n\ndef save_rnn_layers(hidden_layers, output_layers):\n    for i, layer in hidden_layers.items():\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_output\'), hidden_layers[i][\'output\'])\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_state\'), hidden_layers[i][\'state\'])\n    np.save(os.path.join(vsig.out_dir, \'valid_output_layer\'), output_layers)\n    \ndef save_mlp_layers(hidden_layers, output_layers):\n    for i, layer in hidden_layers.items():\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_output\'), layer)\n    np.save(os.path.join(vsig.out_dir, \'valid_output_layer\'), output_layers)\n    \ndef glance_at_tensor(tensor):\n    if len(tensor.shape) == 3:\n        print(tensor[:10, 0, 0])\n        print(tensor[0, :10, 0])\n        print(tensor[0, 0, :10])\n        print(\'\')\n        print(tensor[-10:, -1, -1])\n        print(tensor[-1, -10:, -1])\n        print(tensor[-1, -1, -10:])\n    elif len(tensor.shape) == 4:\n        print(tensor[:10, 0, 0, 0])\n        print(tensor[0, :10, 0, 0])\n        print(tensor[0, 0, :10, 0])\n        print(tensor[0, 0, 0, :10])\n        print(\'\')\n        print(tensor[-10:, -1, -1, -1])\n        print(tensor[-1, -10:, -1, -1])\n        print(tensor[-1, -1, -10:, -1])\n        print(tensor[-1, -1, -1, -10:])\n        \nclassifier_activation = {\'binary\': \'sigmoid\', \'categorical\': \'softmax\'}```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def error_line_trace(x_values, y_values, x):\n    pass\n```\n\nUnused variables:\n- m\n- b'}, {'reason': 'stop', 'result': '```beta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nx_plot = np.linspace(-3, 3, 200)\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def insert_and_time_filter_bloom_filter(capacity, percent_fill=0.9):\n    b_filter = CountingBloomFilter(capacity)\n    now = time.time()\n    for i in range(int(percent_fill*capacity)):\n        b_filter.add(str(i))\n    elapsed_insertion = time.time() - now\n    return b_filter, elapsed_insertion\n```\n\nUnused variables:\n- num_inserted'}, {'reason': 'stop', 'result': "targets = np.array(['Vessel underway (stern)',\n                    'Power-driven vessel underway (>50m in length) (starboard)',\n                    'Power-driven vessel underway (>50m in length) (port)',\n                    'Power-driven vessel underway (>50m in length) (bow)',\n                    'Power-driven vessel underway (<50m in length) (starboard)',\n                    'Power-driven vessel underway (<50m in length) (port)',\n                    'Power-driven vessel underway (<50m in length) (bow)'], dtype=np.dtype('unicode_'))```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "fig = plt.figure(figsize = (12, 7))\n\nax1 = fig.add_subplot(231)\np = ax1.pcolor(inc,scr, ZU)\nc = ax1.contour(inc,scr, ZU, [0.92,0.94,0.96,0.98], colors=['k','k','k','k'])\nplt.clabel(c, manual = [(0.02,0.25), (0.05,0.25), (0.07,0.15), (0.09,0.05)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax1)\nax1.set_ylabel('Screening Rate (years $^{-1}$)')\nt = ax1.text(0.05, 0.45, 'Uninfected', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax1.set_ylim(0, 0.5)\nax1.set_xlim(0, 0.1)\n\nax2 = fig.add_subplot(232)\np = ax2.pcolor(inc,scr, ZS)\nc = ax2.contour(inc,scr, ZS, (0.001,0.002,0.003), colors='k', manual=True)\nplt.clabel(c, manual = [(0.03,0.15), (0.06,0.15), (0.09,0.15)], fmt='%1.3f')\ncb = fig.colorbar(p, ax=ax2)\nt = ax2.text(0.05, 0.45, 'Infected, Symptomatic', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax2.set_ylim(0, 0.5)\nax2.set_xlim(0, 0.1)\n\nax3 = fig.add_subplot(233)\np = ax3.pcolor(inc,scr, ZA)\nc = ax3.contour(inc,scr, ZA, (0.02,0.04,0.06,0.08), colors='k')\nplt.clabel(c, manual = [(0.02,0.25), (0.05,0.25), (0.07,0.15), (0.09,0.05)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax3)\nt = ax3.text(0.05, 0.45, 'Infected, Asymptomatic', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax3.set_ylim(0, 0.5)\nax3.set_xlim(0, 0.1)\n\nax4 = fig.add_subplot(234)\np = ax4.pcolor(inc,scr, Zprev)\nc = ax4.contour(inc,scr, Zprev, (0.02,0.04,0.06,0.08), colors='k')\nplt.clabel(c, manual = [(0.02,0.25), (0.05,0.25), (0.07,0.15), (0.09, 0.05)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax4)\nax4.set_xlabel('Incidence  (years $^{-1}$)')\nax4.set_ylabel('Screening Rate (years $^{-1}$)')\nt = ax4.text(0.05, 0.45, 'Prevalence', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax4.set_ylim(0, 0.5)\nax4.set_xlim(0, 0.1)\n\nax5 = fig.add_subplot(235)\np = ax5.pcolor(inc,scr, Ztest)\nc = ax5.contour(inc,scr, Ztest, (0.2,0.4), colors='k')\nplt.clabel(c, manual = [(0.045,0.2), (0.045,0.5)], fmt='%1.1f')\ncb = fig.colorbar(p, ax=ax5)\nax5.set_xlabel('Incidence  (years $^{-1}$)')\nt = ax5.text(0.05, 0.45, 'Testing Rate', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax5.set_ylim(0, 0.5)\nax5.set_xlim(0, 0.1)\n\nax6 = fig.add_subplot(236)\np = ax6.pcolor(inc,scr, Zdiag)\nc = ax6.contour(inc,scr, Zdiag, (0.02,0.04,0.06), colors='k')\nplt.clabel(c, manual = [(0.04,0.2), (0.06,0.4), (0.09,0.35)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax6)\nax6.set_xlabel('Incidence (years $^{-1}$)')\nt = ax6.text(0.05, 0.45, 'Diagnosis Rate', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax6.set_ylim(0, 0.5)\nax6.set_xlim(0, 0.1)\n\nplt.show()```"}, {'reason': 'stop', 'result': "for (name, prd) in ('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf):\n    prd.fit(X_train, y_train)```\n\nUnused variables:\n- name"}, {'reason': 'stop', 'result': '# Include the observations, which are Bernoulli\nwith model:\n    obs = pm.Bernoulli("obs", p, observed=occurrences)\n    # To be explained in chapter 3\n    step = pm.Metropolis()\n    trace = pm.sample(18000, step=step)\n    burned_trace = trace[1000:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Action Directions\nnorth = (0, 1)\nsouth = (0,-1)\nwest = (-1, 0)\neast = (1, 0)\n\npolicy = {\n    (0, 2): east,  (1, 2): east,  (2, 2): east,\n    (0, 1): north,                (2, 1): north,\n    (0, 0): north, (1, 0): west,  (2, 0): west,\n}\n```\n\nUnused variables:\n- `south`\n- `policy[(3, 2)]`\n- `policy[(3, 1)]`\n- `policy[(3, 0)]`'}, {'reason': 'stop', 'result': 'sum(1 for completed_process, outcome in outcomes if completed_process.stderr == "")```\n\nUnused variables:\n- `outcome`'}, {'reason': 'stop', 'result': 'reset_graph()\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "#Create a monthly mean PM2.5 for the dataset\nmonthly_data = After2009.resample('M', dim='time', how='mean', keep_attrs=True)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '```#-------------Part 2----------------\n\n# Overall mean & covariance\ntrain_data = np.vstack((c1.train, c2.train))\ntest_data  = np.vstack((c1.test, c2.test))\ncov_est    = np.cov(test_data.T)   # Transopose data to get correct covariance\n```\n\nUnused variables:\n- `mu_est`'}, {'reason': 'stop', 'result': "backend = 'ibmqx2'   # Backend where you execute your program; in this case, on the Real Quantum Chip online \ncircuits = ['Circuit']   # Group of circuits to execute\nshots = 1024           # Number of shots to run the program (experiment); maximum is 8192 shots.\nmax_credits = 3          # Maximum number of credits to spend on executions. \nqp.set_api(Qconfig.APItoken, Qconfig.config['url']) # set the APIToken and API url\n\nresult_real = qp.execute(circuits, backend, shots=shots, max_credits=max_credits, wait=10, timeout=240)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# tranform each class\nY1 = np.matmul(X1,v2)\nY2 = np.matmul(X2,v2)\n\n## show me dont tell me\nax = plt.gca()\nax.hist(Y1,color='blue', alpha=0.5, label='1')\nax.hist(Y2,color='yellow', alpha=0.5, label='2')\nplt.legend(loc='upper right')\nplt.xlabel('y')\n\nY = np.matmul(X,v2)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "#directory and filename\ndir = 'data/processed/'\nfile = 'revenue_2009_2016.csv'```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'f_bar = util.draw_latent_func(GPC, optimize_theta, kernel, x_new, x, y)```\n\nUnused variables:\n- var'}, {'reason': 'stop', 'result': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 1].Valid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 1].Valid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "a = pd.read_csv('https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/ade_sales.csv')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': '# Create DensityForest instance\nclf_df = DensityForest(**ps_df.best_params)```\n\nUnused variables:\n- `default_params`'}, {'reason': 'stop', 'result': "# Test and training set\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n# Combine into one dataset for the purposes of cleaning, and make sure that index continues\ndata_full = pd.concat([train, test])#ignore_index = True)```\n\nUnused variables:\n- `data_full`"}, {'reason': 'stop', 'result': '# a small subset of our input/output pairs\nXsmall = X[:10000,:,:]\nysmall = y[:10000,:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'N = old_faithful_df.shape[0]'}, {'reason': 'stop', 'result': "old_faithful_df = get_rdataset('faithful', cache=True).data[['waiting']]```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "df = pd.read_csv('data.csv',\n                 header=None,\n                 usecols=['authors'])\n\n# print df\n```\n\nUnused variables:\n- `year`\n- `journal`\n- `media`\n- `paper_name`"}, {'reason': 'stop', 'result': "# Pymc model\nX_shared = theano.shared(X)\nminibatch_size = 500\nX_minibatch = pm.Minibatch(X, minibatch_size)\nnp.random.seed(45)\nwith pm.Model() as model:\n    pi = pm.Dirichlet('pi', np.ones(K))\n    comp_dist = []\n    mu = []\n    sigma_sq = []\n    cov = []\n    for i in range(K):\n        mu.append(pm.Normal('mu%i' % i, 127, 80, shape=D))\n        sigma_sq.append(pm.InverseGamma('sigma_sq%i' % i, 1, 1, shape=D))\n        cov.append(tt.nlinalg.alloc_diag(sigma_sq[i]))\n        comp_dist.append(pm.MvNormal.dist(mu=mu[i], cov=cov[i]))\n    xobs = pm.Mixture('x_obs', pi, comp_dist,\n            observed=X_shared)```\n\nUnused variables:\n- X_minibatch"}, {'reason': 'stop', 'result': "import p5_util\nfilename='./data/arr_keras_X_y_train_test.dump'\n(X_train, X_test, y_train, y_test) = p5_util.object_load(filename)```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "fig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n        label='DP sample CDFs');\nax.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\nax.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\nax.set_title(r'$\\alpha = {}$'.format(alpha));\nax.legend(loc=2);```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'seed = ["Hello World"]\ncgi_runner = FunctionCoverageRunner(cgi_decode)\nm = MutationCoverageFuzzer(seed)\nresults = m.runs(cgi_runner, 10000)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def broad_less_than_50_meters_port():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible.\n    """\n    white = (255, 255, 255)\n    red = (255, 0, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 201)\n        distance_bw_left_endpoint = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = distance_bw_left_endpoint + light_width\n        running_light_start = tall_masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if distance_bw_left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(distance_bw_left_endpoint - 20, distance_bw_left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(25, distance_bw_left_endpoint + 20)\n        running_light_area = running_light_loc + light_width\n        new_array[taller_masthead_light:tall_masthead_height, distance_bw_left_endpoint:tall_masthead_width] = white\n        new_array[running_light_start:running_light_width, running_light_loc: running_light_area] = red\n        new_array = new_array.flatten()\n        all_broad_images[i] = new_array\n\n    return all_broad_images\n```\n\nUnused variables:\n- black'}, {'reason': 'stop', 'result': 'def roc_auc(labels, predictions, thresholds, get_fpr_tpr=True):\n    tpr = []\n    fpr = []\n    for th in thresholds:    \n        # Compute number of true positives\n        tp_cases = tf.where((tf.greater_equal(predictions, th)) & \n                            (tf.equal(labels, 1)))\n        tp = tf.size(tp_cases)\n        \n        # Compute number of true negatives\n        tn_cases = tf.where((tf.less(predictions, th)) & \n                            (tf.equal(labels, 0)))\n        tn = tf.size(tn_cases)\n        \n        # Compute number of false positives\n        fp_cases = tf.where((tf.greater_equal(predictions, th)) & \n                            (tf.equal(labels,0)))\n        fp = tf.size(fp_cases)\n        \n        # Compute number of false negatives\n        fn_cases = tf.where((tf.less(predictions, th)) & \n                            (tf.equal(labels,1)))\n        fn = tf.size(fn_cases)\n        \n        # Compute True Positive Rate for this threshold\n        tpr_th = tp/(tp + fn)\n        \n        # Compute the False Positive Rate for this threshold\n        fpr_th = fp/(fp + tn)\n        \n        # Append to the entire True Positive Rate list\n        tpr.append(tpr_th)\n        \n        # Append to the entire False Positive Rate list\n        fpr.append(fpr_th)\n        \n    # Approximate area under the curve using Riemann sums and the trapezoidal rule\n    auc_score = 0\n    for i in range(0, len(thresholds)-1):\n        height_step = tf.abs(fpr[i+1]-fpr[i])\n        b1 = tpr[i]\n        b2 = tpr[i+1]\n        step_area = height_step*(b1+b2)/2\n        auc_score += step_area\n    return auc_score, fpr, tpr\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'ids_no_zero_count_in_last_three_week = [i for i in shop_info.index.tolist() if i not in ids_zero_morethan1_count_in_last_three_week and i not in ids_cos_sim_high_3_wk]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'disting_bet_all = sorted(betweenness_dict.iteritems(), key=operator.itemgetter(1), reverse=True)\ndisting_degree_all = sorted(degree_dict.iteritems(), key=operator.itemgetter(1), reverse=True)\ndisting_bet_graph = gig_comp_graph.copy(as_view=False)\ndisting_degree_graph = gig_comp_graph.copy(as_view=False)\n\nremoval_evolution(disting_bet_all, disting_bet_graph)\nremoval_evolution(disting_degree_all, disting_degree_graph)```\n\nUnused variables:\n- bet_removal_evolution\n- degree_removal_evolution'}, {'reason': 'stop', 'result': 'stbd_broad_lt_50m_images = broad_less_than_50_meters_starboard()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'qp = QuantumProgram(specs=Q_SPECS)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'import sympy as sym\nfrom sympy import *\nA, U, S = symbols("A U S")\nalpha_UA, alpha_AU, alpha_US, alpha_SU  = symbols("alpha_UA alpha_AU alpha_US alpha_SU")\n\nmodel_dyn = [\n    alpha_UA*U - alpha_AU*A,\n    alpha_AU*A + alpha_SU*S - (alpha_UA + alpha_US)*U,\n    alpha_US*U - alpha_SU*S,\n    A + U + S - 1 # this equation sets the total population size to 1\n    ]\n\n# steady-state solution\nsol_dyn = solve(model_dyn, A, U, S)\n\n# functions for calculating the proportion of the population in each compartment at \n# steady state, given transition rates between compartments\ndyn_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A] + sol_dyn[S])\nU_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[U])\nA_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A])\nS_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[S])\n\nsol_dyn\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'movies_norm = moviesnormal.mean(axis=1).round(decimals=5)\nuser_norm = moviesnormal.mean().round(decimals=5)```\n\nUnused variables:\n- movies_norm'}, {'reason': 'stop', 'result': '#plot important features except for playlist_pid\nfig, ax = plt.subplots(1,1, figsize = (20,15))\ny_pos = np.arange(len(not_pid_features) - 3)\nax.barh(not_pid_features[3:], not_pid_importances[3:], align = "center", color = "mediumspringgreen")\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\nax.grid(alpha = 0)\nax.set_xlabel("Importance", fontsize = 20)\nax.set_ylabel("Features", fontsize = 20)\nax.yaxis.set_tick_params(labelsize=12)\nax.xaxis.set_tick_params(labelsize=15)\n\nfig.savefig("Feature Importance.png", bbox_inches="tight")```\n\nUnused variables: `y_pos`'}, {'reason': 'stop', 'result': "# Split the data into features and target label\nincome_raw = data['income']\nfeatures_raw = data.drop('income', axis = 1)\n\n# Visualize skewed continuous features of original data\nvs.distribution(data)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# Spase PCA\nimport pywt\n\nclass SPC(object):\n    \n    def __init__(self, number_of_components,max_iter=10, threshold_val=1.5 ):\n        \n        \n        """Initialize the SPC object\n        \n        Positional arguments:\n        number_of_components -- the number of sparse principal components \n        to compute, must be between 1 and p (total number of features)\n        \n        Keyword argument:\n        max_iter -- the number of iterations to perform (default=10)\n        threshold_val -- value of the lambda regularisation \n        parameter (default=10)\n        """\n        self.number_of_components=number_of_components\n        self.max_iter=max_iter\n        self.threshold_val=threshold_val\n    \n    def fit(self, X_):\n        """learn the sparse pc of a data matrix, return sparse estimates\n        of the left and right singular vectors (U and V respectively) \n        as well as the standard principal components loading matrix W\n    \n        Positional arguments:\n        X_ -- training data matrix, as numpy ndarray\n        \n        """ \n        print("computing sparse principal components...")\n        print("computing SVD of data matrix...")\n        U, s, V = np.linalg.svd(X_, full_matrices=True)  \n        cnt = 0\n        self.U = U\n        self.W = V.T\n        def normalize(vector):\n            norm=np.linalg.norm(vector)\n            if norm>0:\n                return vector/norm\n            else:\n                return vector\n        print("starting iterations...")\n        while True:\n           \n            self.V = pywt.threshold(np.dot(U[:self.number_of_components],X_), self.threshold_val)\n            self.U = np.dot(self.V,X_.T)\n            self.U = np.array([normalize(u_i) for u_i in self.U])\n            if cnt%2==0:\n                print("{} out of {} iterations".format(cnt,self.max_iter))\n            cnt += 1\n            if cnt == self.max_iter:\n                self.V = np.array([normalize(v_i) for v_i in self.V])\n                break\n        print("...finish")\n        return self.U, self.V, self.W\n    \n    def transform(self, X_, k=2):\n        X_reduced_spca     = np.dot(X_, np.dot(self.V[:k].T, self.V[:k]))\n        return X_reduced_spca\nmy_spca  = SPC(2,3000,0.1)\nmy_spca.fit(X)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n\ndata = np.array([4, 14, 6, 11, 3, 14, 8, 17, 17, 12, 10, 18])\ncat = np.array([1, 3, 2, 1, 2, 2, 3, 1, 3, 2, 3, 1])\n```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '# The three players\' heights, in meters:\nklay =  2.01 # Klay Thompson is 6\'7"\nsteph = 1.91 # Steph Curry is 6\'3"\nkevin = 2.06 # Kevin Durant is officially 6\'9", but many suspect that he is taller.\n             # (Further complicating matters, membership of the "Splash Triplets" \n             #  is disputed, since it was originally used in reference to \n             #  Klay Thompson, Steph Curry, and Draymond Green.)\n\n# We\'d like to look at all 3 pairs of heights, compute the absolute\n# difference between each pair, and then find the smallest of those\n# 3 absolute differences.  This is left to you!  If you\'re stuck,\n# try computing the value for each step of the process (like the\n# difference between Klay\'s heigh and Steph\'s height) on a separate\n# line and giving it a name (like klay_steph_height_diff).\n```\n\n\nUnused variables:\n- min_height_difference'}, {'reason': 'stop', 'result': "stable_wave_model = shop_info.loc[:,'wave_model2_1':'wave_model2_7']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'import numpy as np\n\nR = np.array([[5.]])'}, {'reason': 'stop', 'result': "from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')/255\ntest_tensors = paths_to_tensor(test_files).astype('float32')/255```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'max_norm_reg = max_norm_regularizer(threshold=1.0)\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_reg, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_reg, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'reload(pccsims)\ncoco = pccsims.pyCoCo(pcc.utils.b(filter_path), pcc.utils.b(coco_root_path))\n\nflux, flux_err = coco.simulate(b"SN2007uy", \n                    z_obs, 0.0, 0.0, 0.0, 3.1, \n                    mjdmax, mjd_to_sim, \n                    filters_to_sim)```\n\nUnused variables:\n- flux, flux_err (commented out lines)'}, {'reason': 'stop', 'result': 'def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n\n    gray = grayscale(image)\n\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray,kernel_size)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges,vertices)\n \n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    line_image = np.copy(image)*0 # creating a blank to draw lines on\n\n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap,vertices)\n\n    # Draw the lines on the edge image\n    result = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n    \n    return result\n```\n\nUnused variables:\n- `line_image`'}, {'reason': 'stop', 'result': "null_info_dataframe = reduce(lambda left,right: pandas.merge(left,right,on='index'), [percent_null,\n                                                                                 percent_filled,\n                                                                                 filled_count_series,\n                                                                                 null_count_series])```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "```fig, ax = plt.subplots(figsize=(8, 6))\n\nplot_w = np.arange(K) + 1\n\nax.bar(plot_w - 0.5, trace['w'].mean(axis=0), width=1., lw=0);\n\nax.set_xlim(0.5, K);\nax.set_xlabel('Component');\n\nax.set_ylabel('Posterior expected mixture weight');```"}, {'reason': 'stop', 'result': 'n_epochs = 20\nbatch_size = 200\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "```python\n# TODO: Apply PCA by fitting the good data with only two dimensions\npca = None\n\n# TODO: Transform the good data using the PCA fit above\nreduced_data = None\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n```\n\nUnused variables:\n- pca_samples"}, {'reason': 'stop', 'result': "batch_size = 128\nembedding_size = 128\nskip_window = 1\nnum_skips = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  \n    embeddings = tf.Variable(\n        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(\n        tf.truncated_normal([vocabulary_size, embedding_size],\n                            stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n  \n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    loss = tf.reduce_mean(\n        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n  \n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(\n        normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))```\n\nUnused variables:\n- skip_window\n- num_skips"}, {'reason': 'stop', 'result': "### Coletando as fontes de dados\ntrain_csv = pd.read_csv('train.csv', sep = ',', encoding = 'UTF-8')\ntest_csv = pd.read_csv('test.csv', sep = ',', encoding = 'UTF-8')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'def lane_detection(image):\n    gray = grayscale(image)\n    # Define a kernel size and apply Gaussian smoothing\n    blur_gray = gaussian_blur(gray, 5)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 180\n    edges = canny(blur_gray, low_threshold, high_threshold) \n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0,imshape[0]),(450, 320), (490, 320), (imshape[1],imshape[0])]], dtype=np.int32)\n\n    masked_edges = region_of_interest(edges, vertices)\n\n    # Define the Hough transform parameters\n    rho = 2 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 15     # minimum number of votes (intersections in Hough grid cell)\n    min_line_len = 40 #minimum number of pixels making up a line\n    max_line_gap = 20    # maximum gap in pixels between connectable line segments\n\n    # Run Hough on edge detected image\n    line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\n\n    # Create a "color" binary image to combine with line image\n    color_edges = np.dstack((edges, edges, edges)) \n\n    # Draw the lines on the edge image\n    lines_edges = weighted_img(line_image, image, α=0.8, β=1., λ=0.)\n    return lines_edges\n```\n\nUnused variables:\n- kernel_size'}, {'reason': 'stop', 'result': '```# probamos con n_components para reduccion pero no afecto\nshrinkage = np.linspace(0.1,1.0).tolist()\nshrinkage.append(\'auto\')\n\nparametros_LDA_lsqr_eigen = {\n    "solver": ["lsqr"],\n    "shrinkage": shrinkage,\n    "n_components": range(1, 20)\n}\n\n(tiempo_LDA_lsqr_eigen, grid_lda) = correr_y_mostrar(\n    LDA(),\n    parametros_LDA_lsqr_eigen,\n    5,\n    10\n)```\n\nUnused variables: `priors`'}, {'reason': 'stop', 'result': 'data4 = data3[data3.star_name == nlist[0]]\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'pnicer_emap = ext_pnicer_discrete.build_map(bandwidth=5 / 60, metric="gaussian", use_fwhm=True)```\n\nUnused variables:\n- `sampling`'}, {'reason': 'stop', 'result': '# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\n\n# TODO: Transform log_samples using the PCA fit above\npca_samples = None\n\n# Generate PCA results plot\npca_results = vs.pca_results(good_data, pca)```\n\nUnused variables:\n- pca'}, {'reason': 'stop', 'result': "# This looks much better, so let's replace the SalePrice with the log-transformed version (will need to exponentiate predictions)\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n# Re-define the full dataset - and work on this until we are ready to split out test and train sets again\ndata_full = pd.concat([train, test], keys = ['train', 'test'])```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "colunasNumericasIdade=['Classe','ParentesIrmao','ParentesFilhos','SaudacaoNum','PassagemPreco']\nx = pd.DataFrame(treino_dropna,columns=colunasNumericasIdade).values\ny = treino_dropna['Idade'].astype('float32')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "data_dir = os.path.abspath(os.path.relpath('../data'))\nimage_dir = os.path.abspath(os.path.relpath('../doc/images'))```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "train, valid, test = df.split_frame(\n    ratios=[0.6,0.2], \n    seed=1234, \n    destination_frames=['train.hex','valid.hex','test.hex']\n)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '#Initialize Weights to zero:\nw = np.zeros(3)'}, {'reason': 'stop', 'result': 'for i in range(1000):\n    try:\n        url = fuzzer()\n        http_program(url)\n        print("Success!")\n    except ValueError:\n        pass\n```\n\nUnused variables:\n- `result`'}, {'reason': 'stop', 'result': 'ot_model_strict = wot.ot.OTModel(adata, epsilon=0.05, lambda1=3, lambda2=50)\ntmap_anno_strict = ot_model_strict.compute_transport_map(7, 7.5)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "```python\n# TODO: Apply PCA by fitting the good data with only two dimensions\npca = PCA(n_components=2)\npca.fit(good_data)\n\n# TODO: Transform the good data using the PCA fit above\nreduced_data = pca.transform(good_data)\n\n# TODO: Transform the sample log-data using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns=['Dimension 1', 'Dimension 2'])\n```\n\nUnused variables:\n- `pca` (redefined after fitting)"}, {'reason': 'stop', 'result': "n_samples_train = 2048\nn_samples_valid = 512\nepochs = 200\ndump_every = 1\nstatus_update_every = 50\n\nx_valid, y_valid, *_ = msig.generate_samples(n_samples_valid, 'tf_tc')\n\nfor i in range(epochs):\n    x_train, y_train, *_ = msig.generate_samples(n_samples_train, 'tf_tc')\n    model.fit(\n        x_train, y_train,\n        epochs=1, \n        validation_data=(x_valid, y_valid),\n        batch_size=batch_size,\n        verbose=1, \n        callbacks=[\n            csvlogger,\n            checkpointer\n        ],\n    )\n    if stateful:\n        model.reset_states()\n\n    if (i + 1) % status_update_every == 0:\n        print('#' * 50)\n        print(f'Epoch: {(i + 1)}/{epochs}')\n        print('#' * 50)\n#         model.save(msig.model_filename)```\n\nUnused variables:\n- dump_every\n- test_dict['epoch']\n- score\n- y_hat"}, {'reason': 'stop', 'result': "```benefits_assesement = [\n    'AssessBenefits1', 'AssessBenefits2', 'AssessBenefits3', 'AssessBenefits4', 'AssessBenefits5', \n    'AssessBenefits6','AssessBenefits7', 'AssessBenefits8', 'AssessBenefits9', 'AssessBenefits10', 'AssessBenefits11'\n]\n\nff = top10_df[benefits_assesement].mode()\nff_p = ff.pivot_table(columns='Country')\nplt.figure(figsize=(14, 8))\nff = pd.Series(index=ff.columns, data=ff.values[0])\nplt.bar(ff.index, ff)\nsns.despine(left=True)\nplt.title('Benefits assessement comparaison', fontsize=21)\n_ = plt.xticks(rotation='vertical')\n```\n\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'K = 9\n```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "backend = 'local_qasm_simulator' \ncircuits = ['Circuit']  # Group of circuits to execute\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "accuracies = []\n\nch = chance[0, :, -1].mean(axis=-1)\nda = deep_all[key][:, -1] / ch\nla = linear_all[key][:, -1] / ch\n\nfor ii in range(4):\n    for jj in range(10):\n        accuracies.append({'subject':ii, 'accuracy':da[ii, jj],\n                           'complexity': 3, 'model': 2})\n        accuracies.append({'subject':ii, 'accuracy':la[ii, jj],\n                           'complexity': 3, 'model': 1})\n    \nch = chance[1, :, -1].mean(axis=-1)\nda = other_deep_accuracy['c'][:, -1] / ch\nla = other_linear_accuracy['c'][:, -1] / ch\nfor ii in range(4):\n    for jj in range(10):\n        accuracies.append({'subject':ii, 'accuracy':da[ii, jj],\n                           'complexity': 2, 'model': 2})\n        accuracies.append({'subject':ii, 'accuracy':la[ii, jj],\n                           'complexity': 2, 'model': 1})\n\nfor ii, t in enumerate(['v', 'p', 'm']):\n    ch = chance[ii+2, :, -1].mean(axis=-1)\n    da = other_deep_accuracy[t][:, -1] / ch\n    la = other_linear_accuracy[t][:, -1] / ch\n    for ii in range(4):\n        for jj in range(10):\n            accuracies.append({'subject':ii, 'accuracy':da[ii, jj],\n                               'complexity': 1, 'model': 2})\n            accuracies.append({'subject':ii, 'accuracy':la[ii, jj],\n                               'complexity': 1, 'model': 1})\ndf = pd.DataFrame.from_dict(accuracies)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '#An example of how to use these functions to provide a directory listing.\ncmdstr = "ls"\nprint("The systemcall method doesn\'t show stdout when used in Jupyter notebook but does from a script.")\nretcode = systemcall(cmdstr)\nprint("")\nprint("With systemcall_pipe you can see the stdout from Jupyter notebook, and can use the results in variables:")\nstdout, stderr = systemcall_pipe(cmdstr)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'np.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '```figsize(12.5, 5)\n\nsimulations = trace["bernoulli_sim"]\nprint(simulations.shape)\n\nplt.title("Simulated dataset using posterior parameters")\nfor i in range(4):\n    ax = plt.subplot(4, 1, i+1)\n    plt.scatter(temperature, simulations[1000*i, :], color="k",\n                s=50, alpha=0.6)```\n\nUnused variables:\n- figsize (first occurrence)'}, {'reason': 'stop', 'result': '# Re-run the model with the Bib numbers as a feature and for the 5K and 10K split times to predict 15K time\n\n### set up data for modeling\nX_15K = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\']]\ny_15K = boston_clean[\'15K Duration\'].values.reshape(-1, 1)\nprint(X_15K.shape, y_15K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_15K, X_test_15K, y_train_15K, y_test_15K = train_test_split(X_15K, y_15K, random_state=29)\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_15K = LinearRegression()\nmodel_15K.fit(X_train_15K, y_train_15K)\n\n# Make predictions\n\npredictions_15K = model_15K.predict(X_test_15K)\n\n# Plot the residuals\n\nplt.scatter(model_15K.predict(X_train_15K), model_15K.predict(X_train_15K) - y_train_15K, c="blue", label="Training Data")\nplt.scatter(model_15K.predict(X_test_15K), model_15K.predict(X_test_15K) - y_test_15K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_15K.min(), xmax=y_test_15K.max())\nplt.title("Residual Plot 15K")\nplt.savefig(\'model_15k.png\')\nplt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'A = A.tocsr()\nprint(type(A))\n\nb = np.random.rand(1000)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'data = ["acacag", "acacgg", "aaaaaacgg"]\nClusteringSeq(data)```\n\nUnused variables:\n- c'}, {'reason': 'stop', 'result': "df_hues = ['Todos','Sin distinguidos Betweenness', 'Sin distinguidos Grado']\ngig_comp_degree = gig_comp_graph.degree()\ngig_bet_degree = gig_bet_graph.degree()\ngig_degree_degree = gig_degree_graph.degree()\ndegree_frequencies1 = Counter([degree for (author, degree) in gig_comp_degree])\ndegree_frequencies2 = Counter([degree for (author, degree) in gig_bet_degree])\ndegree_frequencies3 = Counter([degree for (author, degree) in gig_degree_degree])\ndegree_info = []\nfor k,v in degree_frequencies1.items(): degree_info.append((k,v,df_hues[0]))\nfor k,v in degree_frequencies2.items(): degree_info.append((k,v,df_hues[1]))\nfor k,v in degree_frequencies3.items(): degree_info.append((k,v,df_hues[2]))\ndegree_df = pd.DataFrame(degree_info)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "a = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Invalid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Invalid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'for n in range(nt):\n    un = u.copy()\n    for i in range(1, nx-1):\n        u[i] = un[i] - un[i] * dt / dx *(un[i] - un[i-1]) + nu * dt / dx**2 *\\\n                (un[i+1] - 2 * un[i] + un[i-1])\n    u[0] = un[0] - un[0] * dt / dx * (un[0] - un[-2]) + nu * dt / dx**2 *\\\n                (un[1] - 2 * un[0] + un[-2])\n    u[-1] = u[0]\n        \nu_analytical = numpy.asarray([ufunc(nt * dt, xi, nu) for xi in x])```\n\nUnused variables:\n- `n`'}, {'reason': 'stop', 'result': 'lambdas = np.logspace(-5, 0, 15)\n\n# bluid poly\ntx_tr = build_poly(Strain1_z, 1)\ntx_te = build_poly(Stest1_z, 1)\n\n# ridge regression with different lambda\nrmse_tr = []\nrmse_te = []\nfor ind, lambda_ in enumerate(lambdas):\n    # ridge regression\n    weight = ridge_regression(Strain1_y, tx_tr, lambda_)\n    rmse_tr.append(np.sqrt(2 * compute_mse(Strain1_y, tx_tr, weight)))\n    rmse_te.append(np.sqrt(2 * compute_mse(Stest1_y, tx_te, weight)))\nprint(rmse_tr, rmse_te)```'}, {'reason': 'stop', 'result': 'layout = make_layout(500, 300)'}, {'reason': 'stop', 'result': 'x3 = powers(3)[1]\n\nprint(x3)```\n\nUnused variables:\n- x2\n- x4'}, {'reason': 'stop', 'result': 'array_1 = band1.ReadAsArray().flatten()\narray_2 = band2.ReadAsArray().flatten()\narray_3 = band3.ReadAsArray().flatten()\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'class ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {\'a\': [1, 5, 2, 5, 2, 8],\n               \'b\': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key=\'a\')\n    >> data[\'a\'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[:,[self.key]].astype(np.int64)\n\n    def get_feature_names(self):\n        return [dataset.columns[self.key]]```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "#now for run as well\n\nRTrunanalysis = pd.DataFrame()\nnew_RTlists = [[] for list in range(0,5)]\n\nfor ID in range(10,86):\n    sub = cdat[cdat.subject == ID]\n    for runID in range(0,4):\n        run = sub[sub.RunCounter == runID]\n        validRT_trials = run[run.TrialType == 'Valid'].RT.mean()\n        invalidRT_trials = run[run.TrialType == 'Invalid'].RT.mean()\n        new_RTlists[0].append(ID)\n        new_RTlists[1].append(runID)\n        new_RTlists[2].append(validRT_trials)\n        new_RTlists[3].append(invalidRT_trials)\n    \nRTrunanalysis['SubjectID'] = new_RTlists[0]\nRTrunanalysis['Run'] = new_RTlists[1]\nRTrunanalysis['Valid'] = new_RTlists[2]\nRTrunanalysis['Invalid'] = new_RTlists[3]```\n\nUnused variables:\n- `RTrunanalysis`\n- `new_RTlists`"}, {'reason': 'stop', 'result': 'def dateRange(start, end):\n    days = (datetime.datetime.strptime(end, "%Y-%m-%d") - datetime.datetime.strptime(start, "%Y-%m-%d")).days + 1\n    return [datetime.datetime.strftime(datetime.datetime.strptime(start, "%Y-%m-%d") + datetime.timedelta(i), "%Y-%m-%d") for i in xrange(days)]\n\ndef date_to_week(date):\n    if type(date) == str:\n        date = pd.to_datetime(date).date()\n    return (date - datetime.date(2015,7,7)).days  / 7\n\ndef week_to_date(week_number,return_str=True):\n    if week_to_date:\n        return [(datetime.date(2015,7,7)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d"),(datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d")]\n    return [datetime.date(2015,7,7)+ datetime.timedelta(week_number*7),datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)]\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "## Load symbol and parameters, the parameters can be downloaded from the following link. \n## including the realtimePose-symbol.json and realtimePose-0000.params\n## https://drive.google.com/drive/folders/0BzffphMuhDDMV0RZVGhtQWlmS1U?usp=sharing\noutput_prefix='realtimePose'\nsym, arg_params, _ = mx.model.load_checkpoint(output_prefix, 0)```\n\nUnused variables:\n- aux_params"}, {'reason': 'stop', 'result': 'data = data_ep[pd.notnull(data_ep["mass"])]\ndata = data[pd.notnull(data["semi_major_axis"])]\ndata1 = data[[\'star_name\',\'mass\',\'semi_major_axis\']]\n\nData1=System(data1)'}, {'reason': 'stop', 'result': 'def hypothesis_inlinecounter(text):\n    return np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split(\'\\n\')])[:-1]\n\ndef hypothesis_inside_one(text, single):\n    return re.sub(\'\\{}.*?\\{}\'.format(single, single), lambda m: single+\'#\'*(len(m.group())-2)+single, text)\n\ndef hypothesis_inside_two(text, left, right):\n    hyp = np.full(len(text), -1)\n    inside = False\n    for i in range(len(text) - 1):\n        if text[i] == left:\n            inside = True\n        elif text[i] == right:\n            inside = False\n        if inside:\n            hyp[i+1] = 1\n    return hyp\n\nhypothesis_inside_quotation = lambda x: hypothesis_inside_one(x, \'"\')\nhypothesis_inside_parantheses = lambda x: hypothesis_inside_two(x, \'(\', \')\')\n\ndef hypothesis_comments(text):\n    hyp = np.full(len(text), -1)\n    in_brac_comment = False\n    in_line_comment = False\n    for i in range(len(text)):\n        if text[i:i+2] == \'//\':\n            in_line_comment = True\n        elif text[i] == \'\\n\':\n            in_line_comment = False\n        elif text[i:i+2] == \'/*\':\n            in_brac_comment = True\n        elif text[i:i+2] == \'*/\':\n            in_brac_comment = False\n        if in_brac_comment:\n            hyp[i:i+3] = 1\n        if in_line_comment:\n            hyp[i:i+1] = 1\n    return hyp\n\ndef hypothesis_indentation(text, level):\n    hyp = np.full(len(text), -1)\n    cur_level = 0\n    for i, char in enumerate(text):\n        if char == \'\\n\':\n            cur_level = 0\n        elif char == \'\\t\':\n            cur_level += 1\n        if cur_level >= level:\n            hyp[i] = 1\n    return hyp\n\n# plot_colored_text(text, hypothesis_inlinecounter(text), title=\'Hypothesis: Inline counter\', save_file=\'plots/hyp_inline_counter.png\')\n# plot_colored_text(text, hypothesis_inside_quotation(text), title=\'Hypothesis: Inside quotation\', save_file=\'plots/hyp_inside_quotation.png\')\n# plot_colored_text(text, hypothesis_inside_parantheses(text), title=\'Hypothesis: Inside parantheses\', save_file=\'plots/hyp_inside_parantheses.png\')\n# plot_colored_text(text, hypothesis_comments(text), title=\'Hypothesis: Comments\', save_file=\'plots/hyp_comments.png\')\n# plot_colored_text(text, hypothesis_indentation(text, 1), title=\'Hypothesis: Indent level 1\', save_file=\'plots/hyp_indent_1.png\')\n# plot_colored_text(text, hypothesis_indentation(text, 2), title=\'Hypothesis: Indent level 2\', save_file=\'plots/hyp_indent_2.png\')\n# plot_colored_text(text, hypothesis_indentation(text, 3), title=\'Hypothesis: Indent level 3\', save_file=\'plots/hyp_indent_3.png\')'}, {'reason': 'stop', 'result': 'from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n# TODO: Make a copy of the DataFrame, using the \'drop\' function to drop the given feature\nnew_data = data.drop("Fresh", axis=1)\n# TODO: Split the data into training and testing sets(0.25) using the given feature as the target\n# Set a random state.\nX_train, X_test, y_train, y_test = train_test_split(data, new_data, test_size = 0.25, random_state = 34)\n\n# TODO: Create a decision tree regressor and fit it to the training set\nregressor = DecisionTreeRegressor()\nregressor_fit = regressor.fit(X_train, y_train)\n\n# TODO: Report the score of the prediction using the testing set\nscore = regressor.score(X_test, y_test)\nprint(score)```\n\nUnused variables:\n- make_scorer'}, {'reason': 'stop', 'result': '```reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300\nn_hidden2 = 50\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "colunasTreino=[\n    'PassageiroId',\n    'Idade',\n    'FaixaEtaria',\n    'Classe',\n    'PortoEmbarqueNum',\n    'ParentesIrmao',\n    'ParentesFilhos',\n    'SexoNum',\n    'SaudacaoNum',\n    'PassagemPreco',\n    'FamiliaQtde',\n    'EstaSozinho',\n    'TemCabine',\n    'Sobreviventes'\n]```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '# Separa o dataset em conjunto treino e teste\ntrain_data = keystrokes[:split_data_idx]\ntest_data = keystrokes[split_data_idx:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# google drive sync\n\nsave_to_gdrive = True\ndataset_in_gdrive = False # set to True for speed up without training\n\n\nif save_to_gdrive:\n  from google.colab import drive\n  drive.mount(\'/content/gdrive\')\n  output_dir = os.path.join("/content/gdrive/My Drive", output_dir)\n    \nen_vocab_file = os.path.join(output_dir, "en_vocab")\nzh_vocab_file = os.path.join(output_dir, "zh_vocab")\ncheckpoint_path = os.path.join(output_dir, "checkpoints")\nlog_dir = os.path.join(output_dir, \'logs\')\n\nif dataset_in_gdrive:\n  download_dir = os.path.join(output_dir, "tensorflow-datasets/downloads")\nelse:\n  download_dir = "tensorflow-datasets/downloads"\n    \n# print(f"Save result to {output_dir}")\nclear_output()```\n\nUnused variables:\n- `download_dir`'}, {'reason': 'stop', 'result': 'img_mcmc = get_segment_img(clusters, img, posterior_mu, mcmc=True)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '```reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300\nn_hidden2 = 50\nn_hidden3 = 50\nn_hidden4 = 50\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3")\n    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4")\n    logits = tf.layers.dense(hidden4, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")\n\nlearning_rate = 0.01\nthreshold = 1.0\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ngrads_and_vars = optimizer.compute_gradients(loss)\ncapped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n              for grad, var in grads_and_vars]\ntraining_op = optimizer.apply_gradients(capped_gvs)\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()```\n\nUnused variables: `hidden5`'}, {'reason': 'stop', 'result': "posttest = pd.read_csv('posttest.csv')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')/255\ntest_tensors  = paths_to_tensor(test_files).astype('float32')/255```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '# Sanity check:\nnp.random.seed(1243)\nw = np.zeros((28*28, 10))\nb = np.zeros(10)\n    \nlogp_train, _, _ = sgd_iter(x_train[:5], t_train[:5], w, b)\n```\n\nUnused variables:\n- W'}, {'reason': 'stop', 'result': "colunasTeste=[\n    'PassageiroId',\n    'Idade',\n    'FaixaEtaria',\n    'Classe',\n    'PortoEmbarqueNum',\n    'ParentesIrmao',\n    'ParentesFilhos',\n    'SexoNum',\n    'SaudacaoNum',\n    'PassagemPreco',\n    'EstaSozinho',\n    'TemCabine'\n]```\n\nUnused variables:\n- FamiliaQtde"}, {'reason': 'stop', 'result': "def value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4):\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    \n    while delta > epsilon:\n\n        delta = 0\n        \n        Q = {0: {0: 0,   # state0, action0\n                 1: 0},  # state0, action1\n             1: {2: 0}}  # state1, action2\n        for s in states:\n            for a in actions[s]:\n                for n in next_states:\n                \n                    # Bellman's optimality update rule\n                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n\n            # This step replaces the poilicy improvement step (gets the maximal value)\n            V_kplus1[s] = max(Q[s].items(), key=operator.itemgetter(1))[1]\n            \n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n    \n    # Updates the policy to be greedy with respect to the value function\n    for s in states:\n        PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n    \n    return V_k, k, PI\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '# Note: Use an alpha of .01 when creating the model for this activity\nfrom sklearn.linear_model import Ridge\n\nridge_5K = Ridge(alpha=.01).fit(X_train_5K, y_train_5K)\n\npredictions_5K = ridge_5K.predict(X_test_5K)\n\nr2 = ridge_5K.score(X_test_5K, y_test_5K)\n\nprint(f"R2: {r2}")```\n\nUnused variables:\n- MSE'}, {'reason': 'stop', 'result': "```python\nunique, counts = np.unique(y_train, return_counts=True)\n\nplt.hist(y_train, color='b', label='train', normed=True, bins=range(n_classes+1))\nplt.title('New distribution of the number of images per class \\n for train set')\nplt.xlabel('Class')\nplt.ylabel('Percentage of images')\nplt.show()\n```\n\nUnused variables:\n- `unique_test`\n- `counts_test`"}, {'reason': 'stop', 'result': 'split_data_idx = 20000\ntrain_data = norm_keystrokes[:split_data_idx]\ntest_data = norm_keystrokes[split_data_idx:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "obj = {'name': 'AskDjango', 'since': 2015}```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "from sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_topics=10,\n                                random_state=123,\n                                learning_method='batch')\nlda.fit_transform(X)```\n\nUnused variables:\n- X_topics"}, {'reason': 'stop', 'result': '### Preprocess the data here. Preprocessing steps could include normalization, converting to grayscale, etc.\n### Feel free to use as many code cells as needed.\n\ndef rgb2gray(rgb):\n    gray_img = cv.cvtColor(rgb,cv.COLOR_BGR2GRAY)\n    return np.asarray(gray_img)\n\n# Normalization\ndef normalize(img):\n    return cv.normalize(img, img, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n\n# Find the darkest image in training data\nimg_sums = [sum(sum(rgb2gray(img))) for img in X_train]\nmin_ind = np.argmin(img_sums)\nprint(min_ind)\n\nf, axarr = plt.subplots(1, 2, figsize=(8, 4))\naxarr[0].imshow(X_train[min_ind], cmap="gray")\naxarr[0].set_title(\'Before normalization\')\n\nnormalized_gray = normalize(X_train[min_ind])\naxarr[1].imshow(normalized_gray, cmap="gray")\naxarr[1].set_title(\'After normalization\')\n\n# Normalize all training, validation data\n# Gray all images\nX_train = [rgb2gray(img) for img in X_train]\nX_train = [normalize(img) for img in X_train]\n\nrand_img_indices = plot_imgs(X_train, y_train)\nprint(rand_img_indices)```\n\nUnused variables:\n- `rand_img_indices`'}, {'reason': 'stop', 'result': '# Bokeh options for interaction\nTOOLS="pan,box_zoom,reset,resize,save,hover"\n\n# Read in example log file\nlog_data = pd.read_csv(\'log_file.csv\', index_col=0)\ntime_x = log_data.iloc[ : , 1]\nT_data = log_data.iloc[ : , 2]\nHF_data = log_data.iloc[ : , 3]\n\n# Format plots and display data\np1 = figure(title=\'FF Helmet - Ambient Temp\', x_axis_label = \'Time (s)\', y_axis_label = \'Temperature (°C)\', \n            tools=TOOLS, plot_width=750, plot_height=500)\np1.line(time_x, T_data, color=\'red\', line_width = 3, legend=\'Amb T\')\n\np2 = figure(title=\'FF Helmet - Heat Flux\', x_axis_label = \'Time (s)\', y_axis_label = \'Heat Flux (kW/m²)\', \n            tools=TOOLS, plot_width=750, plot_height=500)\np2.line(time_x, HF_data, color=\'blue\', line_width = 3, line_dash = \'dashed\', legend=\'Heat Flux\')\n\np = vplot(p1, p2)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# reset index, moving the column names into a new series\ndtypes_final = dtypes_dataframe.reset_index()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "RTanalysis = pd.DataFrame()\nlists = [[] for _ in range(0,5)]\n\nfor ID in range(10,86):\n    sub = cdat[cdat.subject == ID]\n    lists[0].append(ID)\n    validRT_trials = sub[sub.TrialType == 'Valid'].RT.mean()\n    invalidRT_trials = sub[sub.TrialType == 'Invalid'].RT.mean()\n    lists[1].append(validRT_trials)\n    lists[2].append(invalidRT_trials)\n    \nRTanalysis['SubjectID'] = lists[0]\nRTanalysis['Valid'] = lists[1]\nRTanalysis['Invalid'] = lists[2]```\n\nUnused variables:\n- `lists`"}, {'reason': 'stop', 'result': "import kf_book.mkf_internal as mkf_internal\n\nvar = 27.5\ndata = mkf_internal.zs_var_275()\nrun(R=var, Q=.02, P=500., plot_P=True, \n    title='$P=500\\, m^2$');```\n\nUnused variables:\n- `track`\n- `zs`"}, {'reason': 'stop', 'result': "# create a list\na_list = ['M', 'o', 'n', 't', 'y', '', 'P', 'y', 't', 'h', 'o', 'n']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_nolm_aer_36 - y_nolm_hitran, label='AER 3.6$-$HITRAN')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables:\n- l"}, {'reason': 'stop', 'result': 'n_neurons = 32\nkernel_size = 5\nstateful = False```\n\nUnused variables:\n- n_hidden'}, {'reason': 'stop', 'result': 'dfTitanic = dfTiDirt.dropna(subset=["Survived", "Pclass", "Age", "Sex"]).copy()```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '# Load pickled data\nimport pickle\n\n# TODO: Fill this in based on where you saved the training and testing data\n\ntraining_file = "../traffic-signs-data/train.p"\nvalidation_file="../traffic-signs-data/valid.p"\ntesting_file = "../traffic-signs-data/test.p"\n\nwith open(training_file, mode=\'rb\') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode=\'rb\') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode=\'rb\') as f:\n    test = pickle.load(f)\n    \nX_train, y_train = train[\'features\'], train[\'labels\']\nX_valid, y_valid = valid[\'features\'], valid[\'labels\']\nX_test, y_test = test[\'features\'], test[\'labels\']```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'import pymc3 as pm\n\nwith pm.Model() as model:\n    parameter = pm.Exponential("poisson_param", 1.0)\n    pm.Poisson("data_generator", parameter)```\n\nUnused variables:\n- data_generator'}, {'reason': 'stop', 'result': 'vectorizer = HashingVectorizer(decode_error=\'ignore\', n_features=2 ** 18,\n                               non_negative=True)\n\n# Iterator over parsed Reuters SGML files.\ndata_stream = stream_reuters_documents()\n\n# We learn a binary classification between the "acq" class and all the others.\n# "acq" was chosen as it is more or less evenly distributed in the Reuters\n# files. For other datasets, one should take care of creating a test set with\n# a realistic portion of positive instances.\nall_classes = np.array([0, 1])\npositive_class = \'acq\'\n\n# Here are some classifiers that support the `partial_fit` method\npartial_fit_classifiers = {\n    \'SGD\': SGDClassifier(),\n    \'Perceptron\': Perceptron(),\n    \'NB Multinomial\': MultinomialNB(alpha=0.01),\n    \'Passive-Aggressive\': PassiveAggressiveClassifier(),\n}\n\n\ndef get_minibatch(doc_iter, size, pos_class=positive_class):\n    """Extract a minibatch of examples, return a tuple X_text, y.\n\n    Note: size is before excluding invalid docs with no topics assigned.\n\n    """\n    data = [(u\'{title}\\n\\n{body}\'.format(**doc), pos_class in doc[\'topics\'])\n            for doc in itertools.islice(doc_iter, size)\n            if doc[\'topics\']]\n    if not len(data):\n        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n    X_text, y = zip(*data)\n    return X_text, np.asarray(y, dtype=int)\n\n\ndef iter_minibatches(doc_iter, minibatch_size):\n    """Generator of minibatches."""\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\n    while len(X_text):\n        yield X_text, y\n        X_text, y = get_minibatch(doc_iter, minibatch_size)\n\n\n# test data statistics\ntest_stats = {\'n_test\': 0, \'n_test_pos\': 0}\n\n# First we hold out a number of examples to estimate accuracy\nn_test_documents = 1000\ntick = time.time()\nX_test_text, y_test = get_minibatch(data_stream, 1000)\nparsing_time = time.time() - tick\ntick = time.time()\nX_test = vectorizer.transform(X_test_text)\nvectorizing_time = time.time() - tick\ntest_stats[\'n_test\'] += len(y_test)\ntest_stats[\'n_test_pos\'] += sum(y_test)\nprint("Test set is %d documents (%d positive)" % (len(y_test), sum(y_test)))\n\n\ndef progress(cls_name, stats):\n    """Report progress information, return a string."""\n    duration = time.time() - stats[\'t0\']\n    s = "%20s classifier : \\t" % cls_name\n    s += "%(n_train)6d train docs (%(n_train_pos)6d positive) " % stats\n    s += "%(n_test)6d test docs (%(n_test_pos)6d positive) " % test_stats\n    s += "accuracy: %(accuracy).3f " % stats\n    s += "in %.2fs (%5d docs/s)" % (duration, stats[\'n_train\'] / duration)\n    return s\n\n\ncls_stats = {}\n\nfor cls_name in partial_fit_classifiers:\n    stats = {\'n_train\': 0, \'n_train_pos\': 0,\n             \'accuracy\': 0.0, \'accuracy_history\': [(0, 0)], \'t0\': time.time(),\n             \'runtime_history\': [(0, 0)], \'total_fit_time\': 0.0}\n    cls_stats[cls_name] = stats\n\nget_minibatch(data_stream, n_test_documents)\n# Discard test set\n\n# We will feed the classifier with mini-batches of 1000 documents; this means\n# we have at most 1000 docs in memory at any time.  The smaller the document\n# batch, the bigger the relative overhead of the partial fit methods.\nminibatch_size = 1000\n\n# Create the data_stream that parses Reuters SGML files and iterates on\n# documents as a stream.\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\ntotal_vect_time = 0.0\n\n# Main loop : iterate on mini-batches of examples\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n\n    tick = time.time()\n    X_train = vectorizer.transform(X_train_text)\n    total_vect_time += time.time() - tick\n\n    for cls_name, cls in partial_fit_classifiers.items():\n        tick = time.time()\n        # update estimator with examples in the current mini-batch\n        cls.partial_fit(X_train, y_train, classes=all_classes)\n\n        # accumulate test accuracy stats\n        cls_stats[cls_name][\'total_fit_time\'] += time.time() - tick\n        cls_stats[cls_name][\'n_train\'] += X_train.shape[0]\n        cls_stats[cls_name][\'n_train_pos\'] += sum(y_train)\n        tick = time.time()\n        cls_stats[cls_name][\'accuracy\'] = cls.score(X_test, y_test)\n        cls_stats[cls_name][\'prediction_time\'] = time.time() - tick\n        acc_history = (cls_stats[cls_name][\'accuracy\'],\n                       cls_stats[cls_name][\'n_train\'])\n        cls_stats[cls_name][\'accuracy_history\'].append(acc_history)\n        run_history = (cls_stats[cls_name][\'accuracy\'],\n                       total_vect_time + cls_stats[cls_name][\'total_fit_time\'])\n        cls_stats[cls_name][\'runtime_history\'].append(run_history)\n\n        if i % 3 == 0:\n            print(progress(cls_name, cls_stats[cls_name]))\n    if i % 3 == 0:\n        print(\'\\n\')```'}, {'reason': 'stop', 'result': "def gp_plot( x_test, y_test, mean_test, covar_test, x_train, t_train, theta, beta ):\n    # x_test: \n    # y_test:    the true function at x_test\n    # mean_test: predictive mean at x_test\n    # var_test:  predictive covariance at x_test \n    # t_train:   the training values\n    # theta:     the kernel parameters\n    # beta:      the precision (known)\n    \n    # the reason for the manipulation is to allow plots separating model and data stddevs.\n    std_total = np.sqrt(np.diag(covar_test))       # includes all uncertainty, model and target noise \n    std_model = np.sqrt(std_total**2 - 1.0/beta) # remove data noise to get model uncertainty in stddev\n    std_combo = std_model + np.sqrt(1.0/beta)    # add stddev (note: not the same as full)\n    \n    plt.plot(x_test, y_test, 'b', lw=3)\n    plt.plot(x_test, mean_test, 'k--', lw=2)\n    plt.fill_between(x_test, mean_test+2*std_combo,mean_test-2*std_combo, color='k', alpha=0.25)\n    plt.fill_between(x_test, mean_test+2*std_model,mean_test-2*std_model, color='r', alpha=0.25)\n    plt.plot(x_train, t_train, 'ro', ms=10)\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'betweenness_dict = nx.algorithms.centrality.betweenness_centrality(gig_comp_graph)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'centers = [np.sort(e, axis=0) for e in centers_kmeans_proj]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "df_train = log_and_placements_aggregated_per_week_df.loc[:39,['client-ip-unique-count']]\ndf_test = log_and_placements_aggregated_per_week_df.loc[40:,['client-ip-unique-count']]```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'class two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)\n    \n    def fit(self, input_data, target, optimizer, num_epochs=500, \n            verbose=50, track_accuracy=True):\n        """ Function to train the model, using the selected optimizer and\n            for the desired number of epochs. It also stores the accuracy\n            of the model after each epoch.\n        """   \n        \n        if track_accuracy:\n            # Initialize list to store the accuracy of the model\n            self.hist_accuracy = []     \n            # Initialize class to compute the accuracy metric\n            accuracy = tfe.metrics.Accuracy()\n\n        for i in range(num_epochs):\n            # Take a step of gradient descent\n            grads = self.grads_fn(input_data, target)\n            optimizer.apply_gradients(zip(grads, self.variables))\n            if track_accuracy:\n                # Predict targets after taking a step of gradient descent\n                logits = self.predict(input_data)\n                preds = tf.argmax(logits, axis=1)\n                # Compute the accuracy\n                accuracy(preds, target)\n                # Get the actual result and add it to our list\n                self.hist_accuracy.append(accuracy.result())\n                # Reset accuracy value (we don\'t want to track the running mean accuracy)\n                accuracy.init_variables()\n```\n\nUnused variables:\n- `verbose`'}, {'reason': 'stop', 'result': 'y_kmeans = []\ncenters_kmeans = []\nfor x in X:\n    kmeans = KMeans(n_clusters=5)\n    kmeans.fit(x)\n    centers_kmeans.append(kmeans.cluster_centers_)\n    y_kmeans.append(kmeans.predict(x))```\n\nUnused variables:\n- i'}, {'reason': 'stop', 'result': '# convert to dataframe and rename a column\ndtypes_dataframe = pandas.DataFrame(dtypes, columns=["data_type"])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '#ignore\n# @pysnooper.snoop()\ndef plot(value, group=None, group_dim=0, mask=None, labels=[], square=False, shape_desc=\'\', width_prec=None, bottom_prec=None, \n         is_string=False, name=\'\', fmt="d", single_plot_size=4, h_dist_ratio=0.7, w_dist_ratio=0.7, linewidths=None):\n\n  shape = value.shape\n  num_groups = shape[0]\n  \n  if hasattr(value, \'numpy\'):\n    if value.dtype in [tf.int32, tf.int64]:\n      value = tf.cast(value, tf.int32)\n    elif value.dtype in [tf.float32, tf.float64] and fmt == "d":\n      fmt = ".2g"\n    \n    value = value.numpy()\n\n  if hasattr(value, \'ndim\'):\n    ndim = value.ndim\n  else:\n    ndim = len(value)\n    \n    \n  if ndim == 2:\n    value = value[np.newaxis, np.newaxis, :, :]\n  if ndim == 3:\n    value = value[np.newaxis, :, :, :]\n  if ndim == 4:\n    pass\n  \n  # decide how to group sub-tensors smartly\n  if not group_dim:\n    group_dim = ndim - 1\n    \n  # generate group identifier tensor for differentiating between\n  # different bactch / sentence\n  if group is None:\n    group_ids = tf.range(num_groups, dtype=tf.int64).numpy()\n    if group_dim == 1:\n      group = group_ids[:, tf.newaxis]\n    elif group_dim == 2:\n      group = group_ids[:, tf.newaxis, tf.newaxis]\n    elif group_dim == 3:\n      group = group_ids[:, tf.newaxis, tf.newaxis, tf.newaxis]\n\n    # broadcast to all groups    \n    group = tf.ones(shape=value.shape) * group\n\n  d0, d1, d2, d3 = value.shape\n\n  # set figure size based on tensor dimensions\n  fig_width = (d3 * 1.0 / 4) * d0 * single_plot_size\n  fig_height = (d2 * 1.0 / 4) * single_plot_size\n  figsize = (fig_width, fig_height)\n  \n  if width_prec is None:\n    width_prec = 1.0 / d0\n  \n  if bottom_prec is None:\n    bottom_prec = 1.0 / d1\n\n  fig = plt.figure(figsize=figsize)\n  fig_title = f\'name: {name}, shape: {shape}\' if name else f\'shape: {shape}\'\n  \n  if shape_desc:\n    fig_title = fig_title + \' = \' + shape_desc\n    \n  for e0 in range(d0):\n\n    # plot 2d array in reverse order since the earlier plot will be covered\n    for e1 in reversed(range(d1)):\n      annot = value[e0, e1]\n\n      # select corresponding matplotlib axis      \n      cur_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3) * w_dist_ratio, \n                             e1 / d2 * h_dist_ratio, \n                             width_prec, \n                             bottom_prec]) \n\n      matrix_id = e0 + e1 * 2\n      \n      if mask is not None:\n        if ndim == 2:\n          mask_idx = e0\n        elif ndim == 3:\n          mask_idx = e1\n        elif ndim ==4:\n          mask_idx = e0\n          \n        # mimic broadcasting\n        if mask.shape[0] == 1:\n          mask_idx = 0\n          \n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, \n                is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n      else:\n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, is_string=is_string, fmt=fmt, square=square)\n      \n      # minic shadowing for each 2d matrix\n      width_delta_prec = 0.0005\n      height_delta_prec = width_delta_prec * d2 / d3\n      \n      for k in range(1, 3):\n        shadow_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3)  * w_dist_ratio - width_delta_prec * k, \n                                  e1 / d2 * h_dist_ratio - height_delta_prec * k, \n                                  width_prec, \n                                  bottom_prec])  \n        \n        if k == 2:\n          linewidths = 1\n        else:\n          linewidths = 0\n          \n        if mask is not None:\n          if ndim == 2:\n            mask_idx = e0\n          elif ndim == 3:\n            mask_idx = e1\n          elif ndim ==4:\n            mask_idx = e0\n            \n          # mimic broadcasting\n          if mask.shape[0] == 1:\n            mask_idx = 0  \n            \n            \n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n        else:\n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, square=square)\n\n      if e0 == 0 and e1 == 0:\n        ax1 = cur_ax\n        \n        if labels:\n            plt.ylabel(labels[-2])\n            plt.xlabel(labels[-1] + \'\\n\' + fig_title)\n        else:\n          plt.xlabel(fig_title)\n\n        # 4D 中的 axis1 說明 label\n#           if len(labels) >= 3:\n#             plt.text(d3 + 2, 1 + 0.5, labels[-3],\n#                      rotation=0, rotation_mode=\'anchor\')\n\n      if e1 == d0 - 1:\n        ax2 = cur_ax\n        \n        \n#       transFigure = fig.transFigure.inverted()\n#       coord1 = transFigure.transform(ax1.transData.transform([d3 + 2 + 0.5, 0]))\n#       coord2 = transFigure.transform(ax2.transData.transform([d3 + 0.5, d2]))\n\n\n#       line = mpl.lines.Line2D((coord1[0],coord2[0]),(coord1[1],coord2[1]), \n#                               transform=fig.transFigure, \n#                               linestyle=\'--\',\n#                               color=\'black\')\n#       fig.lines.append(line)\n```\n\nUnused variables:\n- ax1\n- ax2'}, {'reason': 'stop', 'result': "```python\nfor _, slices in storage.get_slices().items():\n    output = slices.to_output()\n    print(output, '\\n')\n```\n\nUnused variables:\n- `feature_set`"}, {'reason': 'stop', 'result': '# We create tables that will contains every canton we find, so we\'ll be able to match it with the dataframe at the end.\nlogger.debug(\'Beginning of geolocation : creating canton tables\')\ncanton_shortname_table = [] # eg: VD\ncanton_longname_table = []# eg: Vaud\n\n# number of rows analysed. Can be limited for debuging (eg : 10) because the number of requests to Google Maps API is limited !\nMAX_ROWS = math.inf # values between 0 and math.inf \nrow_counter = 0 # will be incremented each time we iterate over a row\n\n# maximum duration of a query to the geocoder, in seconds\ngeocoder_timeout = 5\n\n# We\'re going to use more than one API key if we want to make all the requests !! :@\n# Keys are referenced in a table, se we start with the first key:\nAPIkeynumber = 0\n\n# This function definition makes the geolocator "stubborn" : it uses all the keys that are available and if it gets a timeout error, it just tries again !        \ndef stubborn_geocode(geolocator, address):\n    global APIkeynumber\n    \n    try:\n        geolocator = geopy.geocoders.GoogleV3(api_key=googlemapsapikeys[APIkeynumber])\n        return geolocator.geocode(address, region=\'ch\', timeout=geocoder_timeout)\n    \n    except geopy.exc.GeocoderTimedOut:\n        print("Error : the geocoder timed out. Let\'s try again...")\n        return stubborn_geocode(geolocator, address)\n    \n    except geopy.exc.GeocoderQuotaExceeded:\n        print("Error : The given key has gone over the requests limit in the 24 hour period or has submitted too many requests in too short a period of time. Let\'s try again with a different key...")\n        APIkeynumber = APIkeynumber + 1\n        \n        try:\n            print("Trying API key n°" + str(APIkeynumber) + "...")           \n            return stubborn_geocode(geolocator, address)\n        \n        except IndexError:\n            print("Error : Out of API keys ! We need to request another API key from Google :(")\n            print("When you get a new API key, add it to the json file containing the others keys.")\n            # We have to stop there... the error will be raised and the execution stopped.\n            raise\n\n    \n# Go through the dataframe that contains all universities and institutions\nfor index, row in p3_grant_export_data.iterrows():\n    logger.debug("Iterating over row n°" + str(row_counter) + ":")\n    # initialize variables that will contain canton name for the current row\n    canton_longname = \'N/A\'\n    canton_shortname = \'N/A\'\n    # Check if the university name exists in our index\n    university_name = row[\'University\']\n    institution_name = row[\'Institution\']\n    if university_name in university_canton_dict:\n        # The university has already been located. Let\'s add the canton to the canton table\n        if university_canton_dict[university_name][\'long_name\'] is not None:\n            logger.debug(\'University already exists in dictionary (\' + university_canton_dict[university_name][\'long_name\'] + \')\')\n        else:\n            logger.debug(\'University already exists in dictionary, but no canton is associated to it (it might be outside Switzerland).\')\n        \n        canton_longname = university_canton_dict[university_name][\'long_name\']\n        canton_shortname = university_canton_dict[university_name][\'short_name\']\n    \n    elif institution_name in institution_canton_dict:\n        # The institution has already ben located, so we add its canton to the canton table\n        logger.debug(\'University wasn\'\'t found, but institution already exists in dictionary (\' + institution_canton_dict[institution_name][\'long_name\'] + \')\')\n        \n        canton_longname = institution_canton_dict[institution_name][\'long_name\']\n        canton_shortname = institution_canton_dict[institution_name][\'short_name\']\n    \n    else:\n        # Nor the university neither the institution has been found yet, so we have to geolocate it\n        logger.debug(str(university_name) + \' / \' + str(institution_name) + \' not found in dictionaries, geolocating...\')\n        adr = stubborn_geocode(geolocator, university_name)\n        if adr is None:\n            # No address has been found for this University. So we have to do the same with Institution           \n            adr = stubborn_geocode(geolocator, institution_name)\n            \n        # Now, the address should have been found, either by locating the university or the institution\n        if adr is not None:                 \n            # Check if it\'s a Swiss address and finds the right canton\n            try:\n                swiss_address = False\n                for i in adr.raw[\'address_components\']:\n                    if i["types"][0] == "country" and i["long_name"] == "Switzerland":\n                        # The address is located in Switerland\n                        swiss_address = True\n                # So, we go on only if we found a Swiss address. Otherwise, there is no point to continue.\n                if swiss_address:\n                    for i in adr.raw[\'address_components\']:\n                        if i["types"][0] == "administrative_area_level_1":\n                            # We found a canton !\n                            canton_longname = (i[\'long_name\'])\n                            canton_shortname = (i[\'short_name\'])                          \n                            break\n                \n                \n            \n            except IndexError:\n                # I don\'t know where this error comes from exactly, just debugging... it just comes from this line :\n                # if i["types"][0] == "country" and i["long_name"] == "Switzerland":\n                # For the moment I assume that the the address doesn\'t match the requirements, so it should not be located in Switzerland\n                # Thus, we just forget it and look for the next address.\n                print("IndexError : no canton found for the current row")\n                \n            except KeyError:\n                print("KeyError : no canton found for the current row")\n                print("Current item: n°" + str(len(canton_shortname_table)))\n                # The address doesn\'t act as excpected. There are two possibilities :\n                # - The address doesn\'t contain the field related to the canton\n                # - The address doesn\'t contain the field related to the country\n                # So we don\'t consider this address as a Swiss one and we give up with this one.\n    \n    # Let\'s add what we found about the canton !\n    # If we didn\'t find any canton for the current university/institution, it will just append \'N/A\' to the tables.\n    logger.debug("Appending canton to the table: " + canton_longname)\n    canton_shortname_table.append(canton_shortname)\n    canton_longname_table.append(canton_longname)\n    \n    # We also add it to the university/institution dictionary, in order to limit the number of requests\n    university_canton_dict[university_name] = {}\n    university_canton_dict[university_name][\'short_name\'] = canton_shortname\n    university_canton_dict[university_name][\'long_name\'] = canton_longname\n    institution_canton_dict[institution_name] = {}\n    institution_canton_dict[institution_name][\'short_name\'] = canton_shortname\n    institution_canton_dict[institution_name][\'long_name\'] = canton_longname\n            \n\n    row_counter = row_counter + 1\n    if row_counter >= MAX_ROWS:\n        print("Maximum number of rows reached ! (" + str(MAX_ROWS) + ")")\n        print("Increase the MAX_ROWS variable to analyse more locations")\n        print("No limit : MAX_ROWS = maths.inf")\n        break\n```\n\nUnused variables:\n- `row_counter`\n- `geocoder_timeout`'}, {'reason': 'stop', 'result': 'subset = data.isel(x=slice(x_start, y_stop), y=slice(y_start, y_stop))```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'reset_graph()\n\nsaver = tf.train.import_meta_graph("./my_model_final.ckpt.meta")\n\nX = tf.get_default_graph().get_tensor_by_name("X:0")\ny = tf.get_default_graph().get_tensor_by_name("y:0")\n\nhidden3 = tf.get_default_graph().get_tensor_by_name("dnn/hidden3/Relu:0")\n\nnew_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="new_hidden4")\nnew_logits = tf.layers.dense(new_hidden4, n_outputs, name="new_outputs")\n\nwith tf.name_scope("new_loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("new_eval"):\n    correct = tf.nn.in_top_k(new_logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")\n\nwith tf.name_scope("new_train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()\nnew_saver = tf.train.Saver()```\n\nUnused variables:\n- n_hidden4\n- n_outputs'}, {'reason': 'stop', 'result': "models = {}\nfor l in labels:\n    models[l] = build_model()\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=400)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'y_train_stern = (y_train == 0)'}, {'reason': 'stop', 'result': '# build random forest\nrandom_forest_build(dataset, 1000, .3, n_jobs=-1)```\n\nUnused variables:\n- root_nodes'}, {'reason': 'stop', 'result': "import pandas as pd\nread_filename = './annotations_201712131226.csv'\n\ndf = pd.read_csv(read_filename+'.points', names=('filename', 'x1', 'y1', 'x2', 'y2', 'points'))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'accuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\ndef get_positive_class_probabilities(arr):\n    arr_aux = []\n    for entry in arr:\n        arr_aux.append(entry[1])\n    return arr_aux\n\ndef get_accuracy(y_pred, y_eval_np):\n    return np.mean(y_pred == y_eval_np)\n    \ndef show_prediction_accuracy(y_pred, y_eval_np, x_eval_np):\n    print("Predicciones sobre el test set:\\n {}".format(y_pred))\n    print("Score sobre el test set: {:.2f}".format(np.mean(y_pred == y_eval_np))) # A mano\n    print("Score sobre el test set: {:.2f}".format(arbol.score(x_eval_np, y_eval_np))) # Usando el método score.\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test) )\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train\ndf["Accuracy (validación)"] = accuracy_validation\ndf["AUC ROC (training)"]    = roc_train\ndf["AUC ROC (validación)"]  = roc_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()```'}, {'reason': 'stop', 'result': 'tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\ns = tags[:-1]\ne = tags[1:]\n\nspluse = list(zip(s, e))\n\ndef bigram_counts(sequences):\n    """Return a dictionary keyed to each unique PAIR of values in the input sequences\n    list that counts the number of occurrences of pair in the sequences list. The input\n    should be a 2-dimensional array.\n    \n    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n    """\n    d4 = defaultdict(int)\n    for i in sequences:\n        d4[i] += 1\n            \n    return d4\n\n# TODO: call bigram_counts with a list of tag sequences from the training set\ntag_bigrams = bigram_counts(spluse)\n\nassert len(tag_bigrams) == 144, \\\n       "Uh oh. There should be 144 pairs of bigrams (12 tags x 12 tags)"\nassert min(tag_bigrams, key=tag_bigrams.get) in [(\'X\', \'NUM\'), (\'PRON\', \'X\')], \\\n       "Hmmm...The least common bigram should be one of (\'X\', \'NUM\') or (\'PRON\', \'X\')."\nassert max(tag_bigrams, key=tag_bigrams.get) in [(\'DET\', \'NOUN\')], \\\n       "Hmmm...(\'DET\', \'NOUN\') is expected to be the most common bigram."\nHTML(\'<div class="alert alert-block alert-success">Your tag bigrams look good!</div>\')```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with pm.Model() as model:\n    ℓ = pm.Gamma("ℓ", alpha=2, beta=1)\n    η = pm.HalfCauchy("η", beta=5)\n    \n    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n    gp = pm.gp.Latent(cov_func=cov)\n    \n    f = gp.prior("f", X=X)\n    \n    σ = pm.HalfCauchy("σ", beta=5)\n    y_ = pm.StudentT("y", mu=f, lam=1.0/σ, nu=ν, observed=y)\n    \n    trace = pm.sample(1000)```\n\nUnused variables:\n- ν'}, {'reason': 'stop', 'result': 'U = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\nfor state_action, value in q_agent.Q.items():\n    state, action = state_action\n    if U[state] < value:\n        U[state] = value```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'seed = ["1 + 1"]\nbc = ProgramRunner(program="bc")\nm = MutationFuzzer(seed)\noutcomes = m.runs(bc, trials=100)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'output_dir = "nmt"\nen_vocab_file = os.path.join(output_dir, "en_vocab")\nzh_vocab_file = os.path.join(output_dir, "zh_vocab")\ncheckpoint_path = os.path.join(output_dir, "checkpoints")\nlog_dir = os.path.join(output_dir, \'logs\')\n\nif not os.path.exists(output_dir):\n  os.makedirs(output_dir)```\n\nUnused variables:\n- download_dir'}, {'reason': 'stop', 'result': 'data = pd.read_csv("../data/prices.csv")```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "```python\n# TODO: Exponentiate the centers\ntrue_centers = None\n\n# Display the true centers\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)\n```\n\nUnused variables:\n- log_centers"}, {'reason': 'stop', 'result': "# Determine if the predictions are correct\nis_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n# Calculate the accuracy of the predictions\naccuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n\nprint('Accuracy function created.')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# Bidirectional RNN\nbi_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n    cell_fw=cell_fw,\n    cell_bw=cell_bw,\n    inputs=inpts,\n    sequence_length=length,\n    dtype=tf.float32)\n\noutputs = tf.concat(bi_outputs, -1, name='outputs')\n\npred = tf.layers.dense(inputs=outputs,\n                       units=n_classes,\n                       name='pred')\nprediction = tf.argmax(pred, axis=-1, name='prediction')```\n\nUnused variables: `pred`"}, {'reason': 'stop', 'result': "# See revenue variation across months\nax = sns.boxplot(df.index.month, df.Revenue, color='#2ecc71')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "YOUR_BUCKET_NAME = ' < YOUR S3 BUCKET NAME > '```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'def find_f(K, y, iterations=100):\n    """\n    Finds f using a iterative aproach also finds p(y|f)\n    :param K: np.array(np.array) covariance matrix of data points we have observed\n    :param y: numpy array of the y value of the data points we have observed\n    :iterations: int optional default = 100 number of iterations we will preform to optimize f\n    :return: (numpy array <float> f, numpy array <float> p(y|f)) f is the latent function value for each of the \n    sampled data points, p(y|f) is the probability of y given the latent function we calculated\n    make sure to calculate p(y|f) after having approximated f_hat.\n    """\n    n = len(y)\n    f = np.zeros(n)\n    grad = np.zeros(n)\n    for i in range(iterations):\n        for j in range(n):\n            sigmoid_v = sigmoid(f[j]*y[j])\n            grad[j] = (1-sigmoid_v)*y[j]\n        f = np.array(np.matmul(K, grad)).flatten()\n        \n    y_giv_f = np.zeros(n)\n    for j in range(n):\n        y_giv_f[j] = sigmoid(f[j]*y[j])\n        \n    return f, y_giv_f\n```\n\nUnused variables:\n- K\n- iterations\n- sigmoid_v\n- grad\n- y_giv_f'}, {'reason': 'stop', 'result': 'X = 35\n\nwith model:\n    observations = pm.Binomial("obs", N, observed_proportion, observed=X)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "fig = plt.figure(figsize=(5, 3))\nplt.plot(y_score.sum(axis=0) / y_score.shape[0])\nplt.xlim((0, y_score.shape[1]))\nplt.xlabel('timestep')\nplt.ylim((0, 1))\nplt.ylabel('classifier score')\nplt.grid(True)\nplt.savefig(os.path.join(msig.out_dir, 'mean_accuracy_vs_timestep.png'))```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '# Delete this cell to re-enable tracebacks\nimport sys\nipython = get_ipython()\n\ndef hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n                   exception_only=False, running_compiled_code=False):\n    etype, value, tb = sys.exc_info()\n    value.__cause__ = None  # suppress chained exceptions\n    return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))```\n\nUnused variables:\n- exc_tuple\n- filename\n- tb_offset\n- exception_only\n- running_compiled_code'}, {'reason': 'stop', 'result': '# Plot Frequency again\nsign_frequencies = get_frequencies(y_train_augmented, sign_dict)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nind = np.arange(len(sign_dict))\nwidth = 0.8\n\nrects = ax.bar(ind, sign_frequencies.values(), width, align="edge", alpha=0.5)\nax.set_ylabel(\'Frequency\')\nax.set_title(\'Traffic Sign Classes\')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(sign_frequencies.keys(), rotation=90)\nplt.show()```\n\nUnused variables: \n- classes\n- rects'}, {'reason': 'stop', 'result': "# define map colors\nland_color = '#f5f5f3'\nwater_color = '#cdd2d4'\ncoastline_color = '#f5f5f3'\nborder_color = '#bbbbbb'\nmeridian_color = '#eaeaea'\nmarker_fill_color = '#0033cc'\nmarker_edge_color = 'None'```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'parametrosRF = {\n    "max_features": ["sqrt", "log2", None] + [1, 20, 50, 100, 150, 200],\n    "max_depth": [3, 6, 12],\n    "min_samples_split": [2, 6, 12],\n    "n_estimators": [10, 50, 120, 200]\n}\n\n(_, grid_random_forest) = correr_y_mostrar(\n    RandomForestClassifier(),\n    parametrosRF,\n    5,\n    5\n)\n\nrandomForest = grid_random_forest.best_estimator_\nplot_learning_curve(randomForest, "Learning Curve Random Forest Mejor segun GridSearch", X_dev_np, y_dev_np, cv=5)```\n\nUnused variables:\n- tiempo_random_forest'}, {'reason': 'stop', 'result': 'import random\n\ndef compute_fitness(solution):\n    """\n        This function returns the total distance traveled on the current road trip.\n        \n        The genetic algorithm will favor road trips that have shorter\n        total distances traveled.\n    """\n    \n    solution_fitness = 0.0\n    \n    for index in range(len(solution)):\n        waypoint1 = solution[index - 1]\n        waypoint2 = solution[index]\n        solution_fitness += waypoint_distances[frozenset([waypoint1, waypoint2])]\n        \n    return solution_fitness\n\ndef generate_random_agent():\n    """\n        Creates a random road trip from the waypoints.\n    """\n    \n    new_random_agent = list(all_waypoints)\n    random.shuffle(new_random_agent)\n    return tuple(new_random_agent)\n\ndef mutate_agent(agent_genome, max_mutations=3):\n    """\n        Applies 1 - `max_mutations` point mutations to the given road trip.\n        \n        A point mutation swaps the order of two waypoints in the road trip.\n    """\n    \n    agent_genome = list(agent_genome)\n    num_mutations = random.randint(1, max_mutations)\n    \n    for mutation in range(num_mutations):\n        swap_index1 = random.randint(0, len(agent_genome) - 1)\n        swap_index2 = swap_index1\n\n        while swap_index1 == swap_index2:\n            swap_index2 = random.randint(0, len(agent_genome) - 1)\n\n        agent_genome[swap_index1], agent_genome[swap_index2] = agent_genome[swap_index2], agent_genome[swap_index1]\n            \n    return tuple(agent_genome)\n\ndef shuffle_mutation(agent_genome):\n    """\n        Applies a single shuffle mutation to the given road trip.\n        \n        A shuffle mutation takes a random sub-section of the road trip\n        and moves it to another location in the road trip.\n    """\n    \n    agent_genome = list(agent_genome)\n    \n    start_index = random.randint(0, len(agent_genome) - 1)\n    length = random.randint(2, 20)\n    \n    genome_subset = agent_genome[start_index:start_index + length]\n    agent_genome = agent_genome[:start_index] + agent_genome[start_index + length:]\n    \n    insert_index = random.randint(0, len(agent_genome) + len(genome_subset) - 1)\n    agent_genome = agent_genome[:insert_index] + genome_subset + agent_genome[insert_index:]\n    \n    return tuple(agent_genome)\n\ndef generate_random_population(pop_size):\n    """\n        Generates a list with `pop_size` number of random road trips.\n    """\n    \n    random_population = []\n    for agent in range(pop_size):\n        random_population.append(generate_random_agent())\n    return random_population\n    \ndef run_genetic_algorithm(generations=5000, population_size=100):\n    """\n        The core of the Genetic Algorithm.\n        \n        `generations` and `population_size` must be a multiple of 10.\n    """\n    \n    population_subset_size = int(population_size / 10.)\n    generations_10pct = int(generations / 10.)\n    \n    # Create a random population of `population_size` number of solutions.\n    population = generate_random_population(population_size)\n\n    # For `generations` number of repetitions...\n    for generation in range(generations):\n        \n        # Compute the fitness of the entire current population\n        population_fitness = {}\n\n        for agent_genome in population:\n            if agent_genome in population_fitness:\n                continue\n\n            population_fitness[agent_genome] = compute_fitness(agent_genome)\n\n        # Take the top 10% shortest road trips and produce offspring each from them\n        new_population = []\n        for rank, agent_genome in enumerate(sorted(population_fitness,\n                                                   key=population_fitness.get)[:population_subset_size]):\n            \n            if (generation % generations_10pct == 0 or generation == generations - 1) and rank == 0:\n                print("Generation %d best: %d | Unique genomes: %d" % (generation,\n                                                                       population_fitness[agent_genome],\n                                                                       len(population_fitness)))\n                print(agent_genome)\n                print("")\n\n            # Create 1 exact copy of each of the top road trips\n            new_population.append(agent_genome)\n\n            # Create 2 offspring with 1-3 point mutations\n            for offspring in range(2):\n                new_population.append(mutate_agent(agent_genome, 3))\n                \n            # Create 7 offspring with a single shuffle mutation\n            for offspring in range(7):\n                new_population.append(shuffle_mutation(agent_genome))\n\n        # Replace the old population with the new population of offspring \n        population = new_population\n```\n\nUnused variables:\n- `solution_fitness` in the `compute_fitness` function\n- `num_mutations` in the `mutate_agent` function\n- `mutation` in the `mutate_agent` function\n- `offspring` in the `run_genetic_algorithm` function'}, {'reason': 'stop', 'result': "def plot_decision_boundary(X, y, w, i):\n    x1s = np.arange(7)\n    x2s = (-w[2]-w[0]*x1s)/w[1]\n\n    fig = figure(0, (8,6))\n    #Scatter data\n    scatter(X[:,0], X[:,1], c = y, s = 75)\n    scatter(X[i,0], X[i,1], c = 'r', s = 75)\n    #Plot decision boundary\n    plot(x1s, x2s)\n\n    xlabel('$x_1$', fontsize = 20)\n    ylabel('$x_2$', fontsize = 20)\n    grid(1)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "exp_name = 'regression'\nregression_network = load_model(exps_path, exp_name, 100, camera_parameters_path)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'SCFC = posttest.groupby([\'subjID\'])[\'Q3_SC_ForcedChoicePerformance\'].mean()\nSCFCSEM = pd.Series.std(SCFC) / n\nSCTFC = posttest.groupby([\'subjID\'])[\'Q3_SCT_ForcedChoicePerformance\'].mean()\nSCTFCSEM = pd.Series.std(SCTFC) / n\n\nbarwidth = 0.75\nfig, ax = plt.subplots(figsize=(8, 6))\nrects1 = ax.bar(0.5, SCFC.mean(), barwidth, color=sns.xkcd_rgb[\'green\'], yerr=SCFCSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects4 = ax.bar(1.5, SCTFC.mean(), barwidth, yerr=SCTFCSEM, ecolor=\'k\', edgecolor=sns.xkcd_rgb[\'green\'], linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None)\nax.set_ylim(0, 4.1)\nax.set_xlim(0, 2.5)\nplt.plot([0, 1, 2, 2.5], [2, 2, 2, 2], \'r--\')\nax.set_xticklabels((\'S-C\', \'S-CT\'))\nax.set_xticks([0.5 + barwidth/2, 1.5 + barwidth/2])\nax.set_yticks(np.arange(0, 5, 1))\nplt.title(\'Q3: Match the Scene/Face/House to Its\\nMost Likely Trial Type\', fontsize=18, fontweight="bold")\nplt.ylabel(\'Performance (# Pairs Correct)\', fontsize=17, fontweight="bold")\nplt.xlabel(\'Task Phase\', fontsize=17, fontweight="bold")\nsns.despine()\n\nplt.show()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'detNames = {0: \'Det0\'}  ###REMOVE WHEN YOU GET ACTUAL PSD CUTS ON THEM ALL!\n\ngROOT.ProcessLine(\'HistogramOperations ops\')\ngROOT.ProcessLine(\'lightTables.setBirksParams(1.0,6.90)\')\n\nfor detNum, detName in detNames.iteritems():\n    params = CalibParams(calPath+calNames[detNum])\n    gROOT.ProcessLine(\'vector<TH1*> phs{1} = ops.loadHistograms("33MeVTa_{0}_ls_{1}_fittedPSDCut.root")\'.format(runNum,detNum))\n    gROOT.ProcessLine(\'ops.applyCalibration(phs{0}[1],{1},{2})\'.format(detNum, params.a, params.b))\n    gROOT.ProcessLine(\'TFile *tgt{0} = new TFile("33MeVTa_{0}_ls_{1}_calibFittedPSDCut.root","recreate")\'.format(runNum,detNum))\n    gROOT.ProcessLine(\'phs{0}[1]->Rebin(3)\'.format(detNum))\n    gROOT.ProcessLine(\'phs{0}[1]->Draw()\'.format(detNum))    \n    gROOT.ProcessLine(\'phs{0}[1]->Write()\'.format(str(detNum)))\n    pause()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_nolm_aer, label='AER 3.2')\nplt.plot(f/3e10, y_nolm_hitran, label='HITRAN')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables:\n- l"}, {'reason': 'stop', 'result': 'params = sig.parameters'}, {'reason': 'stop', 'result': '```python\nn_epochs = 20\nbatch_size = 200\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = saver.save(sess, "./my_model_final.ckpt")```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Input data.\n    tf_train_dataset = tf.placeholder(\n        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n  \n    # Variables.\n    layer1_weights = tf.Variable(tf.truncated_normal(\n        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n    layer1_biases = tf.Variable(tf.zeros([depth]))\n    layer2_weights = tf.Variable(tf.truncated_normal(\n        [patch_size, patch_size, depth, depth], stddev=0.1))\n    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n    layer3_weights = tf.Variable(tf.truncated_normal(\n        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n    layer4_weights = tf.Variable(tf.truncated_normal(\n        [num_hidden, num_labels], stddev=0.1))\n    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n  \n    # Model.\n    def model(data):\n        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(conv + layer1_biases)\n        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(conv + layer2_biases)\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        return tf.matmul(hidden, layer4_weights) + layer4_biases\n  \n    # Training computation.\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    \n    # Optimizer.\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n  \n    # Predictions for the training, validation, and test data.\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))```\n\nUnused variables:\n- num_labels"}, {'reason': 'stop', 'result': "# Set the pipelines for categorical variables\ndiscrete_pipe_dog = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['dog'], hashing=None))])\ndiscrete_pipe_cat = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['cat'], hashing=None))])\n\n# Set the pipelines for continuous variables\ncontinuous_pipe_cat = Pipeline(steps=[('Scale', MyScaler(continuous['cat']))])\ncontinuous_pipe_dog = Pipeline(steps=[('Scale', MyScaler(continuous['dog']))])\n\n# Bring the discrete and continuous pipelines together for cats and dogs\nunion_dog = FeatureUnion([('Discrete', discrete_pipe_dog), ('Continuous', continuous_pipe_dog)])\nunion_cat = FeatureUnion([('Discrete', discrete_pipe_cat), ('Continuous', continuous_pipe_cat)])```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\ntest_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n```\n\nUnused variables:\n- test_sentsa"}, {'reason': 'stop', 'result': "def comparePlots(elems, years):\n    '''\n    ta\n    :param elems: list of dataframes that we  want to compare\n    :param years: list of years that we want to anlayze\n    :return: None (prompts the plot)\n    '''\n    for year in years:\n        for elem in elems:\n            plt.plot(x, y2/y2.max())\n            plt.plot(x,y/y.max())```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'elderSlope = array2[array3 == 1]\nelderArea = array1[array3 == 1]\nfoxSlope = array2[array3 == 2]\nfoxArea = array1[array3 == 2]\nhankSlope = array2[array3 == 3]\nhankArea = array1[array3 == 3]\ndrySlope = array2[array3 == 4]\ndryArea = array1[array3 == 4]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "```python\n# Import packages relevant for all models\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Load in modeling data from /data\nmodeling_df = pd.read_csv('data/weather_with_avg_hourly_flight_delay.csv', index_col=False)\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "filename = os.path.join(folder, filename_audio)\nwith h5py.File(filename, 'r') as audio:\n\n    # Display HDF5 attributes.\n    print('Attributes:')\n    for attr in audio.attrs:\n        print('  {} = {}'.format(attr, audio.attrs[attr]))\n    sr = audio.attrs['sr']\n    labels = audio.attrs['labels']\n\n    # Show datasets, their dimensionality and data type.\n    print('Datasets:')\n    for dname, dset in audio.items():\n        print('  {:2}: {:24}, {}'.format(dname, dset.shape, dset.dtype))\n\n    # Choose dataset: Xa, Xs.\n    X = audio.get('Xs')\n\n    # Full dataset.\n    n = X.shape[-1]\n    datinfo(X, 'Full dataset')\n    print(type(X))\n\n    # Load data into memory as a standard NumPy array.\n    X = X[:Ngenres,:Nclips,:Nframes,...]\n    datinfo(X, 'Reduced dataset')\n    print(type(X))\n\n    # Resize in place without memory loading via hyperslab.\n    # Require chunked datasets.\n    #X.resize((Ngenres, Nclips, Nframes, 2, n))\n\n# Squeeze dataset to a 2D array. The auto-encoder does not\n# care about the underlying structure of the dataset.\nX.resize(Ngenres * Nclips * Nframes * 2, n)\nprint('Data: {}, {}'.format(X.shape, X.dtype))\n\n# Independently rescale each feature.\n# To be put in an sklearn Pipeline to avoid transductive learning.\nX -= np.min(X, axis=0)\nX /= np.max(X, axis=0)\n\n# Independently normalize each sample.\n#X /= np.linalg.norm(X, axis=1)[:,np.newaxis]\n\n# Add Gaussian noise.\nif noise_std is not 0:\n    X += np.random.normal(scale=noise_std, size=X.shape)```\n\nUnused variables:\n- `sr`\n- `labels`"}, {'reason': 'stop', 'result': '#load the best model LSTM\nLSTM = load_model("LSTMmodel_"+str(embedding_size)+\'_\'+str(hidden_size))```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'with model:\n    step = pm.Metropolis()\n    pm.sample(100000, step=step, random_seed=SEED)\n    \ntrace = trace_[50000::50]```\n\nUnused variables:\n- `trace_`'}, {'reason': 'stop', 'result': 'wiki_influenza = getWiki("Influenza")'}, {'reason': 'stop', 'result': 'os.listdir("new-traffic-signs/")\nrepo = "new-traffic-signs/"\nX = []\nfor image_locations in os.listdir(repo):\n    if image_locations != \'.DS_Store\':\n        img = mpimg.imread(repo+image_locations)\n        img_resized = cv2.resize(img,(image_size,image_size), interpolation = cv2.INTER_AREA)\n        X.append(img_resized)\n    \nX2 = np.stack(X, axis = 0)\nX2 = X2.astype(np.uint8)'}, {'reason': 'stop', 'result': 'def run_episode_neural_network(show_renderer=False):    \n    with tf.Session() as sess:\n        init.run()\n        obs = env.reset()\n        obs_pretty = obs.reshape(1, nb_neurons_input)\n        done = False\n        final_score = 0\n        while not done:\n            if show_renderer:\n                env.render()\n            proba_move_to_left = output.eval(feed_dict={X: obs_pretty})        \n            if random.uniform(0, 1) < proba_move_to_left:\n                action = 0  # move to left\n            else:\n                action = 1       \n            obs, reward, done, info = env.step(action) \n            if done:\n                break\n            final_score += reward\n    env.reset()\n    env.close()\n    return final_score\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nn_clusters = 2\n\n# TODO: Apply your clustering algorithm of choice to the reduced data \nclusterer = KMeans(n_clusters=n_clusters, random_state=10)\nclusterer = clusterer.fit(reduced_data)\n\n# TODO: Predict the cluster for each data point\npreds = clusterer.predict(reduced_data)\n\n# TODO: Find the cluster centers\ncenters = clusterer.cluster_centers_\n\n# TODO: Predict the cluster for each transformed sample data point\nsample_preds = clusterer.predict(pca_samples)\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\nscore = silhouette_score(reduced_data, preds)\nprint("{} => {}".format(n_clusters, score))```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "#plot the monthly revenues per year\nax = sns.boxplot(y='Revenue', x=df.index.quarter, data=df, color='#2ecc71')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import statsmodels.stats.api as sms\n\n#ValidRTCI\nRTanalysis.Valid.mean()\nsms.DescrStatsW(RTanalysis.Valid).tconfint_mean()```\n\nUnused variables:\n- a'}, {'reason': 'stop', 'result': '# Create input features and targets dataset. Transfer data from numpy to tensors.\nX = tf.constant(wine_data.data)\ny = tf.constant(wine_data.target)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'init = tf.global_variables_initializer()'}, {'reason': 'stop', 'result': 'y_pred = logreg.predict(X_test)'}, {'reason': 'stop', 'result': 'from helpers import *\ndef test_your_least_squares():\n    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n    x, _, _ = standardize(height)\n    y, tx = build_model_data(x, weight)\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # least square or grid search: TODO\n    # this code should compare the optimal weights obtained \n    # by least squares vs. grid search\n    # ***************************************************\n    raise NotImplementedError```\n\nUnused variables:\n- mean_x\n- std_x'}, {'reason': 'stop', 'result': 'X_train_t = (torch.from_numpy(X_train).float().transpose(1,3)).transpose(2,3)\ny_train_t = torch.from_numpy(y_train).long()\n\ntrain_data = torch.utils.data.TensorDataset(X_train_t, y_train_t)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=round(batch_size_train), shuffle=True)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "post_pdf_contribs = sp.stats.norm.pdf(np.atleast_3d(x_plot),\n                                      trace['mu'][:, np.newaxis, :],\n                                      1. / np.sqrt(trace['lambda'] * trace['tau'])[:, np.newaxis, :])\npost_pdfs = (trace['w'][:, np.newaxis, :] * post_pdf_contribs).sum(axis=-1)\n\npost_pdf_low, post_pdf_high = np.percentile(post_pdfs, [2.5, 97.5], axis=0)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "from scipy.optimize import fsolve\n\ntsym, dsym, ssym = symbols('tsym dsym ssym')\n\nmodel_test_diag = [\n    tsym - ( ssym + (1 - A - U)*test_sym ),\n    dsym - ( A*ssym*p_true_pos + U*ssym*p_false_pos + (1 - A - U)*test_sym*p_true_pos )\n    ]\n\nsol_test_diag = solve(model_test_diag, tsym, dsym)\ntest_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[tsym])\ndiag_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[dsym])\n\ndef test_diag_fun(parms):\n    # parms = (incidence, screening rate)\n    inc = parms[0]\n    scr = parms[1]\n    \n    A = A_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1 - p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n    U = U_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1 - p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n    return [test_fun(A, U, scr, att_symp), diag_fun(A, U, scr, att_symp)]\n\n\n# set up a function to simulate system dynamics when perturbed from steady state\nfrom scipy.integrate import odeint\n\ndef dydt(y, t, parms):\n    return([\n    parms[1]*y[1] + parms[3]*y[2] - (parms[0] + parms[2])*y[0],\n    parms[0]*y[0] - parms[1]*y[1],\n    parms[2]*y[0] - parms[3]*y[2]\n    ])```\n\nUnused variables:\n- `test_sym`"}, {'reason': 'stop', 'result': "def find_rocks(img, levels=(110,110,50)):\n    rockpix = ((img[:,:,0] > levels[0]) \\\n                & (img[:,:,1] > levels[1]) \\\n                & (img[:,:,2] < levels[2]))\n    \n    color_select = np.zeros_like(img[:,:,0])\n    color_select[rockpix] = 1\n    \n    return color_select\n    \nrock_map = find_rocks(rock_img)\nfig = plt.figure(figsize=(12,3))\nplt.subplot(121)\nplt.imshow(rock_img)\nplt.subplot(122)\nplt.imshow(rock_map, cmap='gray')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# Loop over features (polygons) in the shapefile\nfor f in tqdm(feats):\n    # Rasterize the polygon into an array\n    rasterized_image = features.rasterize([(shape(f['geometry']),1)],\n                                          out_shape=out_shape,\n                                          transform=new_aff,\n                                          fill=0,\n                                          all_touched=True)\n\n    # Extract from the xarray where the rasterized polygon is\n    region = data.where(rasterized_image == 1)\n    \n    # Combine x and y into a new dimension called allpoints and calculate the mean over it\n    # and then convert to a dataframe with an appropriate name\n    res = region.stack(allpoints=['x','y']).mean(dim='allpoints').to_dataframe(name=f['properties']['LSOA11CD'])\n    \n    # Append to the list of data frames so we can concatenate them all at the end\n    dfs.append(res)\n    \nstats = pd.concat(dfs, axis=1)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "# Define a function to perform a perspective transform\n# I've used the example grid image above to choose source points for the\n# grid cell in front of the rover (each grid cell is 1 square meter in the sim)\n# Define a function to perform a perspective transform\ndef perspect_transform(img, src, dst):\n           \n    M = cv2.getPerspectiveTransform(src, dst)\n    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))# keep same size as input image\n    mask = cv2.warpPerspective(np.ones_like(img[:,:,0]), M, (img.shape[1], img.shape[0]))\n    return warped, mask\n\n\n# Define calibration box in source (actual) and destination (desired) coordinates\n# These source and destination points are defined to warp the image\n# to a grid where each 10x10 pixel square represents 1 square meter\n# The destination box will be 2*dst_size on each side\ndst_size = 5 \n# Set a bottom offset to account for the fact that the bottom of the image \n# is not the position of the rover but a bit in front of it\n# this is just a rough guess, feel free to change it!\nbottom_offset = 6\nsource = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\ndestination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],\n                  [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],\n                  [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], \n                  [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],\n                  ])\nwarped, mask = perspect_transform(grid_img, source, destination)\nfig = plt.figure(figsize=(12,3))\nplt.subplot(121)\nplt.imshow(warped)\nplt.subplot(122)\nplt.imshow(mask, cmap='gray')\n#scipy.misc.imsave('../output/warped_example.jpg', warped)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'import numpy as np\nimport matplotlib.pyplot as plt\n\narr = np.reshape(z2_layers[..., 5], (1000, 1000))\nfig = plt.figure(figsize=(14, 8))\n\nax = plt.subplot(111)\nim = ax.imshow(arr, interpolation="none")\n\nplt.tight_layout()\nplt.show()```\n\nUnused variables:\n- `fig`\n- `im`'}, {'reason': 'stop', 'result': 'rm_latlon = [(-4.928333333333334, -62.733333333333334), (-3.8666666666666667, -61.6775)]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "'''Create nuclides dictionary from data frame'''\n\nget_nuclides( df )"}, {'reason': 'stop', 'result': '# t, the duration of the fall in the experiment, in seconds.\n# Fill this in.\ntime = ...\n\n# The estimated distance the hammer actually fell, in meters.\n# Fill this in.\nestimated_distance_m = ...```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def ridge_regression(y, tx, lambda_):\n    """implement ridge regression."""\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # ridge regression: TODO\n    # ***************************************************\n    raise NotImplementedError\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "#dataframe om alle gediplomeerden in regio Flevoland te selecteren\ndf_locatie_flevoland = df.loc[df['Locatie'] == 'Flevoland']\ndf_aantal_flevoland = df_locatie_flevoland.groupby(['Locatie']).sum()[['2013   AANT','2014   AANT','2015   AANT','2016   AANT','2017   AANT']]```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'random_average_min_path_length = np.average([nx.average_shortest_path_length(C) for C in nx.connected_component_subgraphs(random_degree_graph)])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'stoichs = [[1, 1, 0, 0, -1], [1, 0, -1, 1, 0]]  # our 2 equilibria\nH = [1, 1, 4, 3, 2]\nN = [0, 0, 1, 1, 0]\nO = [0, 1, 0, 0, 1]``` \n\nUnused variables:\n- preserv'}, {'reason': 'stop', 'result': "scores = []\nfor rep in range(number_of_replicates):\n    env.reset()\n    done = False\n    episode_score = 0.0\n    while not done:\n        action, _, _ = \\\n            model_smoothed_fitted_q(env, gamma, RandomForestRegressor, number_of_value_iterations, transition_model_fitter,\n                                    pairwise_kernels_=None, kernel_sums=None, smoothing_method='mse')\n        _, r, done = env.step(action)\n        episode_score += r\n    scores.append(episode_score)\n    print('score: {}'.format(score))\nprint('mean score: {} se: {}'.format(np.mean(scores), np.std(scores) / np.sqrt(number_of_replicates)))```\n\nUnused variables:\n- pairwise_kernel_\n- kernel_sums"}, {'reason': 'stop', 'result': 'correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\naccuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ndef evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples```\n\n\nUnused variables:\n- saver'}, {'reason': 'stop', 'result': 'act_train_svm = preprocessing.scale(act_train)\nact_val_all_svm = preprocessing.scale(act_val_all)\nact_test_svm = preprocessing.scale(act_test)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'combined_valid_acc_list = None\ncombined_train_acc_list = None\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'class MyScaler():\n\n    def __init__(self, cols):\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        self.ss = StandardScaler()\n        self.ss.fit(X[self.cols])\n        return self\n    \n    def transform(self, X):\n        return self.ss.transform(X[self.cols])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'import time\nimport os\nimport importlib\n\npath="/Users/louis/Google Drive/M.Sc-DIRO-UdeM/IFT6135-Apprentissage de représentations/assignment4/"\nif os.path.isdir(path):\n    os.chdir(path)\nelse:\n    os.chdir("./")\nprint(os.getcwd())\n\nimport matplotlib.pyplot as plt\nfrom scipy.misc import imresize\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom torchvision.utils import save_image\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom PIL import Image\nimport itertools\nimport inception_score \nimport GAN_CelebA\nimportlib.reload(GAN_CelebA)\nimportlib.reload(inception_score)\n\nuse_cuda = torch.cuda.is_available()\ntorch.manual_seed(999)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(999)\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "text = 'foo bar\\t baz \\tqux'```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': '# get activations\nact_val_all = get_activations_batch(model_unet, -2, data_val_overlap.im_patches, 20, verbose=True)\n\n# retain only activation weights for which there is a ground truth\nact_val_all = remove_overlap(data_val.imgs, act_val_all, patch_size=64, stride=32)\nact_val = act_val_all[pred_t_val]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "def value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n    while delta > epsilon:\n\n        # Only one sweep of evaluation before taking the max\n        delta = 0\n        for i, j in states:\n            # Now evaluates the value function for each state for every possible action (not just with respect to current policy)\n            for a_idx in range(4): # actions\n\n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Update rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n            # This step replaces the poilicy improvement step\n            V_kplus1[i,j] = np.max(Q[i,j,:])\n\n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n        \n    # Updates the policy to be greedy with respect to the value function\n    for i, j in states:\n        PI[i,j] = np.argmax(Q[i,j,:])\n    \n    return V_k, k, PI\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'inc = linspace(0, 0.1, 101) # incidence - different range\nscr = linspace(0, 0.5, 101) # screening\ninc,scr = meshgrid(inc, scr)\n\n# proportion of population in each compartment\nZU = U_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1-p_asymp), scr*p_true_pos + att_symp*p_true_pos)\nZA = A_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1-p_asymp), scr*p_true_pos + att_symp*p_true_pos)\nZS = S_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1-p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n\nZprev = 1 - ZU\nZtest = scr + ZS*att_symp\nZdiag = (ZA+ZS)*scr*p_true_pos + ZU*scr*p_false_pos + ZS*att_symp*p_true_pos\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# ODR \nimport scipy.odr as odr\ndef formfaktor2(B, winkel):\n    a,amp=B[0],B[1]\n    q=4*np.pi*n/wavelen * np.sin( winkel *gamma /2)\n    return 9 * ( np.sin(q*a) - (q*a)* np.cos(q*a) )**2 /  (q*a)**6 *amp\n\n# Funktion funktioniert für yscale="log" bei ca. a=500nm\nangles=[]\nmeanI=[]\ndmeanI=[]\n\nfor x in range(2,16):\n    name="Data/A/{}.ASC".format(x*10)\n    angles.append(x*10)\n    data=np.genfromtxt(name,skip_header=16,delimiter="\\t",filling_values=-1,comments="\\"")\n    I=data[183:,1]\n    I*=np.sin(gamma*x*10) # Winkelkorrektur\n    meanI.append(np.mean(I))\n    dmeanI.append(np.std(I))\n    \nstart,stop=0,9\n\nlinear = odr.Model(formfaktor2)\nmydata = odr.RealData(meanI[start:stop], angles[start:stop], sx=dmeanI, sy=1)\nmyodr = odr.ODR(mydata, linear, beta0=[1e-9,100])\nmyoutput = myodr.run()\nbeta=myoutput.beta\ndbeta= myoutput.sd_beta\nmyoutput.pprint()\n\nplotting=np.linspace(20,150,1000)\nfig,ax=plt.subplots(dpi=144)\nax.plot(plotting,formfaktor2(beta,plotting))\nax.plot(angles,meanI,label="data")\nax.set(yscale="log")\nax.legend();```\n\nUnused variables: `dbeta`'}, {'reason': 'stop', 'result': '# cross validation for 2 --> TO DO: FOR K\n\nStrain1_z, Stest1_z, Strain1_y, Stest1_y = split_data(z,y,0.8)\n\n# standardize the data (only centered here)\n\n#Strain1_z = standardize(Strain1_z)\n#Stest1_z = standardize(Stest1_z)\n#Strain1_y = standardize(Strain1_y)\n#Stest1_y = standardize(Stest1_y)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'track, zs = compute_dog_data(10, .2, 200)\nP = np.diag([500., 49.])\nf = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=P)\ns = Saver(f)\nxs, _, _, _ = f.batch_filter(zs, saver=s)\ns.to_array()\nplt.plot(s.y);```\n\nUnused variables:\n- `track`\n- `xs`'}, {'reason': 'stop', 'result': "# start off with simplest case for proof of concept\nwave1_coeffs = {\n    'amplitude': {'mean': 0.5, 'delta': 0.05}, \n    'frequency': {'mean': 1.0, 'delta': 0.1},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'A',\n    'color': '#0000ff'\n}\nwave2_coeffs = {\n    'amplitude': {'mean': 0.75, 'delta': 0.075}, \n    'frequency': {'mean': 3.0, 'delta': 0.3},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'B',\n    'color': '#ff0000',\n}\nwave3_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0.1}, \n    'frequency': {'mean': 8.0, 'delta': 0.8},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'C',\n    'color': '#00ff00'\n}\nwave4_coeffs = {\n    'amplitude': {'mean': 1.4, 'delta': 0.1}, \n    'frequency': {'mean': 12.0, 'delta': 1.2},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'D',\n    'color': '#ff00ff'\n}\n\nmwave_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0}, \n    'frequency': {'mean': 1.0, 'delta': 0}, \n    'offset': {'mean': 0, 'delta': 0},\n    'phase': {'mean': 0, 'delta': 1}, \n    'name': 'mixed_wave',\n    'time': {'t_min': 0, 't_max': 2, 'n_timestamps': 4096, 'delta': 0}\n}\n\nsigs_coeffs = [wave1_coeffs, wave2_coeffs, wave3_coeffs, mwave_coeffs, wave4_coeffs]\n\nfeatures=('x', 'dxdt')[0]\nbatch_size = 128\nwindow_size = 4096\nwindow_type = 'sliding'\nnetwork_type = 'TCN'\nsequence_type = 'many2many'\n\nmsig = MixedSignal(\n    sigs_coeffs, \n    *features,\n    window_size=window_size, \n    window_type=window_type, \n    network_type=network_type,\n    sequence_type=sequence_type,\n)\n\nmsig.generate()\nn_classes = msig.n_classes\nn_features = msig.n_features\ninput_shape = (None, n_features) # TCN\n\nprint(msig.inputs.shape)\nprint(msig.mixed_signal.shape)\nprint(msig.one_hots.shape)\nprint(msig.labels.shape)\nprint(msig.n_timestamps)\nprint(msig.n_samples)```\n\nUnused variables:\n- `batch_size`\n- `input_shape` (RNN version)"}, {'reason': 'stop', 'result': "# max fraction\nfrac_max = 0.3\nfor row in df_secs.itertuples():\n    mdl.add_constraint(row.frac <= 0.3)\n\n# sum of fractions equal 100%\nmdl.add_constraint(mdl.sum(df_secs.frac) == 1)\n\n# north america constraint:\n#    - add a 1-0 column equal to 1 \n# compute the scalar product of frac variables and the 1-0 'is_na' column and set a minimum\nmdl.add_constraint(mdl.dot(df_secs.frac, df_secs.is_na) >= .4)\n\n# ensure minimal return on investment\ntarget_return = 9 # return data is expressed in percents\n# again we use scalar product to compute compound return rate\n# keep the expression to use as a kpi.\nactual_return = mdl.dot(df_secs.frac, df_secs['return'])\nmdl.add_kpi(actual_return, 'ROI')\n\n# keep the constraint for later use (more on this later)\nct_return = mdl.add_constraint(actual_return >= 9)```\n\nUnused variables:\n- target_return"}, {'reason': 'stop', 'result': 'x_A_m = 0\nx_B_m = float(x_A_m + L_AB_m.subs(s_d))\nx_D_m = float(x_B_m + L_BD_m.subs(s_d))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'means = X_train.mean(axis=0, keepdims=True)\nstds = X_train.std(axis=0, keepdims=True) + 1e-10\nX_val_scaled = (X_valid - means) / stds\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            X_batch_scaled = (X_batch - means) / stds\n            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n        if epoch % 5 == 0:\n            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n            print(epoch, "검증 세트 정확도:", acc_valid)\n\n    save_path = saver.save(sess, "./my_model_final_selu.ckpt")```\n\nUnused variables:\n- acc_batch'}, {'reason': 'stop', 'result': 'def uncompress_features_labels(file):\n    """\n    Uncompress features and labels from a zip file\n    :param file: The zip file to extract the data from\n    """\n    features = []\n    labels = []\n\n    with ZipFile(file) as zipf:\n        # Progress Bar\n        filenames_pbar = tqdm(zipf.namelist(), unit=\'files\')\n        \n        # Get features and labels from all files\n        for filename in filenames_pbar:\n            # Check if the file is a directory\n            if not filename.endswith(\'/\'):\n                with zipf.open(filename) as image_file:\n                    image = Image.open(image_file)\n                    image.load()\n                    # Load image data as 1 dimensional array\n                    # We\'re using float32 to save on memory space\n                    feature = np.array(image, dtype=np.float32).flatten()\n\n                # Get the the letter from the filename.  This is the letter of the image.\n                label = os.path.split(filename)[1][0]\n\n                features.append(feature)\n                labels.append(label)\n    return np.array(features), np.array(labels)\n\n# Get the features and labels from the zip files\ntrain_features, train_labels = uncompress_features_labels(\'notMNIST_train.zip\')\ntest_features, test_labels = uncompress_features_labels(\'notMNIST_test.zip\')\n\n# Limit the amount of data to work with a docker container\ndocker_size_limit = 150000\ntrain_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n\n# Wait until you see that all features and labels have been uncompressed.\nprint(\'All features and labels uncompressed.\')'}, {'reason': 'stop', 'result': 'V_AD_N = P_A_N\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets("/tmp/data/")```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "```python\nsave_file = './train_model_best.ckpt'\nsaver = tf.train.Saver()\n\nwith tf.Session() as session:\n    saver.restore(session, save_file)\n    feed_dict = {tf_train_dataset : X2_norm, tf_keep_prob : 1}\n    logi = session.run(logits, feed_dict)\n    predicts = session.run(tf.nn.top_k(logi, k=5, sorted=True))\n    proba = session.run(tf.nn.softmax(predicts[0]))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\naccuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsaver = tf.train.Saver()\n\ndef evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        batch_x = np.expand_dims(batch_x, axis=3)\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n```\n\nUnused variables:\n- N/A'}, {'reason': 'stop', 'result': 'specphot = coco.spec_photometry(b"SN2007uy", \n                    z_obs, b"LSST_g")'}, {'reason': 'stop', 'result': "allData = pd.read_csv('data/allData.csv')\nr1 = allData[allData['Round']==1]\nr2 = allData[allData['Round']==2]```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'width = 16\nheight = 8\nshift_x = 2\nshift_y = 6\nim_shape = (20, 20)\n\nbrick = make_synthetic_brick(im_shape, width, height, shift_x, shift_y)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300\nn_hidden2 = 50\nn_hidden3 = 50\nn_hidden4 = 20\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    hidden2_stop = tf.stop_gradient(hidden2)\n    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name="hidden3")\n    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4")\n    logits = tf.layers.dense(hidden4, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)```\n\nUnused variables:\n- n_hidden1\n- n_hidden2\n- n_hidden3\n- n_outputs'}, {'reason': 'stop', 'result': 'test_ratio = 0.2\ntrain_size = int(len(flower_paths_and_classes) * (1 - test_ratio))\n\nnp.random.shuffle(flower_paths_and_classes)\n\nflower_paths_and_classes_train = flower_paths_and_classes[:train_size]\nflower_paths_and_classes_test = flower_paths_and_classes[train_size:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "n_periods = 0\nperiod    = 200\ninterval  = 2\nphase     = 0\namp_scale = 100\n\nplt.figure(figsize=(12,6))\nfig = plt.subplot()\nx = range(len(power_data))\nfig.plot(power_time[n_periods*period:(n_periods+interval)*period], amp_data[n_periods*period:(n_periods+interval)*period,phase]*amp_scale,  label='amp',  marker='o',linewidth=0.3, markersize=1.5)\nfig.plot(power_time[n_periods*period:(n_periods+interval)*period], volt_data[n_periods*period:(n_periods+interval)*period,phase],  label='volt',  marker='o',linewidth=0.3, markersize=1.5)\nfig.plot(power_time[n_periods*period:(n_periods+interval)*period], power_data[n_periods*period:(n_periods+interval)*period],  label='power',  marker='o',linewidth=0.3, markersize=1.5)\nfig.legend();\nfig.set_ylabel('Power [W]')\nfig.set_xlabel('Time [s]')\nfig.set_title('Total Power');```\n\nUnused variables:\n- x"}, {'reason': 'stop', 'result': 'split = StratifiedShuffleSplit(1, .8, 42)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# Let's start by creating our geolocator. We will use Google Maps API:\ngooglemapsapikeyjson = json.loads(open('google_maps_api_keys.json').read())"}, {'reason': 'stop', 'result': 'x, y = util.get_sample_classification_data()\nx_new = np.linspace(-15, 15, 100)\nutil.scatter_raw_data_classification(x, y, y_label = "Class")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with model:\n    pm.Binomial("number_cheaters", 100, p_skewed, observed=35)```\n\nUnused variables:\n- yes_responses'}, {'reason': 'stop', 'result': '# find connection in the specified sequence, center 29 is in the position 15\nlimbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n           [1,16], [16,18], [3,17], [6,18]]\n# the middle joints heatmap correpondence\nmapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n          [55,56], [37,38], [45,46]]```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# plt.rc("savefig", dpi=600) # for high-resolution version\n\nfig = plt.figure(figsize=(12, 7))\n\nax1 = fig.add_subplot(231)\np = ax1.pcolor(inc, scr, ZU)\nc = ax1.contour(inc, scr, ZU, [0.6, 0.7, 0.8, 0.9], colors=[\'k\', \'k\', \'k\', \'k\'])\nplt.clabel(c, manual=[(0.1, 0.05), (0.2, 0.05), (0.4, 0.05)], fmt=\'%1.1f\')\ncb = fig.colorbar(p, ax=ax1)\n# ax1.set_xlabel(\'Incidence\')\nax1.set_ylabel(\'Screening Rate (years $^{-1}$)\')\nt = ax1.text(0.25, 0.45, \'Uninfected\', ha=\'center\', size=\'large\')\nt.set_bbox(dict(facecolor=\'white\', alpha=0.7, edgecolor=\'None\'))\nax1.set_ylim(0, 0.5)\nax1.set_xlim(0, 0.5)\n\nax2 = fig.add_subplot(232)\np = ax2.pcolor(inc, scr, ZS)\nc = ax2.contour(inc, scr, ZS, (0.003, 0.006, 0.009, 0.012), colors=\'k\', manual=True)\nplt.clabel(c, manual=[(0.1, 0.35), (0.2, 0.35), (0.35, 0.35), (0.45, 0.35)])\ncb = fig.colorbar(p, ax=ax2)\nt = ax2.text(0.25, 0.45, \'Infected, Symptomatic\', ha=\'center\', size=\'large\')\nt.set_bbox(dict(facecolor=\'white\', alpha=0.7, edgecolor=\'None\'))\nax2.set_ylim(0, 0.5)\nax2.set_xlim(0, 0.5)\n\nax3 = fig.add_subplot(233)\np = ax3.pcolor(inc, scr, ZA)\nc = ax3.contour(inc, scr, ZA, (0.1, 0.2, 0.3), colors=\'k\')\nplt.clabel(c, manual=[(0.1, 0.1), (0.2, 0.1), (0.4, 0.1)], fmt=\'%1.1f\')\ncb = fig.colorbar(p, ax=ax3)\nt = ax3.text(0.25, 0.45, \'Infected, Asymptomatic\', ha=\'center\', size=\'large\')\nt.set_bbox(dict(facecolor=\'white\', alpha=0.7, edgecolor=\'None\'))\nax3.set_ylim(0, 0.5)\nax3.set_xlim(0, 0.5)\n\nax4 = fig.add_subplot(234)\np = ax4.pcolor(inc, scr, Zprev)\nc = ax4.contour(inc, scr, Zprev, (0.1, 0.2, 0.3), colors=\'k\')\nplt.clabel(c, manual=[(0.1, 0.1), (0.2, 0.1), (0.4, 0.1)], fmt=\'%1.1f\')\ncb = fig.colorbar(p, ax=ax4)\nax4.set_xlabel(\'Incidence  (years $^{-1}$)\')\nax4.set_ylabel(\'Screening Rate (years $^{-1}$)\')\nt = ax4.text(0.25, 0.45, \'Prevalence\', ha=\'center\', size=\'large\')\nt.set_bbox(dict(facecolor=\'white\', alpha=0.7, edgecolor=\'None\'))\nax4.set_ylim(0, 0.5)\nax4.set_xlim(0, 0.5)\n\nax5 = fig.add_subplot(235)\np = ax5.pcolor(inc, scr, Ztest)\nc = ax5.contour(inc, scr, Ztest, (0.2, 0.4, 0.6), colors=\'k\')\nplt.clabel(c, manual=[(0.45, 0.05), (0.45, 0.25), (0.45, 0.45)], fmt=\'%1.1f\')\ncb = fig.colorbar(p, ax=ax5)\nax5.set_xlabel(\'Incidence  (years $^{-1}$)\')\nt = ax5.text(0.25, 0.45, \'Testing Rate\', ha=\'center\', size=\'large\')\nt.set_bbox(dict(facecolor=\'white\', alpha=0.7, edgecolor=\'None\'))\nax5.set_ylim(0, 0.5)\nax5.set_xlim(0, 0.5)\n\nax6 = fig.add_subplot(236)\np = ax6.pcolor(inc, scr, Zdiag)\nc = ax6.contour(inc, scr, Zdiag, (0.1, 0.2), colors=\'k\')\nplt.clabel(c, manual=[(0.2, 0.2), (0.4, 0.25)], fmt=\'%1.1f\')\ncb = fig.colorbar(p, ax=ax6)\nax6.set_xlabel(\'Incidence (years $^{-1}$)\')\nt = ax6.text(0.25, 0.45, \'Diagnosis Rate\', ha=\'center\', size=\'large\')\nt.set_bbox(dict(facecolor=\'white\', alpha=0.7, edgecolor=\'None\'))\nax6.set_ylim(0, 0.5)\nax6.set_xlim(0, 0.5)\n\nplt.show()\n# plt.rc("savefig", dpi=80) # reset resolution\n```\n\nUnused variables:\n- `ax1`\n- `ax2`\n- `ax3`\n- `ax4`\n- `ax5`\n- `ax6`\n- `p`\n- `c`\n- `cb`'}, {'reason': 'stop', 'result': 'data = yearly_data\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'train_dataset = trimmed.sample(frac=0.8, random_state=0)\ntest_dataset = trimmed.drop(train_dataset.index)\n\ntrain_stats = train_dataset.describe()\ntrain_stats = train_stats.transpose()\ntrain_stats```\n\nUnused variables:\n- `train_stats` (before the last assignment)'}, {'reason': 'stop', 'result': "# reload(accuracy)\nreload(slope)\nplt.close()\nf = plt.figure(figsize=(5, 4))\n\nle = .1\nre = .02\nte = .1\nbe = .11\nh_gap = .125\n\nw = .55\nh = 1. - te - be\n\nax_lines = f.add_axes([le, be, w, h])\nax_slopes = f.add_axes([le + w + h_gap, be, 1. - w - h_gap - le - re, h])\n\nkey = fracs[-1]\n\nslope.plot_cv_slope(subjects, deep_all, linear_all, chance[0], training_size, fracs, (ax_lines, ax_slopes),\n                    legend=True, show_significance=True)\n\nx0 = .05\ny0 = 1. - te + .01\nx1 = le + w + h_gap - .075\n\nf.text(x0, y0, 'A', **letter_fontstyle)\nf.text(x1, y0, 'B', **letter_fontstyle)\n\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope.eps'), dpi=300)\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope.png'), dpi=300)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'app_path = os.getcwd()\nfilesep = \'\\\\\' if platform.system() == \'Windows\' else \'/\'\nos.environ[\'GOOGLE_APPLICATION_CREDENTIALS\'] = "creds" + filesep + "sarasmaster-524142bf5547.json"```\n\nUnused variables:\n- `os.chdir(os.getcwd())`'}, {'reason': 'stop', 'result': 'p_A_samples = burned_trace["p_A"]\np_B_samples = burned_trace["p_B"]```\n\nUnused variables:\n- delta_samples'}, {'reason': 'stop', 'result': "from keras import optimizers\n\nrmsprop = optimizers.RMSprop(lr=1e-4)\n\nVGG16Seq.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n```\n\nUnused variables:\n- sgd"}, {'reason': 'stop', 'result': 'import pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv(\'data/exoplanet.eu_catalog.csv\', \n                      usecols=[\'mass\',\'mass_error_min\',\'mass_error_max\',\n                               \'semi_major_axis\',\'semi_major_axis_error_min\',\'semi_major_axis_error_max\',\'star_name\'])\n\nclass System:\n    def __init__(self, data):\n        self.data=data\n        self.system = list(self.data.groupby("star_name").groups.keys())\n        self.Number()\n        self.Mass()\n        self.CenterOfMass()\n        \n    def Number(self):\n        sys = self.data.groupby("star_name")\n        self.N_total = len(sys["mass"])\n        \n    def Mass(self):\n        sys = self.data.groupby("star_name")\n        self.M_total = sys["mass"].sum()\n    \n    def CenterOfMass(self):\n        self.rm_i = self.data["mass"].multiply(self.data["semi_major_axis"])\n        self.data_i = self.data.assign(CM_i = self.rm_i.values) \n        p_system = self.data_i.groupby("star_name")\n        sum_rm = p_system[\'CM_i\'].sum()\n        self.CM = sum_rm.divide(self.M_total)'}, {'reason': 'stop', 'result': "from sklearn.feature_extraction.text import HashingVectorizer\n\n\nvect = HashingVectorizer(decode_error='ignore', \n                         n_features=2**21,\n                         preprocessor=None, \n                         tokenizer=tokenizer)```\n\nUnused variables:\n- SGDClassifier"}, {'reason': 'stop', 'result': 'for n in range(1, nt):\n    rn = rho.copy()\n    for j in range(1, nx):\n        v = (vmax * (1 - rho / rhomax)) * (5 / 18)\n        f1 = v * rho\n        rho[1:] = rn[1:] - dt / dx * (f1[1:] - f1[0:-1])\n        rho[0] = 10\n```\n\nUnused variables:\n- `n`\n- `j`'}, {'reason': 'stop', 'result': "SkyPresence = posttest.groupby(['subjID'])['Q2_SceneSkyPresence'].mean()\nSkyPresenceSEM = pd.Series.std(SkyPresence) / n\nColorScheme = posttest.groupby(['subjID'])['Q2_SceneColorScheme'].mean()\nColorSchemeSEM = pd.Series.std(ColorScheme) / n\nTreeFreq = posttest.groupby(['subjID'])['Q2_SceneTreeFrequency'].mean()\nTreeFreqSEM = pd.Series.std(TreeFreq) / n\n\nImageType = posttest.groupby(['subjID'])['Q2_ImageType'].mean()\nImageTypeSEM = pd.Series.std(ImageType) / n\nFeatureType = posttest.groupby(['subjID'])['Q2_FeatureType'].mean()\nFeatureTypeSEM = pd.Series.std(FeatureType) / n\nLightType = posttest.groupby(['subjID'])['Q2_LightType'].mean()\nLightTypeSEM = pd.Series.std(LightType) / n\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "```python\n# Generate hypothesis data\ndef gen_hyp_data(model, N, text_len=500):\n    texts, hiddens, hyps = [], [], []\n    for i in range(N):\n        text, hidden = generate(model, '\\n\\n', text_len, 0.8, True)\n        hidden = hidden.reshape(hidden.shape[0], -1)\n        hyp = hypothesis_inlinecounter(text)\n        hiddens.append(hidden)\n        hyps.append(hyp)\n        texts.append(text)\n    return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n\n# Generate train and test data\ntrain_texts, _, train_hiddens = gen_hyp_data(model_gru, 500)\ntest_texts, _, test_hiddens = gen_hyp_data(model_gru, 2)\n```\n\nUnused variables:\n- train_hyps\n- test_hyps"}, {'reason': 'stop', 'result': '# get all predictions in training and test set\n# training set\ny_pred_tr = model_unet.predict(data_train_overlap.im_patches, verbose=1)\ny_pred_tr = remove_overlap(data_train.imgs, y_pred_tr, 64, 32)\ny_pred_label_tr = get_y_pred_labels(y_pred_tr, background=True)\n\n# validation set\ny_pred_val = model_unet.predict(data_val_overlap.im_patches, verbose=1)\ny_pred_val = remove_overlap(data_val.imgs, y_pred_val, 64, 32)\ny_pred_label_val = get_y_pred_labels(y_pred_val, background=True)\n\n# test set\ny_pred_te = model_unet.predict(data_test_overlap.im_patches, batch_size=20, verbose=1)\ny_pred_te = remove_overlap(data_test.imgs, y_pred_te, 64, 32)\ny_pred_label_te = get_y_pred_labels(y_pred_te, background=True)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '```python\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = saver.save(sess, "./my_model_final.ckpt")\n```\n\nUnused variables:\n- `extra_update_ops`'}, {'reason': 'stop', 'result': "```result_dir = '/Users/nishant/ray_results/2018-08-28_22-18-1581dc8109'\nfname = result_dir + '/iter_vars.pkl'\nwith open(fname, 'rb') as file:\n    itervars = pickle.load(file)```\n\nUnused variables:\n- `result_dir`"}, {'reason': 'stop', 'result': "from sklearn.datasets import fetch_mldata\nmnist = fetch_mldata('MNIST original')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# Create Negative Samples\nnegative_samples = pd.merge(\n    pd.merge(\n        tracks_df, playlist_map_df_negative, left_index=True, right_on='track_uri'),\n    playlist_df,\n    on='playlist_pid')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nplt.scatter(X[:,0], X[:, 1], c=y)\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "images_seq, labels_seq = loadGapData('data/gapdet/large/',\n                                     slider=slider_size,\n                                     seq=True)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'dt = 1.\nR_var = 10\nQ_var = 0.01\nx = np.array([[10.0, 4.5]]).T\nP = np.diag([500, 49])\nF = np.array([[1, dt],\n              [0,  1]])\nH = np.array([[1., 0.]])\nR = np.array([[R_var]])\nQ = Q_discrete_white_noise(dim=2, dt=dt, var=Q_var)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "detNames = {0: 'Det0', 2: 'Det45', 4: 'Det90'}```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "with pm.Model() as model:\n    alpha = pm.Gamma('alpha', 1., 1.)\n    beta = pm.Beta('beta', 1, alpha, shape=K)\n    w = pm.Deterministic('w', stick_breaking(beta))\n    \n    mu = pm.Uniform('mu', 0., 300., shape=K)\n    obs = pm.Mixture('obs', w, pm.Poisson.dist(mu), observed=sunspot_df['sunspot.year'])```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\ntf.random.set_seed(9527)\n\n# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\nq = emb_inp\nk = emb_inp\n```\n\nUnused variables:\n- `v`'}, {'reason': 'stop', 'result': '### Train your model here.\n### Calculate and report the accuracy on the training and validation set.\n### Once a final model architecture is selected, \n### the accuracy on the test set should be calculated and reported as well.\n### Feel free to use as many code cells as needed.\nx = tf.placeholder(tf.float32, (None, 32, 32, 3))\ny = tf.placeholder(tf.int32, (None))\n``` \n\nUnused variables: \n- `one_hot_y`'}, {'reason': 'stop', 'result': 'earthquakeTimes, earthquakeMoments = countQuakes(earthquakePlot, blockNum, timeStepShort)\nearthquakeMagnitudes = list(map(np.log, earthquakeMoments))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "a = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'y_pred = y_prob.idxmax(axis=1)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'threshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\ntf.assign(weights, clipped_weights)```\n\nUnused variables:\n- clip_weights'}, {'reason': 'stop', 'result': 'num_channels = 1\n```\n\nUnused variables:\n- image_size'}, {'reason': 'stop', 'result': 'def is_valid_url(url):\n    try:\n        http_program(url)\n        return True\n    except ValueError:\n        return False\n```\n\nUnused variables:\n- `result`'}, {'reason': 'stop', 'result': 'alpha = integral.trapez(u1_z, 0, 0, len(u1_z)-1)\nbeta = integral.trapez(u2_z, 0, 0, len(u2_z)-1)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    \n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n    \n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n\n    self.dropout = tf.keras.layers.Dropout(rate)\n        \n  def call(self, x, training, mask):\n    input_seq_len = tf.shape(x)[1]\n    \n    x = self.embedding(x)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :input_seq_len, :]\n\n    x = self.dropout(x, training=training)\n    \n    for enc_layer in self.enc_layers:\n      x = enc_layer(x, training, mask)\n      \n    return x\n```\n\nUnused variables:\n- `num_layers`\n- `rate`'}, {'reason': 'stop', 'result': "# do the reduction but in a different way\ncov = X.T @ X /(X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:,idx]\nproject_X = X @ vec\nproject_V = vec.T @ vec\n\ninversed_vec = np.linalg.inv(vec)\ninversed_vec = inversed_vec - inversed_vec.mean(1)\nrevert_X = project_X @ inversed_vec\nrevertedV = project_V @ inversed_vec.T\n\n\ndef plot():\n    plt.figure(figsize=(15,5)); \n\n    plt.subplot(131); \n    plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\n    plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n    pca11 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca12 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True); \n\n    plt.subplot(132); \n    plt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red', alpha=0.5)\n    plt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\n    pca21 = plt.arrow(0, 0, *project_V[:,0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca22 = plt.arrow(0, 0, *project_V[:,1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True); \n\n    plt.subplot(133); \n    plt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red', alpha=0.5)\n    plt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\n    pca21 = plt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca22 = plt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True); \n\n    plt.show()\nplot()```\n\nUnused variables:\n- idx"}, {'reason': 'stop', 'result': "GCS_PATTERN = 'gs://flowers-public/*/*.jpg'\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # flower labels (folder names in the data)```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': '# make a one-step forecast\ndef forecast_lstm(model, batch_size, X):\n    X = X.reshape(1, 1, len(X))\n    yhat = model.predict(X, batch_size=batch_size)\n    return yhat[0, 0]\n\n# load the dataset\nseries = read_csv(filename, header=0, parse_dates=[0], index_col=0, squeeze=True)\n\n# transform data to be stationary\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\ntrain, test = supervised_values[0:-12], supervised_values[-12:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "rows, cols = 10, 6\nfig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n\nfor sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '#Only run once!\nX = np.hstack((X, np.ones((X.shape[0], 1))))\n\n#Initialize Weights to zero:\nw = np.zeros(X.shape[1])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# split our dataset into training / testing sets\ntrain_test_split = int(np.ceil(2*len(y)/float(3)))   # set the split point\n\n# partition the training set\nX_train = X[:train_test_split,:]\ny_train = y[:train_test_split]\n\n# keep the last chunk for testing\nX_test = X[train_test_split:,:]\ny_test = y[train_test_split:]\n\n# NOTE: to use keras's RNN LSTM module our input must be reshaped to [samples, window size, stepsize] \nX_train = np.reshape(X_train, (X_train.shape[0], window_size, 1))\nX_test = np.reshape(X_test, (X_test.shape[0], window_size, 1))```\n\nUnused variables:\n- `X_train` (before reshaping)\n- `X_test` (before reshaping)"}, {'reason': 'stop', 'result': "I = Variable(name='I', num_states=2) #Influenza\nS = Variable(name='S', num_states=2) #Smokes\nST = Variable(name='ST', num_states=2) #SoreThreat\nF = Variable(name='F', num_states=2) #Fever\nB = Variable(name='B', num_states=2) #Bronchitis\nC = Variable(name='C', num_states=2) #Coughing\nW = Variable(name='W', num_states=2) #Wheezing\n\nf_I = Factor(name='p(I)',\n                 f=np.array([0.95, 0.05]),\n                 neighbours=[I])\n\nf_S = Factor(name='p(S)',\n                 f=np.array([0.8, 0.2]),\n                 neighbours=[S])\n\nprob_ST = [\n    [ #ST = 0\n        0.999, # I = 0\n        0.7 # I = 1\n    ],\n    [ #ST = 1\n        0.001, # I = 0\n        0.3 # I = 1\n    ]\n]\nf_ST = Factor(name='p(ST |I)',\n                 f=np.array(prob_ST),\n                 neighbours=[ST, I])\n\nprob_F = [\n    [ #F = 0\n        0.95, # I = 0\n        0.1 # I = 1\n    ],\n    [ #F = 1\n        0.05, # I = 0\n        0.9 # I = 1\n    ]\n]\nf_F = Factor(name='p(F |I)',\n                 f=np.array(prob_F),\n                 neighbours=[F, I])\n\nprob_B = [\n    [ #B = 0\n        [ # I = 0\n          0.9999, # S = 0\n          0.3     # S = 1\n        ],\n        [ # I = 1\n          0.1,  # S = 0\n          0.01  # S = 1\n        ]\n    ],\n    [  #B = 1\n        [ # I = 0\n          0.0001, # S = 0\n          0.7     # S = 1\n        ],\n        [ # I = 1\n          0.9,  # S = 0\n          0.99  # S = 1\n        ]\n    ]\n]\nf_B = Factor(name='p(B |I, S)',\n                 f=np.array(prob_B),\n                 neighbours=[B, I, S])\n\nprob_C = [\n    [ #C = 0\n        0.93, # B = 0\n        0.2 # B = 1\n    ],\n    [ #C = 1\n        0.07, # B = 0\n        0.8 # B = 1\n    ]\n]\nf_C = Factor(name='p(C |B)',\n                 f=np.array(prob_C),\n                 neighbours=[C, B])\n\n\nprob_W = [\n    [ #W = 0\n        0.999, # B = 0\n        0.4 # B = 1\n    ],\n    [ #W = 1\n        0.001, # B = 0\n        0.6 # B = 1\n    ]\n]\nf_W = Factor(name='p(W |B)',\n                 f=np.array(prob_W),\n                 neighbours=[W, B])\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "```python\ndef view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n    \n    return fig, axes\n```\n\nUnused variables:\n- `im`"}, {'reason': 'stop', 'result': '```def show_results(label):\n    _, mae, mse = models[label].evaluate(normed_test_data, test_labels[label], verbose=0)\n\n    print("Testing set Mean Abs Error: {:5.2f} um".format(mae))\n    print("Testing set RMS: {:5.2f} um".format(np.sqrt(mse)))\n\n    test_predictions = models[label].predict(normed_test_data).flatten()\n\n    plt.scatter(test_labels[label], test_labels[label] - test_predictions)\n    plt.xlabel(\'True Values [um]\')\n    plt.ylabel(\'Residuals [um]\')\n    minx, maxx = min(test_labels[label]), max(test_labels[label])\n    plt.plot([minx, maxx], [0, 0])\n    plt.show()```\n\nUnused variables:\n- `loss`'}, {'reason': 'stop', 'result': 'learn = Learner(data, simple_cnn((3, 16, 16, 2)), metrics=[accuracy, error_rate], callback_fns=[CSVLogger])```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "words_to_train = ['FISH', 'BOOK', 'VEGETABLE', 'FUTURE', 'JOHN']```\n\nUnused variables:\n- timeit"}, {'reason': 'stop', 'result': '```python\ndef plot_2d(value, ax=None, group=None, mask=None, matrix_id=0, mat_as_group=False, \n            group_id=None, linewidths=0, is_string=False, fmt="d", square=False):\n  \n  if hasattr(value, "numpy"):\n    value = value.numpy()\n  if group is not None and hasattr(group, "numpy"):\n    group = group.numpy()\n  if mask is not None and hasattr(mask, "numpy"):\n    mask = tf.squeeze(mask)\n    mask = tf.ones_like(value) * mask\n    mask = mask.numpy()\n    \n\n  cmaps = [\'PuOr\', \'tab20b\', \'RdBu\']\n  group_id = int(group[0][0])\n  cmap = cmaps[group_id]\n  \n  if is_string:\n    fmt = \'\'\n  \n  sns.heatmap(group, \n              fmt=fmt,\n              cmap=cmap,\n              annot=value, \n              cbar=False, \n              xticklabels=False, \n              yticklabels=False, \n              square=square,\n              mask=mask,\n              linewidths=linewidths,\n              ax=ax)```\n```\n\nUnused variables: `matrix_id, mat_as_group`'}, {'reason': 'stop', 'result': "dfTiDirt = pd.read_csv('titanic_data.csv')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'reset_graph()\n\nimport tensorflow as tf\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n\ntraining = tf.placeholder_with_default(False, shape=(), name=\'training\')\n\nhidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")\nbn1_act = tf.nn.elu(hidden1)\n\nhidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")\nbn2_act = tf.nn.elu(hidden2)\n\nlogits_before_bn = tf.layers.dense(bn2_act, n_outputs, name="outputs")\nlogits = tf.layers.batch_normalization(logits_before_bn, training=training,\n                                       momentum=0.9)```\n\nUnused variables:\n- bn1\n- bn2'}, {'reason': 'stop', 'result': "unique, counts = np.unique(y_train, return_counts=True)\nunique_test, counts_test = np.unique(y_test, return_counts=True)\n\nplt.hist(y_train, color = 'b', label = 'train', normed = True, bins=range(n_classes+1))\nplt.hist(y_test, color = 'r', alpha = 0.5, label = 'test', normed = True, bins=range(n_classes+1))\nplt.title('Distribution of the number of images per class \\n for train and test set')\nplt.xlabel('Class')\nplt.ylabel('Percentage of images')\nplt.show()\n```\n\nUnused variables:\n- `counts`\n- `counts_test`"}, {'reason': 'stop', 'result': '## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to generate more \n## "logging" output when this notebook is run \n## on the Travis Continuous Integration\n## platform to test the code as well as\n## speeding up the run using a smaller\n## dataset for debugging\n\nif \'TRAVIS\' in os.environ:\n    gs_lr_tfidf.verbose=2\n    X_train = df.loc[:250, \'review\'].values\n    y_train = df.loc[:250, \'sentiment\'].values\n    X_test = df.loc[25000:25250, \'review\'].values\n    y_test = df.loc[25000:25250, \'sentiment\'].values```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '#Setup Axis\nstate=df_county_data[\'State\']\ngrad_rate = df_county_data["Graduation Rate"]\nx_axis = np.arange(len(state))\n\n# Create a bar chart based upon the above data\nplt.bar(x_axis, grad_rate, color="b", align="center")\nplt.title("County Graduation Rates")\nplt.xlabel("Counties")\nplt.ylabel("Graduation Rates")\nplt.text(140, 0.6, "Note:\\nGraduation Rates for all counties in NJ, NY, & PA.")\n\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Graduation_Rates.png", bbox_inches = "tight")\nplt.show()\n```\n\nUnused variables:\n- county\n- tick_locations'}, {'reason': 'stop', 'result': '# show sample images\nfig = plt.figure(figsize=(12, 5))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(data[i], cmap=\'gray\', interpolation=\'none\')\n    plt.title("Ground Truth: {}".format(labels[i]))\n    plt.xticks([])\n    plt.yticks([])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '#TRAINING\nChatbot.fit(x= [X_[:dataset_cut,:], padded_T[:dataset_cut,:-1]], y = T_[:dataset_cut,1:], \n                                    epochs=epochs, \n                                    validation_split=validation_split, \n                                    batch_size=batch_size,\n                                    callbacks = [callback]).history```\n\nUnused variables:\n- logs_chatbot'}, {'reason': 'stop', 'result': "```python\nwaves_std = np.std(waves, axis=0)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 6))\n\nax[0].plot(xindex, waves_std)\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=waves_std)\nax[1].set_xlim((xmin, xmax))\nax[1].grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'std_dev_analysis.png'), bbox_inches='tight')\n```\n\nUnused variables:\n- `i_std`"}, {'reason': 'stop', 'result': 'optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n                                       momentum=0.9)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'n_epochs = 10\nbatch_size = 40\nn_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n\nwith tf.Session() as sess:\n    init.run()\n    inception_saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n\n    for epoch in range(n_epochs):\n        print("Epoch", epoch, end="")\n        for iteration in range(n_iterations_per_epoch):\n            print(".", end="")\n            X_batch, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        print("  Train accuracy:", acc_train)\n\n        save_path = saver.save(sess, "./my_flowers_model")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'raveled = np.ravel(test_black)'}, {'reason': 'stop', 'result': 'county = df_county_data["County Name"]\nhouse_size = df_county_data["Household Size"]\n\nx_axis = np.arange(len(house_size))\n\n# Create a bar chart based upon the above data\nplt.bar(x_axis, house_size, color="r", align="center")\nplt.title("County Household Size")\nplt.xlabel("Counties")\nplt.ylabel("Household Size")\nplt.text(140, 2.5, "Note:\\nHousehold size for all counties in NJ, NY, & PA.")\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Household_Size.png", bbox_inches = "tight")\nplt.show()```\n\nUnused variables:\n- tick_locations'}, {'reason': 'stop', 'result': '```n_epochs = 40\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        if epoch % 5 == 0:\n            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n            print(epoch, "배치 데이터 정확도:", acc_batch, "검증 세트 정확도:", acc_valid)\n\n    save_path = saver.save(sess, "./my_model_final.ckpt")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'seed_input = "http://www.google.com/search?q=fuzzing"'}, {'reason': 'stop', 'result': 'unique_datatypes = list(merged["data_type"].unique())\n\nprint("Unique datatypes in this dataset are: {}".format(unique_datatypes))\n\nif "float64" in unique_datatypes or "int64" in unique_datatypes:\n    rounded_places = 4\n    \n    for column in ["mean", "std", "min", "25%", "50%", "75%", "max"]:\n        merged[column] = merged[column].astype(float).round(rounded_places)\nelse:\n    round_values = False```'}, {'reason': 'stop', 'result': '# remove class\nn_classes = 9\nnames_keep = np.asarray(names)\nnames_keep = names_keep.tolist()\nprint("classes to keep: " + str(names_keep))```\n\nUnused variables:\n- classes_to_keep'}, {'reason': 'stop', 'result': "data = xr.open_mfdataset(r'C:\\MAIACData\\nc_monthly_daily\\*.nc')['data']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "RM_D_Nm = sy.Symbol('RM_{D}[Nm]')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': '```python\n# Getting the tsv file from the given link provided using request library\nimport requests\nimport pandas as pd\n\nurl = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"\nresponse = requests.get(url)\n\nwith open(\'image_predictions.tsv\', \'wb\') as file:\n    file.write(response.content)\n    \n# Reading the file\nimg_predictions = pd.read_csv(\'image_predictions.tsv\', sep=\'\\t\')\n```\n\nUnused variables: None'}]