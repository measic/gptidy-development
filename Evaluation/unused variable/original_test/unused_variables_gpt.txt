[{'reason': 'stop', 'result': 'with tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = saver.save(sess, "./my_model_final.ckpt")```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '#Kolommen zijn de kolommen uit de df_totaalSBB dataframe. Dit gaat dan om 2013 AANT etc. Dit is voor de eerste groepering. \nkolommen_zwolle = list(df_aantal_zwolle)\n\n#Sectoren haalt alle unieke waardes uit het dataframe voor de kolom SECTORUNIT SBB.\nlocaties_zwolle = list(df_locatie_zwolle[\'Locatie\'].unique())\n\n#Voeg per kolom alle sectoren toe. Dit wordt dan de data voor de x-as. \n#Kolom is bijvoorbeeld 2013 AANT en hier worden vervolgens alle sectoren aan toegevoegd.\nx = [ (kolom, locatie) for kolom in kolommen_zwolle for locatie in locaties_zwolle ]\n\n#Data voor de y-as\n#Data doorlopen om de totalen op de juiste volgorde in een array te plaatsen.\n#Zelfde volgorde zoals hierboven staat voor de gegevens voor de x-as\n\ncounts = []\n\nfor kolom in kolommen_zwolle:\n    for locatie in locaties_zwolle:\n        counts.append(df_aantal_zwolle.loc[locatie][kolom])\n\n#Teken grafiek\nsource = ColumnDataSource(data=dict(x=x, counts=counts))\n\naz = figure(x_range=FactorRange(*x), plot_height=400, title="Aantal MBO Afgestudeerden Regio Zwolle")\n\naz.vbar(x=\'x\', top=\'counts\', width=0.8, source=source)\n\naz.width=900\naz.y_range.start = 0\naz.x_range.range_padding = 0.1\naz.xaxis.major_label_orientation = 1\naz.xgrid.grid_line_color = None\n\n\n#update functie om nieuwe data(selecties) weer te geven, doet nu niets...\n#def update():\n\n#callback om updates elke 100ms op te halen\n#doc.add_periodic_callback(update, 100)\n\nshow(az)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "img[250-10:250+10, 300:400] = (0,0,1)\nimg[200:300, 350-10:350+10] = (0,0,1)\nplt.imshow(img[200:400, 300:500], interpolation='bilinear')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "from ipywidgets import interact\nfrom kf_book.book_plots import IntSlider, FloatSlider\n\ndef plot_FPFT(F00, F01, F10, F11, covar):   \n    plt.figure()\n    dt = 1.\n    x = np.array((0, 0.))\n    P = np.array(((1, covar), (covar, 2)))\n    F = np.array(((F00, F01), (F10, F11)))\n    plot_covariance_ellipse(x, P)\n    plot_covariance_ellipse(x, F @ P @ F.T, ec='r')\n    plt.gca().set_aspect('equal')\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n    plt.xlabel('position')\n    plt.ylabel('velocity')\n    plt.show()\n                 \ninteract(plot_FPFT, \n         F00=IntSlider(value=1, min=0, max=2), \n         F01=FloatSlider(value=1, min=0, max=2, description='F01(dt)'),\n         F10=FloatSlider(value=0, min=0, max=2),\n         F11=FloatSlider(value=1, min=0, max=2),\n         covar=FloatSlider(value=0, min=0, max=1));```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "# Students should write code here\ndistances = pairwise_distances(tf_idf, tf_idf[0:3], metric='euclidean')\ncluster_assignment = np.argmin(distances, axis=1)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "boston_females = boston_clean.loc[boston_clean['F'] == 1]\nboston_females```"}, {'reason': 'stop', 'result': '#Animal types\np = Bar(data, label=\'OutcomeType\', values=\'AnimalType\', agg=\'count\', stack=\'AnimalType\',\n        title="Outcomes by Animal Type", legend=\'top_right\')\nshow(p)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Integrand function\nH = 0\ndef f(x, H):\n    return (x - 5) * np.exp(-(x / 2 - 3)) + H\n\nx = np.arange(2, 10, 0.1)\nplt.plot(x, f(x, H), 'b')\nplt.show()\n```\n\nUnused variables:\n- `P`\n- `plotly`\n- `py`\n- `ff`"}, {'reason': 'stop', 'result': 'learn = Learner(data, simple_cnn((3,16,16,2)), metrics=accuracy)\nfit_odd_shedule(learn, 1e-3)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'import pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv(\'data/exoplanet.eu_catalog.csv\', \n                      usecols=[\'mass\',\'mass_error_min\',\'mass_error_max\',\n                               \'semi_major_axis\',\'semi_major_axis_error_min\',\'semi_major_axis_error_max\',\'star_name\'])\n\nclass System:\n    def __init__(self, data):\n        self.data=data\n        self.system = list(self.data.groupby("star_name").groups.keys())\n        self.Number()\n        self.Mass()\n        self.CenterOfMass()\n        \n    def Number(self):\n        sys = self.data.groupby("star_name")\n        self.N_total = len(sys["mass"])\n        \n    def Mass(self):\n        sys = self.data.groupby("star_name")\n        self.M_total = sys["mass"].sum()\n    \n    def CenterOfMass(self):\n        self.rm_i = self.data["mass"].multiply(self.data["semi_major_axis"])\n        self.data_i = self.data.assign(CM_i = self.rm_i.values) \n        p_system = self.data_i.groupby("star_name")\n        sum_rm = p_system[\'CM_i\'].sum()\n        self.CM = sum_rm.divide(self.M_total)'}, {'reason': 'stop', 'result': 'twitter_archive_clean.info()'}, {'reason': 'stop', 'result': 'P = np.diag([500., 49.])\nMs, Ps = run(count=50, R=10, Q=0.01)```\n\nUnused variables:\n- P'}, {'reason': 'stop', 'result': 'np.column_stack(([1,2], [3,4], [5,6]))```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '# Lista que armazenará as acurácias de teste durante o treinamento.\ntest_acc = []\n\n# Variável utilizada para manter o controle de qual a maior acurácia\n# encontrada até então.\nmax_acc = 0\n\n# Inicializamos a Session do Tensorflow\nwith tf.Session() as sess:\n    \n    # Chamamos a função que efetivamente inicializa as variáveis criadas.\n    sess.run(tf.global_variables_initializer())\n    \n    # Vamos treinar a rede por 100 épocas\n    for epoch in range(100):\n        # Cada época é composta de 200 batches de tamanho 100.\n        for i in range(200):\n            \n            # Armazena o batch a ser processado no momento.\n            batch = next_batch(100, i)\n            \n            # Atualiza os pesos da rede.\n            train_step.run(feed_dict={x: batch[0], y: batch[1]})\n            \n            # Mede a acurácia da rede atual e a armazena na lista do\n            # histórico.\n            test_acc.append(test_accuracy())\n            print("Ep {}: Batch #{} - Acc: {}%".format(epoch, i, 100*test_acc[-1]))\n            \n            # Caso a acurácia atual seja maior que a previamente maior,\n            # salvamos os pesos atuais e atualizamos o valor de max_acc.\n            if test_acc[-1] > max_acc:\n                max_acc = test_acc[-1]\n                save_path = saver.save(sess, "./weights/model_e{}b{}_{}.ckpt".format(epoch, i, time.time()))\n                print("Model saved in file: %s" % save_path)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'X[0,:],T[0,:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'l = [1,2,3,4]\n\nprint(type(l))\nprint(l)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'rmean = numpy.mean(rho)\nvmean = (vmax*(1-rmean/rhomax))*(5/18)\nprint(\'The Average velocity at t=3 minutes is\',"%.2f" %vmean,\'m/s\')```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'uprime = u.diff(x)\nuprime```'}, {'reason': 'stop', 'result': "##  for each row calculate Hill N10\nfor i in range(len(data)):\n    swi = 0.0\n    for x in range(len(data.iloc[0]))[4:last]:\n        if data.iloc[i][x] > 0:\n            swi += -(data.iloc[i][x]/sum(data.iloc[i][4:last][data.iloc[i][4:last]>0])) * math.log((data.iloc[i][x]/sum(data.iloc[i][4:last][data.iloc[i][4:last]>0])))\n    data.loc[i,'N10'] = math.exp(swi)/len(data.iloc[i][4:last][data.iloc[i][4:last]>0])```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "accuracies = []\n\nch = chance[0, :, -1].mean(axis=-1)\nda = deep_all[key][:, -1] / ch\nla = linear_all[key][:, -1] / ch\n\nfor ii in range(4):\n    for jj in range(10):\n        accuracies.append({'subject':ii, 'accuracy':da[ii, jj],\n                           'complexity': 3, 'model': 2})\n        accuracies.append({'subject':ii, 'accuracy':la[ii, jj],\n                           'complexity': 3, 'model': 1})\n    \nch = chance[1, :, -1].mean(axis=-1)\nda = other_deep_accuracy['c'][:, -1] / ch\nla = other_linear_accuracy['c'][:, -1] / ch\nfor ii in range(4):\n    for jj in range(10):\n        accuracies.append({'subject':ii, 'accuracy':da[ii, jj],\n                           'complexity': 2, 'model': 2})\n        accuracies.append({'subject':ii, 'accuracy':la[ii, jj],\n                           'complexity': 2, 'model': 1})\n\nfor ii, t in enumerate(['v', 'p', 'm']):\n    ch = chance[ii+2, :, -1].mean(axis=-1)\n    da = other_deep_accuracy[t][:, -1] / ch\n    la = other_linear_accuracy[t][:, -1] / ch\n    for jj in range(10):\n        accuracies.append({'subject':ii, 'accuracy':da[ii, jj],\n                           'complexity': 1, 'model': 2})\n        accuracies.append({'subject':ii, 'accuracy':la[ii, jj],\n                           'complexity': 1, 'model': 1})\ndf = pd.DataFrame.from_dict(accuracies)```\n\nUnused variables:\n- `ii` (inside the third nested loop)"}, {'reason': 'stop', 'result': 'sparsity = sparse_codes(Z)'}, {'reason': 'stop', 'result': 'plt.plot(np.arange(21955, 23184), a2_x[21955:23184])\nplt.grid()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with model:\n    pm.Bernoulli("second_flips", 0.5, shape=N, testval=np.random.binomial(1, 0.5, N))```\n\nUnused variables:\n- `second_coin_flips`'}, {'reason': 'stop', 'result': '```img[...] = 0\ncurve1 = y<np.floor(500-0.001*(x-400)**2)\ncurve2 = y>np.floor(400-0.0005*(x-400)**2)\n\nimg[curve1 & curve2] = (1,0,0)\n\nplt.imshow(img, interpolation="bilinear")```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "5 / 'five'```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'sessions.columns'}, {'reason': 'stop', 'result': 'test_images_np_gray = normalize_and_grayscale(test_images_np)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# escogemos aleatoriamente una ilera en los datos\nilera_al = rand(1:150,1)\n\n# extraemos los datos de esta ilera en una variable nueva\nx_muestra = x[ilera_al,:]\n\n# calculamos el producto interno de esta muestra con cada uno de los vectores promedio\nprod_1 = x_muestra*prom_1\'\nprod_2 = x_muestra*prom_2\'\nprod_3 = x_muestra*prom_3\'\n\n# imprimimos los resultados\nprint("el producto interno del vector muestra con de los vectores promedio correspondientes a las especies de plantas son: \\n")\nprint("\\n setosa ", prod_1)\nprint("\\n versicolor ", prod_2)\nprint("\\n virginica ", prod_3)\nprint("\\n\\ny la especie correcta del vector muestra es: \\n")\nprint("\\n ", especies[ilera_al])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "```python\ndef get_name_ext(timestr, beta, learning_rate, learn_decay, separator = '_'):\n    return timestr + separator + str(beta) + separator + str(learning_rate) + separator + str(learn_decay)\n\ndef combine_acc_lists(beta, lr, ld, acc_list_temp, acc_list):\n    length_acc_list_temp = len(acc_list_temp)\n    beta_list = [beta] * length_acc_list_temp\n    lr_list = [lr] * length_acc_list_temp\n    ld_list = [ld] * length_acc_list_temp\n    t = np.vstack((np.asarray(acc_list_temp),beta_list,lr_list,ld_list))\n    \n    if(acc_list is None):\n        acc_list = t\n    else:\n        acc_list = np.concatenate((acc_list, t), axis = 1)\n    \n    return acc_list\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# Create Dataset\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=500, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)'}, {'reason': 'stop', 'result': '# length of the string: number of characters in string\nlen(s)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def plotImg(arr, **kwargs):\n    \n    avg = mean(arr.flatten())\n    std = stdev(arr.flatten())\n    \n    fig,ax = plt.subplots(1)\n    pl = plt.imshow(arr, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', \\\n                    vmin=np.percentile(arr, 10), vmax=np.percentile(arr, 90))\n    if \'window\' in kwargs:\n        window = kwargs.get(\'window\')\n        rect = patches.Rectangle(window[0], window[1][1], window[1][0], edgecolor=\'r\', facecolor=\'none\')\n        ax.add_patch(rect)\n    plt.colorbar().set_label(\'Detector Value (ADU)\')\n    plt.xlabel(\'pixels(x)\')\n    plt.ylabel(\'pixels(y)\')\n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'selection_window.png\')\n    plt.show()\n\ndef plotAll(array2D, **kwargs):\n    """\n    Input:  \'array2D\': 2D image array\n    Output: show plot of histogram and image\n    """\n    \n    arr = array2D.flatten()\n    \n    avg = mean(arr)\n    std = stdev(arr)\n    \n    sigma = kwargs.get(\'sigma\', 2)\n    low = int(np.round((avg-sigma*std)))\n    high = int(np.round((avg+sigma*std)))\n    rng = kwargs.get(\'rng\', [low, high])\n    exp = kwargs.get(\'exp\')\n    if \'nbins\' in kwargs:\n        nbins = kwargs.get(\'nbins\')\n        bin_size = (rng[1]-rng[0])/nbins\n    else:\n        bin_size = kwargs.get(\'bin_size\', 1)\n    \n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=[18,6])\n    \n    # Histogram\n    #===========\n    hr = np.arange(rng[0], rng[1]+1, bin_size)\n    hist = []\n    for i in range(len(hr)):\n        try:\n            counts = len(np.where((arr >= hr[i]) & (arr < hr[i+1]))[0])\n        except:\n            counts = 0\n        hist.append(counts)\n    ax1.step(hr, hist, color=\'k\')\n\n    #mean and median lines\n    ax1.axvline(avg, color=\'b\', label=r\'$\\bar{x}=%s$\'%(np.round(avg,2)))\n#     ax1.axvline(med, color=\'b\', label=r\'$\\tilde{x}=%s$\'%(np.round(med,2)), linestyle=\'dashed\')\n    \n    #sigma levels\n    if kwargs.get(\'show_level\', True) == True:\n        for i in np.arange(1,sigma+1):\n            if i == 1:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05, label=r\'$s=\\pm %s$\'%(np.round(std,2)))\n            else:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05)\n                \n                \n    #poisson distribution\n    xarray = np.arange(rng[0]-10, rng[1]+10, 1)\n    pdist = poisson_approx(xarray, avg)\n    pdist = max(hist)/max(pdist)*pdist\n    ax1.plot(xarray, pdist, color=\'r\', label=r\'$P_{Poisson}(\\bar{x})$\')\n    std_expected = math.sqrt(avg)\n    ax1.axvspan(avg - std_expected, avg + std_expected, facecolor=\'r\', alpha=0.05, \\\n                label=r\'$\\sigma=\\pm %s$\'%(np.round(std_expected,2)))\n    \n    #gaussian distribution\n    gdist = gaussian(xarray, avg, std)\n    gdist = max(hist)/max(gdist)*gdist\n    ax1.plot(xarray, gdist, color=\'c\', label=r\'$P_{Gaussian}(\\bar{x}, s)$\')\n    \n    ax1.legend(loc=\'upper left\')\n    ax1.set_xlabel(\'Counts (ADU)\')\n    ax1.set_ylabel(\'Frequency\')\n    \n    if \'exp\' in kwargs:\n        ax1.set_title(\'Combined Histogram (Exposure Time: %s sec)\'%(exp))\n    ax1.set_xlim(rng)\n    \n    # Image\n    #===========\n    hrng = kwargs.get(\'hrng\', [np.percentile(arr, 10), np.percentile(arr, 90)])\n    pl = ax2.imshow(array2D, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', vmin=hrng[0], vmax=hrng[1])\n    fig.colorbar(pl, ax=ax2, fraction=0.046, pad=0.04).set_label(\'Detector Value (ADU)\')\n    \n    ax2.set_xlabel(\'pixels(x)\')\n    ax2.set_ylabel(\'pixels(y)\')\n    ax2.set_title(\'Combined Image\')\n    \n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'exposure%s.png\'%(exp))\n    plt.show()```\n\nUnused variables:\n- std\n- med\n- Npix'}, {'reason': 'stop', 'result': '#ignore\n# mask = tf.squeeze(inp_mask, axis=1)\n# draw = True\noutput, attention_weights = scaled_dot_product_attention_demo(q, k, v, mask, draw=draw)\nprint("output:", output)\nprint("-" * 20)\nprint("attention_weights:", attention_weights)```\n\nUnused variables:\n- mask\n- draw'}, {'reason': 'stop', 'result': 'data = [len(s) for c in translate_conv for s in c]\n\nplt.violinplot(data, showmedians=True, showextrema=False)\nplt.ylabel("length of sentences")\nplt.title("ViolinPlot (Showing density of tokenized sentences length)")\nplt.show()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "def plot_k_vs_heterogeneity(k_values, heterogeneity_values):\n    plt.figure(figsize=(7,4))\n    plt.plot(k_values, heterogeneity_values, linewidth=4)\n    plt.xlabel('K')\n    plt.ylabel('Heterogeneity')\n    plt.title('K vs. Heterogeneity')\n    plt.rcParams.update({'font.size': 16})\n    plt.tight_layout()\n\nfilename = 'kmeans-arrays.npz'\n\nheterogeneity_values = []\nk_list = [2, 10, 25, 50, 100]\n\nif os.path.exists(filename):\n    arrays = np.load(filename)\n    centroids = {}\n    cluster_assignment = {}\n    for k in k_list:\n        print k\n        sys.stdout.flush()\n        score = compute_heterogeneity(tf_idf, k, arrays['centroids_{0:d}'.format(k)], arrays['cluster_assignment_{0:d}'.format(k)])\n        heterogeneity_values.append(score)\n    \n    plot_k_vs_heterogeneity(k_list, heterogeneity_values)\n\nelse:\n    print('File not found. Skipping.')```\n\nUnused variables:\n- centroids\n- cluster_assignment"}, {'reason': 'stop', 'result': 'import time\ntuning_job_name = "forecast-tuning-{}".format(datetime.now().strftime("%d%H%M"))\n\nsmclient = boto3.client(\'sagemaker\')\nsmclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name,\n                                           HyperParameterTuningJobConfig=tuning_job_config,\n                                           TrainingJobDefinition=training_job_definition)\nstatus = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)[\'HyperParameterTuningJobStatus\']\nprint(status)\nwhile status != \'Completed\' and status != \'Failed\':\n    time.sleep(60)\n    status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)[\'HyperParameterTuningJobStatus\']\n    print(status)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "unicode_string.encode('utf32')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "unique_countries = ['Croatia', 'Argentina', 'Canada', 'Mexico', 'Italy', 'South Korea', 'USA', 'Morocco', 'Malta', 'Finland']```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# convert a string to a list by type casting:\n\nprint(s)\n\ns2 = list(s)```\n\nUnused variables:\n- s2'}, {'reason': 'stop', 'result': 'import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom pybbn.generator.bbngenerator import generate_multi_bbn, convert_for_exact_inference\nfrom pybbn.generator.bbngenerator import convert_for_drawing\nfrom pybbn.pptc.inferencecontroller import InferenceController\n\nnp.random.seed(37)\n\ng, p = generate_multi_bbn(5, max_iter=5)\nm_bbn = convert_for_exact_inference(g, p)\nnx_multi_bbn = convert_for_drawing(m_bbn)\n\ng, p = generate_singly_bbn(5, max_iter=10)\ns_bbn = convert_for_exact_inference(g, p)\nnx_singly_bbn = convert_for_drawing(s_bbn)```\n\nUnused variables:\n- `nx` (unused import)\n- `plt` (unused import)\n- `warnings` (unused import)'}, {'reason': 'stop', 'result': "def kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):\n    heterogeneity = {}\n    \n    min_heterogeneity_achieved = float('inf')\n    best_seed = None\n    final_centroids = None\n    final_cluster_assignment = None\n    \n    for i in xrange(num_runs):\n        \n        # Use UTC time if no seeds are provided \n        if seed_list is not None: \n            seed = seed_list[i]\n            np.random.seed(seed)\n        else: \n            seed = int(time.time())\n            np.random.seed(seed)\n        \n        # Use k-means++ initialization\n        initial_centroids = get_initial_centroids(data, k, seed=0)\n        \n        # Run k-means\n        centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter=400,\n                                       record_heterogeneity=None, verbose=True)\n        \n        # To save time, compute heterogeneity only once in the end\n        heterogeneity[seed] = compute_heterogeneity(data, k, centroids, cluster_assignment)\n        \n        if verbose:\n            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n            sys.stdout.flush()\n        \n        # if current measurement of heterogeneity is lower than previously seen,\n        # update the minimum record of heterogeneity.\n        if heterogeneity[seed] < min_heterogeneity_achieved:\n            min_heterogeneity_achieved = heterogeneity[seed]\n            best_seed = seed\n            final_centroids = centroids\n            final_cluster_assignment = cluster_assignment\n    \n    # Return the centroids and cluster assignments that minimize heterogeneity.\n    return final_centroids, final_cluster_assignment\n```\n\nUnused variables:\n- `heterogeneity` (not used after initialization)\n- `min_heterogeneity_achieved` (not used after initialization)\n- `best_seed` (not used after initialization)"}, {'reason': 'stop', 'result': 'from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms_test = sqrt(mean_squared_error(y_test, y_test_predict))\nrms_test\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "new_pc = eigvecs[:,-2:]\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = new_pc - new_pc.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()```\n\nUnused variables:\n- pca11\n- pca12\n- pca21\n- pca22"}, {'reason': 'stop', 'result': "def comparePlots(elems, years):\n    '''\n    ta\n    :param elems: list of dataframes that we  want to compare\n    :param years: list of years that we want to anlayze\n    :return: None (prompts the plot)\n    '''\n    for year in years:\n        for elem in elems:\n            plt.plot(x, y2/y2.max())\n            plt.plot(x,y/y.max())```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'sys.getsizeof(A)'}, {'reason': 'stop', 'result': "##  for each row calculate Hill N2\nfor i in range(len(data)):\n    N = 0.0\n    for x in range(4, last):\n        if data.iloc[i][x] > 0:\n            N += data.iloc[i][x]\n\n    lam = 0.0\n    for x in range(4, last):\n        lam += (data.iloc[i][x]/N) * (data.iloc[i][x]/N)\n    data.loc[i,'N2'] = 1/lam```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'port_broad_images_gt_50m = broad_greater_than_50_meters_port()```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '## transpose matrix\nprint(A.T)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "for iteration in itervars:\n    print(eval_grads(process_weights(iteration['weights'])))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# split out the test and train sets again\ntrain = data_full.ix['train']\ntest = data_full.ix['test']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'import scipy.stats as stats\n\nnor = stats.norm\nx = np.linspace(-8, 7, 150)\nmu = (-2, 0, 3)\ntau = (.7, 1, 2.8)\ncolors = ["#348ABD", "#A60628", "#7A68A6"]\n\nfor _mu, _tau, _color in zip(mu, tau, colors):\n    plt.plot(x, nor.pdf(x, _mu, scale=1./_tau),\n             label="$\\mu = %d,\\;\\\\tau = %.1f$" % (_mu, _tau), color=_color)\n    plt.fill_between(x, nor.pdf(x, _mu, scale=1./_tau), color=_color,\n                     alpha=.33)\n\nplt.legend(loc="upper right")\nplt.xlabel("$x$")\nplt.ylabel("density function at $x$")\nplt.title("Probability distribution of three different Normal random \\\nvariables")'}, {'reason': 'stop', 'result': '# Run GO enrichment analysis for each community\ncomm_enrich_scores = []\nfor community in communities:\n    subset = [gene[0] for gene in membership if gene[1] == community]\n\n    # Exclude communities smaller than 5:\n    if len(subset) > 5:\n        goea_results = goeaobj.run_study(subset);\n        sig_results = [result for result in goea_results if result.p_fdr_bh < 0.05]\n\n        enriched_mem_ratios = []\n        for res in sig_results:\n            enriched_mem_ratios.append(res.ratio_in_study[0] / res.ratio_in_study[1])\n\n        if len(enriched_mem_ratios):\n            avg_enriched_ratio = sum(enriched_mem_ratios) / len(enriched_mem_ratios)\n            comm_enrich_scores.append([community, avg_enriched_ratio])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '#Setup Axis\nstate=df_county_data[\'State\']\ngrad_rate = df_county_data["Graduation Rate"]\nx_axis = np.arange(len(state))\n\n# Create a bar chart based upon the above data\nplt.bar(x_axis, grad_rate, color="b", align="center")\nplt.title("County Graduation Rates")\nplt.xlabel("Counties")\nplt.ylabel("Graduation Rates")\nplt.text(140, 0.6, "Note:\\nGraduation Rates for all counties in NJ, NY, & PA.")\n\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Graduation_Rates.png", bbox_inches = "tight")\nplt.show()\n```\n\nUnused variables:\n- county\n- tick_locations'}, {'reason': 'stop', 'result': "# Split the data into features and target label\nincome_raw = data['income']\nfeatures_raw = data.drop('income', axis = 1)\n\n# Visualize skewed continuous features of original data\nvs.distribution(data)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'from aipython.searchBranchAndBound import DF_branch_and_bound\n\ns_dfbb = DF_branch_and_bound(problem=search_simple1)\n\n# Visualization options\ns_dfbb.sleep_time = 0.2 # The time, in seconds, between each step in auto solving\ns_dfbb.line_width = 4.0 # The thickness of edges\ns_dfbb.text_size = 15 # The fontsize of the text\ns_dfbb.detail_level = 1 # 0=no text, 1=truncated text, 2=full text\ns_dfbb.show_edge_costs = True\ns_dfbb.show_node_heuristics = True\n\n# Display the widget\ndisplay(s_dfbb)\ns_dfbb.search()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# Common imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# setting random seed\nnp.random.seed(42)\n\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nplt.rcParams[\'axes.labelsize\'] = 14\nplt.rcParams[\'xtick.labelsize\'] = 12\nplt.rcParams[\'ytick.labelsize\'] = 12\n\n\nCHAPTER_ID = "ensembles"\n\ndef image_path(fig_id):\n    \n    if os.path.exists(os.path.join("images", CHAPTER_ID)):\n        return os.path.join("images", CHAPTER_ID, fig_id)\n    os.mkdir(os.path.join("images", CHAPTER_ID))\n    return os.path.join("images", CHAPTER_ID, fig_id)\n\ndef save_fig(fig_id, tight_layout=True):\n    print("Saving figure", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(image_path(fig_id) + ".png", format=\'png\', dpi=300)```\n\nUnused variables:\n- `matplotlib.pyplot`'}, {'reason': 'stop', 'result': "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport IPython\nfrom scipy.stats import norm\nfrom sys import version \nimport multiprocessing\n\nprint ' Reproducibility conditions for this notebook '.center(90,'-')\nprint 'Python version:     ' + version\nprint 'Numpy version:      ' + np.__version__\nprint 'IPython version:    ' + IPython.__version__\nprint 'Multiprocessing:    ' + multiprocessing.__version__\nprint '-'*90\n```\n\nUnused variables:\n- ABCMeta\n- abstractmethod\n- ceil\n- mean\n- time\n- os"}, {'reason': 'stop', 'result': 'plt.imshow(black_array)\nplt.show()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "try:\n    university_canton_dict = json.loads(open('university_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for universities has not been saved yet. Let''s create a new dictionary.')\n    university_canton_dict = {}\n    \ntry:\n    institution_canton_dict = json.loads(open('institution_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for institutions has not been saved yet. Let''s create a new dictionary.')\n    institution_canton_dict = {}\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n\n    gray = grayscale(image)\n\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray,kernel_size)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges,vertices)\n \n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    line_image = np.copy(image)*0 # creating a blank to draw lines on\n\n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap,vertices)\n\n    # Draw the lines on the edge image\n    result = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n    \n    return result\n```\n\nUnused variables:\n- `line_image`'}, {'reason': 'stop', 'result': "fig, ax = plt.subplots(figsize=(8, 6))\n\nax.hist(sunspot_df['sunspot.year'].values, bins=40, normed=True, lw=0, alpha=0.75);\n\nax.fill_between(x_plot, post_pmf_low, post_pmf_high,\n                 color='gray', alpha=0.45)\nax.plot(x_plot, post_pmfs.mean(axis=0),\n        c='k', label='Posterior expected density');\n\nax.set_xlabel('Yearly sunspot count');\nax.set_yticklabels([]);\nax.legend(loc=1);```\n\nUnused variables:\n- `post_pmfs[0]`\n- `post_pmfs[::200].T`"}, {'reason': 'stop', 'result': "passageirosMedia = passageiros['PassagemPreco'].mean()\npassageirosMedia```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "### Coletando as fontes de dados\ntrain_csv = pd.read_csv('train.csv', sep = ',', encoding = 'UTF-8')\ntest_csv = pd.read_csv('test.csv', sep = ',', encoding = 'UTF-8')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'def runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n    l1 = LogisticRegression(penalty=\'l1\', tol=.01, \n                            solver="liblinear", multi_class="ovr",\n                            max_iter=500, C=c_param)\n    # Fit model\n    l1.fit(train_data, train_labels) \n    # Predict\n    predict = l1.predict(dev_data)\n    # Get precision, recall, f1 scores\n    scores = precision_recall_fscore_support(dev_labels, predict, average=\'weighted\')  \n    \n    # Get the features with non-zero coefficients.  We will use\n    # this list to reduce the features for the\n    # following logistic regression with L2 regularization\n    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n    non_zero_names = np.array(list(train_data.columns))[non_zero_sums] \n    \n    return {\'scores\': scores, \'non_zero_genes\': non_zero_names}```\n\nUnused variables:\n- `names`'}, {'reason': 'stop', 'result': '# precision-recall curves\n\n# msr\ny_scores = (-get_acc_net_msr(y_pred_te)).flatten()\nprecision_msr, recall_msr, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_msr = metrics.average_precision_score(y_true, y_scores)\nauroc_msr = metrics.roc_auc_score(y_true, y_scores)\nfpr_msr, tpr_msr, _ = metrics.roc_curve(y_true, y_scores)\n\n# margin\ny_scores = (-get_acc_net_max_margin(y_pred_te)).flatten()\nprecision_margin, recall_margin, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_margin = metrics.average_precision_score(y_true, y_scores)\nauroc_margin = metrics.roc_auc_score(y_true, y_scores)\nfpr_margin, tpr_margin, _ = metrics.roc_curve(y_true, y_scores)\n\n# entropy\ny_scores = (-get_acc_net_entropy(y_pred_te)).flatten()\nprecision_entropy, recall_entropy, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_entropy = metrics.average_precision_score(y_true, y_scores)\nauroc_entropy = metrics.roc_auc_score(y_true, y_scores)\nfpr_entropy, tpr_entropy, _ = metrics.roc_curve(y_true, y_scores)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'E01'}, {'reason': 'stop', 'result': "### Load in necessary libraries for data input and normalization\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%load_ext autoreload\n%autoreload 2\n\nfrom my_answers import *\n\n### load in and normalize the dataset\ndataset = np.loadtxt('datasets/normalized_apple_prices.csv')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "# Plot Resolution function\ngROOT.ProcessLine('TF1* resolution = sim0->getResolutionFunction()')\ngROOT.ProcessLine('resolution->Draw()')```\n\nUnused variables:\n- `c2`"}, {'reason': 'stop', 'result': 'fig = plt.figure(figsize=(8,6)) \nx = df_county_data["Speak a language other than English"]\ny = df_county_data["Graduation Rate"] \nplt.scatter(x, y, color="g", marker="o", alpha=0.9) \n\n#Calculate and add R2 value\nmask = ~np.isnan(x) & ~np.isnan(y)\n\n#Add regression line\nsns.regplot(df_county_data["Speak a language other than English"], \n              df_county_data["Graduation Rate"], color=\'r\',label = "Speak a language other than English" )\n\n# Incorporate the other graph properties\nplt.title("High School Graduation Rates and ESL by County")\nplt.ylabel("Graduation Rate")\nplt.xlabel("Speak a language other than English") \n\nplt.legend(loc=\'best\')\nplt.grid(True)\nsns.set_style(\'whitegrid\')\nplt.text(65, 0.925, "Note:\\nAreas with one or more Foreign languages beside English language \\ntend to have a Lower graduation rate.")\n\nplt.savefig("Images/County_Grad_Speak a language other than English3.png", bbox_inches = "tight")\nplt.show()```\n\nUnused variables:\n- mask'}, {'reason': 'stop', 'result': 'logits = LeNetTrafficSign(x, n_classes)\nprint(logits)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)```\n\nUnused variables:\n- `training_operation`'}, {'reason': 'stop', 'result': '%matplotlib inline\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'SEED = 5132290 # from random.org\n\nnp.random.seed(SEED)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "canton_shortname_series = pd.Series(canton_shortname_table, name='Canton Shortname')\ncanton_shortname_series.size\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'if le is not None:\n    dictenc(ae.E, enc=True)\n    atoms(ae.E)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'print( np.linalg.norm(np.eye(len(b)) - np.linalg.inv(np.triu(A)) @ A) )```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'plt.imshow(port_broad_images_gt_50m[8].reshape(255, 255, 3))\nplt.show()```'}, {'reason': 'stop', 'result': "#def plot_k_vs_heterogeneity(k_values, heterogeneity_values):\n#    plt.figure(figsize=(7,4))\n#    plt.plot(k_values, heterogeneity_values, linewidth=4)\n#    plt.xlabel('K')\n#    plt.ylabel('Heterogeneity')\n#    plt.title('K vs. Heterogeneity')\n#    plt.rcParams.update({'font.size': 16})\n#    plt.tight_layout()\n\n#start = time.time()\n#centroids = {}\n#cluster_assignment = {}\nheterogeneity_values = []\nk_list = [2, 10, 25, 50, 100]\nseed_list = [0, 20000, 40000, 60000, 80000, 100000, 120000]\n\nfor k in k_list:\n    heterogeneity = []\n    centroids[k], cluster_assignment[k] = kmeans_multiple_runs(tf_idf, k, maxiter=400,\n                                                               num_runs=len(seed_list),\n                                                               seed_list=seed_list,\n                                                               verbose=True)\n    score = compute_heterogeneity(tf_idf, k, centroids[k], cluster_assignment[k])\n    heterogeneity_values.append(score)\n\nplot_k_vs_heterogeneity(k_list, heterogeneity_values)\n\n#end = time.time()\n#print(end-start)```"}, {'reason': 'stop', 'result': '# plt.style.use("ggplot")```\n\nUnused variables:\n- `plt.style.use("seaborn-whitegrid")`\n- `plt.style.use("fivethirtyeight")`'}, {'reason': 'stop', 'result': 'learning_rate = 0.01\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    optimizer.minimize(loss)```\n\nUnused variables:\n- training_op'}, {'reason': 'stop', 'result': "# define model\nmodel = Sequential()\nmodel.add(LSTM(n_neurons, batch_input_shape=(batch_size, window_size, 1), stateful=stateful, return_sequences=True))\nmodel.add(LSTM(n_neurons, stateful=stateful, return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()```\n\nUnused variables:\n- `model.add(Masking(mask_value=-1, input_shape=(1,)))`\n- `model.add(LSTM(n_neurons, input_shape=(msig.window_size, 1), return_sequences=True))`\n- `model.add(LSTM(n_neurons, input_shape=self.input_shape, return_sequences=False, dropout=0.5))`\n- `model.add(LSTM(n_neurons, input_shape=(2, 1)))`\n- `model.add(LSTM(n_neurons, input_shape=(window_size, 2)))`\n- `model.add(LSTM(n_neurons, batch_input_shape=(window_size, 1, 1), stateful=stateful, return_sequences=True))`\n- `model.add(LSTM(n_neurons, batch_input_shape=(window_size, 1, 1), stateful=stateful))`\n- `model.add(LSTM(n_neurons))`\n- `for _ in range(n_layers):`\n- `    model.add(LSTM(n_neurons, stateful=stateful, return_sequences=True))`\n- `model.add(LSTM(n_neurons, stateful=stateful))`\n- `model.add(Dense(n_classes, activation='softmax'))`\n- `model.add(Dropout(0.5))`\n- `model.add(Reshape((n_neurons, n_classes, 1)))`"}, {'reason': 'stop', 'result': "평점['신과함께']```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "import datalab.bigquery as bq\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "# de-duplicate rows with identical neighborhood, city, and state\ndf_ggl = df_ggl.drop_duplicates(subset=['neighborhood', 'city', 'state'], keep='first')\nprint(len(df_ggl))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "plt.figure(figsize=(14, 8))\nsns.countplot(data=df, x='Country', hue='Exercise', palette='Paired', order=df['Country'].value_counts()[:10].index)\nsns.despine(left=True)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "import cvxopt\n\ndef compute_multipliers(X, t):\n    N = X.shape[0]\n    K = computeK(t.reshape(-1,1) * X)\n    \n    P = cvxopt.matrix(K)\n    q = cvxopt.matrix(-np.ones(N))\n    A = cvxopt.matrix(t).T\n    b = cvxopt.matrix(0.0)\n\n    sol = cvxopt.solvers.qp(P, q, A=A, b=b)\n    a = np.array(sol['x'])\n\n    return a\n```\n\nUnused variables:\n- G\n- h"}, {'reason': 'stop', 'result': 'V_iso = IsotypicComponent(V, n)\nfor value in V_iso.values():\n    show(value.basis())```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "```python\ntrain_acc_list_ = 'train_acc_list_.p'\nvalid_acc_list_ = 'valid_acc_list_.p'\n\nwith open(train_acc_list_, mode='rb') as f:\n    pickle.load(f)\nwith open(valid_acc_list_, mode='rb') as f:\n    pickle.load(f)\n\nplt.plot(train_acc_list_)\nplt.plot(valid_acc_list_)\nplt.title('accuracy for training and validation set by epoch')\n```\n\nUnused variables:\n- `train_acc_list_`\n- `valid_acc_list_`"}, {'reason': 'stop', 'result': 'coco.get_fit_params()'}, {'reason': 'stop', 'result': 'url = \'http://mattmahoney.net/dc/\'\n\ndef maybe_download(filename, expected_bytes):\n  """Download a file if not present, and make sure it\'s the right size."""\n  if not os.path.exists(filename):\n    filename, _ = urlretrieve(url + filename, filename)\n  statinfo = os.stat(filename)\n  if statinfo.st_size == expected_bytes:\n    print(\'Found and verified %s\' % filename)\n  else:\n    print(statinfo.st_size)\n    raise Exception(\n      \'Failed to verify \' + filename + \'. Can you get to it with a browser?\')\n  return filename\n\nmaybe_download(\'text8.zip\', 31344016)'}, {'reason': 'stop', 'result': 'rspEwidth=0.5\nrspEmax=40\nrspLwidth=0.03\nrspLmax=40.0```\n\nUnused variables:\n- rspEmin\n- rspLmin'}, {'reason': 'stop', 'result': 'def get_neighbour_messages(sender, receiver):\n    messages = []\n    for n in sender.neighbours:\n        if n is not receiver:\n            messages.append(sender.in_msgs[n])\n    return messages\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'reset_graph()\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "import numpy as np\n\nC = np.array([C])\nprint(C)\nprint('transpose\\n',C.transpose())\nprint('A more compact way to transpose where T is an alias to the transpose function')\nprint(C.T)\nC = np.array([6,8])\nprint(np.matmul(C,C.T))```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "challenge_output = 'extra.mp4'\nclip2 = VideoFileClip('challenge.mp4')\nchallenge_clip = clip2.fl_image(process_image)\n%time challenge_clip.write_videofile(challenge_output, audio=False)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'plt.scatter(boston_residuals_df[\'boston_models\'], boston_r2, c="blue", label="R2")\nplt.title("R2 for each model")\nplt.savefig(\'R2.png\')\nplt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'with open("obj.json", "rt", encoding="utf8") as f:\n    json_string = f.read()\n    obj = json.loads(json_string)\n    print(obj.keys())\n    print(obj[\'name\'], obj[\'since\'])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "import numpy as np\n\nx = np.array([[[[[[1],[2],[3]]]]]])\nprint(f'1x.shape)\nprint(np.squeeze(x).shape)\nvisulize_array(np.squeeze(x),'np.squeeze(x)')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'def broad_greater_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has two masthead lights and one running light\n    visible for a starboard orientation.\n    """\n    white = (255, 255, 255)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 126)\n        shorter_masthead_light = np.random.randint(130, 186)\n        left_endpoint = np.random.randint(20, 126)\n        right_endpoint = np.random.randint(125, 211)\n        running_light_height_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 16)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = left_endpoint + light_width\n        short_masthead_height = shorter_masthead_light + light_width\n        short_masthead_width = right_endpoint + light_width\n        running_light_start = shorter_masthead_light + running_light_height_diff\n        running_light_width = running_light_start + light_width\n        if right_endpoint - left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(left_endpoint - 20, left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(left_endpoint, right_endpoint)\n        running_light_area = running_light_loc + light_width\n        new_view[taller_masthead_light:tall_masthead_height, left_endpoint:tall_masthead_width] = white\n        new_view[shorter_masthead_light:short_masthead_height, right_endpoint:short_masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```\n\nUnused variables:\n- black'}, {'reason': 'stop', 'result': 'mu = Partition([2,1])\nn = 3\nv = vandermonde(mu)\ndeg_v = v.degree()\ngenerator = {v.multidegree() : [v]}\nlist_op = partial_derivatives(v.parent())\nW1 = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n\nop_pol = polarization_operators(2, deg_v)\nW2 = PolarizedSpace(IsotypicComponent(W1, n), op_pol)\nshow(character(W2))\nfor v in W2.values():\n    show(v.basis())```\n\nUnused variables:\n- r'}, {'reason': 'stop', 'result': 'from tensorflow_graph_in_jupyter import show_graph\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'm = sfdmap.SFDMap()'}, {'reason': 'stop', 'result': "# same way to create a dictionary with nested lists\nmy_dictionary = dict( [ ['var1', 1], ['var2', 2], ['var3', 4] ] )\nprint(my_dictionary)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# reload(accuracy)\nreload(slope)\nplt.close()\nf = plt.figure(figsize=(5, 4))\n\nle = .12\nre = .02\nte = .1\nbe = .11\nh_gap = .13\n\nw = .5\nh = 1. - te - be\n\nax_lines = f.add_axes([le, be, w, h])\nax_slopes = f.add_axes([le + w + h_gap, be, 1. - w - h_gap - le - re, h])\n\nkey = fracs[-1]\n\nslope.plot_cv_slope(subjects, deep_all, linear_all, chance[0], training_size, fracs, (ax_lines, ax_slopes),\n                    legend=True, normalize_chance=False)\n\nx0 = .05\ny0 = 1. - te + .02\nx1 = le + w + h_gap - .075\n\nf.text(x0, y0, 'A', **letter_fontstyle)\nf.text(x1, y0, 'B', **letter_fontstyle)\n\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.eps'), dpi=300)\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.png'), dpi=300)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# mjd_to_sim\nmjd_to_sim = mjd_to_sim - (mjd_to_sim[0] - 54450)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def plot_incomesExpensesProfits(bankdata):\n    data = buildIncomesExpenses(bankdata)\n    \n    dataProfit = buildProfit(bankdata)\n    \n    # Profits per bank\n    for barProfit in dataProfit:\n        trace_profit = go.Scatter(\n            x = barProfit.x,\n            y = barProfit.y,\n            name = barProfit.name,\n            mode = \'markers\'\n        )\n        data.append(trace_profit)\n    \n    # Total profits\n    totalProfits = {}\n    for barProfit in dataProfit:\n        for date, value in zip(barProfit.x, barProfit.y):\n            key = date.strftime("%m/%Y")\n            if key in totalProfits:\n                totalProfits[key] += value\n            else:\n                totalProfits[key] = value\n    \n    xs = []\n    ys = []\n    for key, value in totalProfits.items():\n        xs.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n        ys.append(value)\n    \n    trace_profit = go.Scatter(\n        x = xs,\n        y = ys,\n        name = \'Profit\',\n        mode = \'markers\',\n        marker = dict(\n            size = 10,\n            line = dict(\n                width = 2,\n            )\n        )\n    )\n    data.append(trace_profit)\n    \n    layout = go.Layout(\n      xaxis = dict(\n          title = \'Date\',\n          rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\'1 month\',\n                     step=\'month\',\n                     stepmode=\'backward\'),\n                dict(count=3,\n                     label=\'3 months\',\n                     step=\'month\',\n                     stepmode=\'backward\'),\n                dict(count=6,\n                     label=\'6 months\',\n                     step=\'month\',\n                     stepmode=\'backward\'),\n                dict(count=1,\n                    label=\'1 year\',\n                    step=\'year\',\n                    stepmode=\'backward\'),\n                dict(step=\'all\')\n            ])\n        ),\n        rangeslider=dict(\n            visible = True\n        ),\n        type=\'date\'\n      ),\n      yaxis = dict(title = \'Amount (\' + CURRENCY + \')\'),\n      barmode = \'relative\',\n      title = \'Incomes, expenses and profit\'\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename=\'barmode-relative\')```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '#helper in getting top features and making visual\n#convert relevant dataframe columns to lowercase so we can compare with top feature output\n\ntrack_artist_lower_df = tracks_df["track_artist_uri"].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df["track_album_uri"].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged["track_uri"].apply(lambda x: x.lower())\n\n#Take a uri and return album, artist or song title in plain english\ndef get_translation(uri_type, uri):\n    \n    track = False  #if a single track/song is input as uri, I need to handle it differently\n    if uri_type == "track_artist_uri":\n        df = track_artist_lower_df\n        col = "track_artist_name"\n    elif uri_type == "track_album_uri":\n        df = track_album_lower_df\n        col = "track_album_name"\n    elif uri_type == "track_uri":\n        df = merged_track_uri_lower_df\n        col = "track_name"\n        track = True   #Handle track_name differently by going to the merged df\n    for i in range(len(tracks_df)):\n        if df[i] == uri:\n            if track == True:\n                return merged.iloc[i][col]\n            return tracks_df.iloc[i][col]\n            break```'}, {'reason': 'stop', 'result': 'test_set = asl.build_test(features_ground)\nprint("Number of test set items: {}".format(test_set.num_items))\nprint("Number of test set sentences: {}".format(len(test_set.sentences_index)))```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# some global variables\n\ndata_filepath = "./data/"\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'for i in range(10):\n    print(repr(mutate("A quick brown fox")))```\n\nUnused variables:\n- `i`'}, {'reason': 'stop', 'result': 'r = 2 \ndeg = 2\nop_pol = polarization_operators(r, deg)\nV_pol = PolarizedSpace(V_iso, op_pol)\nfor value in V_pol.values():\n    show(value.basis())```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'winkel=np.linspace(20,150,1000)\n\nfig, ax=plt.subplots(dpi=144)\nax.errorbar(anglesA, meanIA ,label="Data A: Expected 35 nm ",xerr=1,yerr=dmeanIA, marker="",ls="",lw=1)\nax.errorbar(angles, meanI ,label="Data B: Expected 180 nm",xerr=1,yerr=dmeanI, marker="",ls="",lw=1)\n\nax.plot(winkel,formfaktor(winkel,35e-9,100),label="Example: {:.2f} nm".format(35))\nax.plot(winkel,formfaktor(winkel,180e-9,70),label="Example: {:.2f} nm".format(180))\n\nax.set(xlabel=r"Angle $\\theta \\: [°]$", ylabel="Form factor $P(q)$", title="Comparison of Data and Expected Radii",\n       yscale="log",ylim=[1e-1,150])\nax.legend(frameon=False)\nfig.savefig("Plots/Comparison.png")```'}, {'reason': 'stop', 'result': 'def kde_fit(array):\n    dens = sm.nonparametric.KDEUnivariate(array)\n    dens.fit(bw=2)#bw = bandwidth; how smooth\n    density = dens.density\n    value = dens.support\n    return density, value\n\nelder_slope_density, elder_slope_value = kde_fit(elderSlope.astype(np.double))\nelder_area_density, elder_area_value = kde_fit(elderArea.astype(np.double))\nfox_slope_density, fox_slope_value = kde_fit(foxSlope.astype(np.double))\nfox_area_density, fox_area_value = kde_fit(foxArea.astype(np.double))'}, {'reason': 'stop', 'result': "from keras.applications.resnet50 import ResNet50\n\n# define ResNet50 model\nResNet50_model = ResNet50(weights='imagenet')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "VAR_GENE_DS_PATH = 'data/ExprMatrix.var.genes.h5ad'\nCELL_DAYS_PATH = 'data/cell_days.txt'\nSERUM_CELL_IDS_PATH = 'data/serum_cell_ids.txt'\nCELL_GROWTH_PATH = 'data/growth_gs_init.txt'\n\n# load data\nadata = wot.io.read_dataset(VAR_GENE_DS_PATH, obs=[CELL_DAYS_PATH, CELL_GROWTH_PATH], obs_filter=SERUM_CELL_IDS_PATH)\nadata.shape\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'import statsmodels.stats.api as sms\n\n#ValidRTCI\nRTanalysis.Valid.mean()\nsms.DescrStatsW(RTanalysis.Valid).tconfint_mean()```\n\nUnused variables:\n- a'}, {'reason': 'stop', 'result': 'edge_cols = []\nfor edge in G.edges(data=True):\n    if edge[2]["sign"] == -1:\n        edge_cols.append("red")\n    else:\n        edge_cols.append("green")\n\nfig, ax = plt.subplots(figsize=(15, 10))\nnx.draw_shell(G, ax=ax, with_labels=True, node_size=1000, node_color="white", \n              edge_color=edge_cols)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# the variance matrix\nvar = {\n    "treasury": [0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    "hardware": [0, 19, -2, 4, 1, 1, 1, 0.5, 10, 5],\n    "theater": [0, -2, 28, 1, 2, 1, 1, 0, -2, -1],\n    "telecom": [0, 4, 1, 22, 0, 1, 2, 0, 3, 4],\n    "brewery": [0, 1, 2, 0, 4, -1.5, -2, -1, 1, 1],\n    "highways": [0, 1, 1, 1, -1.5, 3.5, 2, 0.5, 1, 1.5],\n    "cars": [0, 1, 1, 2, -2, 2, 5, 0.5, 1, 2.5],\n    "bank": [0, 0.5, 0, 0, -1, 0.5, 0.5, 1, 0.5, 0.5],\n    "software": [0, 10, -2, 3, 1, 1, 1, 0.5, 25, 8],\n    "electronics": [0, 5, -1, 4, 1, 1.5, 2.5, 0.5, 8, 16]\n}\n\ndfv = pd.DataFrame(var, index=securities, columns=securities)\ndfv\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "df_hues = ['Todos','Sin distinguidos Betweenness', 'Sin distinguidos Grado']\ngig_comp_degree = gig_comp_graph.degree()\ngig_bet_degree = gig_bet_graph.degree()\ngig_degree_degree = gig_degree_graph.degree()\ndegree_frequencies1 = Counter([degree for (author, degree) in gig_comp_degree])\ndegree_frequencies2 = Counter([degree for (author, degree) in gig_bet_degree])\ndegree_frequencies3 = Counter([degree for (author, degree) in gig_degree_degree])\ndegree_info = []\nfor k,v in degree_frequencies1.items(): degree_info.append((k,v,df_hues[0]))\nfor k,v in degree_frequencies2.items(): degree_info.append((k,v,df_hues[1]))\nfor k,v in degree_frequencies3.items(): degree_info.append((k,v,df_hues[2]))\ndegree_df = pd.DataFrame(degree_info)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# Test with the test_images subdirs\nfor graph in (output_graph_orig, output_graph):\n    print("\\nTesting with graph=%s\\n" % graph)\n    for subdir in (\'house_with_pool\', \'house_without_pool\'):\n        test_dir = os.path.join(test_images_dir, subdir)\n        for f in os.listdir(test_dir):\n            if f.endswith(JPEG_EXTENSIONS):\n                tf.reset_default_graph()\n                image = os.path.join(test_dir, f)\n                print(image)\n                %run ../image_retraining/label_image.py --image=$image --graph=$graph --labels=$output_labels```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'with tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                              name="hidden1") # 동결층 재사용\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n                              name="hidden2") # 동결층 재사용\n    hidden2_stop = tf.stop_gradient(hidden2)\n    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n                              name="hidden3") # 동결하지 않고 재사용\n    logits = tf.layers.dense(hidden3, n_outputs, name="outputs") # 새로 만듦!```\n\nUnused variables:\n- hidden4'}, {'reason': 'stop', 'result': "if use_attention is False:\n    all_decoded = []\n    for beam_i in range(beam_width):\n        all_decoded.append([])\n        decoded = decode_ids(input_ids, bm_output_ids[:,:,beam_i])\n        for dec in decoded:\n            all_decoded[-1].append(dec[1])\n\n    print('\\n'.join(\n        '{} ---> {}'.format(dec[0], ' / '.join(d[i] for d in all_decoded))\n                            for i, dec in enumerate(decoded)\n    ))```\n\nUnused variables:\n- `inputs`"}, {'reason': 'stop', 'result': "if __name__=='__main__':\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    import matplotlib\n    %matplotlib inline\n\n    x = -Q_training['Ras.Stat']\n    y = Q_training['NPM1.3542']\n    z = -Q_training['FIBRINOGEN']\n    area = Q_training['PCA_Sq_38']*25+75 # 0 to 15 point radiuses\n    colors = Q_training['resp.simple']\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(x.name)\n    ax.set_ylabel(y.name)\n    ax.set_zlabel(z.name)\n    ax.scatter(x, y, z,s=area, c=colors,cmap=matplotlib.cm.coolwarm_r)\n    #Blue is 1 and red is 0\n    plt.show()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'print(str(len([comm for comm in comm_enrich_scores if comm[1] >= .5]) / len(comm_enrich_scores)))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'log_and_placements_aggregated_per_week_df = log_aggregated_per_week_df.join(placement_summary_df)\nlog_and_placements_aggregated_per_week_df.head()```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "an_integer = int('hello')"}, {'reason': 'stop', 'result': 'bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(splitter="random", max_leaf_nodes=16, random_state=42),\n    n_estimators=500, bootstrap=True, n_jobs=-1, random_state=42)```\n\nUnused variables:\n- max_samples'}, {'reason': 'stop', 'result': "data = []\nfor sem in semester_codes:\n    data.append(\n        go.Bar(\n            x=get_semester_asana(all_df, sem)[(get_semester_asana(all_df, sem)['Duration'].astype('timedelta64[D]') < 30)]['Duration'].value_counts(normalize=True).keys().days,\n            y=get_semester_asana(all_df, sem)[(get_semester_asana(all_df, sem)['Duration'].astype('timedelta64[D]') < 30)]['Duration'].value_counts(normalize=True).values,\n            name=semester_names[sem],\n            marker={ 'color': semester_colors[sem] }\n        )\n    )\n\nlayout = go.Layout(\n    barmode='group'\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='grouped-bar')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "num_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  tf.global_variables_initializer().run()\n  print('Initialized')\n  average_loss = 0\n  for step in range(num_steps):\n    batch_data, batch_labels = generate_batch(\n      batch_size, num_skips, skip_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += l\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss = average_loss / 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print('Average loss at step %d: %f' % (step, average_loss))\n      average_loss = 0\n    # note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in range(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8 # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n        log = 'Nearest to %s:' % valid_word\n        for k in range(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log = '%s %s,' % (log, close_word)\n        print(log)\n  final_embeddings = normalized_embeddings.eval()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'f1 = lambda x: x**2\n\n# is equivalent to \n\ndef f2(x):\n    return x**2```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Precision-Recall Curve\n# order according to increasing score\nscores_pr = [pr_auc_msr, pr_auc_margin, pr_auc_entropy, pr_auc_dropout, pr_auc_gmm, pr_auc_svm, pr_auc_df]\nrecalls = [recall_msr, recall_margin, recall_entropy, recall_dropout, recall_gmm, recall_svm, recall_df]\nprecisions = [precision_msr, precision_margin, precision_entropy, precision_dropout, \n              precision_gmm, precision_svm, precision_df]\n\nnames_methods = np.array([\'MSR\', \'Margin\', \'Entropy\', \'Dropout\', \'GMM\', \'OC SVM\', \'DF\'])\nscores_order = np.argsort(scores_pr)\ncolors_lines = plt.cm.rainbow(np.linspace(0, 1, len(scores_pr)))[:, :3]\n\n# plot\nfig = plt.figure(figsize=(6, 6))\nfor i in scores_order:\n    plt.step(recalls[i], precisions[i], where=\'post\', c=colors_lines[i])\n\nplt.xlabel(\'Recall\')\nplt.ylabel(\'Precision\')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.grid(alpha=.3)\nfig.axes[0].spines[\'right\'].set_visible(False)\nfig.axes[0].spines[\'top\'].set_visible(False)\nplt.legend([str.format(\'%s: %.2f\') % (names_methods[i], scores_pr[i]) for i in scores_order], title="PR AUC")\nplt.savefig("../Figures/Zurich/Metrics/PR_pred_ED.pdf", bbox_inches=\'tight\', pad_inches=0)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '96 ** 8\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "runClassifier('best_fit_10000', logit_c_param=.1, svm_c_param=.01)\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "for deep in [True, False]:\n    for ii, s in enumerate(subjects):\n        string = '{}, {}  '.format(subject_labels[s], 'deep' if deep else 'logistic')\n        if deep:\n            data = deep_all[1.][ii, 2]\n        else:\n            data = linear_all[1.][ii, 2]\n        string = string + '&{} $\\pm$ {}\\%  '.format(np.around(100*data.mean(), 1),\n                                                np.around(100*data.std(), 1))\n        for key in ['sens', 'spec', 'prec', 'f1']:\n            if deep:\n                data = deep_cv_stats[key][ii]\n            else:\n                data = linear_cv_stats[key][ii]\n            string = string + '&{} $\\pm$ {}\\%  '.format(np.around(100*data.mean(), 1),\n                                                    np.around(100*data.std(), 1))\n        string = string + ' \\\\\\\\'\n        print(string)\n    string = '{}, {}  '.format('Subj. average', 'deep' if deep else 'logistic')\n    if deep:\n        data = deep_all[1.][:, 2]\n    else:\n        data = linear_all[1.][:, 2]\n    string = string + '&{} $\\pm$ {}\\%  '.format(np.around(100*data.mean(), 1),\n                                            np.around(100*data.std(), 1))\n    for key in ['sens', 'spec', 'prec', 'f1']:\n        if deep:\n            data = deep_cv_stats[key]\n        else:\n            data = linear_cv_stats[key]\n        string = string + '&{} $\\pm$ {}\\%  '.format(np.around(100*data.mean(), 1),\n                                                np.around(100*data.std(), 1))\n    string = string + ' \\\\\\\\'\n    print(string)\n    print()```\n\nUnused variables:\n- `deep`\n- `ii`\n- `s`"}, {'reason': 'stop', 'result': '# gamma_final_mean.shape\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'print(2)'}, {'reason': 'stop', 'result': "param={}\nparam['octave'] = 3\nparam['use_gpu'] = 1\nparam['starting_range'] = 0.8\nparam['ending_range'] = 2\nparam['scale_search'] = [0.5, 1, 1.5, 2]\nparam['thre1'] = 0.1\nparam['thre2'] = 0.05\nparam['thre3'] = 0.5\nparam['mid_num'] = 4\nparam['min_num'] = 10\nparam['crop_ratio'] = 2.5\nparam['bbox_ratio'] = 0.25\nparam['GPUdeviceNumber'] = 3\n\nimport scipy\nprint heatmap_avg.shape\n\n#plt.imshow(heatmap_avg[:,:,2])\nfrom scipy.ndimage.filters import gaussian_filter\nall_peaks = []\npeak_counter = 0\n\nfor part in range(19-1):\n    x_list = []\n    y_list = []\n    map_ori = heatmap_avg[:,:,part]\n    map = gaussian_filter(map_ori, sigma=3)\n    \n    map_left = np.zeros(map.shape)\n    map_left[1:,:] = map[:-1,:]\n    map_right = np.zeros(map.shape)\n    map_right[:-1,:] = map[1:,:]\n    map_up = np.zeros(map.shape)\n    map_up[:,1:] = map[:,:-1]\n    map_down = np.zeros(map.shape)\n    map_down[:,:-1] = map[:,1:]\n    \n    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param['thre1']))\n    peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]) # note reverse\n    peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n    id = range(peak_counter, peak_counter + len(peaks))\n    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n\n    all_peaks.append(peaks_with_score_and_id)\n    peak_counter += len(peaks)```\n\nUnused variables:\n- x_list\n- y_list"}, {'reason': 'stop', 'result': '## display first 5 rows of data frame with new index\ndata.iloc[0:5]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '```python\n# finding minimum discount price out of hotel name - checking date - discount code group and fixing data\nmost_checkins["Discount Price"] = most_checkins.groupby([\'Hotel Name\', \'Checkin Date\', \'Discount Code\'])["Discount Price"].transform(\'min\')\nmost_checkins.drop_duplicates(subset=["Hotel Name", "Checkin Date", "Discount Code"], inplace=True)\nmost_checkins.sort_values(by=["Hotel Name", "Checkin Date", "Discount Code"], ascending=True, inplace=True)\nmost_checkins[\'Discount Price\'].replace(sys.maxsize, -1, inplace=True)\n\n# taking only needed data\ncheckin_hotel_discount = most_checkins[["Hotel Name", "Checkin Date", "Discount Code", "Discount Price"]].reset_index()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "df_totaalprovincie.plot.bar(figsize=(8,5.0),title='Aantal MBO 4 Afgestudeerden per Provincie')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'plt.plot(np.unique(a1), np.poly1d(np.polyfit(a1, a2, 10))(np.unique(a1)))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "treino['Cabine'] = treino['Cabine'].fillna('SEM_CABINE')\nteste['Cabine'] = teste['Cabine'].fillna('SEM_CABINE')\nuniao['Cabine'] = uniao['Cabine'].fillna('SEM_CABINE')\nuniao.info()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "utf8_string.decode('cp949')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "stage_analyze=pd.DataFrame(columns=['dog_stage','number','rating','retweet','favorite'])\nstage_analyze['dog_stage']=['doggo','floofer','pupper','puppo']\nfor each_stage in stage_analyze.dog_stage:\n    stage_df=df[df.dog_stage==each_stage]\n    stage_analyze.loc[stage_analyze.dog_stage== each_stage,'number']=stage_df.shape[0]\n    stage_analyze.loc[stage_analyze.dog_stage== each_stage,'rating']=stage_df['rating'].mean()\n    stage_analyze.loc[stage_analyze.dog_stage== each_stage,'retweet']=stage_df['retweet_count'].mean()\n    stage_analyze.loc[stage_analyze.dog_stage== each_stage,'favorite']=stage_df['favorite_count'].mean()\n    \n    \nstage_analyze[['dog_stage','number']].plot(x='dog_stage',kind='bar',title='Dog Stages Distribution')\n\nstage_analyze[['dog_stage','retweet','favorite']].plot(x='dog_stage',kind='bar',title='Dog Stage-Retweet&Favorite Analysis')\n\nstage_analyze[['dog_stage','rating']].plot(x='dog_stage',kind='bar',title='Dog Stage-Rating Analysis')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'l[0]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Display sample log-data after having a PCA transformation applied\ndf_display = pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values)\ndf_display.index = log_samples.index\ndisplay(df_display)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'for model_name, model in [(\'linux_lstm\', model_lstm), (\'linux_gru\', model_gru)]:\n    print("MODEL: ", model_name)\n    y_true, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n                                         train_len=95, test_len=1,\n                                         save_hyp=\'plots/{}_hyp_inlinecounter.png\'.format(model_name),\n                                         save_diag=\'plots/{}_diag_inlinecounter.png\'.format(model_name),\n                                         save_resp=\'plots/{}_resp_inlinecounter.png\'.format(model_name))```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "'''Generate a list of nuclides with 1% or more fission yield'''\n\nfp_1_percent = list() # this list will be used later\n\nfor fp in cfpy_az:\n    if fp.yield_percent >= 1.0:\n        fp_1_percent.append(fp)\n        \nprint('# of FP nuclides = ', len(fp_1_percent))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'len(stern_images)\nlen(bow_images_gt_50m)\nlen(port_broad_images_gt_50m)\nlen(stbd_broad_images_gt_50m)```\n\nUnused variables:\n- bow_lights_lt_50_images\n- port_broad_lt_50m_images\n- stbd_broad_lt_50m_images'}, {'reason': 'stop', 'result': '# For each feature find the data points with extreme high or low values\nd_unique_idx = dict()\nprint "Number of Data points considered outliers for the feature\\n"\nfor feature in log_data.keys():\n    \n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data[feature], 25)\n    \n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data[feature], 75)\n    \n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = 1.5*(Q3-Q1)\n    \n    # Display the outliers\n    df_out_lier = log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))]\n    i_nfeatures = df_out_lier.shape[0]\n    print "{:18s}\\t{:18s}".format(feature, str(i_nfeatures))\n    for x in df_out_lier.index:\n        if x not in d_unique_idx.keys():\n            d_unique_idx[x] = 1\n        else:\n            d_unique_idx[x] += 1\n\nprint "-------------------------"\nprint "TOTAL: Outliers: {} | Unique Outliers: {}".format(sum(d_unique_idx.values()),\n                                                         len(d_unique_idx.keys()))\n# filtering just the repeated outliers\nprint "\\n\\nData points considered outliers for more than one feature:"\ndf_aux = log_data.loc[[x[0] for x in d_unique_idx.iteritems() if x[1]>1]]\ndf_aux[\'count\'] = None\ndf_aux[\'count\'] = [d_unique_idx[x] for x in df_aux.index]\ndf_aux```'}, {'reason': 'stop', 'result': "corpora = []\nfor name in name_corpus:\n    try:\n        os.stat(corpus_path+name)\n        with open(corpus_path+name, 'rb') as f:\n            corpora.append(pickle.load(f))\n    except FileNotFoundError:\n        # int to string\n        with open(corpus_path+'kor_'+name, 'rb') as f:\n            corpus = pickle.load(f)\n        corpus = [[str(pid) for pid in line] for line in corpus]\n        with open(corpus_path+'kor_'+name,'wb') as f:\n            pickle.dump(corpus, f)\n        with open(corpus_path+'eng_'+name, 'rb') as f:\n            corpus = pickle.load(f)\n        corpus = [[str(pid) for pid in line] for line in corpus]\n        with open(corpus_path+'eng_'+name,'wb') as f:\n            pickle.dump(corpus, f)\n        # 한글&영문 corpus 병합\n        with open(corpus_path+'kor_'+name, 'rb') as f:\n            kor = pickle.load(f)\n        with open(corpus_path+'eng_'+name, 'rb') as f:\n            eng = pickle.load(f)\n        merged = kor+eng\n        with open(corpus_path+name, 'wb') as f:\n            pickle.dump(merged, f)\n        corpora.append(merged)```\n\nUnused variables:\n- `corpus`"}, {'reason': 'stop', 'result': 'a.head()'}, {'reason': 'stop', 'result': 'df_per_day_test.head()'}, {'reason': 'stop', 'result': "# Create a list\na_list = [1, 2, 3, 'hi', True, (1 + 3j), 9.9, ['another list!']]\nprint(a_list)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'def get_model_params():\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n\ndef restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + "/Assign")\n                  for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "df['rating'].describe()"}, {'reason': 'stop', 'result': '# Sanity check:\nnp.random.seed(1243)\nw = np.zeros((28*28, 10))\nb = np.zeros(10)\n    \nlogp_train, _, _ = sgd_iter(x_train[:5], t_train[:5], w, b)\n```\n\nUnused variables:\n- W'}, {'reason': 'stop', 'result': '# Transform the dataset to make it stationary\ndifferenced = difference(series, 1)\nprint(differenced.head())```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# snname = "SN2007uy"\nsnname = "SN2013ge"\n\nsn = pcc.classes.SNClass(snname)\n\nphot_path = os.path.join(coco_root_path, "data/lc/", snname + ".dat")\nspeclist_path = os.path.join(str(coco_root_path),"lists/" + snname + ".list")\nrecon_filename = os.path.abspath(os.path.join(str(coco_root_path), "recon/", snname + ".dat"))\n\nprint(phot_path)\nsn.load_phot(path = phot_path)\nsn.get_lcfit(recon_filename)\n\nsn.load_list(path = speclist_path)\nsn.load_spec()\nsn.plot_lc(multiplot = False, mark_spectra=True)```\n\nUnused variables:\n- `snname`\n- `speclist_path`\n- `recon_filename`'}, {'reason': 'stop', 'result': "log_clf = LogisticRegression(random_state=42)\nrnd_clf = RandomForestClassifier(random_state=42)\nsvm_clf = SVC(random_state=42, probability=True)\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='soft')\n\nvoting_clf.fit(X_train, y_train)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'v = vandermonde(Partition([2,1]))\nv```\n\nUnused variables:\n- n'}, {'reason': 'stop', 'result': 'with tf.Session() as sess:\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = saver.save(sess, "./my_new_model_final.ckpt")    ```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '```airports = pd.read_csv(\'test/airports.csv\')\nairports = airports[airports[\'traffic\'] > 8000]\nchart_3 = MapChart(dataframe=airports, \n                   projection="albersUsa", \n                   region="US", \n                   unit=\'airport\', \n                   value=\'traffic\',\n                   canvas_height=800,\n                   canvas_width=1200\n                  )\\\n    .addTooltip()\\\n    .addMarker({"shape": \'circle\', "color": "steelblue", "scale": 0.5, "opacity": 0.5})\n\nchart_3.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# from bokeh.palettes import Spectral6\n\n# Kolommen zijn de kolommen uit de df_totaalSBB dataframe. Dit gaat dan om 2013 AANT etc. Dit is voor de eerste groepering. \n# kolommen = list(df_totaalSBB)\n\n# Sectoren haalt alle unieke waardes uit het dataframe voor de kolom SECTORUNIT SBB.\n# sectoren = df[\'SECTORUNIT SBB\'].unique()\n# sectorenMultiplied = []\n\n# Voeg per kolom alle sectoren toe. Dit wordt dan de data voor de x-as. \n# Kolom is bijvoorbeeld 2013 AANT en hier worden vervolgens alle sectoren aan toegevoegd.\n# x = [ (sector, kolom) for sector in sectoren for kolom in kolommen ]\n\n# Data voor de y-as\n# Data doorlopen om de totalen op de juiste volgorde in een array te plaatsen.\n# Zelfde volgorde zoals hierboven staat voor de gegevens voor de x-as\n\n# counts = []\n\n# for sector in sectoren:\n#     for kolom in kolommen:\n#         counts.append(df_totaalSBB.loc[sector][kolom])\n\n# cyl_cmap = factor_cmap(\'cyl\', palette=Spectral5, factors=sorted(df.cyl.unique())\n# Teken grafiek\n# source = ColumnDataSource(data=dict(x=x, counts=counts, sectoren=sectorenMultiplied, color=Spectral6))\n# p = figure(x_range=FactorRange(*x), plot_height=400, title="Totalen per SBB sector per jaar")\n# p.vbar(x=\'x\', top=\'counts\', width=0.8, legend=\'sectoren\', color=\'color\', source=source)\n\n# p.width=1000\n# p.y_range.start = 0\n# p.x_range.range_padding = 0.1\n# p.xaxis.major_label_orientation = \'vertical\'\n# p.xgrid.grid_line_color = None\n\n# p.legend.orientation = "horizontal"\n# p.legend.location = "top_center"\n\n# p.xaxis[0].group_text_font_size = \'0pt\'\n# show(p)```\n\nUnused variables:\n- kolommen\n- sectoren\n- sectorenMultiplied\n- counts\n- cyl_cmap\n- source\n- p'}, {'reason': 'stop', 'result': 'from BITS.util.proc import run_command\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "indicator_list = ['oxygen','bqi','din_winter','ntot_summer', 'ntot_winter', 'dip_winter', 'ptot_summer', 'ptot_winter', 'biov', 'chl', 'secchi']\nfor indicator in indicator_list:\n    w.apply_indicator_data_filter(step = 2, \n                          subset = subset_uuid, \n                          indicator = indicator)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'fig = plt.figure()\nfig.add_subplot()\n\ny1 = earningsgrowth10k\ny2 = earningsgrowth10k_18k\ny3 = earningsgrowth18k_32k\ny4 = earningsgrowth32kk\n\nx_axis = [x  for x in range(1,9)]\nx_axis2 = [x + 0.2 for x in range(1,9)]\nx_axis3 = [x + 0.4for x in range(1,9)]\nx_axis4 = [x + 0.6 for x in range(1,9)]\n\nplt.bar(x_axis, y1, width = 0.2, label=\'Growth for Tier:10k\')\nplt.bar(x_axis2, y2, width = 0.2, label=\'Growth for Tier:10k-18k\')\nplt.bar(x_axis3, y3, width = 0.2, label=\'Growth for Tier:18k-32k\')\nplt.bar(x_axis4, y4, width = 0.2, label=\'Growth for Tier: 32k\')\n\nplt.title("Earnings Growth years 6-10")\nplt.xlabel("Region")\nplt.ylabel("Earnings Growth")\nplt.legend(title=["Earnings Growth years 6-10"])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n#save fig\n#plt.savefig("Plots/Worth_it.png")```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# 方便比較不同實驗/ 不同超參數設定的結果\nrun_id = f"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc"\ncheckpoint_path = os.path.join(checkpoint_path, run_id)\nlog_dir = os.path.join(log_dir, run_id)\n\n# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n# 一般來說你會想存下模型以及 optimizer 的狀態\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\n# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# 如果在 checkpoint 路徑上有發現檔案就讀進來\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  \n  # 用來確認之前訓練多少 epochs 了\n  last_epoch = int(ckpt_manager.latest_checkpoint.split("-")[-1])\n  print(f\'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。\')\nelse:\n  last_epoch = 0\n  print("沒找到 checkpoint，從頭訓練。")```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# More thresholds means higher granularity for the area under the curve approximation\n# Feel free to experiment with the number of thresholds\nnum_thresholds = 1000 \nthresholds = tf.lin_space(0.0, 1.0, num_thresholds).numpy()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# plot the function and the derivative for the activations sigmoid, tanh and ReLU.\nimport numpy as np\nfrom scipy.special import expit\nimport matplotlib.pyplot as plt\n\nn = 100\nxs = np.linspace(-3, 3, n)\nReLu = np.maximum(xs,0)\nd_ReLu = np.concatenate((np.zeros(int(n/2)), np.ones(int(n/2))))\n\ntanh = np.tanh(xs)\nd_tanh = 1- tanh**2\n\nsig = expit(xs)\nd_sig = sig*(1- sig)\n\nplt.figure(figsize=(20,5))\n# plot ReLu:\nplt.subplot(1,3, 1)\nplt.plot(xs, ReLu, label = \'ReLu\')\nplt.plot(xs, d_ReLu, label = \'d_Relu\')\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\nplt.title("ReLu(x) Plot")\nplt.ylim(-1.1, 1.1)\nplt.legend()\n\n# plot tanh:\nplt.subplot(1,3,2)\nplt.plot(xs, tanh, label = \'tanh\')\nplt.plot(xs, d_tanh, label = \'d_tanh\')\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\nplt.title("tanh(x) Plot")\nplt.ylim(-1.1, 1.1)\nplt.legend()\n\n# plot sigmoid:\nplt.subplot(1,3,3)\nplt.plot(xs, sig, label = \'sigmoid\')\nplt.plot(xs, d_sig, label = \'d_sigmoid\')\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\nplt.title("Sigmoid(x) Plot")\nplt.ylim(-1.1, 1.1)\nplt.legend()\n    \nplt.suptitle(\'Activation functions\')\nplt.show()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "from sklearn.datasets import fetch_mldata\nimport os\nimport numpy as np\n\n# Fetch the data\ntry:\n    mnist = fetch_mldata('MNIST original', data_home='.')\nexcept Exception:\n    raise FileNotFoundError('Please download mnist-original.mat from Canvas and put it in %s/mldata' % os.getcwd())\ndata, target = mnist.data, mnist.target.astype('int')\n\n# Shuffle\nindices = np.arange(len(data))\nnp.random.seed(123)\nnp.random.shuffle(indices)\ndata, target = data[indices].astype('float32'), target[indices]\n\n# Normalize the data between 0.0 and 1.0:\ndata /= 255. \n\n# Split\nx_train, x_valid, x_test = data[:50000], data[50000:60000], data[60000: 70000]\nt_train, t_valid, t_test = target[:50000], target[50000:60000], target[60000: 70000]```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "data = []\nfor i, sem in enumerate(semester_codes):\n    data.append(\n        go.Bar(\n            x=get_semester_asana(all_df, sem)['Overdue'].value_counts(normalize=True).keys().days,\n            y=get_semester_asana(all_df, sem)['Overdue'].value_counts(normalize=True).values,\n            name=semester_names[sem],\n            marker={ 'color': semester_colors[sem] },\n            yaxis='y' + str(i+1)\n        )\n    )\n\nfig = tools.make_subplots(rows=4, cols=2, subplot_titles=list(semester_names.values()))\n\nfig.append_trace(data[0], 1, 1)\nfig.append_trace(data[1], 1, 2)\nfig.append_trace(data[2], 2, 1)\nfig.append_trace(data[3], 2, 2)\nfig.append_trace(data[4], 3, 1)\nfig.append_trace(data[5], 3, 2)\nfig.append_trace(data[6], 4, 1)\nfig.append_trace(data[7], 4, 2)\n\nfor i, sem in enumerate(semester_codes):\n    fig['layout']['xaxis' + str(i+1)].update(range=[-28, 28])\n    fig['layout']['yaxis' + str(i+1)].update(range=[0, 0.7])\n\nfig.layout.update(height=1000)\nfig.layout.update(title='Overdue Spread')\n\niplot(fig, filename='overdue spread')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'sim = acnsim.Simulator(cn, sch, events, start, period=period)```\n\nUnused variables:\n- verbose'}, {'reason': 'stop', 'result': "x3 = 26.9\ny3 = 55.7\nz3 = 'Third record.'\nzz3 = ['an', 'example','list']\ndata3 = np.array([13,15,17,25.7,34.6,88])\ndatalist = []\ndata_enc = list(np.array(data,dtype=float))\ndata2_enc = list(np.array(data2,dtype=float))\ndata3_enc = list(data3)\ndatalist.append({'x':x, 'y':y, 'z':z, 'data':data_enc})\ndatalist.append({'x':x2, 'y':y2, 'z':z2, 'data':data2_enc})\ndatalist.append({'x':x3, 'y':y3, 'z':z3, 'zz':zz3, 'data':data3_enc})\nprint(datalist)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'from igraph import *\nfrom datetime import datetime\n\ng = Graph.GRG(30, 0.5)\ng.summary()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# BK\n```\n\nUnused variables:\n- datasource'}, {'reason': 'stop', 'result': '```python\nmodeloRandomForestClassifier = RandomForestClassifier()\nmodeloRandomForestClassifier.fit(x, y)\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'from BITS.seq.plot import DotPlot\n```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'threshold = 1.0\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ngrads_and_vars = optimizer.compute_gradients(loss)\ncapped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n              for grad, var in grads_and_vars]\ntraining_op = optimizer.apply_gradients(capped_gvs)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def can_send_message(sender, receiver):\n    for n in sender.neighbours:\n        if n is not receiver and n not in sender.in_msgs:\n            return False\n    \n    return True\n\n# Do the results make sense?\nprint (can_send_message(X, X_prior))\nprint (can_send_message(X_prior, X))\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "boulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,   \n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n    \n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n    \n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n    \n    # yoga\n    'yoga': boulder_6b / 5,\n    \n    # ARC\n    'arc': boulder_6b_plus / 5,\n    \n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n    \n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%' : boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n    \n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n    \n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n    \n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n    \n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit',]\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+',]\n\n\ndef calc_score(row):\n    s = [row[key] * scores_translator[key] for key in scores_translator]\n    return sum(s)\n\n\ndef calc_hangboard_scores(row):\n    s = [row[key] * scores_translator[key] for key in hangboard]\n    return sum(s)\n        \n    \ndef calc_gym_scores(row):\n    s = [row[key] * scores_translator[key] for key in gymnastics]\n    return sum(s)\n\ndef calc_climbing_scores(row):\n    s = [row[key] * scores_translator[key] for key in climbing]\n    return sum(s)\n\ndef calc_arc_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['arc']]\n    return sum(s)\n\ndef calc_technique_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['tech', 'visual']]\n    return sum(s)\n\ndef calc_power_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['power', 'speed', 'speed+']]\n    return sum(s)\n\ndef calc_yoga_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['yoga', ]]\n    return sum(s)"}, {'reason': 'stop', 'result': '!grep -A 2 add_argument taxifare/trainer/task.py\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '```python\n# 메모장을 여시고, 메모장에서 "가나다라마바사아자차카타파하" 를 입력하고,\n# c:\\dev 디렉토리에 저장\n\nf = open(\'메모장.txt\', \'rb\')\ndata = f.read()\nf.close()\n\ndata.decode(\'utf8\')  # utf8, cp949\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'rnd_clf.oob_score_```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'uploaded```'}, {'reason': 'stop', 'result': "print('Number of classes: ', len(np.unique(wine_data.target)))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import numpy as np\n\nn_data_points = 5\nidx = np.arange(n_data_points)\nwith model:\n    lambda_ = pm.math.switch(tau >= idx, lambda_1, lambda_2)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "def plot_incomesExpenses(bankdata):\n    data = buildIncomesExpenses(bankdata)\n    layout = go.Layout(\n      xaxis = dict(title = 'Date'),\n      yaxis = dict(title = 'Amount (' + CURRENCY + ')'),\n      barmode = 'relative',\n      title = 'Incomes and expenses'\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename='barmode-relative')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'm $ m \\$ V3 1 0 1\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "result.get_counts('Circuit')"}, {'reason': 'stop', 'result': "# starten van de webserver :)\napps = {'/': Application(FunctionHandler(make_document))}\n\nserver = Server(apps, port=5006, allow_websocket_origin=['localhost:5006','nm-interactive.net:5006'])\nserver.start()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "runClassifier('genes_all', logit_c_param=.1, svm_c_param=.01)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "correlation.update_multivariate_relevancies(k=3, fixed_features=['1'])```\n\nUnused variables:\n- runs"}, {'reason': 'stop', 'result': '# Predictions is an array of predicted values\nprint(predictions_5K)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'lr_clf = LogisticRegression()\nlr_clf.fit(X_train_norm, y_train)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'predictions_10K'}, {'reason': 'stop', 'result': "from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path, height=224, width=224):\n    ''' Loads RGB image as PIL.Image.Image type of given Height x Width dimensions\n    '''\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(height, width))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'def test_accuracy(user, W, b):\n    # Função que mede a acurácia do modelo em um conjunto de teste.\n    predictions = forward_pass(test_data, W, b)[0]\n    accuracy = np.mean((predictions > 0.5) == (test_labels == user))\n    return accuracy, predictions\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'print "\\n".join(\n    textfilter.normalize_text(\n        sample_lyrics,\n        sentences=True\n    )\n)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "statinfo = os.stat(pickle_file)\nprint('Compressed pickle size:', statinfo.st_size)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "gROOT.ProcessLine('sim0->findGammaCalibrationConstants()')"}, {'reason': 'stop', 'result': 'from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Useful starting lines\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n%load_ext autoreload\n%autoreload 2\n\nfrom implementation import *\nfrom utilities import *```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'x = np.linspace(-5, 5, 50)\nparams = [1, 2, 1, 1]\nK = periodic_kernel(x, x, params)\nutil.visiualize_kernel(K)```\n\nUnused variables:\n- sigma_n'}, {'reason': 'stop', 'result': '# TODO: Number of training examples\nn_train = len(X_train)\nn_valid = len(X_valid)\n\n# TODO: Number of testing examples.\nn_test = len(X_test)\n\n# TODO: What\'s the shape of an traffic sign image?\nimage_shape = X_train[0].shape\n\n# TODO: How many unique classes/labels there are in the dataset.\nn_classes = len(set(y_test))\n\nprint("Number of training examples =", n_train)\nprint("Number of validation examples =", n_valid)\nprint("Number of testing examples =", n_test)\nprint("Image data shape =", image_shape)\nprint("Number of classes =", n_classes)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'class getReviewPerMovie(object):\n    def __init__(self,movieNumber):\n        self.site = \'http://www.imdb.com/title/\'+str(movieNumber)+\'/reviews\'\n        self.movie_no = movieNumber\n        \n    def getLength(self,type_re):\n        first_page = self.site +\'?filter=\'+ type_re + \';filter=\'+ type_re +\';start=\'+\'0\'\n        html_l = requests.get(first_page)\n        soup_l = BeautifulSoup(html_l.text,"html.parser")   \n        tbl = soup_l.findAll(\'table\')[1]\n        row_n = tbl.find_all(\'tr\')\n        str_len = row_n[0].find_all(\'td\')[0].text\n        regex = r"(Page).(\\d).(of).(\\d*)(.+)"\n        len_review = re.sub(regex, \'\\\\4\',str_len)\n        return(len_review)\n    \n    def category(self,ca_re):\n        reviews = []\n        rev=[]\n        try:        \n            print(self.movie_no)\n            page_length = int(self.getLength(str(ca_re)))\n         \n            for i in range(0,page_length):\n\n                link= self.site +\'?filter=\'+ ca_re + \';filter=\'+ ca_re +\';start=\'+ str(i*10)\n                print(link)\n                html_link = requests.get(link)\n                soup = BeautifulSoup(html_link.text,"html.parser")   \n                review_soup = soup.find("div", {"id": "tn15content"})\n\n                for div_re in review_soup.find_all(\'div\'):\n                    user_info_dict = {}\n                    for user_info in div_re.find_all(\'img\',alt=True):\n                        user_info_dict[\'rating\']=user_info.get(\'alt\')\n                        user_info_dict[\'author\']=re.sub(r"(/user/)(ur\\d+)(/)",\'\\\\2\',user_info.a.get(\'href\'))\n                        user_info_dict[\'title\']=div_re.h2.text\n                        user_info_dict[\'movie\']=self.movie_no\n                        if ca_re == \'love\':\n                            user_info_dict[\'categorie\']=\'postive\'\n                        else:\n                            user_info_dict[\'categorie\']=\'negative\'\n                        if user_info.small is not None:\n                            reg = r"(<small>)(\\d+)( out of )(\\d+)(.+)"\n                            user_info_dict[\'usefulness\'] = re.sub(reg,\'\\\\2/\\\\4\',str(div_re.small))\n                            reviews.append(user_info_dict)\n\n                for re_text in review_soup.find_all(\'p\'):\n                    review_text = {}\n                    if re_text.getText() not in [\'*** This review may contain spoilers ***\',\n                                                 \'Add another review\']:\n                        review_text[\'review\'] =re_text.get_text().replace(\'\\n\',\' \')\n                        rev.append(review_text)\n\n            for i in range(0,len(reviews)):\n                if len(reviews) == len(rev):\n                    reviews[i][\'review\'] = rev[i][\'review\']\n                else:\n                    print(\'the length of the user info and review is not the same\',\'...\\n\',\n                         \'user info has length: \',str(len(reviews)),\'...\\n\',\n                          \'review has length: \',str(len(rev)),\'...\\n\')\n\n            return reviews\n                \n        except ValueError:\n            pass\n```\n\nUnused variables:\n- `reviews`\n- `i` (inside the second for loop)'}, {'reason': 'stop', 'result': 'with open(exportpath + "profile.json", "w") as fileobj:\n    json.dump(profile_dict, fileobj, default = convert_invalid_values)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "d2['i2':'i4']"}, {'reason': 'stop', 'result': '%display latex\n%runfile character.py```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'print(bigquery.WriteDisposition.WRITE_TRUNCATE)\nprint(bigquery.WriteDisposition.WRITE_APPEND)\nprint(bigquery.SourceFormat.NEWLINE_DELIMITED_JSON)\nprint(bigquery.SourceFormat.CSV)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "import plotly\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import plot, trace_values, build_layout\n\nx_values = list(range(0, 6))\n\nfour_x_plus_fifteen_values = list(map(lambda x: output_at(four_x_plus_fifteen, x),x_values))\nfour_x_plus_fifteen_trace = trace_values(x_values, four_x_plus_fifteen_values, mode = 'lines')\nplot([four_x_plus_fifteen_trace])```\n\nUnused variables:\n- layout"}, {'reason': 'stop', 'result': 'l[1:3] = ["s", "m"]\n\nprint(l)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "# Import Asset Class data\n\ndataset = 'asset_class' # values: 'asset_class','ftse','sectors','sp100'\n\ncorr_tensor = np.load('../Data/%s/%s_corr.npy' % (dataset, dataset))\ndates = np.load('../Data/%s/%s_dates.npy' % (dataset, dataset))\nnodes = np.load('../Data/%s/%s_nodes.npy' % (dataset, dataset))\nnodes_descr = np.load('../Data/%s/%s_nodes_descr.npy' % (dataset, dataset))"}, {'reason': 'stop', 'result': 'from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, oob_score=True, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\n\ny_pred_rf = rnd_clf.predict(X_test)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler()\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\nfeatures_raw[numerical] = scaler.fit_transform(data[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(features_raw.head(n = 1))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "print(d2['PRON']['we'])```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "doripa['scores'].corr(doripa['total6'])```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeature_dict = {i:label for i,label in zip(\n                range(4),\n                  (\'sepal length in cm\',\n                  \'sepal width in cm\',\n                  \'petal length in cm\',\n                  \'petal width in cm\', ))}\n\nlabel_dict = {0: \'Setosa\', 1: \'Versicolor\', 2:\'Virginica\'}\n\n\ndef plot_data(X, title):\n\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))\n\n    for ax,cnt in zip(axes.ravel(), range(4)):  \n\n        # set bin sizes\n        min_b = math.floor(np.min(X[:,cnt]))\n        max_b = math.ceil(np.max(X[:,cnt]))\n        bins = np.linspace(min_b, max_b, 25)\n\n        # plottling the histograms\n        for lab,col in zip(range(0,3), (\'blue\', \'red\', \'green\')):\n            ax.hist(X[y==lab, cnt],\n                   color=col,\n                   label=\'%s\' %label_dict[lab],\n                   bins=bins,\n                   alpha=0.5,)\n        ylims = ax.get_ylim()\n\n        # plot annotation\n        leg = ax.legend(loc=\'upper right\', fancybox=True, fontsize=8)\n        leg.get_frame().set_alpha(0.5)\n        ax.set_ylim([0, max(ylims)+2])\n        ax.set_xlabel(feature_dict[cnt])\n        ax.set_title(\'Iris histogram #%s\' %str(cnt+1))\n\n        # hide axis ticks\n        ax.tick_params(axis="both", which="both", bottom="off", top="off",  \n            labelbottom="on", left="off", right="off", labelleft="on")\n\n        # remove axis spines\n        ax.spines["top"].set_visible(False)  \n        ax.spines["right"].set_visible(False)\n        ax.spines["bottom"].set_visible(False)\n        ax.spines["left"].set_visible(False)    \n\n    axes[0][0].set_ylabel(\'count\')\n    axes[1][0].set_ylabel(\'count\')\n\n    fig.tight_layout()       \n\n    plt.show()\n\ndef plot_pca(X,title):\n\n    ax = plt.subplot(111)\n\n    for label,marker,color in zip(\n        range(0,3),(\'^\', \'s\', \'o\'),(\'blue\', \'red\', \'green\')):\n\n        plt.scatter(x=X[:,0][y == label],\n                y=X[:,1][y == label],\n                marker=marker,\n                color=color,\n                alpha=0.5,\n                label=label_dict[label]\n                )\n\n    plt.xlabel(\'PC1\')\n    plt.ylabel(\'PC2\')\n\n    leg = plt.legend(loc=\'upper left\', fancybox=True)\n    leg.get_frame().set_alpha(0.5)\n    plt.title(title)\n\n    # hide axis ticks\n    plt.tick_params(axis="both", which="both", bottom="off", top="off",  \n            labelbottom="on", left="off", right="off", labelleft="on")\n\n    # remove axis spines\n    ax.spines["top"].set_visible(False)  \n    ax.spines["right"].set_visible(False)\n    ax.spines["bottom"].set_visible(False)\n    ax.spines["left"].set_visible(False)    \n\n    plt.tight_layout()\n    plt.grid()\n\n    plt.show()\n    \ndef plot_lda(X, title):\n\n    ax = plt.subplot(111)\n    for label,marker,color in zip(\n        range(0,3),(\'^\', \'s\', \'o\'),(\'blue\', \'red\', \'green\')):\n\n        plt.scatter(x=X[:,0][y == label],\n                    y=X[:,1][y == label] * -1, # flip the figure\n                    marker=marker,\n                    color=color,\n                    alpha=0.5,\n                    label=label_dict[label])\n\n    plt.xlabel(\'LD1\')\n    plt.ylabel(\'LD2\')\n\n    leg = plt.legend(loc=\'upper right\', fancybox=True)\n    leg.get_frame().set_alpha(0.5)\n    plt.title(title)\n\n    # hide axis ticks\n    plt.tick_params(axis="both", which="both", bottom="off", top="off",  \n            labelbottom="on", left="off", right="off", labelleft="on")\n\n    # remove axis spines\n    ax.spines["top"].set_visible(False)  \n    ax.spines["right"].set_visible(False)\n    ax.spines["bottom"].set_visible(False)\n    ax.spines["left"].set_visible(False)    \n\n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n```\n\nUnused variables: `plot_data`, `plot_pca`, `plot_lda`'}, {'reason': 'stop', 'result': 'barwidth = 0.75\nfig, ax = plt.subplots(figsize=(9, 7))\nrects1 = ax.bar(0.5,RTanalysis.Valid.mean(),barwidth,yerr=RTstderror.Valid,ecolor=\'k\',edgecolor=sns.xkcd_rgb[\'green\'],linewidth = 2, facecolor=\'none\',error_kw=dict(lw=3))\nrects2 = ax.bar(1.5,RTanalysis.Invalid.mean(),barwidth,color=sns.xkcd_rgb[\'green\'],yerr=RTstderror.Invalid,ecolor=\'k\',error_kw=dict(lw=3))\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None)\nax.set_ylim(550,610)\nax.set_xlim(0,2.5)\nax.set_xticklabels((\'Valid\', \'Invalid\'))\nax.set_xticks([0.5 + barwidth/2, 1.5 + barwidth/2])\nax.set_yticks(np.arange(550, 611, 10))\nplt.title(\'S-S Phase RT\', fontsize=26,fontweight="bold")\nplt.ylabel(\'Reaction Time (ms)\', fontsize=24,fontweight="bold")\nplt.xlabel(\'Trial Type\', fontsize=24,fontweight="bold")\nsns.despine()\n\nplt.show()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'fuzzer(char_start=32, char_range=96)'}, {'reason': 'stop', 'result': "params['x'].annotation```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '%%bash\ngrep -A 15 "add_engineered(" taxifare/trainer/model.py | grep -E "^\\s*[^#]*=" | grep -vE "^\\s*[^#]*=" | sed -E "s/^\\s*([^#]*)=.*/\\1/"'}, {'reason': 'stop', 'result': "by_period['mean'].get_value('mean_ek_value')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "I'm sorry, but I cannot directly access or modify files on your system. However, if you provide me with the code inside the file, I can help you identify and remove the unused variable definitions."}, {'reason': 'stop', 'result': '# 超參數\nd_model = 4\nnum_heads = 2\ndff = 8\ndec_layer = DecoderLayer(d_model, num_heads, dff)\n\n# 來源、目標語言的序列都需要 padding mask\ninp_padding_mask = create_padding_mask(inp)\ntar_padding_mask = create_padding_mask(tar)\n\n# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\nlook_ahead_mask = create_look_ahead_mask(tar.shape[-1])\ncombined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n\n# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\ndec_out, _, _ = dec_layer(\n    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n\nprint("emb_tar:", emb_tar)\nprint("-" * 20)\nprint("enc_out:", enc_out)\nprint("-" * 20)\nprint("dec_out:", dec_out)\nassert emb_tar.shape == dec_out.shape\nprint("-" * 20)```'}, {'reason': 'stop', 'result': 'prediction_probabilities = logreg.predict_proba(X_test)\nprint("Accuracy:",metrics.accuracy_score(y_test, y_pred))\nprint("Precision:",metrics.precision_score(y_test, y_pred))\nprint("Recall:",metrics.recall_score(y_test, y_pred))\nprint("Log loss= ",log_loss(y_test, prediction_probabilities))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'log_and_placements_aggregated_per_week_df["client-ip-unique-count"].hist()```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "score2 = model2.evaluate(np.array(X_dev_feature), y_dev_hot, batch_size=16)\n\nprint '\\nAccuracy on new dataset with no unknown columns is', score2[1]```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "def hypothesis_inlinecounter(text):\n    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split('\\n')])[:-1]\n    return hyp\n\ndef hypothesis_capswords(text):\n    hyp = np.concatenate([np.full(len(x)+1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x)+1, -1) for x in text.split('\\n')])[:-1]\n    return hyp\n\ndef hypothesis_pos(text, pos_tag):\n    hyp = text.replace('1', '0')\n    for word, tag in pynlpir.segment(text):\n        if tag == pos_tag:\n            hyp = hyp.replace(word, '1'*len(word), 1)\n        else:\n            hyp = hyp.replace(word, '0'*len(word), 1)\n    hyp = [1 if x=='1' else -1 for x in re.sub('[^1]', '0', hyp)]\n    \n    return hyp\n\ndef hypothesis_verbs(text):\n    return hypothesis_pos(text, 'verb')\n\ndef hypothesis_nouns(text):\n    return hypothesis_pos(text, 'noun')"}, {'reason': 'stop', 'result': 'm3 = USMap()\nm3.zoom = 11\nm3.display()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "#directory and filename\ndir = 'data/processed/'\nfile = 'revenue_2009_2016.csv'```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(len(dico))\nheights = [elem[1] for elem in sorted_list]\nplt.loglog(x, heights)\n\nplt.title("log-log words frequency")\nplt.ylabel("Frequency")\nplt.xlabel("Index")\nplt.show()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# This will give us the last three elements\nprint(a_list[-3:])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "#GANDeconv_t9900_h100_train3_ep48.critic5\n#GANDeconv_t9900_h100_train3_ep45\n#GANDeconv_t9900_h100_train3_ep21\n#GANDeconv_t9900_h100_train3_ep45\nfilename='GANDeconv_t9900_h100_train3_ep45' #'GANDeconvolution_t2000_h100_ep20.c5'\nhidden_dim = 100\nG,D,train_hist = GAN_CelebA.loadCheckpoint(filename,hidden_dim,use_cuda=use_cuda)\nepoch_num=len(train_hist['D_losses'])\nGAN_CelebA.show_result(G,D,epoch_num, hidden_dim, show=True,save=True, path='figures/'+filename+'.pdf', use_cuda=use_cuda)\n\nplt.plot(range(0,epoch_num),train_hist['D_losses'],label='D_loss')\nplt.plot(range(0,epoch_num),train_hist['G_losses'],label='G_loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Deconvolution GAN, total time:'+str(int(train_hist['total_ptime'][-1]/60))+'minutes')\nplt.legend()\nplt.savefig('figures/'+filename+'_Loss.pdf')\nplt.show()"}, {'reason': 'stop', 'result': '# Get the length of the filtered dictionary (where frequency of the word == 1)\nnum_word_1 = len({k: v for k,v in dico.items() if v == 1})\nnum_word_tot = len(dico)\n"{un} / {tot} words appearing only once".format(un=num_word_1, tot=num_word_tot)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '```python\ndef readData(folder):\n    """\n    Read all frames from a given directory into one matrix.\n    input:  directory to folder containing frames\n    output: array3d (dimension: xpixel x ypixel x # frames)\n            array2D (dimension: 1D flattened img x # frames)\n    """\n    \n    files = os.listdir(folder)\n\n    array3D = []\n    array2D = []\n    for ff in files:\n        arr = fits.getdata(folder + ff)\n        array3D.append(arr)\n        array2D.append(arr.flatten())\n    \n    return np.array(array3D)\n\ndef combineFrame(data_array):\n    """\n    Combine image frames and return 1D array.\n    input:  data array (dimension: # 1D detector counts x # frames)\n    output: 1D combined array of detector counts (ADU)\n    """\n    \n    return np.median(data_array, axis=0)\n```\n\nUnused variables:\n- `array2D`'}, {'reason': 'stop', 'result': "Xs = X[:,-1::-1][-1::-1,:]\nplt.imshow(Xs,interpolation='nearest',cmap=plt.cm.gray);```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'rspEwidth=1\nrspEmax=40\nrspLwidth=0.03\nrspLmax=40.0```\n\nUnused variables:\n- rspEmin\n- rspLmin'}, {'reason': 'stop', 'result': '# a small subset of our input/output pairs\nXsmall = X[:10000,:,:]\nysmall = y[:10000,:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '### Resposta: Não. Mas futuramente pode ser possível.```'}, {'reason': 'stop', 'result': 'np.mean(r_precisions)'}, {'reason': 'stop', 'result': '# autoreload for automatically reloading changes made in my_model_selectors and my_recognizer\n%load_ext autoreload\n%autoreload 2\n\nfrom my_model_selectors import SelectorConstant\n\ndef train_all_words(features, model_selector):\n    training = asl.build_training(features)  # Experiment here with different feature sets defined in part 1\n    sequences = training.get_all_sequences()\n    Xlengths = training.get_all_Xlengths()\n    model_dict = {}\n    for word in training.words:\n        model = model_selector(sequences, Xlengths, word, \n                        n_constant=3).select()\n        model_dict[word]=model\n    return model_dict\n\nmodels = train_all_words(features_ground, SelectorConstant)\nprint("Number of word models returned = {}".format(len(models)))```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'exclude = ["AmBe", "CalibData"]   #File prefixes for the current calibration run to exclude\ntmpName = \'a\'\nfor filename in os.listdir(path):\n    name = os.path.splitext(filename)[0]\n            \n    if filename.endswith(".root") and name.split(\'_\')[1] == runNum \\\n          and  name.split(\'_\')[0] == "Background":\n        print "Adding: ", name\n        gROOT.ProcessLine(\'TFile *{0} = new TFile("{1}.root","update")\'.format(tmpName, name))\n        gROOT.ProcessLine(\'eventTree->Draw("m_amplitude>>rebin{0}(1250,0,35000)")\'.format(name.split(\'_\')[3]))\n        gROOT.ProcessLine(\'{}->Write()\'.format(tmpName)) \n        gROOT.ProcessLine(\'{}->Close()\'.format(tmpName))\n        tmpName = chr(ord(tmpName) + 1)\n        pause()     \n    elif filename.endswith(".root") and name.split(\'_\')[1] == runNum: \n        if name.split(\'_\')[0] not in exclude:\n            print "Adding: ", name\n            gROOT.ProcessLine(\'TFile *{0} = new TFile("{1}.root")\'.format(tmpName, name))\n            gROOT.ProcessLine(\'eventTree->Draw("m_amplitude>>{0}(1250,0,35000)")\'.format(name.split(\'_\')[0]))\n            \n            gROOT.ProcessLine(\'{}->cd()\'.format(detNames[int(name.split(\'_\')[3])]))\n            gROOT.ProcessLine(\'{}->Write()\'.format(name.split(\'_\')[0]))            \n            \n            gROOT.ProcessLine(\'{}->Close()\'.format(tmpName))\n            tmpName = chr(ord(tmpName) + 1)\n            pause() \n```\n\nUnused variables:\n- `tmpName`'}, {'reason': 'stop', 'result': 'for n in range(1, nt):\n    rn = rho.copy()\n    \n    for j in range(1, nx):\n        v = (vmax * (1 - rho / rhomax)) * (5 / 18)\n        f1 = v * rho\n        rho[1:] = rn[1:] - dt / dx * (f1[1:] - f1[0:-1])\n        rho[0] = 20\n```\n\nUnused variables:\n- `n`\n- `j`'}, {'reason': 'stop', 'result': "vote_dfs = pd.read_excel(os.path.join(PATH, 'source_data', '20161206_sov.xlsx'), sheetname=None, header=3)\n\n# Since the sheet names get cut off, we can fix them using the Contents tab\ncontents = vote_dfs['Contents'].copy()\ncontents.columns = ['key', 'name']\ncontents = contents.iloc[1:]\ncontents['key'] = contents['key'].astype(int)\ncontents = contents.set_index('key').to_dict(orient='index')\n\nfixed_names = {}\nfor cut_name in vote_dfs:\n    if cut_name == 'Contents':\n        continue\n    key = int(cut_name[:3])\n    if key in contents:\n        prefix = cut_name[:6]\n        postfix = contents[key]['name']\n        fixed_names[prefix + postfix] = vote_dfs[cut_name]```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import os\nimport sys\nfrom ROOT import gROOT\nimport numpy as np\n\nsys.path.insert(0,os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Support\'))\nsys.path.insert(0,os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Unfolding\'))\nfrom Utilities import pause\nfrom Root import CalibParams\n\noutPath = "/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/Inputs/"\nrspPath= \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Simulated/PHS/ResponseMatrices/simSideResponse20Mil.root\'\ncalPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/CalibData/\'\n\nos.chdir(outPath)\nprint \'Currently working in: \\n {}\'.format(os.getcwd())\n\ndetNames = {0: \'Det0\'}\ncalNames = {0: \'CalibParams_0.txt\'}```\n\nUnused variables:\n- rspPath\n- calPath'}, {'reason': 'stop', 'result': 'import cv2\nimport numpy as np\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    # defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    # defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    # filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    # returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((*img.shape, 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\n# Python 3 has support for cool math symbols.\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': '%bash\nMODEL_NAME="babyweight"\nMODEL_VERSION="soln"\nMODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/trained_model/export/exporter/ | tail -1)\necho "Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes"\ngcloud ml-engine models create ${MODEL_NAME} --regions $REGION\ngcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version 1.4\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'visualize_document_clusters(wiki, tf_idf, centroids[k](), cluster_assignment[k](), k, map_index_to_word)```\n\nUnused variables:\n- `k`'}, {'reason': 'stop', 'result': 'def gp_log_likelihood(x_train, t_train, theta, beta, C=None, invC=None):\n    \n    if C is None:\n        K = computeK(x_train, x_train, theta)\n        C = K + (1/beta)*np.eye(len(K))\n    \n    if invC is None:\n        invC = np.linalg.inv(C)\n    \n    lp = np.log(1/np.sqrt((np.linalg.det(2 * np.pi * C))) * np.exp((-1/2) * t_train.T.dot(invC).dot(t_train)))\n    \n    return lp, C, invC\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "num_classes = 10\nnp.random.seed(133)\n\ndef maybe_extract(filename, force=False):\n  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n  if os.path.isdir(root) and not force:\n    # You may override by setting force=True.\n    print('%s already present - Skipping extraction of %s.' % (root, filename))\n  else:\n    print('Extracting data for %s. This may take a while. Please wait.' % root)\n    tar = tarfile.open(filename)\n    sys.stdout.flush()\n    tar.extractall()\n    tar.close()\n  data_folders = [\n    os.path.join(root, d) for d in sorted(os.listdir(root))\n    if os.path.isdir(os.path.join(root, d))]\n  if len(data_folders) != num_classes:\n    raise Exception(\n      'Expected %d folders, one per class. Found %d instead.' % (\n        num_classes, len(data_folders)))\n  print(data_folders)\n  return data_folders\n  \ntrain_folders = maybe_extract(train_filename)\ntest_folders = maybe_extract(test_filename)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '```# Create Pandas DataFrame for results\npd.DataFrame(data={"date": date_array, "density": density_array, \n                   "frustration_score": frustration_array}).to_csv("ResultCSVs/structbal_quant_%s_res.csv" % (dataset))```'}, {'reason': 'stop', 'result': "VGG16Seq.add(Dense(nb_labels, activation='relu'))```\n\nUnused variables:\n- Dropout"}, {'reason': 'stop', 'result': "features_count = 784\nlabels_count = 10\n\n# TODO: Set the features and labels tensors\nfeatures = tf.placeholder(tf.float32)\nlabels = tf.placeholder(tf.float32)\n\n# TODO: Set the weights and biases tensors\nweights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\nbiases = tf.Variable(tf.zeros(n_labels))\n\n### DON'T MODIFY ANYTHING BELOW ###\n\n#Test Cases\nfrom tensorflow.python.ops.variables import Variable\n\nassert features._op.name.startswith('Placeholder'), 'features must be a placeholder'\nassert labels._op.name.startswith('Placeholder'), 'labels must be a placeholder'\nassert isinstance(weights, Variable), 'weights must be a TensorFlow variable'\nassert isinstance(biases, Variable), 'biases must be a TensorFlow variable'\n\nassert features._shape == None or (\\\n    features._shape.dims[0].value is None and\\\n    features._shape.dims[1].value in [None, 784]), 'The shape of features is incorrect'\nassert labels._shape  == None or (\\\n    labels._shape.dims[0].value is None and\\\n    labels._shape.dims[1].value in [None, 10]), 'The shape of labels is incorrect'\nassert weights._variable._shape == (784, 10), 'The shape of weights is incorrect'\nassert biases._variable._shape == (10), 'The shape of biases is incorrect'\n\nassert features._dtype == tf.float32, 'features must be type float32'\nassert labels._dtype == tf.float32, 'labels must be type float32'\n\n# Feed dicts for training, validation, and test session\ntrain_feed_dict = {features: train_features, labels: train_labels}\nvalid_feed_dict = {features: valid_features, labels: valid_labels}\ntest_feed_dict = {features: test_features, labels: test_labels}\n\n# Linear Function WX + b\nlogits = tf.matmul(features, weights) + biases\n\nprediction = tf.nn.softmax(logits)\n\n# Cross entropy\ncross_entropy = -tf.reduce_sum(labels * tf.log(prediction), axis=1)\n\n# some students have encountered challenges using this function, and have resolved issues\n# using https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n# please see this thread for more detail https://discussions.udacity.com/t/accuracy-0-10-in-the-intro-to-tensorflow-lab/272469/9\n\n# Training loss\nloss = tf.reduce_mean(cross_entropy)\n\n# Create an operation that initializes all variables\ninit = tf.global_variables_initializer()\n\n# Test Cases\nwith tf.Session() as session:\n    session.run(init)\n    session.run(loss, feed_dict=train_feed_dict)\n    session.run(loss, feed_dict=valid_feed_dict)\n    session.run(loss, feed_dict=test_feed_dict)\n    biases_data = session.run(biases)\n\nassert not np.count_nonzero(biases_data), 'biases must be zeros'\n\nprint('Tests Passed!')```\n\nUnused variables:\n- labels_count\n- n_features\n- n_labels"}, {'reason': 'stop', 'result': "### plot some figures we got wrong\n\nfig = plt.figure(figsize=(20, 12))\n\nfor i, ix in enumerate(np.random.choice(test_tensors.shape[0], size=24, replace=False)):\n    ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(test_tensors[ix]))\n    # correct img ix\n    true_index = np.argmax(test_targets[ix])\n    # predicted img ix\n    pred_index = predictions[ix]\n    ax.set_title('{}\\n[{}]'.format(dog_names[pred_index], dog_names[true_index]),\n                color=('blue' if pred_index == true_index else 'red'))\nplt.tight_layout()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': '"""\n# callbacks (evaluated every epoch)\n# show loss and accuracy figures after each epoch\ncallback_plot = PlotLosses()\n\n# stop early if after several epochs the accuracy doesn\'t improve\ncallback_earlystop = EarlyStopping(monitor=\'val_loss\', min_delta=1e-4, patience=24, verbose=1, mode=\'auto\')\n\n# decrease learning rate when accuracy stops improving\ncallback_lr = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.5, patience=12, verbose=1, mode=\'auto\',\n                                epsilon=1e-4, cooldown=0, min_lr=1e-8)\n\n# checkpoint to save weights at every epoch (in case of interruption)\nfile_path = "weights-improvement.hdf5"\ncallback_checkpoint = ModelCheckpoint(file_path, monitor=\'val_acc\', verbose=0, save_best_only=True, mode=\'max\')\n\ncallback_tensorboard = TensorBoard(log_dir=\'./tensorboard\', histogram_freq=0, write_graph=True, write_images=True)\n\n# model setup\nbatch_size = 20\nepochs = 300\n\n\ndef model_train(model, data_augmentation):\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(batch_generator(x_train, y_train,\n                                        batch_size=batch_size, data_augmentation=data_augmentation),\n                        steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n                        epochs=epochs,\n                        verbose=1,\n                        class_weight=class_weights,  # weights for loss function\n                        validation_data=(x_val, y_val),\n                        callbacks=[callback_earlystop,\n                                   callback_lr,\n                                   # callback_checkpoint,\n                                   callback_plot,\n                                   callback_tensorboard],\n                        workers=cpu_count(),\n                        use_multiprocessing=True)\n```'}, {'reason': 'stop', 'result': 'q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2)```\n\nUnused variables:\n- alpha'}, {'reason': 'stop', 'result': "# Save the model as pickle file\n\nimport joblib\n\njoblib.dump(model_30K, 'model_30K.pkl')```\n\nUnused variables:\n- pickle"}, {'reason': 'stop', 'result': 'q_agent.Q'}, {'reason': 'stop', 'result': 'data.columns'}, {'reason': 'stop', 'result': '#Select monthly mean PM2.5 data for the LSOA\nts = monthly_data.isel(x=1103, y=1045).load()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '## display first 5 rows of data frame with new index\ndata.iloc[0:5]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '## matrix addition\nprint(A + B)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# TODO: Apply your clustering algorithm of choice to the reduced data \n\n# TODO: Predict the cluster for each data point\npreds = None\n\n# TODO: Find the cluster centers\ncenters = None\n\n# TODO: Predict the cluster for each transformed sample data point\nsample_preds = None\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\nscore = None```\n\nUnused variables:\n- clusterer'}, {'reason': 'stop', 'result': 'data_new = [line[1:] for line in data]\n\nfor line in data_new:\n    print(line)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_lm - y_lm_36, label='LM 3.2$-$LM 3.6')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables:\n- l"}, {'reason': 'stop', 'result': 'l = []\n\nprint(l)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "import sys\nsys.path.append('../python')\nfrom virtual_stations import get_waterlevel\nfrom misc import get_precipitation, get_pet, get_label_tree, startswith_label, get_mask, get_masks, str2datetime, get_peq_from_df, gcs_get_dir\nfrom models import gr4hh\nfrom mcmc_utils import dist_map, get_likelihood_logp, get_prior_logp\n\nfrom mcmc import smc, dist\nfrom datetime import timedelta\nimport random\nimport subprocess\nimport pickle\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport xarray as xr\nimport gcsfs\nfrom dask.distributed import Client\n\nis_pangeo_data = False # True if in Pangeo binder, False if in laptop\nif is_pangeo_data:\n    from dask_kubernetes import KubeCluster as Cluster\n    n_workers = 10\nelse:\n    from dask.distributed import LocalCluster as Cluster\n    n_workers = 4\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'from IPython.display import display\nfrom IPython.display import HTML\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\ndef get_cuota_mensual_total(p, i, n):\n    num = p * i * (i+1)**n\n    den = (i+1)**n - 1\n    return num/den\n\ndef get_deuda_capital(p, i, n, t):\n    coef = (((1+i)**t) - 1) / (((1+i)**n) - 1)\n    return p * ( 1 - coef )\n\ndef mostrar_evolucion_credito(P, I, N):\n    \n    p = P\n    i = I / 12.0 * 0.01\n    n = N * 12\n    \n    cuota_total = get_cuota_mensual_total(p, i, n)\n    print("Tu cuota mensual es de AR$ " + str(cuota_total) + "\\n")\n\n    cuota_capital = []\n    cuota_interes = []\n    deuda_capital = []\n    t_array = []\n    print("Este es tu listado de cuotas por mes: \\n")\n    print("MES \\t CAPITAL \\t INTERESES \\t CAPITAL ADEUDADO")\n\n    for t in range(1, n+1):    \n\n        t_array.append(t)\n        cuota_capital.append(get_deuda_capital(p, i, n, t-1) - get_deuda_capital(p, i, n, t))\n        cuota_interes.append(cuota_total - cuota_capital[t-1])\n        deuda_capital.append(get_deuda_capital(p, i, n, t))\n\n        print(str(t) + "         " + \\\n              str(\'%.2f\'%(cuota_capital[t-1])) + "         " + \\\n              str(\'%.2f\'%(cuota_interes[t-1])) + "         " + \\\n              str(\'%.2f\'%(deuda_capital[t-1]))\n             )\n\ndef get_cuota_mensual_total_2(xs):\n    print("hi")\n    p=1000000\n    i=0.14\n    out = []\n    for n in xs:\n        num = p * i * (i+1)**n\n        den = (i+1)**n - 1\n        out.append(num/den)\n    return out\n\n```\n\n\nUnused variables:\n- meses\n- get_mes\n- nbinteract'}, {'reason': 'stop', 'result': '```HTML("""\n<video width="960" height="540" controls>\n  <source src="{0}">\n</video>\n""".format(yellow_output))```'}, {'reason': 'stop', 'result': '## Create a dataframe playlist with songs of your choice\n## This code might need a little tweaking, I tried not to spend much time on it,\n## but it will add a new playlist to the end of merged.\n\n## Will need to be fixed so that new playlist appended gets negative samples now that merged is \n## created after negative samples.\n\n## As you can see below, we can look at the end of merged and see that a new playlist pid has been \n## added and columns are filled in.  The function is going to get the songs from merged.  If the\n## songs are not in merged, it\'s not going to work.  Might be better to use track_uri\'s instead.\n\nsongs1 = ["The Wolves", "Coffee", "Song For You", "Heart Of Gold",\n         "Typical Situation", "Ants Marching", "Uncatena"]\n\ndef create_my_playlist(songs: list, name, description):\n    my_playlist = pd.DataFrame()\n    for song in songs:\n        my_playlist = my_playlist.append(merged.loc[merged.track_name == song].iloc[0][tracks_df.columns.append(pd.Index(["track_uri"]))])\n    playlist_info = pd.DataFrame(columns = list(playlist_df.columns))\n    playlist_info["playlist_collaborative"] = pd.Series("false")\n    playlist_info["playlist_description"] = description\n    playlist_info["playlist_modified_at"] = 1496793600\n    playlist_info["playlist_name"] = name\n    playlist_info["playlist_num_edits"] = 1\n    playlist_info["playlist_num_followers"] = 1\n    playlist_info["playlist_pid"] = np.max(merged.playlist_pid) + 1\n    playlist_info["playlist_num_artists"] = my_playlist["track_artist_uri"].nunique()\n    playlist_info["playlist_num_tracks"] = len(my_playlist)\n    playlist_info["playlist_duration_ms"] = np.sum(my_playlist["track_duration_ms"])\n    playlist_info["playlist_num_albums"] = my_playlist["track_album_uri"].nunique()\n    playlist_info = pd.concat([playlist_info] * len(songs))\n    playlist_info.index = list(my_playlist.index)\n    result_df =  pd.concat([my_playlist, playlist_info], axis=1)\n    result_df.index = range(len(merged), len(merged) + len(songs))\n    return result_df\n\n#Try function, and check end of merged, for example\nmy_playlist = create_my_playlist(songs1, "indie", "indie rock upbeat")\nmerged = merged.append(my_playlist)\nmerged.iloc[-30:]```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'import pandas as pd\nimport numpy as np\nimport requests\nimport tweepy\nimport json\nimport re\nimport matplotlib.pyplot as plt\nimport warnings\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'tm.minimize(tm.sum(x[i,j]*costs.get((i,j), 0)))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n    training_op = optimizer.minimize(loss)    \n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n    \ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'import numpy as np\n\nA = np.array([[1, 0, 0],\n              [4, 0, 0],\n              [2, 3, 5]])\n\nprint(np.linalg.det(A))\nprint(np.linalg.matrix_rank(A)) \n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'str.upper?```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'class MutationFuzzer(MutationFuzzer):\n    def mutate(self, inp):\n        return mutate(inp)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'import os\nos.listdir("test_images/")```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '#indeedians = from indeedemployeesnapshot yesterday today\n#group by ldap, full_name ```\n\n\nUnused variables:\n- indeedians'}, {'reason': 'stop', 'result': 'koppeltabel = pd.read_sql_query("SELECT * FROM koppeltabel", engine)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'import warnings\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\n\n# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(X, arima_order):\n    # prepare training dataset\n    train = X.loc[:39,[\'client-ip-unique-count\']]\n    test= X.loc[40:,[\'client-ip-unique-count\']]\n    \n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    model = sm.tsa.ARIMA(train, order=arima_order).fit()\n    predictions = model.predict(40,42,dynamic=True)\n    error = mean_absolute_error(test, predictions)\n    return error\n\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype(\'float32\')\n    best_score, best_cfg = float("inf"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mae = evaluate_arima_model(dataset, order)\n                    if mae < best_score:\n                        best_score, best_cfg = mae, order\n                    print(\'ARIMA%s MAE=%.3f\' % (order,mae))\n                except:\n                    continue\n    print(\'Best ARIMA%s MAE=%.3f\' % (best_cfg, best_score))\n\n\n# load dataset\np_values = [0, 1, 2, 4,6,8,10]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings("ignore")\nevaluate_models(log_and_placements_aggregated_per_week_df.loc[:,[\'client-ip-unique-count\']], p_values, d_values, q_values)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "Pixel['LSOA'] = 'E01017182'```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "#Create a monthly mean PM2.5 for the dataset\nmonthly_data = After2009.resample('M', dim='time', how='mean', keep_attrs=True)```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'heads_proba = 0.51 # The probability of head is 0.51 as it is a biased coin\n\n# coin_tosses contains the coin tosses for 10 series of 10000 times of coin tosses\ncoin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32) \ncumulative_sum_of_number_of_heads = np.cumsum(coin_tosses, axis=0)\ncumulative_heads_ratio = cumulative_sum_of_number_of_heads / np.arange(1, 10001).reshape(-1, 1)\ncumulative_heads_ratio\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '#ignore\noutput, attention_weights = scaled_dot_product_attention_demo(q, k, v, draw=True)\n```\n\nUnused variables:\n- mask'}, {'reason': 'stop', 'result': 'import plotly\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\ndata_matrix = [["N", "I MC", "I Real", "I MC-I Real","sigma","I MC-I Real/sigma"]]\nfor i in range (0,len(N)):\n    element=[10*N[i], results[i], I_real,results[i]-I_real,sigmas[i],(results[i]-I_real)/sigmas[i]]\n    data_matrix.append(element)\n    \nplotly.tools.set_credentials_file(username=\'guneykan\', api_key=\'Yu3MsgD6Zlfbb0B3S5Mx\')\ntable = ff.create_table(data_matrix)\npy.iplot(table)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "### Plot everything - the original series as well as predictions on training and testing sets\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# plot original series\nplt.plot(dataset,color = 'k')\n\n# plot training set prediction\nsplit_pt = train_test_split + window_size \nplt.plot(np.arange(window_size,split_pt,1),train_predict,color = 'b')\n\n# plot testing set prediction\nplt.plot(np.arange(split_pt,split_pt + len(test_predict),1),test_predict,color = 'r')\n\n# pretty up graph\nplt.xlabel('day')\nplt.ylabel('(normalized) price of Apple stock')\nplt.legend(['original series','training fit','testing fit'],loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()```"}, {'reason': 'stop', 'result': "%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')```\n\nUnused variables:\n- `%debug`"}, {'reason': 'stop', 'result': "train1, test1 = log_and_placements_aggregated_per_week_df.iloc[:39,1], log_and_placements_aggregated_per_week_df.iloc[39:,1]\n\nmodel1 = ExponentialSmoothing(train1, seasonal='mul', seasonal_periods=12).fit()\npred1 = model1.predict(start=test1.index[0], end=test1.index[-1])\n\nmean_absolute_error(test1, pred1)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import numpy as np\nfrom scipy.optimize import curve_fit\n\ndef expectedFit(x, a, b):\n    """\n    Returns the expected fit for the histogram\n    \n    Arguments: x - the x value in the equation\n               a - the first fit parameter\n               b - the second fit paramter\n               \n    Returned: The expected fit function\n    """\n    return a * np.exp(-b * x)\n\noccurenceRegion = bigOccurences[7:14] # Only fits region of interest\nmagnitudeRegion = bigMagnitudes[7:14]\n\nparameters, covariance = curve_fit(expectedFit, magnitudeRegion, occurenceRegion)\n\nfitX = magnitudeRegion\n\naFit = parameters[0]\nbFit = parameters[1]\n\nfitY = aFit * np.exp(-bFit * magnitudeRegion)\n\nprint("A = ", aFit, "b = ", bFit)\n```\n\nUnused variables:\n- fitX\n- fitY'}, {'reason': 'stop', 'result': '# google drive sync\n\nsave_to_gdrive = True\ndataset_in_gdrive = False # set to True for speed up without training\n\n\nif save_to_gdrive:\n  from google.colab import drive\n  drive.mount(\'/content/gdrive\')\n  output_dir = os.path.join("/content/gdrive/My Drive", output_dir)\n    \nen_vocab_file = os.path.join(output_dir, "en_vocab")\nzh_vocab_file = os.path.join(output_dir, "zh_vocab")\ncheckpoint_path = os.path.join(output_dir, "checkpoints")\nlog_dir = os.path.join(output_dir, \'logs\')\n\nif dataset_in_gdrive:\n  download_dir = os.path.join(output_dir, "tensorflow-datasets/downloads")\nelse:\n  download_dir = "tensorflow-datasets/downloads"\n    \n# print(f"Save result to {output_dir}")\nclear_output()```\n\nUnused variables:\n- `download_dir`'}, {'reason': 'stop', 'result': 'list(merged[(merged["data_type"] == "int64") | \n            (merged["data_type"] == "float64")]["index"])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '## compute the inverse of a matrix\nprint(np.linalg.inv(A))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '%ls\n```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'using StatsBase\n\n# separamos los datos que nos servirán para entrenar\nx_entren = [\n    caracteristicas[dig_0_aleat[1:950],:];\n    caracteristicas[dig_1_aleat[1:950],:];\n    caracteristicas[dig_2_aleat[1:950],:];\n    caracteristicas[dig_3_aleat[1:950],:];\n    caracteristicas[dig_4_aleat[1:950],:];\n    caracteristicas[dig_5_aleat[1:950],:];\n    caracteristicas[dig_6_aleat[1:950],:];\n    caracteristicas[dig_7_aleat[1:950],:];\n    caracteristicas[dig_8_aleat[1:950],:];\n    caracteristicas[dig_9_aleat[1:950],:];\n]\ny_entren = [\n    clase_de_digito[dig_0_aleat[1:950],:];\n    clase_de_digito[dig_1_aleat[1:950],:];\n    clase_de_digito[dig_2_aleat[1:950],:];\n    clase_de_digito[dig_3_aleat[1:950],:];\n    clase_de_digito[dig_4_aleat[1:950],:];\n    clase_de_digito[dig_5_aleat[1:950],:];\n    clase_de_digito[dig_6_aleat[1:950],:];\n    clase_de_digito[dig_7_aleat[1:950],:];\n    clase_de_digito[dig_8_aleat[1:950],:];\n    clase_de_digito[dig_9_aleat[1:950],:];\n]\n\n\n# separamos los datos que nos servirán para probar\nx_prueba = [\n    caracteristicas[dig_0_aleat[951:end],:];\n    caracteristicas[dig_1_aleat[951:end],:];\n    caracteristicas[dig_2_aleat[951:end],:];\n    caracteristicas[dig_3_aleat[951:end],:];\n    caracteristicas[dig_4_aleat[951:end],:];\n    caracteristicas[dig_5_aleat[951:end],:];\n    caracteristicas[dig_6_aleat[951:end],:];\n    caracteristicas[dig_7_aleat[951:end],:];\n    caracteristicas[dig_8_aleat[951:end],:];\n    caracteristicas[dig_9_aleat[951:end],:];\n]\ny_prueba = [\n    clase_de_digito[dig_0_aleat[951:end],:];\n    clase_de_digito[dig_1_aleat[951:end],:];\n    clase_de_digito[dig_2_aleat[951:end],:];\n    clase_de_digito[dig_3_aleat[951:end],:];\n    clase_de_digito[dig_4_aleat[951:end],:];\n    clase_de_digito[dig_5_aleat[951:end],:];\n    clase_de_digito[dig_6_aleat[951:end],:];\n    clase_de_digito[dig_7_aleat[951:end],:];\n    clase_de_digito[dig_8_aleat[951:end],:];\n    clase_de_digito[dig_9_aleat[951:end],:];\n]\n\n# removemos las caracteristicas globales promedio de x_entren y x_prueba\nx_prom = mean(x_entren,1)\n\nx_entren = x_entren - x_prom.*ones(size(x_entren))\nx_prueba = x_prueba - x_prom.*ones(size(x_prueba))'}, {'reason': 'stop', 'result': 'rnd_clf.feature_importances_\n```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'def prepare_image_with_tensorflow(image, target_width=299, target_height=299, max_zoom=0.2):\n    """Zooms and crops the image randomly for data augmentation."""\n\n    # First, let\'s find the largest bounding box with the target size ratio that fits within the image\n    image_shape = tf.cast(tf.shape(image), tf.float32)\n    height = image_shape[0]\n    width = image_shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = tf.cond(crop_vertically,\n                         lambda: width,\n                         lambda: height * target_image_ratio)\n    crop_height = tf.cond(crop_vertically,\n                          lambda: width / target_image_ratio,\n                          lambda: height)\n\n    # Now let\'s shrink this bounding box by a random factor (dividing the dimensions by a random number\n    # between 1.0 and 1.0 + `max_zoom`.\n    resize_factor = tf.random_uniform(shape=[], minval=1.0, maxval=1.0 + max_zoom)\n    crop_width = tf.cast(crop_width / resize_factor, tf.int32)\n    crop_height = tf.cast(crop_height / resize_factor, tf.int32)\n    box_size = tf.stack([crop_height, crop_width, 3])   # 3 = number of channels\n\n    # Let\'s crop the image using a random bounding box of the size we computed\n    image = tf.random_crop(image, box_size)\n\n    # Let\'s also flip the image horizontally with 50% probability:\n    image = tf.image.random_flip_left_right(image)\n\n    # The resize_bilinear function requires a 4D tensor (a batch of images)\n    # so we need to expand the number of dimensions first:\n    image_batch = tf.expand_dims(image, 0)\n\n    # Finally, let\'s resize the image to the target dimensions. Note that this function\n    # returns a float32 tensor.\n    image_batch = tf.image.resize_bilinear(image_batch, [target_height, target_width])\n    image = image_batch[0] / 255  # back to a single image, and scale the colors from 0.0 to 1.0\n    return image\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '## display first 5 rows of data frame with new index\ndata.iloc[0:5]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'import apache_beam as beam\nimport datetime\n\ndef to_csv(rowdict):\n    # pull columns from BQ and create a line\n    import hashlib\n    import copy\n    CSV_COLUMNS = \'weight_pounds,is_male,mother_age,plurality,gestation_weeks\'.split(\',\')\n    \n    # create synthetic data where we assume that no ultrasound has been performed\n    # and so we don\'t know sex of the baby. Let\'s assume that we can tell the difference\n    # between single and multiple, but that the errors rates in determining exact number\n    # is difficult in the absence of an ultrasound.\n    no_ultrasound = copy.deepcopy(rowdict)\n    w_ultrasound = copy.deepcopy(rowdict)\n\n    no_ultrasound[\'is_male\'] = \'Unknown\'\n    if rowdict[\'plurality\'] > 1:\n      no_ultrasound[\'plurality\'] = \'Multiple(2+)\'\n    else:\n      no_ultrasound[\'plurality\'] = \'Single(1)\'\n      \n    # Change the plurality column to strings\n    w_ultrasound[\'plurality\'] = [\'Single(1)\', \'Twins(2)\', \'Triplets(3)\', \'Quadruplets(4)\', \'Quintuplets(5)\'][rowdict[\'plurality\']-1]\n    \n    # Write out two rows for each input row, one with ultrasound and one without\n    for result in [no_ultrasound, w_ultrasound]:\n      data = \',\'.join([str(result[k]) if k in result else \'None\' for k in CSV_COLUMNS])\n      key = hashlib.sha224(data).hexdigest()  # hash the columns to form a key\n      yield str(\'{},{}\'.format(data, key))\n  \ndef preprocess(in_test_mode):\n    job_name = \'preprocess-babyweight-features\' + \'-\' + datetime.datetime.now().strftime(\'%y%m%d-%H%M%S\')\n    \n    if in_test_mode:\n        print \'Launching local job ... hang on\'\n        OUTPUT_DIR = \'./preproc\'\n    else:\n        print \'Launching Dataflow job {} ... hang on\'.format(job_name)\n        OUTPUT_DIR = \'gs://{0}/babyweight/preproc/\'.format(BUCKET)\n    \n    options = {\n        \'staging_location\': os.path.join(OUTPUT_DIR, \'tmp\', \'staging\'),\n        \'temp_location\': os.path.join(OUTPUT_DIR, \'tmp\'),\n        \'job_name\': job_name,\n        \'project\': PROJECT,\n        \'teardown_policy\': \'TEARDOWN_ALWAYS\',\n        \'max_num_workers\': 3,   # CHANGE THIS IF YOU HAVE MORE QUOTA\n        \'no_save_main_session\': True\n    }\n    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n    if in_test_mode:\n        RUNNER = \'DirectRunner\'\n    else:\n        RUNNER = \'DataflowRunner\'\n    p = beam.Pipeline(RUNNER, options=opts)\n    query = """\nSELECT\n  weight_pounds,\n  is_male,\n  mother_age,\n  plurality,\n  gestation_weeks,\n  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\nFROM\n  publicdata.samples.natality\nWHERE year > 2000\nAND weight_pounds > 0\nAND mother_age > 0\nAND plurality > 0\nAND gestation_weeks > 0\nAND month > 0\n    """\n  \n    if in_test_mode:\n        query = query + \' LIMIT 100\' \n  \n    for step in [\'train\', \'eval\']:\n        if step == \'train\':\n            selquery = \'SELECT * FROM ({}) WHERE MOD(ABS(hashmonth),4) < 3\'.format(query)\n        else:\n            selquery = \'SELECT * FROM ({}) WHERE MOD(ABS(hashmonth),4) = 3\'.format(query)\n\n        (p \n         | \'{}_read\'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=selquery, use_standard_sql=True))\n         | \'{}_csv\'.format(step) >> beam.FlatMap(to_csv)\n         | \'{}_out\'.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, \'{}.csv\'.format(step))))\n        )\n \n    job = p.run()\n  \npreprocess(in_test_mode=False)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "d4.loc['Colorado', 'two']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'The provided code does not contain any variable definitions, so there are no unused variables to remove.'}, {'reason': 'stop', 'result': "def make_arrays(nb_rows, img_size):\n  if nb_rows:\n    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n    labels = np.ndarray(nb_rows, dtype=np.int32)\n  else:\n    dataset, labels = None, None\n  return dataset, labels\n\ndef merge_datasets(pickle_files, train_size, valid_size=0):\n  num_classes = len(pickle_files)\n  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n  train_dataset, train_labels = make_arrays(train_size, image_size)\n  vsize_per_class = valid_size // num_classes\n  tsize_per_class = train_size // num_classes\n    \n  start_v, start_t = 0, 0\n  end_v, end_t = vsize_per_class, tsize_per_class\n  end_l = vsize_per_class+tsize_per_class\n  for label, pickle_file in enumerate(pickle_files):       \n    try:\n      with open(pickle_file, 'rb') as f:\n        letter_set = pickle.load(f)\n        # let's shuffle the letters to have random validation and training set\n        np.random.shuffle(letter_set)\n        if valid_dataset is not None:\n          valid_letter = letter_set[:vsize_per_class, :, :]\n          valid_dataset[start_v:end_v, :, :] = valid_letter\n          valid_labels[start_v:end_v] = label\n          start_v += vsize_per_class\n          end_v += vsize_per_class\n                    \n        train_letter = letter_set[vsize_per_class:end_l, :, :]\n        train_dataset[start_t:end_t, :, :] = train_letter\n        train_labels[start_t:end_t] = label\n        start_t += tsize_per_class\n        end_t += tsize_per_class\n    except Exception as e:\n      print('Unable to process data from', pickle_file, ':', e)\n      raise\n    \n  return valid_dataset, valid_labels, train_dataset, train_labels\n            \n            \ntrain_size = 200000\nvalid_size = 10000\ntest_size = 10000\n\n_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n\nprint('Testing:', test_dataset.shape, test_labels.shape)```\n\nUnused variables:\n- `valid_dataset`\n- `valid_labels`\n- `train_dataset`\n- `train_labels`"}, {'reason': 'stop', 'result': 'corr_returns = returns.corr(method="pearson")\n\nplt.figure(figsize=(9, 5))\nsns.heatmap(\n    corr_returns,\n    cbar=True,\n    annot=True,\n    annot_kws={"size": 15},\n    xticklabels=tickers,\n    yticklabels=tickers,\n    cmap="bone_r")\nplt.title("Correlation Matrix")\nplt.show()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# Plot graph density for each threshold. \ndensity_distr = []\n\nfor i in np.arange(0.1, 1, 0.1):\n    density_array = []\n    for j in range(1, int(num_examples/5)):\n        corr_mat = corr_tensor[j*5, :, :].copy()\n        corr_mat[(corr_mat > -1*i) & (corr_mat < i)] = 0\n        G, density = make_graph(corr_mat, nodes, \'signed\')\n        \n        density_array.append(density)\n    \n    density_distr.append(density_array)\n\nplt.boxplot(density_distr)\nplt.ylabel("Graph Density")\nplt.xlabel("Correlation Threshold (10^-1)")\nplt.title("Density vs Threshold")\nplt.show()```\n\nUnused variables:\n- thresh_array'}, {'reason': 'stop', 'result': 'def mutate(s):\n    """Return s with a random mutation applied"""\n    mutators = [\n        delete_random_character,\n        insert_random_character,\n        flip_random_character\n    ]\n    mutator = random.choice(mutators)\n    return mutator(s)```\n\nUnused variables:\n- `mutator`'}, {'reason': 'stop', 'result': 'X_test_refined = pd.DataFrame([])\nr_precisions = []\npbar = tqdm(data_test.groupby([\'playlist_pid\']))\nfor pid, df in pbar:\n    labels = y_test.loc[df.index]\n    targets = dataset.loc[labels.index].track_duration_ms\n    positive_targets = dataset.loc[labels[labels == 1].index].index\n    negative_tracks = dataset.loc[X_test[(X_test.playlist_pid != pid)]\n                                  .index].track_duration_ms\n    new_df = df.drop(\'track_duration_ms\', axis=1)\n    new_test = negative_tracks.append(targets)\n    new_df = pd.concat([new_df.head(1)] * len(new_test))\n    test_playlist_tracks = pd.concat(\n        [new_df.reset_index(drop=True), new_test.reset_index(drop=True)],\n        axis=1).set_index(new_test.index)\n    test_playlist_tracks = (test_playlist_tracks-test_playlist_tracks.mean())/(test_playlist_tracks.std()+1e-8)\n    X_test_refined = X_test_refined.append(test_playlist_tracks)\n    y_prob = pd.DataFrame(\n        lr_clf.predict_proba(test_playlist_tracks),\n        index=test_playlist_tracks.index)\n    y_prob = y_prob.sort_values(by=[1], ascending=False)\n    if len(positive_targets) > 0:\n        r_precisions.append(r_precision(positive_targets, y_prob.index))\n    pbar.set_description("{}".format(np.mean(r_precisions)))```\n\nUnused variables:\n- `pbar`\n- `pid`\n- `labels`\n- `positive_targets`'}, {'reason': 'stop', 'result': 'fruits.index'}, {'reason': 'stop', 'result': 'plt.imshow(test_black)\nplt.show()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def precision(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n    tp = tf.diag_part(conf_matrix)\n    precision_scores = tp/(tp_and_fp)\n    if weights:\n        precision_score = tf.multiply(precision_scores, weights)/tf.reduce_sum(weights)\n    else:\n        precision_score = tf.reduce_mean(precision_scores)        \n    return precision_score\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'y[i]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def vandermonde(gamma, r=0):\n    n = gamma.size()\n    if r == 0:\n        r = 1\n    P = DiagonalPolynomialRing(QQ, n, r, inert=1) \n    X = P.variables()\n    Theta = P.inert_variables()\n    return matrix([[x**i[1]*theta**i[0] for i in gamma.cells()] \n                   for x,theta in zip(X[0],Theta[0])]).determinant()```\n\nUnused variables:\n- `n`\n- `P`'}, {'reason': 'stop', 'result': 'import sys\n\nfrom pnicer import ApparentMagnitudes\nfrom pnicer.utils.auxiliary import get_resource_path\n\n%matplotlib inline```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'fpr, tpr, _ = roc_curve(y_test, prediction[:,1])\nplt.plot(fpr, tpr);```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'l[1] = "p"\nl[2] = "t"\n\nprint(l)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "top_travel_cities = ['Solta', 'Greenville', 'Buenos Aires', 'Los Cabos', 'Walla Walla Valley', 'Marakesh', 'Albuquerque', 'Archipelago Sea', 'Iguazu Falls', 'Salina Island', 'Toronto', 'Pyeongchang']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': '# Import the three supervised learning models from sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nk=999 #random seed\n\n# Initialize the three models\nclf_A = SVC(random_state=k)\nclf_B = KNeighborsClassifier(n_neighbors=5)\nclf_C = RandomForestClassifier(random_state=k)\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\nsamples_1 = int(len(X_train)/100)\nsamples_10 = int(len(X_train)/10)\nsamples_100 = len(X_train)\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)\n\n# Run metrics visualization for the three supervised learning models chosen\nvs.evaluate(results, accuracy, fscore)```\n\nUnused variables:\n- LinearSVC\n- k\n- results'}, {'reason': 'stop', 'result': '# Create overlapping sets\nset1 = set([1,2,3,4,5])\nset2 = set([4,5,6,7,8])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "### Load the images and plot them here.\n### Feel free to use as many code cells as needed.\nimport glob\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2 as cv\n\nfig, axs = plt.subplots(2,3, figsize=(10, 6))\nfig.subplots_adjust(hspace = .2, wspace=.001)\naxs = axs.ravel()\n\nfor i, img in enumerate(glob.glob('./my-signs/*.png')):\n    image = cv.imread(img)\n    axs[i].axis('off')\n    axs[i].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n    image = cv.resize(image, (32,32))\n\nmy_images = np.asarray(my_images)\nprint(my_images.shape)```\n\nUnused variables:\n- `csv`\n- `random`\n- `shuffle`\n- `tensorflow`"}, {'reason': 'stop', 'result': '# Visualize how growth rates change with growth iterations\nplt.scatter(tmap_annotated.obs[\'g0\'],tmap_annotated.obs[\'g1\'])\nplt.xlabel("g0")\nplt.ylabel("g1")\nplt.title("Input vs Output Growth Rates")\nplt.show()```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "data_index = 0\n\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [ skip_window ]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\nprint('data:', [reverse_dictionary[di] for di in data[:8]])\n\nfor num_skips, skip_window in [(2, 1), (4, 2)]:\n    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])```\n\nUnused variables:\n- `data_index` (unused outside the function `generate_batch`)"}, {'reason': 'stop', 'result': "# Selecting certian numerical features\nX_select = X_Num_Cov[space]\n# Dropping the missing values from the feature space\nX_select = X_select.dropna()\n# Creating the output space\nY = X_select['reviews_per_month']\nX_select = X_select.drop(['reviews_per_month'], axis=1)\nprint(X_select.info())\nX_select.head()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "#Find x and y coordinates from Easting and Northing values for the LSOA\na = PM25.attrs['affine']\na = rasterio.Affine.from_gdal(*a)\n~a * (439040, 115775)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'cigar.string'}, {'reason': 'stop', 'result': 'def print_policy(policy, terminal_states):\n    \n    idx_to_symbol = {0:\'\\u2190\', 1:\'\\u2191\', 2:\'\\u2192\', 3:\'\\u2193\'}\n    \n    border_str = "\\u00B7 "\n    for i in range(policy.shape[0]):\n        border_str += "\\u2015 "\n    border_str += "\\u00B7 "\n    \n    for i in range(policy.shape[0]):\n        \n        string = ""\n        for j in range(policy.shape[1]):\n            \n            if (i,j) in terminal_states:\n                string += \'\\u25A0 \'\n            else:\n                string += idx_to_symbol[policy[i, j]]+" "\n        \n        print(string)\n    \n    return\n```\n\nUnused variables:\n- `border_str`'}, {'reason': 'stop', 'result': 'r_precisions = []\nfor pid, df in tqdm(X_test.groupby(["playlist_pid"])):\n    labels = y_test.loc[df.index]  # change y_test to the output probs from clf\n    preds = y_pred.loc[df.index]\n    targets = dataset.loc[labels[labels == 1].index].track_uri\n    predictions = dataset.loc[preds[preds == 1].index].track_uri.unique()\n    if len(targets) > 0:\n        r_precisions.append(r_precision(targets, predictions))\nnp.mean(r_precisions)    ```\n\nUnused variables:\n- pid\n- df'}, {'reason': 'stop', 'result': '```python\n# plot first 3 PCA components\nplot_pts_3d(act_test[:, :3], data_test.gt_patches.flatten(), classes_to_keep, colors, subsample_pct=.0003)\n\nprint("Variance explained by first 3 components: %.2f" % np.sum(pca.explained_variance_ratio_[:3]))\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'learn.recorder.plot_lr(show_moms=True)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg3.fit(X, y2 - tree_reg2.predict(X))```\n\nUnused variables:\n- y3'}, {'reason': 'stop', 'result': 'full_name = full_name.upper()\nprint(full_name)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'stats.ttest_rel(posttest.Q2_ImageType, posttest.Q2_LightType)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'X_test_refined = pd.DataFrame([])\nr_precisions = []\npbar = tqdm(data_test.groupby([\'playlist_pid\']))\nfor pid, df in pbar:\n    labels = y_test.loc[df.index]\n    targets = dataset.loc[labels.index].track_duration_ms\n    positive_targets = dataset.loc[labels[labels == 1].index].index\n    negative_tracks = dataset.loc[X_test[(X_test.playlist_pid != pid)].index].track_duration_ms\n    new_df = df.drop(\'track_duration_ms\', axis=1)\n    new_test = negative_tracks.append(targets)\n    new_df = pd.concat([new_df.head(1)] * len(new_test))\n    test_playlist_tracks = pd.concat(\n        [new_df.reset_index(drop=True), new_test.reset_index(drop=True)],\n        axis=1).set_index(new_test.index)\n    test_playlist_tracks = (test_playlist_tracks-test_playlist_tracks.mean())/(test_playlist_tracks.std()+1e-8)\n    X_test_refined = X_test_refined.append(test_playlist_tracks)\n    y_prob = pd.DataFrame(\n        lr_clf.predict_proba(test_playlist_tracks),\n        index=test_playlist_tracks.index)\n    y_prob = y_prob.sort_values(by=[1], ascending=False)\n    if len(positive_targets) > 0:\n        r_precisions.append(r_precision(positive_targets, y_prob.index))\n    pbar.set_description("{}".format(np.mean(r_precisions)))```\n\nUnused variables:\n- `pbar`\n- `pid`\n- `labels`\n- `positive_targets`'}, {'reason': 'stop', 'result': '```python\nX = np.linspace(-1,1,256)\nXp = np.linspace(-1,1,32)\nplt.plot(0.5*X + 0.5,Hart(X,1)[1],"k-",label= "H = 1")\nplt.plot(0.5*Xp + 0.5,Hart(Xp,1)[1],"b.",label= "H = 1")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "vocabulary_size = 50000\n\ndef build_dataset(words):\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count = unk_count + 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\nprint('Most common words (+UNK)', count[:5])\nprint('Sample data', data[:10])```\n\nUnused variables:\n- `words`"}, {'reason': 'stop', 'result': "# GLOBAL VARIABLES\nBOOKINGS = '/content/gdrive/My Drive/Colab Notebooks/bookings.csv'```\n\nUnused variables:\n- GEO_O"}, {'reason': 'stop', 'result': 'alpha = integral.trapez(u1_z, 0, 0, len(u1_z)-1)\nbeta = integral.trapez(u2_z, 0, 0, len(u2_z)-1)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'cd gd/My Drive\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "```#aggregate mean value for plotting\ndf0_10k_grouped = df0_10k_edit9.groupby(['region']).mean()\n\ndf10_18k_grouped = df10_18k_edit.groupby(['region']).mean()\n\ndf18_32_grouped = df18_32.groupby(['region']).mean()\n\ndf32_grouped = df32.groupby(['region']).mean()\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '#train model on PCA, 2 components\nmodel_2 = RandomForestClassifier().fit(X_train_2d, y_train)\nmodel_2.score(X_test_2d, y_test)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'l = g.layout_circle()\np = plot(g, layout=l, bbox = (300, 300), margin=20)\np```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Reindex the dataframe to make the match with cantons\np3_grant_export_data_reindex = p3_grant_export_data.reset_index(drop=True)\np3_grant_export_data_reindex\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it\n# get an appropriately sized chunk of characters from the text\nstart_inds = [100,200,300,400,500,600,700]\n\n# load in weights\nmodel.load_weights(\'model_weights/best_RNN_small_textdata_weights.hdf5\')\nfor s in start_inds:\n    start_index = s\n    input_chars = text[start_index: start_index + window_size]\n\n    # use the prediction function\n    predict_input = predict_next_chars(model,input_chars,num_to_predict = 100)\n\n    # print out input characters\n    print(\'------------------\')\n    input_line = \'input chars = \' + \'\\n\' +  input_chars + \'"\' + \'\\n\'\n    print(input_line)\n\n    # print out predicted characters\n    line = \'predicted chars = \' + \'\\n\' +  predict_input + \'"\' + \'\\n\'\n    print(line)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '```gROOT.ProcessLine(\'sim0->writeStateToDisk("CalibParams_0.txt")\')```'}, {'reason': 'stop', 'result': '```python\n# TODO: Select three indices of your choice you wish to sample from the dataset\nindices = [1, 271, 413]\n\n# Create a DataFrame of the chosen samples\nsamples = pd.DataFrame(data.loc[indices], columns=data.keys()).reset_index(drop=True)\nsamples.index = indices\nprint("Chosen samples of wholesale customers dataset:")\ndisplay(samples)\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "gROOT.ProcessLine('obj0->findGammaCalibrationConstants()')"}, {'reason': 'stop', 'result': 'img_predictions.info()'}, {'reason': 'stop', 'result': "import os\nos.environ['BUCKET'] = BUCKET\nos.environ['PROJECT'] = PROJECT\nos.environ['REGION'] = REGION```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'from Coverage import Coverage\n\n```\n\nUnused variables:\n- population_coverage'}, {'reason': 'stop', 'result': "X = feature_data[feature_data['season_t'] <= stop_tournament]\nX['season_t'].describe()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# get index of predicted dog breed for each image in test set\nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "```log_aggregated_per_week_df = pd.read_csv('../data/LogsAggregatedData_per_week.csv')\nlog_aggregated_per_week_df.set_index('calendar-year-week', inplace=True)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# t-SNE visualization after PCA\ntsne_all = tsne.fit_transform(act_test[dataset_subset_indices])```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'Chatbot = load_model("Chatbot")'}, {'reason': 'stop', 'result': 's_d.update({A_mm2: 12700.0,\n            I_mm4: 113e6,\n            h_mm: 229,\n            S_mm3: 990e3})```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '### União do Treino e Teste \njoin = pd.concat([train[colunasTeste], test])```\n\nUnused variables:\n- frames'}, {'reason': 'stop', 'result': 'movies_norm = moviesnormal.mean(axis=1).round(decimals=5)\nuser_norm = moviesnormal.mean().round(decimals=5)```\n\nUnused variables:\n- movies_norm'}, {'reason': 'stop', 'result': 'b = 3\nb *= 3\nprint(b)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Import libraries necessary for this project\nimport pandas as pd\n\n# Import supplementary visualizations code visuals.py\nimport visuals as vs\n\n# Load the wholesale customers dataset\ntry:\n    data = pd.read_csv("customers.csv")\n    data.drop([\'Region\', \'Channel\'], axis = 1, inplace = True)\n    print("Wholesale customers dataset has {} samples with {} features each.".format(*data.shape))\nexcept:\n    print("Dataset could not be loaded. Is the dataset missing?")```\n\nUnused variables:\n- numpy (np)\n- IPython.display (display)'}, {'reason': 'stop', 'result': '# ROC\n# order according to increasing score\nscores_auc = [auroc_msr, auroc_margin, auroc_entropy, auroc_dropout, auroc_gmm, auroc_svm, auroc_df]\nfprs = [fpr_msr, fpr_margin, fpr_entropy, fpr_dropout, fpr_gmm, fpr_svm, fpr_df]\ntprs = [tpr_msr, tpr_margin, tpr_entropy, tpr_dropout, tpr_gmm, tpr_svm, tpr_df]\nscores_order = np.argsort(scores_auc)\ncolors_lines = plt.cm.rainbow(np.linspace(0, 1, len(scores_auc)))[:, :3]\n\n# plot\nfig = plt.figure(figsize=(6, 6))\nfor i in scores_order:\n    plt.step(fprs[i], tprs[i], where=\'post\', c=colors_lines[i])\n\nplt.plot([0, 1], [0, 1], \'--\', c=\'gray\')\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.grid(alpha=.3)\nfig.axes[0].spines[\'right\'].set_visible(False)\nfig.axes[0].spines[\'top\'].set_visible(False)\nplt.legend([str.format(\'%s: %.2f\') % (names_methods[i], scores_auc[i]) for i in scores_order], title="AUROC")\nplt.savefig("../Figures/Zurich/Metrics/ROC_pred_ED.pdf", bbox_inches=\'tight\', pad_inches=0)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '# creating the image output directory\nvideo_output_directory = "test_videos_output"\nif not os.path.exists(video_output_directory):\n    os.makedirs(video_output_directory)\n\nwhite_output = \'test_videos_output/solidWhiteRight.mp4\'\n## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n## To do so add .subclip(start_second,end_second) to the end of the line below\n## Where start_second and end_second are integer values representing the start and end of the subclip\n## You may also uncomment the following line for a subclip of the first 5 seconds\n##clip1 = VideoFileClip("test_videos/solidWhiteRight.mp4").subclip(0,5)\n# start_second = 0\n# end_second = 10\n# clip1 = VideoFileClip("test_videos/solidWhiteRight.mp4").subclip(start_second,end_second)\nclip1 = VideoFileClip("test_videos/solidWhiteRight.mp4")\nwhite_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n%time white_clip.write_videofile(white_output, audio=False)```\n\nUnused variables:\n- start_second\n- end_second'}, {'reason': 'stop', 'result': "# TODO: Inverse transform the centers\nlog_centers = pca.inverse_transform(centers)\n\n# TODO: Exponentiate the centers\ntrue_centers = np.exp(log_centers)\n\n# Display the true centers\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "a = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 1].Valid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 1].Valid).tconfint_mean()```\n\nUnused variables:\n- None"}]