[{'reason': 'stop', 'result': 'def plot_accuracy(x, y, x_legend):\n    """Plot accuracy as a function of x."""\n    x = np.array(x)\n    y = np.array(y)\n    plt.title(\'Classification accuracy as a function of %s\' % x_legend)\n    plt.xlabel(\'%s\' % x_legend)\n    plt.ylabel(\'Accuracy\')\n    plt.grid(True)\n    plt.plot(x, y)\n\nrcParams[\'legend.fontsize\'] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\'accuracy_history\'])\n    plot_accuracy(n_examples, accuracy, "training examples (#)")\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc=\'best\')\n\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\'runtime_history\'])\n    plot_accuracy(runtime, accuracy, \'runtime (s)\')\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc=\'best\')\n\n# Plot fitting times\nplt.figure()\nfig = plt.gcf()\ncls_runtime = []\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\'total_fit_time\'])\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\'Vectorization\')\nbar_colors = [\'b\', \'g\', \'r\', \'c\', \'m\', \'y\']\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\'runtime (s)\')\nax.set_title(\'Training Times\')\n\n\ndef autolabel(rectangles):\n    """attach some text vi autolabel on rectangles."""\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width() / 2.,\n                1.05 * height, \'%.4f\' % height,\n                ha=\'center\', va=\'bottom\')\n\nautolabel(rectangles)\nplt.show()\n\n# Plot prediction times\nplt.figure()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\'prediction_time\'])\ncls_runtime.append(parsing_time)\ncls_names.append(\'Read/Parse\\n+Feat.Extr.\')\ncls_runtime.append(vectorizing_time)\ncls_names.append(\'Hashing\\n+Vect.\')\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\nplt.setp(plt.xticks()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\'runtime (s)\')\nax.set_title(\'Prediction Times (%d instances)\' % n_test_documents)\nautolabel(rectangles)\nplt.show()```\n\nUnused variables:\n- `cls_name`\n- `stats`'}, {'reason': 'stop', 'result': 'P = np.diag([500., 49.])\nMs, Ps = run(count=50, R=10, Q=0.01)```\n\nUnused variables:\n- P'}, {'reason': 'stop', 'result': 'barwidth = 0.75\nfig, ax = plt.subplots(figsize=(9, 7))\nrects1 = ax.bar(0.5,SkyPresence.mean(),barwidth,color=sns.xkcd_rgb[\'green\'],yerr=SkyPresenceSEM,ecolor=\'k\',error_kw=dict(lw=3))\nrects2 = ax.bar(1.5,ColorScheme.mean(),barwidth,color=(0.3,0.9,0.3),yerr=ColorSchemeSEM,ecolor=\'k\',error_kw=dict(lw=3))\nrects3 = ax.bar(2.5,TreeFreq.mean(),barwidth,color=(0.15,1,0.15),yerr=TreeFreqSEM,ecolor=\'k\',error_kw=dict(lw=3))\nrects4 = ax.bar(4,ImageType.mean(),barwidth,yerr=ImageTypeSEM,ecolor=\'k\',edgecolor=sns.xkcd_rgb[\'green\'],linewidth = 2,facecolor=\'none\', error_kw=dict(lw=3))\nrects5 = ax.bar(5,FeatureType.mean(),barwidth,yerr=FeatureTypeSEM,ecolor=\'k\',edgecolor=(0.3,0.9,0.3),linewidth = 2,facecolor=\'none\', error_kw=dict(lw=3))\nrects6 = ax.bar(6,LightType.mean(),barwidth,yerr=LightTypeSEM,ecolor=\'k\',edgecolor=(0.15,1,0.15),linewidth = 2, facecolor=\'none\',error_kw=dict(lw=3))\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None)\nax.set_ylim(0,100)\nax.set_xlim(0,7.5)\nax.set_xticklabels((\'SP\',\'CS\',\'TF\',\'IT\',\'FT\',\'LT\'))\nax.set_xticks([0.5 + barwidth/2, 1.5 + barwidth/2, 2.5 + barwidth/2, 4 + barwidth/2, 5 + barwidth/2, 6 + barwidth/2])\nax.set_yticks(np.arange(0, 101, 10))\nplt.title(\'Q2: Rate the Frequency at Which These Perceptual Categories\\nPredicted an Easy/Hard Color-Word Trial\', fontsize=18,fontweight="bold")\nplt.ylabel(\'<-- Less Likely      More Likely -->\', fontsize=17,fontweight="bold")\nplt.xlabel(\'S-C Phase                 S-CT Phase\', fontsize=17,fontweight="bold")\nsns.despine()\n\nplt.show()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '## get unique river segment names\nsegments = data.RiverSeg.unique()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "def define_graph():\n    \n    tf_train_dataset = tf.placeholder(tf.float32, \n                          shape=[None, image_size, image_size, num_channels])\n    tf_train_labels = tf.placeholder(tf.float32, \n                          shape=[None, n_classes])\n    tf_train_labels_cls = tf.argmax(tf_train_labels, dimension=1)\n\n    tf_beta = tf.placeholder(tf.float32)\n    tf_keep_prob = tf.placeholder(tf.float32)\n    tf_learning_rate = tf.placeholder(tf.float32)\n\n    layer_width = {\n        'layer_1': 48,\n        'layer_2': 64,\n        'layer_3': 128,\n        'fc1': 512,\n        'fc2': 512\n    }\n\n    weights = {\n        'layer_1': create_conv_weight(input_feature_map = num_channels, output_feature_map = layer_width['layer_1']\n                                      , filter_height = 3, filter_width = 3, weight_name = 'W_L1'),\n\n        'layer_2': create_conv_weight(input_feature_map = layer_width['layer_1'], output_feature_map = layer_width['layer_2']\n                                      , filter_height = 3, filter_width = 3, weight_name = 'W_L2'),\n\n        'layer_3': create_conv_weight(input_feature_map = layer_width['layer_2'], output_feature_map = layer_width['layer_3']\n                                      , filter_height = 3, filter_width = 3, weight_name = 'W_L3'),\n\n        'fc1': create_fc_weight(input_feature_map = 2048, output_feature_map = layer_width['fc1']\n                                    , weight_name = 'W_F1'),\n\n        'fc2': create_fc_weight(input_feature_map = layer_width['fc1'], output_feature_map = layer_width['fc2']\n                                    , weight_name = 'W_F2'),\n\n        'out': create_fc_weight(input_feature_map = layer_width['fc2'], output_feature_map = n_classes\n                                    , weight_name = 'W_out')\n    }\n\n    biases = {\n        'layer_1': tf.Variable(tf.constant(0.0, shape=[layer_width['layer_1']]), name='b_L1'),\n        'layer_2': tf.Variable(tf.constant(0.0, shape=[layer_width['layer_2']]), name='b_L2'),\n        'layer_3': tf.Variable(tf.constant(0.0, shape=[layer_width['layer_3']]), name='b_L3'),\n        'fc1': tf.Variable(tf.constant(0.0, shape=[layer_width['fc1']]), name='b_F1'),\n        'fc2': tf.Variable(tf.constant(0.0, shape=[layer_width['fc2']]), name='b_F2'),\n        'out': tf.Variable(tf.constant(0.0, shape=[n_classes]), name='b_out')\n    }\n\n    # Layer 1\n    conv1 = conv2d(tf_train_dataset, weights['layer_1'], biases['layer_1'], is_relu = True)\n    conv1 = maxpool2d(conv1)\n\n    # Layer 2\n    conv2 = conv2d(conv1, weights['layer_2'], biases['layer_2'], is_relu = True)\n    conv2 = maxpool2d(conv2)\n\n    # Layer 3\n    conv3 = conv2d(conv2, weights['layer_3'], biases['layer_3'], is_relu = True)\n    conv3 = maxpool2d(conv3)\n    conv3_drop = tf.nn.dropout(conv3, tf_keep_prob)\n\n    # Flatten\n    flat, num_fc_layers = flatten_layer(conv3_drop)\n\n    # Fully connected layer\n    fc1 = tf.add(tf.matmul(flat, weights['fc1']), biases['fc1'])\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, tf_keep_prob)\n\n    fc2 = tf.add(tf.matmul(fc1, weights['fc2']), biases['fc2'])\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, tf_keep_prob)\n\n    # Output Layer - class prediction\n    logits = tf.add(tf.matmul(fc2, weights['out']), biases['out'], name='logits')\n    train_prediction = tf.nn.softmax(logits) \n\n    regularizers = (tf.nn.l2_loss(weights['layer_1']) \n                + tf.nn.l2_loss(weights['layer_2']) \n                + tf.nn.l2_loss(weights['layer_3']) \n                + tf.nn.l2_loss(weights['fc1'])\n                + tf.nn.l2_loss(weights['fc2'])\n                + tf.nn.l2_loss(weights['out']))\n\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n                                                    labels=tf_train_labels)\n\n    loss = tf.reduce_mean(cross_entropy) + tf_beta*regularizers \n    tf.summary.scalar('loss', loss) \n\n    with tf.name_scope('SGD'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=tf_learning_rate).minimize(loss) #AdamOptimizer #GradientDescentOptimizer\n\n    with tf.name_scope('accuracy'):\n        with tf.name_scope('correct_prediction'):\n\n          labels_pred_cls = tf.argmax(train_prediction,dimension = 1) \n          correct_prediction = tf.equal(labels_pred_cls, tf_train_labels_cls)\n\n        with tf.name_scope('accuracy'):\n          accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n          tf.summary.scalar('accuracy', accuracy_operation) \n\n    summary_op = tf.summary.merge_all()\n    \n    return tf_train_dataset, tf_train_labels, tf_train_labels_cls, tf_beta, tf_keep_prob, tf_learning_rate, weights, biases,conv1, conv2, conv3, flat, fc1, fc2, logits, train_prediction, regularizers,  cross_entropy, loss, optimizer, labels_pred_cls, correct_prediction, accuracy_operation, accuracy_operation, summary_op  \n```"}, {'reason': 'stop', 'result': "a = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 2].Valid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 2].Valid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'rspEwidth=0.5\nrspLwidth=0.03\n```\n\nUnused variables:\n- rspEmin\n- rspEmax\n- rspLmin\n- rspLmax'}, {'reason': 'stop', 'result': "# split out the test and train sets again\ntrain = data_full.ix['train']\ntest = data_full.ix['test']```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "```python\n# TODO: Apply PCA by fitting the good data with only two dimensions\npca = None\n\n# TODO: Transform the good data using the PCA fit above\nreduced_data = None\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n```\n\nUnused variables:\n- pca_samples"}, {'reason': 'stop', 'result': 'X_test, y_test = prepare_batch(flower_paths_and_classes_test, batch_size=len(flower_paths_and_classes_test))```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "print(root_directory)\npaths = {'user_id': user_id, \n         'workspace_directory': root_directory + '/workspaces', \n         'resource_directory': root_directory + '/resources'}```\n\nUnused variables:\n- log_directory\n- test_data_directory"}, {'reason': 'stop', 'result': 'import os\nos.listdir("test_images/")```\n\nUnused variables:\n- files'}, {'reason': 'stop', 'result': '# get indices of correctly / incorrectly predicted pixels\n# with error in known classes\n# only known classes (novely detection)\n# get indices of correctly / incorrectly predicted pixels\n# with error in known classes\n# only known classes (novely detection)\n\npred_t_tr = (data_train.gt_patches != 0) & (data_train.gt_patches == y_pred_label_tr)\npred_f_tr = (data_train.gt_patches != 0) & (data_train.gt_patches != y_pred_label_tr)\n\npred_t_val = (data_val.gt_patches != 0) & (data_val.gt_patches == y_pred_label_val)\npred_f_val = (data_val.gt_patches != 0) & (data_val.gt_patches != y_pred_label_val)\n\npred_t_te = (data_test.gt_patches != 0) & (data_test.gt_patches == y_pred_label_te)\npred_f_te = (data_test.gt_patches != 0) & (data_test.gt_patches != y_pred_label_te)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "from docplex.mp.model import Model\n\ntm = Model(name='transportation')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "from keras.applications.resnet50 import ResNet50\n\n# define ResNet50 model\nResNet50_model = ResNet50(weights='imagenet')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'data = np.array([[1., 2., 0.],\n                 [0., 0., 0.],\n                 [2., 2., 0.]])\ncentroids = np.array([[0.5, 0.5, 0.],\n                      [0., -0.5, 0.]])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': '```python\n# LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_lda = lda.fit(X_iris, y).transform(X_iris)\n\n## PCA See no need to use labels\npca = sklearnPCA(n_components=2)\nX_pca = pca.fit_transform(X_iris)\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'data = ["acacag", "acacgg", "aaaaaacgg"]\nClusteringSeq(data)```\n\nUnused variables:\n- c'}, {'reason': 'stop', 'result': 'import tensorflow as tf\nimport numpy as np\nfrom sklearn.utils import shuffle\n\nX_train_normal = np.array(X_train/255 - 0.5)\nX_valid_normal = np.array(X_valid/255 - 0.5)\nX_test_normal = np.array(X_test/255 - 0.5)\n\nEPOCHS = 15\nBATCH_SIZE = 128\n```\n\nUnused variables:\n- `tf`\n- `shuffle`'}, {'reason': 'stop', 'result': "df_stats = pd.read_csv(msig.training_stats_filename)\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n# span = 111//30 = 3\nspan = epochs // 100\n\nloss_dict = {\n    'loss': df_stats.loss,\n    'val_loss': df_stats.val_loss,\n}\n\nalphas = {\n    'loss': 0.3,\n    'val_loss': 0.3,\n}\n\nlegend_labels = []\nfor key, value in loss_dict.items():\n    ax1.plot(value, alpha=alphas[key])\n    legend_labels.append(key)\n\nax1.set_title(r'timestamps = {}, window_size = {}'.format(msig.n_timestamps, msig.window_size))\nax1.set_xlabel(r'epoch')\nax1.set_xlim((0, len(df_stats.acc)))\nax1.set_ylabel(r'loss')\nax1.set_ylim((0, None))\nax1.grid(True)\nax1.legend(legend_labels)\n\nacc_dict = {\n    'acc': df_stats.acc,\n    'val_acc': df_stats.val_acc,\n}\n\nalphas = {\n    'acc': 0.3,\n    'val_acc': 0.3,\n}\n\nlegend_labels = []\nfor key, value in acc_dict.items():\n    ax2.plot(value, alpha=alphas[key])\n    legend_labels.append(key)\n\nax2.set_title(r'neurons = {}, batch_size = {}'.format(n_neurons, batch_size))\nax2.set_xlabel(r'epoch')\nax2.set_xlim((0, len(df_stats.acc)))\nax2.set_ylabel(r'accuracy')\nax2.set_ylim((None, 1))\nax2.grid(True)\nax2.legend(legend_labels)\n\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'loss_accuracy.png'))\nplt.show()```\n\nUnused variables:\n- loss (hw)\n- val_loss (hw)\n- acc (hw)\n- val_acc (hw)"}, {'reason': 'stop', 'result': '# kernel PCA\ngamma = 100\nsq_x  = -2 * X @ X.T + np.sum(X**2,1) + np.sum(X**2,1,keepdims=True)\nK     = np.exp(-gamma * sq_x)\nN     = K.shape[0]\none_n = np.ones((N,N)) / N\nK2    = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\neigvals, eigvecs = np.linalg.eigh(K2)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# 2 1d ODEs\ndef ode(state,t):\n    # x is the first component of state vector\n    x = state[0]\n\n    # Compute state derivative\n    dx = .4* np.square(x) - 2\n\n    # Return the state derivative\n    return [dx]```\n\nUnused variables:\n- y'}, {'reason': 'stop', 'result': 'import warnings\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\n\n# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(X, arima_order):\n    # prepare training dataset\n    train = X.loc[:39,[\'client-ip-unique-count\']]\n    test= X.loc[40:,[\'client-ip-unique-count\']]\n    \n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    model = sm.tsa.ARIMA(train, order=arima_order).fit()\n    predictions = model.predict(40,42,dynamic=True)\n    error = mean_absolute_error(test, predictions)\n    return error\n\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype(\'float32\')\n    best_score, best_cfg = float("inf"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mae = evaluate_arima_model(dataset, order)\n                    if mae < best_score:\n                        best_score, best_cfg = mae, order\n                    print(\'ARIMA%s MAE=%.3f\' % (order,mae))\n                except:\n                    continue\n    print(\'Best ARIMA%s MAE=%.3f\' % (best_cfg, best_score))\n\n\n# load dataset\np_values = [0, 1, 2, 4,6,8,10]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings("ignore")\nevaluate_models(log_and_placements_aggregated_per_week_df.loc[:,[\'client-ip-unique-count\']], p_values, d_values, q_values)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Valid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Valid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '```python\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super(CustomSchedule, self).__init__()\n    \n    self.d_model = d_model\n    self.warmup_steps = warmup_steps\n    \n  def __call__(self, step):\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n    \n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n  \nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)\n```\n\nUnused variables:\n- `arg1`\n- `arg2`'}, {'reason': 'stop', 'result': 'four_x_cubed_plus_eleven_x_squared = [(4, 3), (11, 2)]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "# Load pickled data\nimport pickle\n\n# TODO: Fill this in based on where you saved the training and testing data\ntraining_file = '../traffic-signs-data/train.p'\nvalidation_file = '../traffic-signs-data/valid.p'\ntesting_file = '../traffic-signs-data/test.p'\n\nwith open(training_file, mode='rb') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode='rb') as f:\n    test = pickle.load(f)\n    \nX_train, y_train = train['features'], train['labels']\nX_valid, y_valid = valid['features'], valid['labels']\n\nX_test, y_test = test['features'], test['labels']```\n\nUnused variables: \n- csv\n- matplotlib.pyplot as plt\n- numpy as np\n- cv2 as cv\n- random\n- sklearn.utils.shuffle\n- tensorflow as tf\n- tensorflow.contrib.layers.flatten"}, {'reason': 'stop', 'result': 'x, y = util.get_sample_classification_data()\nx_new = np.linspace(-15, 15, 100)\nutil.scatter_raw_data_classification(x, y, y_label = "Class")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "fig, ax = plt.subplots(figsize=(8, 6))\n\nax.hist(sunspot_df['sunspot.year'].values, bins=40, normed=True, lw=0, alpha=0.75);\nax.plot(x_plot, post_pmfs.mean(axis=0),\n        c='k', label='Posterior expected density');\nax.plot(x_plot, (trace['w'][:, np.newaxis, :] * post_pmf_contribs).mean(axis=0)[:, 0],\n        '--', c='k', label='Posterior expected\\nmixture components\\n(weighted)');\nax.plot(x_plot, (trace['w'][:, np.newaxis, :] * post_pmf_contribs).mean(axis=0),\n        '--', c='k');\n\nax.set_xlabel('Yearly sunspot count');\nax.set_yticklabels([]);\nax.legend(loc=1);```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "# reload(accuracy)\nreload(slope)\nplt.close()\nf = plt.figure(figsize=(5, 4))\n\nle = .12\nre = .02\nte = .1\nbe = .11\nh_gap = .13\n\nw = .5\nh = 1. - te - be\n\nax_lines = f.add_axes([le, be, w, h])\nax_slopes = f.add_axes([le + w + h_gap, be, 1. - w - h_gap - le - re, h])\n\nkey = fracs[-1]\n\nslope.plot_cv_slope(subjects, deep_all, linear_all, chance[0], training_size, fracs, (ax_lines, ax_slopes),\n                    legend=True, normalize_chance=False)\n\nx0 = .05\ny0 = 1. - te + .02\nx1 = le + w + h_gap - .075\n\nf.text(x0, y0, 'A', **letter_fontstyle)\nf.text(x1, y0, 'B', **letter_fontstyle)\n\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.eps'), dpi=300)\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.png'), dpi=300)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# turn into 1-d arrays, and only use values > 0 (sometimes no-value data is very large negative number)\narray1 = array_1[array_1>0]\narray2 = array_2[array_1>0]\narray3 = array_3[array_1>0]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'flux, flux_err = coco.simulate(b"SN2007uy", \n                    z_obs, 0.0, 0.0, 0.0, 3.1, \n                    mjdmax, mjd_to_sim, \n                    filters_to_sim)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "def character_quotient(M, N, n, r, left_basis=s, right_basis=s):\n    b_tot = M.basis()\n    b_ideal = N.basis()\n    charac = 0\n    q = PolynomialRing(QQ,'q',r).gens()\n    \n    for nu in Partitions(n):\n        basis_nu_tot = {}\n        basis_nu_ideal = {}\n        charac_nu = 0\n        # Get the nu_isotypic part of the bases\n        for key, value in b_tot.iteritems():\n            if Partition(key[1]) == nu:\n                basis_nu_tot[key[0]] = value\n        for key, value in b_ideal.iteritems():\n            if Partition(key[1]) == nu:\n                basis_nu_ideal[key[0]] = value\n                \n        # Use the degrees to compute the character\n        for deg, b in basis_nu_tot.iteritems():\n            charac_nu += sum(prod(q[i]**deg[i] for i in range(0,len(deg))) for p in b)\n        for deg, b in basis_nu_ideal.iteritems():\n            charac_nu -= sum(prod(q[i]**deg[i] for i in range(0,len(deg))) for p in b)\n        if charac_nu != 0 :\n            if left_basis == s :\n                charac_nu = s.from_polynomial(charac_nu).restrict_partition_lengths(r,exact=False)           \n            else:\n                charac_nu = left_basis.from_polynomial(charac_nu)      \n            # Make the tensor product with s[nu]\n            charac += tensor([charac_nu, right_basis(s(nu))])\n            \n    return charac\n```\n\nUnused variables: `M, N, r`"}, {'reason': 'stop', 'result': '#plot important features except for playlist_pid\nfig, ax = plt.subplots(1,1, figsize = (20,15))\ny_pos = np.arange(len(not_pid_features) - 3)\nax.barh(not_pid_features[3:], not_pid_importances[3:], align = "center", color = "mediumspringgreen")\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\nax.grid(alpha = 0)\nax.set_xlabel("Importance", fontsize = 20)\nax.set_ylabel("Features", fontsize = 20)\nax.yaxis.set_tick_params(labelsize=12)\nax.xaxis.set_tick_params(labelsize=15)\n\nfig.savefig("Feature Importance.png", bbox_inches="tight")```\n\nUnused variables:\n- y_pos'}, {'reason': 'stop', 'result': '# Separa o dataset em conjunto treino e teste\ntrain_data = keystrokes[:split_data_idx]\ntest_data = keystrokes[split_data_idx:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "ex = flor.Experiment('risecamp_demo').__enter__()"}, {'reason': 'stop', 'result': 'transformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, dropout_rate)\n\nprint(f"""這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\nd_model: {d_model}\nnum_heads: {num_heads}\ndff: {dff}\ninput_vocab_size: {input_vocab_size}\ntarget_vocab_size: {target_vocab_size}\ndropout_rate: {dropout_rate}\n\n""")'}, {'reason': 'stop', 'result': 'model_lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1],\n                     max_iter = 50000, cv = 10).fit(X_train, y)\n# Coefficients of each predictor:\ncoef_lasso = pd.Series(model_lasso.coef_, index = X_train.columns)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Set up parameters:    \nk_size = 3\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline(image, \n                                   k_size = k_size,\n                                   low_thresh = low_thresh,\n                                   high_thresh = high_thresh,\n                                   L2gradient = L2gradient,\n                                   rho = rho,\n                                   theta = theta,\n                                   min_votes = min_votes,\n                                   min_line_len = min_line_len,\n                                   max_line_gap = max_line_gap,\n                                   angle = angle,\n                                   angle_thresh = angle_threshold,\n                                   debug = False)\n    return result\n```\n\nUnused variables:\n- vertex_ratio_h\n- vertex_ratio_v'}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_lm, label='AER-LM 3.2')\nplt.plot(f/3e10, y_lm_36, label='AER-LM 3.6')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables:\n- l"}, {'reason': 'stop', 'result': '##  Define existing output location\n##  i.e. Y:\\LRMF\\graphics\\Columbia\noutput = input("Please provide a full-path output directory:")\n```'}, {'reason': 'stop', 'result': 'dcmdmi = (data4.semi_major_axis - com) / mtot\ndcmdri = data4.mass / mtot\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# Code specific to window_type == sliding\n\nif msig.sequence_type == 'many2many':\n    y_score_mean = y_score.sum(axis=1) / y_score.shape[1]\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\n    y_score_unshifted_clipped_mean = y_score_unshifted_clipped.sum(axis=1) / y_score.shape[1]\nelse:\n    y_score_mean = y_score.sum(axis=1) / y_score.shape[1]\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\n    y_score_unshifted_clipped_mean = y_score_unshifted_clipped.sum(axis=1) / y_score.shape[1]```\n\nUnused variables:\n- j\n- s0\n- s1\n- y_hat\n- y_true\n- y_true_value\n- y_pred\n- y_pred_value\n- y_penalty\n- np.min(y_score)\n- np.max(y_score)"}, {'reason': 'stop', 'result': 'seed = ["Hello World"]\ncgi_runner = FunctionCoverageRunner(cgi_decode)\nm = MutationCoverageFuzzer(seed)\nresults = m.runs(cgi_runner, 10000)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def bow_lights_lt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    """\n    white = (255, 255, 255)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 201)\n        tall_mh_height = taller_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(56)\n        running_light_dist_vert = np.random.randint(tall_mh_height, tall_mh_height + 51)\n        new_array[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_array[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_array[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_array = new_array.flatten()\n        all_bow_images[i] = new_array\n    \n    return all_bow_images\n```\n\nUnused variables:\n- black\n- red\n- green'}, {'reason': 'stop', 'result': '#dataframe om alle gediplomeerden in Categorie "Meerdere" te selecteren\ndf_me = output.loc[output[\'Categorie\'] == \'Meerdere\']\ndf_gme = df_me.groupby([\'Gemeente\']).sum()[[\'2013   AANT\',\'2014   AANT\',\'2015   AANT\',\'2016   AANT\',\'2017   AANT\']]'}, {'reason': 'stop', 'result': 'U, Sigma, Vt = np.linalg.svd(norm_keystrokes)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_nolm_aer, label='AER')\nplt.plot(f/3e10, y_nolm_aer_arts, label='AER_ARTS')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables:\n- l"}, {'reason': 'stop', 'result': 'import math\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef filterFn(alpha, prev, new):\n    return alpha * new + (1 - alpha) * prev\n\ndef clearUnjitter():\n    global previous\n    previous = {\n        "right": [0, 0, 0, 0], # x1, y1, x2, y2\n        "left": [0, 0, 0, 0]\n    }\n\ndef unjitter(side, value):\n    global previous\n    alpha = 0.6\n    # destructure value\n    x1 = value[0]\n    y1 = value[1]\n    x2 = value[2]\n    y2 = value[3]\n    # set start value on first frame\n    if previous[side][0] == 0:\n        previous[side][0] = x1\n    if previous[side][1] == 0:\n        previous[side][1] = y1\n    if previous[side][2] == 0:\n        previous[side][2] = x2\n    if previous[side][3] == 0:\n        previous[side][3] = y2\n    # calculate filtered results\n    x1 = filterFn(alpha, previous[side][0], x1)\n    y1 = filterFn(alpha, previous[side][1], y1)\n    x2 = filterFn(alpha, previous[side][2], x2)\n    y2 = filterFn(alpha, previous[side][3], y2)\n    # set new values to previous\n    previous[side][0] = x1\n    previous[side][1] = y1\n    previous[side][2] = x2\n    previous[side][3] = y2\n    return [x1, y1, x2, y2]\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=5):\n    global previous\n    sides = {\n        "left": [],\n        "right": []\n    }\n    yMin = img.shape[0]\n    xHalf = img.shape[1] / 2\n    yMax = 315\n    drawn = []\n    if lines != None:\n        for line in lines:\n            for x1,y1,x2,y2 in line:\n                slope = ((y2 - y1) / (x2 - x1))\n                if slope > 0.50 and slope < 0.8: # Right line, because of positive slope (y-reversed)\n                    sides["right"].append([x1,y1,x2,y2]) \n                elif slope < -0.50 and slope > -0.8: # Left line because of negative slope\n                    sides["left"].append([x1,y1,x2,y2])\n    yHalf = None\n    for side in sides:\n        avgSlope = None\n        totalSlope = 0\n        totalWeight = 0\n        xAvg = None\n        yAvg = None\n        for x1,y1,x2,y2 in sides[side]:\n            slope = (y2 - y1) / (x2 - x1)\n            length = math.sqrt(abs(x2-x1)^2+abs(y2 - y1)^2)\n            if xAvg == None:\n                xAvg = (x1 + x2) / 2\n                yAvg = (y1 + y2) / 2\n            else:\n                xAvg = (xAvg + ((x1 + x2) / 2)) / 2\n                yAvg = (yAvg + ((y1 + y2) / 2)) / 2\n            totalSlope += slope * length\n            totalWeight += length\n        if totalWeight > 0:\n            avgSlope = totalSlope / totalWeight\n        if avgSlope != None and xAvg != None and yAvg != None:\n            yIntercept = -(avgSlope * xAvg) + yAvg\n            xMax = (yMax - yIntercept) / avgSlope\n            xMin = (yMin - yIntercept) / avgSlope\n            if side == "right":\n                offset = 20\n            else:\n                offset = -20\n            _yHalf = avgSlope * (xHalf + offset) + yIntercept \n            if yHalf == None:\n                yHalf = _yHalf\n            else:\n                xHalf = ((yHalf - yIntercept) / avgSlope) - offset\n            points = unjitter(side, [xMin, yMin, (xHalf + offset), yHalf])\n            cv2.line(img, (int(points[0]), int(points[1])), (int(points[2]), int(points[3])), color, thickness)\n        else:\n            points = unjitter(side, previous[side])\n            cv2.line(img, (int(points[0]), int(points[1])), (int(points[2]), int(points[3])), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((*img.shape, 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': "RTanalysis = pd.DataFrame()\nlists = [[] for _ in range(0,5)]\n\nfor ID in range(10,86):\n    sub = cdat[cdat.subject == ID]\n    lists[0].append(ID)\n    validRT_trials = sub[sub.TrialType == 'Valid'].RT.mean()\n    invalidRT_trials = sub[sub.TrialType == 'Invalid'].RT.mean()\n    lists[1].append(validRT_trials)\n    lists[2].append(invalidRT_trials)\n    \nRTanalysis['SubjectID'] = lists[0]\nRTanalysis['Valid'] = lists[1]\nRTanalysis['Invalid'] = lists[2]```\n\nUnused variables:\n- `lists`"}, {'reason': 'stop', 'result': 'print(\'Loading case data ...\')\n\ncases_bestfit_10000          = pd.read_csv("pancancer_case_features_bestfit_10000_topgenes_2000.csv")\ncases_bestfit_15000          = pd.read_csv("pancancer_case_features_bestfit_15000_topgenes_3000.csv")\ncases_bestfit_15000_allgenes = pd.read_csv("pancancer_case_features_bestfit_15000_topgenes_None.csv")\ncases_allgenes               = pd.read_csv("pancancer_case_features_all.csv")\nall_data = {\n    \'best_fit_10000\':          getDataAndLabels(cases_bestfit_10000),\n    \'best_fit_15000\':          getDataAndLabels(cases_bestfit_15000),\n    \'best_fit_15000_allgenes\': getDataAndLabels(cases_bestfit_15000_allgenes),\n    \'genes_all\':               getDataAndLabels(cases_allgenes)\n}\nprint("done.")```\n\nUnused variables:\n- cases_800\n- cases_1000\n- cases_1500'}, {'reason': 'stop', 'result': "for number in range(____,____,____):\n    square = number * number\n    cube = number * number * number\n    print('{0} squared is {1} and cubed is {2}.'.format(number, square, cube))```\n\nUnused variables:\n- square\n- cube"}, {'reason': 'stop', 'result': "x_test, y_test = msig.generate(sequence_code='1tf_1tc')\nscore = model.evaluate(x_test, y_test, batch_size=batch_size)\ny_hat = model.predict(x_test, batch_size=batch_size)\n\ny_true = np.argmax(y_test, axis=-1)\ny_pred = np.argmax(y_hat, axis=-1)\n\ny_correct = (y_pred == y_true) * 1\ny_fail = (y_pred != y_true) * 1\n\nprint('x_test  {}'.format(x_test.shape))\nprint('y_test  {}'.format(y_test.shape))\nprint('y_true  {}'.format(y_true.shape))\nprint(score)\nprint('y_hat   {}'.format(y_hat.shape))\nprint('y_pred  {}'.format(y_pred.shape))\n\nprint(y_hat[0, :4])\nprint(y_true[0, :10])\nprint(y_pred[0, :10])\nprint(y_fail[0, :10])\n\ni_fail = np.where(y_fail)[1]\nprint(i_fail.shape)\n\ny_true_colors = np.hstack([msig.waves[i].color for i in y_true[0]])\ny_pred_colors = np.hstack([msig.waves[i].color for i in y_pred[0]])\n\nprint('y_pred_colors {}'.format(y_pred_colors.shape))"}, {'reason': 'stop', 'result': '#plot\n\nfig, ax = plt.subplots(1,1, figsize = (20,15))\ny_pos = np.arange(len(feature_names))\nax.barh(feature_names, top50importance, align = "center", color = "mediumspringgreen")\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\nax.grid(alpha = 0)\nax.set_xlabel("Importance", fontsize = 20)\nax.set_ylabel("Features", fontsize = 20)\nax.yaxis.set_tick_params(labelsize=12)\nax.xaxis.set_tick_params(labelsize=15)\n\nfig.savefig("Feature Importance.png", bbox_inches="tight")```\n\nUnused variables: `y_pos`'}, {'reason': 'stop', 'result': 'import numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.datasets import fetch_mldata\nmnist = fetch_mldata(\'MNIST original\')\n\nX, y = mnist["data"], mnist["target"]\nshuffle_index = np.random.permutation(70000)\nX, y = X[shuffle_index], y[shuffle_index]\nX_train60, X_test, y_train60, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\nX_train, y_train = X_train60[:5000], y_train60[:5000]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Re-run the model with the Bib numbers as a feature and for the 5K split times to predict 10K time\n\n### set up data for modeling\nX_10K = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\']]\ny_10K = boston_clean[\'10K Duration\'].values.reshape(-1, 1)\nprint(X_10K.shape, y_10K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_10K, X_test_10K, y_train_10K, y_test_10K = train_test_split(X_10K, y_10K, random_state=29)\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_10K = LinearRegression()\nmodel_10K.fit(X_train_10K, y_train_10K)\n\n# Make predictions\n\npredictions_10K = model_10K.predict(X_test_10K)\n\n# Plot the residuals\n\nplt.scatter(model_10K.predict(X_train_10K), model_10K.predict(X_train_10K) - y_train_10K, c="blue", label="Training Data")\nplt.scatter(model_10K.predict(X_test_10K), model_10K.predict(X_test_10K) - y_test_10K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_10K.min(), xmax=y_test_10K.max())\nplt.title("Residual Plot 10K")\nplt.savefig(\'model_10k.png\')\nplt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "models = {}\nfor l in labels:\n    models[l] = build_model()\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=400)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'def broad_less_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible starboard orientation.\n    """\n    white = (255, 255, 255)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        masthead_light = np.random.randint(50, 201)\n        mh_horiz = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        masthead_height = masthead_light + light_width\n        masthead_width = mh_horiz + light_width\n        running_light_start = masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if mh_horiz < 2 * light_width:\n            running_light_loc = np.random.randint(mh_horiz - 20, mh_horiz + 21)\n        else:\n            running_light_loc = np.random.randint(mh_horiz - 20, 211)\n        running_light_area = running_light_loc + light_width\n        new_view[masthead_light:masthead_height, mh_horiz:masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```\n\nUnused variables:\n- black'}, {'reason': 'stop', 'result': 'from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n\n# TODO: Make a copy of the DataFrame, using the \'drop\' function to drop the given feature\nnew_data = data.drop("Fresh", axis=1)\n# TODO: Split the data into training and testing sets(0.25) using the given feature as the target\n# Set a random state.\nX_train, X_test, y_train, y_test = train_test_split(data, new_data, test_size = 0.25, random_state = 34)\n\n\n\n# TODO: Create a decision tree regressor and fit it to the training set\nregressor = DecisionTreeRegressor()\nregressor_fit = regressor.fit(X_train, y_train)\n\n# TODO: Report the score of the prediction using the testing set\nscore = regressor.score(X_test, y_test)\nprint(score)```\n\nUnused variables:\n- make_scorer'}, {'reason': 'stop', 'result': '```python\ndef importInflunet(path):\n    \'\'\'\n    Reads the Influnet data and creates a unique multiindex dataframe of the format\n    \n    (year,week) - incidence\n    \n    :param path: location of the influnet folder\n    :return: compacted version of \n    \'\'\'\n    \n    df = pd.concat([pd.read_csv(path+t, names=["time", "incidence"], sep=" ", header=1, usecols=[0,4], decimal=",") for t in listdir(path)], ignore_index=True)\n    df[["year","week"]] = df["time"].str.split("-", expand=True).astype(int)\n    df.drop(["time"], axis=1, inplace=True)\n    df = df.set_index(["year","week"])\n    df.sortlevel(inplace=True)\n    df = df.astype(float)\n    df = df.loc[2008:]\n    return df\n\ndef padInflunet(aux, year):\n    \'\'\'\n    The influnet dataset lacks information about the weeks that do not belog to the flu season (usally, but not necessarly, from week 17 to 40).\n    This functions fills the dataset with empty position in order to match the wikipedia format.\n    \n    :param aux: Influnet dataframe from a specific year\n    :param year: year of the previous Influnet dataframe\n    :return: padded version of the original dataframe\n    \'\'\'\n    year_weeks = aux.index.values[-1]\n    week_range = range(1,year_weeks+1)\n    aux = aux.reindex(week_range, fill_value=0)\n    aux["year"] = year\n    aux["week"] = week_range\n    \n    aux.set_index([\'year\', \'week\'], append=False, inplace=True)\n    return aux\n\n\ndef getInflunet(path = "/home/aalto/Desktop/DE/hw2/influnet/data/"):\n    \'\'\'\n    import and reformat the original Influnet dataset\n    \n    :param path: \n    :return: clean and padded version of the Influnet dataset\n    \'\'\'\n    \n    df = importInflunet(path);\n    previous = None\n    for x,y in df.index.values:\n        if previous == None:\n            df2 = reindexDF(df.loc[x], x)\n        elif x != previous:\n            df2 = df2.append(reindexDF(df.loc[x], x))\n        previous = x\n    return df\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "```python\n# TODO: Apply PCA by fitting the good data with only two dimensions\npca = PCA(n_components=2)\npca.fit(good_data)\n\n# TODO: Transform the good data using the PCA fit above\nreduced_data = pca.transform(good_data)\n\n# TODO: Transform log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns=['Dimension 1', 'Dimension 2'])\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "melted_stats = pd.melt(stats.reset_index(), id_vars='time', var_name='LSOA').dropna()```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'workspace_alias = ekos.get_alias_for_unique_id(workspace_unique_id=workspace_uuid)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "x = sy.Symbol('x')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "SkyPresence = posttest.groupby(['subjID'])['Q2_SceneSkyPresence'].mean()\nSkyPresenceSEM = pd.Series.std(SkyPresence) / n\nColorScheme = posttest.groupby(['subjID'])['Q2_SceneColorScheme'].mean()\nColorSchemeSEM = pd.Series.std(ColorScheme) / n\nTreeFreq = posttest.groupby(['subjID'])['Q2_SceneTreeFrequency'].mean()\nTreeFreqSEM = pd.Series.std(TreeFreq) / n\n\nImageType = posttest.groupby(['subjID'])['Q2_ImageType'].mean()\nImageTypeSEM = pd.Series.std(ImageType) / n\nFeatureType = posttest.groupby(['subjID'])['Q2_FeatureType'].mean()\nFeatureTypeSEM = pd.Series.std(FeatureType) / n\nLightType = posttest.groupby(['subjID'])['Q2_LightType'].mean()\nLightTypeSEM = pd.Series.std(LightType) / n\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'X_test_t = (torch.from_numpy(X_test).float().transpose(1,3)).transpose(2,3)\ny_test_t = torch.from_numpy(y_test).long()\n\ntest_data = torch.utils.data.TensorDataset(X_test_t, y_test_t)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=True)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '#InvalidRTCI\nRTanalysis.Invalid.mean()\nsms.DescrStatsW(RTanalysis.Invalid).tconfint_mean()```\n\nUnused variables:\n- a'}, {'reason': 'stop', 'result': 'init = tf.global_variables_initializer()'}, {'reason': 'stop', 'result': '# create hyperameter and search criteria lists (ranges are inclusive..exclusive))\nhyper_params_tune = {\'max_depth\' : list(range(new_min,new_max+1,1)),\n                \'sample_rate\': [x/100. for x in range(20,101)],\n                \'col_sample_rate\' : [x/100. for x in range(20,101)],\n                \'col_sample_rate_per_tree\': [x/100. for x in range(20,101)],\n                \'col_sample_rate_change_per_level\': [x/100. for x in range(90,111)],\n                \'min_rows\': [2**x for x in range(0,int(math.log(train.nrow,2)-1)+1)],\n                \'nbins\': [2**x for x in range(4,11)],\n                \'nbins_cats\': [2**x for x in range(4,13)],\n                \'min_split_improvement\': [0,1e-8,1e-6,1e-4],\n                \'histogram_type\': ["UniformAdaptive","QuantilesGlobal","RoundRobin"]}\nsearch_criteria_tune = {\'strategy\': "RandomDiscrete",\n                   \'max_runtime_secs\': 3600,  ## limit the runtime to 60 minutes\n                   \'max_models\': 100,  ## build no more than 100 models\n                   \'stopping_rounds\' : 5,\n                   \'stopping_metric\' : "AUC",\n                   \'stopping_tolerance\': 1e-3\n                   }```\n\nUnused variables: `seed`'}, {'reason': 'stop', 'result': 'fig, ax = plt.subplots(figsize=(16,8))\nbp = dfBabies.boxplot(column="weight", by="smoke", ax=ax, return_type="dict")\n\nfor column in bp:\n    for box in column[\'boxes\']:\n        box.set(color=\'steelblue\', linewidth=2)\n    \n    for whisker in column[\'whiskers\']:\n        whisker.set(color=\'gray\', linewidth=2)\n\n    for cap in column[\'caps\']:\n        cap.set(color=\'gray\', linewidth=2)\n\n    for cap in column[\'medians\']:\n        cap.set(color=\'green\', linewidth=2, alpha=0.5)\n\n    for cap in column[\'fliers\']:\n        cap.set(markerfacecolor=\'steelblue\', linewidth=2, marker=\'s\', markersize=6, alpha=0.5)\n\nax.set_title(\'Weight of Smoker\\\'s Babies vs Non-Smoker\\\'s Babies\', fontsize=18)\nax.set_ylabel("Weight (in Ounces)", fontsize=16)\n\nshort_names = ["Non-Smoker", "Smoker"]\nplt.xticks(range(1,len(short_names)+1),short_names, rotation=90, fontsize=16)\n\nplt.suptitle("")\nax.set_xlabel("")\n\nax.grid(alpha=0.25)'}, {'reason': 'stop', 'result': 'num_examples = 9\n```\n\nUnused variables:\n- im_shape'}, {'reason': 'stop', 'result': "ae = auto_encoder(m=m, ls=ls, ld=ld, le=le, lg=lg, rtol=rtol, xtol=None, N_inner=N_inner, N_outer=N_outer)\ntstart = time.time()\nZ = ae.fit_transform(X, L)\ntime_features = time.time() - tstart\nprint('Elapsed time: {:.0f} seconds'.format(time_features))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'dt = 1.\nR_var = 10\nQ_var = 0.01\nx = np.array([[10.0, 4.5]]).T\nP = np.diag([500, 49])\nF = np.array([[1, dt],\n              [0,  1]])\nH = np.array([[1., 0.]])\nR = np.array([[R_var]])\nQ = Q_discrete_white_noise(dim=2, dt=dt, var=Q_var)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '#GENERATE LOGARITHMIC BIN SPACING\nnumBins = 21\n\narea_bins_elder = np.logspace(np.log10(min(elderArea)), np.log10(max(elderArea)), numBins)\narea_bins_fox = np.logspace(np.log10(min(hankArea)), np.log10(max(hankArea)), numBins)\narea_bins_hank = np.logspace(np.log10(min(hankArea)), np.log10(max(foxArea)), numBins)\narea_bins_dry = np.logspace(np.log10(min(hankArea)), np.log10(max(dryArea)), numBins)\n\n\n#Return the indices_elder of the bins to which each value in input array belongs.\nindices_elder = np.digitize(elderArea, area_bins_elder)\nindices_fox = np.digitize(foxArea, area_bins_fox)\nindices_hank = np.digitize(hankArea, area_bins_hank)\nindices_dry = np.digitize(dryArea, area_bins_dry)\n\n#COMPUTE MEAN, MEDIAN AND STANDARD DEVIATION OF SLOPE IN EACH AREA BIN\nbin_means_elder = [elderSlope[indices_elder == i].mean() for i in range(1, len(area_bins_elder))]\nbin_medians_elder = [np.median(elderSlope[indices_elder == i]) for i in range(1, len(area_bins_elder))]\nbin_stds_elder = [elderSlope[indices_elder == i].std() for i in range(1, len(area_bins_elder))]\n\nbin_means_fox = [foxSlope[indices_fox == i].mean() for i in range(1, len(area_bins_fox))]\nbin_medians_fox = [np.median(foxSlope[indices_fox == i]) for i in range(1, len(area_bins_fox))]\nbin_stds_fox = [foxSlope[indices_fox == i].std() for i in range(1, len(area_bins_fox))]\n\nbin_means_hank = [hankSlope[indices_hank == i].mean() for i in range(1, len(area_bins_hank))]\nbin_medians_hank = [np.median(hankSlope[indices_hank == i]) for i in range(1, len(area_bins_hank))]\nbin_stds_hank = [hankSlope[indices_hank == i].std() for i in range(1, len(area_bins_hank))]\n\nbin_means_dry = [drySlope[indices_dry == i].mean() for i in range(1, len(area_bins_dry))]\nbin_medians_dry = [np.median(drySlope[indices_dry == i]) for i in range(1, len(area_bins_dry))]\nbin_stds_dry = [drySlope[indices_dry == i].std() for i in range(1, len(area_bins_dry))]\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'moviesavg_std = moviesstd.mean(axis=1).round(decimals=5)\nmoviesavg_std```'}, {'reason': 'stop', 'result': 'trace_delta_f_four_x_plus_fifteen = delta_f_trace(four_x_plus_fifteen, 2, 1)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'init = tf.global_variables_initializer()'}, {'reason': 'stop', 'result': 'dt = .1\nx = np.array([0., 0.]) \nkf = pos_vel_filter(x, P=500, R=5, Q=0.1, dt=dt)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'import pymc3 as pm\n\nwith pm.Model() as model:\n    p = pm.Uniform("freq_cheating", 0, 1)```\n\nUnused variables:\n- N'}, {'reason': 'stop', 'result': '# Import pandas and read in csv file as a dataframe\nimport pandas as pd\n# Change the path below to your data directory\n# If you are in a locale (e.g., Europe) that uses \',\' as the decimal separator\n# change the \'.\' to \',\'\ndf = pd.read_csv(\'../test_dataset/robot_log.csv\', delimiter=\';\', decimal=\'.\')\ncsv_img_list = df["Path"].tolist() # Create list of image pathnames\n# Read in ground truth map and create a 3-channel image with it\nground_truth = mpimg.imread(\'../calibration_images/map_bw.png\')\nground_truth_3d = np.dstack((ground_truth*0, ground_truth*255, ground_truth*0)).astype(np.float)\n\n# Creating a class to be the data container\n# Will read in saved data from csv file and populate this object\n# Worldmap is instantiated as 200 x 200 grids corresponding \n# to a 200m x 200m space (same size as the ground truth map: 200 x 200 pixels)\n# This encompasses the full range of output position values in x and y from the sim\nclass Databucket():\n    def __init__(self):\n        self.images = csv_img_list  \n        self.xpos = df["X_Position"].values\n        self.ypos = df["Y_Position"].values\n        self.yaw = df["Yaw"].values\n        self.worldmap = np.zeros((200, 200, 3)).astype(np.float)\n        self.ground_truth = ground_truth_3d # Ground truth worldmap\n\n# Instantiate a Databucket().. this will be a global variable/object\n# that you can refer to in the process_image() function below\ndata = Databucket()\n```\n\nUnused variables:\n- `data.count`'}, {'reason': 'stop', 'result': 'q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2)```\n\nUnused variables:\n- alpha'}, {'reason': 'stop', 'result': '# Create iterator for feeding BiRNN\ntrain_iterator = BucketDataIterator(trainImages,\n                                    trainLabels,\n                                    num_buckets,\n                                    train=True)\ntest_iterator = BucketDataIterator(testImages,\n                                   testLabels,\n                                   1,\n                                   train=False)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "data = pd.DataFrame({'labels':labels, 'count':count})```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'beta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\ntheta = P0.rvs(size=(N, K))\n\ndpm_pdf_components = f(x_plot[np.newaxis, np.newaxis, :], theta[..., np.newaxis])\ndpm_pdfs = (w[..., np.newaxis] * dpm_pdf_components).sum(axis=1)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'def error(x_values, y_values, x):\n    pass\n```\n\nUnused variables:\n- m\n- b'}, {'reason': 'stop', 'result': 'min_error = np.min(errors)\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# t-SNE visualization\ntsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=500)\ntsne_all = tsne.fit_transform(act_test[dataset_subset_indices])```\n\nUnused variables:\n- `tsne_y`'}, {'reason': 'stop', 'result': "train_corpus, test_corpus = preprocessing.clean_corpus()\ntest_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\ntrain_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n\ntrain_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\ntest_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n\npreprocessing.bag_of_word(train_corpus['corpus'].values, test_corpus['corpus'].values, min_df=10)```\n\nUnused variables:\n- train_uid\n- test_uid"}, {'reason': 'stop', 'result': "num_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  tf.global_variables_initializer().run()\n  print('Initialized')\n  average_loss = 0\n  for step in range(num_steps):\n    batch_data, batch_labels = generate_batch(\n      batch_size, num_skips, skip_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += l\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss = average_loss / 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print('Average loss at step %d: %f' % (step, average_loss))\n      average_loss = 0\n    # note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in range(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8 # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n        log = 'Nearest to %s:' % valid_word\n        for k in range(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log = '%s %s,' % (log, close_word)\n        print(log)\n  final_embeddings = normalized_embeddings.eval()```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 's = fit.summary()'}, {'reason': 'stop', 'result': "dfTiDirt = pd.read_csv('titanic_data.csv')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'hyperparameter = ex.literal(v = 5, name="hyperparameters")\n#Define the model training and evaluation action and final artifacts\ndo_test = ex.action(train_test, [X_train, X_test, y_train, y_test, hyperparameter])\nreport = ex.artifact(\'report.csv\', \'report\', do_test)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# maak document, met daarin grafiek. Hier zullen (uiteindelijk) meerdere grafieken moeten worden \n# weergegeven met meerdere opties.\n\ndef make_document(doc):\n    #Kolommen zijn de kolommen uit de df_totaalSBB dataframe. Dit gaat dan om 2013 AANT etc. Dit is voor de eerste groepering. \n    kolommen = list(df_totaalSBB)\n\n    #Sectoren haalt alle unieke waardes uit het dataframe voor de kolom SECTORUNIT SBB.\n    sectoren = list(df[\'SECTORUNIT SBB\'].unique())\n\n    #Voeg per kolom alle sectoren toe. Dit wordt dan de data voor de x-as. \n    #Kolom is bijvoorbeeld 2013 AANT en hier worden vervolgens alle sectoren aan toegevoegd.\n    x = [ (kolom, sector) for kolom in kolommen for sector in sectoren ]\n\n    #Data voor de y-as\n    #Data doorlopen om de totalen op de juiste volgorde in een array te plaatsen.\n    #Zelfde volgorde zoals hierboven staat voor de gegevens voor de x-as\n\n    counts = []\n\n    for kolom in kolommen:\n        for sector in sectoren:\n            counts.append(df_totaalSBB.loc[sector][kolom])\n\n    #Teken grafiek\n    source = ColumnDataSource(data=dict(x=x, counts=counts))\n\n    p = figure(x_range=FactorRange(*x), plot_height=400, title="Totalen per SBB sector per jaar")\n\n    p.vbar(x=\'x\', top=\'counts\', width=0.8, source=source)\n\n    p.width=900\n    p.y_range.start = 0\n    p.x_range.range_padding = 0.1\n    p.xaxis.major_label_orientation = 1\n    p.xgrid.grid_line_color = None\n\n\n    #update functie om nieuwe data(selecties) weer te geven\n    def updateSBB(attr, old, new):\n        kolommen = [jaren.labels[i] for i in jaren.active]\n        x = [ (kolom, sector) for kolom in kolommen for sector in sectoren ]\n\n        counts = []\n\n        for kolom in kolommen:\n            for sector in sectoren:\n                counts.append(df_totaalSBB.loc[sector][kolom])\n\n        newSource = ColumnDataSource(data=dict(x=x, counts=counts))\n        source.data.update(newSource.data)\n    \n    \n    jaren = CheckboxGroup(labels=kolommen, active = [0, 1, 2, 3 ,4])\n    jaren.on_change(\'active\', updateSBB)\n\n    layout = row(p, jaren)\n\n    #callback om updates elke 100ms op te halen\n    #doc.add_periodic_callback(update, 100)\n\n    doc.title = "Sectorunit SBB chart..."\n    doc.add_root(layout)'}, {'reason': 'stop', 'result': "doripa = sessions[sessions.monkey == 'doripa']"}, {'reason': 'stop', 'result': 'import numpy as np\n\nA = np.array([[54, 14, -11, 2], \n              [14, 50, -4, 29],\n              [-11, -4, 55, 22],\n              [2, 29, 22, 95]]\n            )\nb = np.array([1, 1, 1, 1])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "# Set the pipelines for categorical variables\ndiscrete_pipe_dog = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['dog'], hashing=None))])\ndiscrete_pipe_cat = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['cat'], hashing=None))])\n\n# Set the pipelines for continuous variables\ncontinuous_pipe_cat = Pipeline(steps=[('Scale', MyScaler(continuous['cat']))])\ncontinuous_pipe_dog = Pipeline(steps=[('Scale', MyScaler(continuous['dog']))])\n\n# Bring the discrete and continuous pipelines together for cats and dogs\nunion_dog = FeatureUnion([('Discrete', discrete_pipe_dog), ('Continuous', continuous_pipe_dog)])\nunion_cat = FeatureUnion([('Discrete', discrete_pipe_cat), ('Continuous', continuous_pipe_cat)])"}, {'reason': 'stop', 'result': '# construct pipeline\npipe_dt = make_pipeline(\n    MinMaxScaler(),\n    DecisionTreeRegressor()\n)\n\n# create the parameter grid for hyperparameter tuning\nparam_grid_dt = {\n    \'decisiontreeregressor__max_features\':["auto", "sqrt", "log2", None],\n    \'decisiontreeregressor__max_depth\':range(1, 10),\n    \'decisiontreeregressor__min_samples_leaf\':range(1, 4)\n}\n\n# perform grid search of pipeline\ndt_grid = GridSearchCV(pipe_dt, param_grid_dt)\n\n# use results to create model on training data\ndt_grid.fit(train_features, train_outcome)\n\n# find the best parameters from the grid search\ndt_best_params = dt_grid.best_params_\n\n# find the score of our model on the test data\ndt_grid_score = dt_grid.score(test_features, test_outcome)\n\n# find the mean absolute error of our model on the test data\ndt_mae = mean_absolute_error(dt_grid.predict(test_features), test_outcome)\n\n# find the explained variance score of our model on the test data\ndt_evs = explained_variance_score(dt_grid.predict(test_features), test_outcome)```\n\nUnused variables:\n- `pipe_dt`\n- `param_grid_dt`'}, {'reason': 'stop', 'result': "# First, we've written down the values of the 3 universal\n# constants that show up in Newton's formula.\n\n# G, the universal constant measuring the strength of gravity.\ngravity_constant = 6.674 * 10**-11\n\n# M, the moon's mass, in kilograms.\nmoon_mass_kg = 7.34767309 * 10**22\n\n# R, the radius of the moon, in meters.\nmoon_radius_m = 1.737 * 10**6\n\n# The distance the hammer should have fallen over the\n# duration of the fall, in meters, according to Newton's\n# law of gravity.  The text above describes the formula\n# for this distance given by Newton's law.\n# **YOU FILL THIS PART IN.**\n# YOUR CODE HERE\nraise NotImplementedError()\n\n# Here we've computed the difference between the predicted\n# fall distance and the distance we actually measured.\n# If you've filled in the above code, this should just work.\ndifference = predicted_distance_m - estimated_distance_m\ndifference\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# Create Negative Samples\nnegative_samples = pd.merge(\n    pd.merge(\n        tracks_df, playlist_map_df_negative, left_index=True, right_on='track_uri'),\n    playlist_df,\n    on='playlist_pid')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "B2_NTOT_WINTER_SETTINGS = lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter']\nlv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].settings.get_value('EK G/M', 22)\n```\n\nUnused variables:\n- B2_NTOT_WINTER_SETTINGS"}, {'reason': 'stop', 'result': '```python\n##  Ask for input file, full path\n##  i.e. Y:\\LRMF\\R_tables\\columbia_river_orig.csv\ninputFile = input("Please provide a full-path input file:")\n\ndata = pd.read_csv(inputFile,header=0)\n```\n\nUnused variables:\n- `inputFile` (commented out line)'}, {'reason': 'stop', 'result': 'from sklearn.decomposition import PCA\n\n# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components=6)\npca.fit(good_data)\n\n# TODO: Transform the sample log-data using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Generate PCA results plot\npca_results = rs.pca_results(good_data, pca)```\n\nUnused variables:\n- pca'}, {'reason': 'stop', 'result': 'p, r, f, s = metrics.precision_recall_fscore_support(y_test_flattened[filter_items], y_pred_flattened[filter_items])```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'data4 = data3[data3.star_name == nlist[0]]\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'batch_idx, (example_data, example_targets) = next(enumerate(test_loader))```\n\nUnused variables:\n- examples'}, {'reason': 'stop', 'result': 'with tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss, var_list=train_vars)```\n\nUnused variables:\n- train_vars'}, {'reason': 'stop', 'result': 'class two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)\n    \n    def fit(self, input_data, target, optimizer, num_epochs=500, \n            verbose=50, track_accuracy=True):\n        """ Function to train the model, using the selected optimizer and\n            for the desired number of epochs. It also stores the accuracy\n            of the model after each epoch.\n        """   \n        \n        if track_accuracy:\n            # Initialize list to store the accuracy of the model\n            self.hist_accuracy = []     \n            # Initialize class to compute the accuracy metric\n            accuracy = tfe.metrics.Accuracy()\n\n        for i in range(num_epochs):\n            # Take a step of gradient descent\n            grads = self.grads_fn(input_data, target)\n            optimizer.apply_gradients(zip(grads, self.variables))\n            if track_accuracy:\n                # Predict targets after taking a step of gradient descent\n                logits = self.predict(input_data)\n                preds = tf.argmax(logits, axis=1)\n                # Compute the accuracy\n                accuracy(preds, target)\n                # Get the actual result and add it to our list\n                self.hist_accuracy.append(accuracy.result())\n                # Reset accuracy value (we don\'t want to track the running mean accuracy)\n                accuracy.init_variables()\n```\n\nUnused variables:\n- `verbose`'}, {'reason': 'stop', 'result': "detNames = {0: 'Det0', 2: 'Det45', 4: 'Det90'}```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'with tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "result_test = []\nresult_train = []\ntot = 0\nfor string in ['share','comment','zan','content_len','链接','//@','@','#','【','《','\\[']:\n    temp = []\n    for i in test[string+'_histogram']:\n        if isinstance(i,int):\n            temp.append(np.zeros(shape=8))\n            tot +=1\n        else:\n            temp.append(i[0])\n    result_test.append(np.asarray(temp))\n    temp = []\n    for i in train[string+'_histogram']:\n        temp.append(i[0])\n    result_train.append(np.asarray(temp))\n    \n    train.drop(string+'_histogram',axis=1,inplace=True)\n    test.drop(string+'_histogram',axis=1,inplace=True)\ntrain.drop(['pid','uid'],inplace=True,axis = 1)\ntest.drop(['pid','uid'],inplace=True,axis = 1)\n\ntrain_y = train[['share','comment','zan']].values\ntrain.drop(['share','comment','zan'],axis = 1,inplace=True)\ntrain_x = train.values\ntest_x  = test.values\nfor i in result_train:\n    train_x = np.c_[train_x,i]\nfor i in result_test:\n    test_x = np.c_[test_x,i]\nnp.save('processed_data/train3_np',train_x)\nnp.save('processed_data/test3_np',test_x)\nnp.save('processed_data/target3_np',train_y)```\n\nUnused variables:\n- tot"}, {'reason': 'stop', 'result': 'def derivative_of(list_of_terms, x_value, delta_x):\n    pass\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n    \n    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data[feature],q=25)\n    \n    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile( log_data[feature],q=75)\n    \n    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers \n    print("Data points considered outliers for the feature \'{}\':".format(feature))\n    display(\n        log_data[\n            ~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    \n# OPTIONAL: Select the indices for data points you wish to remove\noutliers  = []\n\n# Remove the outliers, if any were specified\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)```\n\nUnused variables: `outliers`'}, {'reason': 'stop', 'result': 'transition_model_fitter = MultivariateLinear```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "for feature_set, slices in storage.get_slices().items():\n    dict_version = slices.to_dict()\n    print(dict_version, '\\n')```\n\nUnused variables:\n- `feature_set`"}, {'reason': 'stop', 'result': 'import pandas as pd\n\nwaypoint_distances = {}\nwaypoint_durations = {}\nall_waypoints = set()\n\nwaypoint_data = pd.read_csv("my-waypoints-dist-dur.tsv", sep="\\t")\n\nfor i, row in waypoint_data.iterrows():\n    waypoint_distances[frozenset([row.waypoint1, row.waypoint2])] = row.distance_m\n    waypoint_durations[frozenset([row.waypoint1, row.waypoint2])] = row.duration_s\n    all_waypoints.update([row.waypoint1, row.waypoint2])```\n\nUnused variables:\n- numpy (imported but not used)\n- np (defined but not used)'}, {'reason': 'stop', 'result': '# Re-run the model with the Bib numbers as a feature and for the 5K, 10K 15K 20K and Half split times to predict 25K time\n\n### set up data for modeling\nX_25K = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\', \'20K Duration\', \'Half Duration\']]\ny_25K = boston_clean[\'25K Duration\'].values.reshape(-1, 1)\nprint(X_25K.shape, y_25K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_25K, X_test_25K, y_train_25K, y_test_25K = train_test_split(X_25K, y_25K, random_state=29)\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_25K = LinearRegression()\nmodel_25K.fit(X_train_25K, y_train_25K)\n\n# Make predictions\n\npredictions_25K = model_25K.predict(X_test_25K)\n\n# Plot the residuals\n\nplt.scatter(model_25K.predict(X_train_25K), model_25K.predict(X_train_25K) - y_train_25K, c="blue", label="Training Data")\nplt.scatter(model_25K.predict(X_test_25K), model_25K.predict(X_test_25K) - y_test_25K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_25K.min(), xmax=y_test_25K.max())\nplt.title("Residual Plot 25K")\nplt.savefig(\'model_25k.png\')\nplt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "API_KEY = 'DEMO_TOKEN'\nevents = acnsim.acndata_events.generate_events(API_KEY, site, start, end, period, voltage, default_battery_power)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '#load the best model LSTM\nLSTM = load_model("LSTMmodel_"+str(embedding_size)+\'_\'+str(hidden_size))```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "```python\ndef validate_hypothesis(model, diag_classifier, hypothesis, train_len=50,\n                        test_len=1, text_len=500, temperature=0.8,\n                        save_hyp=None, save_diag=None, save_resp=None):\n    # Generate hypothesis data\n    def gen_hyp_data(model, N, text_len=500):\n        texts, hiddens, hyps = [], [], []\n        for i in range(N):\n            text, hidden = generate(model, '\\n\\n', text_len, temperature, True)\n            hidden = hidden.reshape(hidden.shape[0], -1)\n            hyp = hypothesis(text)\n            hiddens.append(hidden)\n            hyps.append(hyp)\n            texts.append(text)\n        return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n\n    # Generate train and test data\n    _, train_hyps, train_hiddens = gen_hyp_data(model, train_len)\n    test_texts, test_hyps, test_hiddens = gen_hyp_data(model, test_len)\n    print(pearsonr(train_hiddens, train_hyps))\n    print(pearsonr(test_hiddens, test_hyps))\n\n    # Train Diagnostic Classifier\n    diag_classifier.fit(train_hiddens, train_hyps)\n    \n    # Predict with Diagnostic Classifier\n    pred_hyps = diag_classifier.predict(test_hiddens)\n    \n    # Find responsible neuron\n    resp_neuron = np.argmax(np.abs(diag_classifier.coef_))\n    print(resp_neuron)\n    \n    # Plot results\n    if save_hyp:\n        plot_colored_text(test_texts[:text_len], test_hyps[:text_len],\n                          title='Formed Hypothesis',\n                          save_file=save_hyp)\n    if save_diag:\n        plot_colored_text(test_texts[:text_len], pred_hyps[:text_len],\n                          title='Diagnostic Classifier Prediction',\n                          save_file=save_diag)\n    if save_resp:\n        plot_colored_text(test_texts[:text_len], test_hiddens[:text_len, resp_neuron],\n                          title='Most Responsible Neuron {}'.format(resp_neuron),\n                          save_file=save_resp)\n        \n    gc.collect()\n    \n    return test_hyps, pred_hyps\n```\n\nUnused variables:\n- `del(train_hyps)`\n- `del(train_hiddens)`\n- `del(test_texts)`\n- `del(test_hiddens)`"}, {'reason': 'stop', 'result': 'k = 3\nheterogeneity = []\ninitial_centroids = get_initial_centroids(tf_idf, k, seed=0)\ncentroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n                                       record_heterogeneity=heterogeneity, verbose=True)\nplot_heterogeneity(heterogeneity, k)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def dateRange(start, end):\n    days = (datetime.datetime.strptime(end, "%Y-%m-%d") - datetime.datetime.strptime(start, "%Y-%m-%d")).days + 1\n    return [datetime.datetime.strftime(datetime.datetime.strptime(start, "%Y-%m-%d") + datetime.timedelta(i), "%Y-%m-%d") for i in xrange(days)]\n\ndef date_to_week(date):\n    if type(date) == str:\n        date = pd.to_datetime(date).date()\n    return (date - datetime.date(2015,7,7)).days  / 7\n\ndef week_to_date(week_number,return_str=True):\n    if week_to_date:\n        return [(datetime.date(2015,7,7)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d"),(datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d")]\n    return [datetime.date(2015,7,7)+ datetime.timedelta(week_number*7),datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)]\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "A_m2, I_m4, h_m, S_m3 = sy.symbols('A[m^{2}], I[m^{4}], h[m], S[m^{3}]')\nA_mm2 = A_m2 * (1e3) ** 2\nI_mm4 = I_m4 * (1e3) ** 4\nh_mm = h_m * 1e3\nS_mm3 = S_m3 * (1e3) ** 3```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "posttest = pd.read_csv('posttest.csv')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "```python\n# Load samples from generator taken while training\nwith open('train_samples.pkl', 'rb') as f:\n    pkl.load(f)```"}, {'reason': 'stop', 'result': "```python\nsave_file = './train_model_best.ckpt'\nsaver = tf.train.Saver()\n\nwith tf.Session() as session:\n    saver.restore(session, save_file)\n    feed_dict = {tf_train_dataset : X2_norm, tf_keep_prob : 1}\n    logi = session.run(logits, feed_dict)\n    predicts = session.run(tf.nn.top_k(logi, k=5, sorted=True))\n    proba = session.run(tf.nn.softmax(predicts[0]))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'seed = ["1 + 1"]\nbc = ProgramRunner(program="bc")\nm = MutationFuzzer(seed)\noutcomes = m.runs(bc, trials=100)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def sample_withplot(loglike_model, prior_transform_model, datafile, priorRange):\n    data_file = io.get_data_file_path(datafile)\n    data_x, data_xerr, data_y, data_yerr = io.load_data(data_file)\n\n    # n: number of parameters, len(priorRange)\n    n = len(priorRange)\n\n    def new_loglike_model(theta):\n        return loglike_model(theta, (data_x, data_xerr, data_y, data_yerr))\n\n    def new_prior_transform_model(theta):\n        return prior_transform_model(theta, priorRange)\n\n    result = nestle.sample(new_loglike_model, new_prior_transform_model, n)\n\n    print(\'log evidence\')\n    print(result.logz)\n\n    print(\'numerical (sampling) error on logz\')\n    print(result.logzerr)\n\n    print(\'array of sample parameters\')\n    print(result.samples)\n\n    print(\'array of weights associated with each sample\')\n    print(result.weights)\n\n    import matplotlib.pyplot as plt\n    import corner\n\n    p_fit, cov_fit = nestle.mean_and_cov(result.samples, result.weights)\n\n    plt.figure()\n    plt.errorbar(data_x, data_y, yerr=data_yerr, fmt=\'*\')\n    plt.xlabel("r (kpc)")\n    plt.ylabel(\'V (km/s)\')\n    plt.title("Results of using the model to fit the DM rotational velocity distribution")\n    xplot = [5 + 5 * i for i in range(40)]\n    yplot = [model.model_NFW(xplot[i], p_fit) for i in range(40)]\n    plt.plot(xplot, yplot)\n    plt.show()\n\n    fig = corner.corner(result.samples, weights=result.weights, labels=[\'a\', \'rho0\'],\n                        range=[0.99999, 0.99999], bins=30)\n    plt.show()\n\n    return result\n```\n\nUnused variables:\n- `result.logzerr`\n- `p_fit`\n- `cov_fit`'}, {'reason': 'stop', 'result': 'M_Nm = sy.Piecewise((M_AB_Nm.subs(s_d), x<=x_B_m),\n                    (M_BD_Nm.subs(s_d), x_B_m<x))```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'x_plot = np.arange(250)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "colors = ['#330700', '#f2553d', '#73341d', '#e57e39', '#736256', '#8c5e00', '#402b00', '#f2deb6', '#f2ce3d', \n          '#595843', '#798020', '#ccff00', '#293300']\nextractPolyFromKML(regionspath)\nprint('extracted regions from KML file')```\n\nUnused variables:\n- urbanRegions"}, {'reason': 'stop', 'result': "merged = pd.merge(\n    pd.merge(\n        tracks_df, playlist_map_df, left_index=True, right_on='track_uri'),\n    playlist_df,\n    on='playlist_pid')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "from tensorflow.python.framework import ops\nops.reset_default_graph()\n\nbatch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Input data.\n    tf_train_dataset = tf.placeholder(\n        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n  \n    # Variables.\n    layer1_weights = tf.Variable(tf.truncated_normal(\n        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n    layer1_biases = tf.Variable(tf.zeros([depth]))\n    layer2_weights = tf.Variable(tf.truncated_normal(\n        [patch_size, patch_size, depth, depth], stddev=0.1))\n    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n    layer3_weights = tf.Variable(tf.truncated_normal(\n        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n    layer4_weights = tf.Variable(tf.truncated_normal(\n        [num_hidden, num_labels], stddev=0.1))\n    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n  \n    # Model.\n    def model(data):\n        # layer 1 convo. max_pool 2x2\n        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n        pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(pool + layer1_biases)\n        # layer 2 convo. max_pool 2x2\n        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n        pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(pool + layer2_biases)\n        # layer 3 fully connected.\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        return tf.matmul(hidden, layer4_weights) + layer4_biases\n  \n    # Training computation.\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    \n    # Optimizer.\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n  \n    # Predictions for the training, validation, and test data.\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'seed_input = "http://www.google.com/search?q=fuzzing"\nvalid_inputs = set()\ntrials = 20\n\nfor i in range(trials):\n    inp = mutate(seed_input)\n    if is_valid_url(inp):\n        valid_inputs.add(inp)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# Let's start by creating our geolocator. We will use Google Maps API:\ngooglemapsapikeyjson = json.loads(open('google_maps_api_keys.json').read())"}, {'reason': 'stop', 'result': "train_uid = access_data.load_processed_data('uid&pid_train')\ntest_uid  = access_data.load_processed_data('uid&pid_test')\n\ntrain ,test = access_data.load_raw_data()\n\ntrain_corpus = access_data.load_processed_data('cleaned&segment_train')\ntest_corpus  = access_data.load_processed_data('cleaned&segment_test')                                               \ntrain = pd.concat([train_uid,train,train_corpus],axis=1)\ntest  = pd.concat([test_uid,test,test_corpus],axis=1)\n\ntrain.drop([0,1],axis=1,inplace=True)\ntest.drop([0,1],axis=1,inplace=True)\n\ntrain.columns = ['pid','uid','time','share','comment','zan','raw_corpus','clean&segment','链接','//@','@','#','【','《','\\[']\ntest.columns = ['pid','uid','time','raw_corpus','clean&segment','链接','//@','@','#','【','《','\\[']\ntrain['uid'] = train['uid'].astype(np.uint16)\ntest['uid']  = test['uid'].astype(np.uint16)\nl = ['链接','//@','@','#','【','《','\\[']\n\nfor string in l :\n    train[string] = train[string].astype(np.int8)\n    test[string]  = test[string].astype(np.int8)\n\n#在training set和test set中和用户发送微博的总数量\ntot = pd.concat([pd.DataFrame(train['uid']),pd.DataFrame(test['uid'])])\nc = pd.DataFrame(tot['uid'].value_counts())\nc.columns = ['tot_counts']\ntrain = train.merge(c,left_on='uid',right_index=True,how='left')\ntest  = test.merge(c,left_on='uid',right_index=True,how='left')\n\n# 用户出现在训练集的次数\nc = pd.DataFrame(train['uid'].value_counts())\nc.columns = ['train_counts']\ntrain = train.merge(c,left_on='uid',right_index=True,how='left')\ntest  = test.merge(c,left_on='uid',right_index=True,how='left')\n\ntest.fillna(-1,inplace=True)\ntrain['tot_counts'] = train['tot_counts'].astype(np.int32)\ntrain['train_counts'] = train['train_counts'].astype(np.int32)\n\ntest['tot_counts'] = test['tot_counts'].astype(np.int32)\ntest['train_counts'] = test['train_counts'].astype(np.int32)\n\naddr1 = setting.raw_data_dir + 'basic_train'\naddr2 = setting.raw_data_dir + 'basic_test'\n\nlda_result = np.load('processed_data/lda_result_version3.npy')\nlda_result = pd.DataFrame(lda_result,columns=['topic_%d' %i for i in range(0,25)])\n\nfor string in ['topic_%d' %i for i in range(0,25)]:\n    train[string] = lda_result.loc[:train.shape[0]-1,string].values\n    test[string] = lda_result.loc[train.shape[0]:,string].values\n\n#train.to_pickle(addr1)\n#test.to_pickle(addr2)```\n\nUnused variables:\n- addr1\n- addr2"}, {'reason': 'stop', 'result': 'predictions_5K = model_5K.predict(X_test_5K)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# tranform each class\nY1 = np.matmul(X1,v2)\nY2 = np.matmul(X2,v2)\n\n## show me dont tell me\nax = plt.gca()\nax.hist(Y1,color='blue', alpha=0.5, label='1')\nax.hist(Y2,color='yellow', alpha=0.5, label='2')\nplt.legend(loc='upper right')\nplt.xlabel('y')\n\nY = np.matmul(X,v2)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "logsPath = '../data/logs'\ngeoLiteIPDBPath = '../data/GeoLite2-City_20181009/GeoLite2-City.mmdb'\n\n#Create a myLogReader object\nmyLogReader = mlr.log()\n#Open Reader\nmyLogReader.openReader(geoLiteIPDBPath)```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "'''Plot function for nuclides half-life'''\n\ndef plot_nuclides(nuclides):\n    \n    from matplotlib import pyplot as plt # import the pyplot function of the matplotlib package\n    (fig, ax) = plt.subplots(figsize=(18,7))\n\n    ax.plot([nc.Z for nc in nuclides.values()], [nc.half_life/3600/24/365 for nc in nuclides.values()], \n            ' ',color='black', marker='x',markersize=10)\n    \n    min_z = min([nc.Z for nc in nuclides.values()])\n    max_z = max([nc.Z for nc in nuclides.values()])\n\n    ax.xaxis.set_ticks(range(min_z, max_z+1,2))\n    ax.set_xlim((min_z-1,max_z+1))\n\n    plt.xlabel(r'Nuclide Z Number',fontsize=18)\n    plt.ylabel(r'$T_{1/2} [a]$',fontsize=18)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=16)\n    \n    ay1 = ax.twiny()\n    ay1.set_xlim(ax.get_xlim())\n    ay1.set_xticks([])\n    from mendeleev import element\n    ay1.set_xticks(range(min_z,max_z+1), [element(z).symbol for z in range(min_z,max_z+1)])\n    ay1.set_xticklabels([element(z).symbol for z in range(min_z,max_z+1)],minor=True,fontsize=12)\n\n    min_a = min([nc.A for nc in nuclides.values()])\n    max_a = max([nc.A for nc in nuclides.values()])\n    \n    plt.title(r'%i Nuclides: $%i \\leq A \\leq %i$ '%(len(nuclides),min_a,max_a),fontsize=22)\n    ax.grid(True)\n    plt.yscale('log')\n    plt.show()\n\n    return\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'nlist = number.index[number.star_name > 2]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '```initial_w = np.zeros(Strain1_z.shape[1])    #w is an array of size = number of features (i.e. number of columns in z)```'}, {'reason': 'stop', 'result': '```reset_graph()\n\nn_inputs = 28 * 28  # MNIST\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")```\n\nUnused variables:\n- n_hidden1'}, {'reason': 'stop', 'result': "list1 = pd.read_csv('listings.csv')"}, {'reason': 'stop', 'result': "# Get the components.\n\n# get the circuit by Name\ncircuit = qp.get_circuit('Circuit')```\n\nUnused variables:\n- quantum_r\n- classical_r"}, {'reason': 'stop', 'result': "pd.options.display.max_colwidth = 400\npd.read_csv(join(path, 'survey_results_schema.csv'), low_memory=False)```\n\nUnused variables:\n- schema"}, {'reason': 'stop', 'result': "가사 = '''내 몸이 확\n머리가 휙\n돌아가 어때\n지금 기분이\n우리는 마치\n자석 같아\n서로를 안고\n또 밀쳐내니까\n너는 날 미치게 하고\n정신 못 차리게 해\n그래 알겠지 넌 참\n날 기가 막히게 해\n너는 날 춤추게 하고\n술 없이 취하게 해\n그래 알겠지 넌 참\n착한 날 독하게 해\n우리 둘만의\n이 영화에\n진짜 주인공은 너였어 baby\n그래 넌 오늘도 너답게\n화려한 주인공처럼\n그저 하던 대로 해\n그게 악역이라도\n나를 슬픈 엔딩이라도\nthe show must go on\nthe show must go on\n너는 늘 끝장을 보고\n모든 걸 덮으려 해\n왜 날개를 달아주고\n추락하자 해\n너는 늘 착하지 라며\n날 눈물 삼키게 하잖아\n그래 알겠지 알겠지\n넌 날 혹하게 해\n우리 둘만의\n이 드라마에\n진짜 주인공은 너였어 baby\n그래 넌 오늘도 너답게\n화려한 주인공처럼\n그저 하던 대로 해\n그게 악역이라도\n나를 슬픈 엔딩이라도\nthe show must go on\nthe show must go on\n그저 하던 대로 해\n그게 악역이라도\n나를 슬픈 엔딩이라도\n넌 너여야만 해\n내가 아플지라도\n슬픈 엔딩이라도\nthe show must go on\nthe show must go on\nyou must go on'''"}, {'reason': 'stop', 'result': 'ADPagent = PassiveADPAgent(policy, sequential_decision_environment)\nfor i in range(200):\n    run_single_trial(ADPagent, sequential_decision_environment)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "#plot the Monthly Revenue Averages to determine overall monthly comparison performance\nax = sns.lineplot(df.index.month, df['Revenue'], color='#2ecc71', label='Revenue')```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# find connection in the specified sequence, center 29 is in the position 15\nlimbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n           [1,16], [16,18], [3,17], [6,18]]\n# the middle joints heatmap correpondence\nmapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n          [55,56], [37,38], [45,46]]```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'bivar = sorted(filter(lambda r: len(r[0]) == 1, storage.relevancies.relevancy.iteritems()), \n               key=lambda r: r[1], reverse=True)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'with pm.Model() as model:\n    lambda_1 = pm.Exponential("lambda_1", 1.0)\n    lambda_2 = pm.Exponential("lambda_2", 1.0)\n\nnew_deterministic_variable = lambda_1 + lambda_2```\n\nUnused variables:\n- tau'}, {'reason': 'stop', 'result': '```reset_graph()\n\nn_inputs = 28 * 28  # MNIST\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")```\n\nUnused variables:\n- n_hidden1\n- n_hidden2\n- n_outputs'}, {'reason': 'stop', 'result': "dfBabyDirt = pd.read_csv('http://www.stat.berkeley.edu/~statlabs/data/babies.data', delim_whitespace=True)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# TODO add features for polar coordinate values where the nose is the origin\n# Name these 'polar-rr', 'polar-rtheta', 'polar-lr', and 'polar-ltheta'\n# Note that 'polar-rr' and 'polar-rtheta' refer to the radius and angle\n\nfeatures_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'v = vandermonde(Partition([2,1]))\nv```\n\nUnused variables:\n- n'}, {'reason': 'stop', 'result': '# create OTModel\not_model = wot.ot.OTModel(adata)```\n\nUnused variables:\n- epsilon\n- lambda1\n- lambda2'}, {'reason': 'stop', 'result': 'X = tf.get_default_graph().get_tensor_by_name("X:0")\ny = tf.get_default_graph().get_tensor_by_name("y:0")\n\naccuracy = tf.get_default_graph().get_tensor_by_name("eval/accuracy:0")\n\ntraining_op = tf.get_default_graph().get_operation_by_name("GradientDescent")```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'class Preprocessor:\n    \n    def __init__(self, train_data_file, train_label_file, train_ids_file,\n                 instr_file, test_data_file=None, test_ids_file=None):\n        """A class to process and reformat data\n        for use in learning models"""\n        \n        # initialize the data the data filenames\n        self.train_data_file = train_data_file\n        self.train_label_file = train_label_file\n        self.train_ids_file = train_ids_file\n        self.instr_file = instr_file\n        \n        # test data is optional\n        self.test_data_file = test_data_file\n        self.test_ids_file = test_ids_file\n        \n    def read_data(self):\n        """Reads in data from the files passed to constructor"""\n        \n        # read in the data\n        train_X_df = pd.read_csv(self.train_data_file)\n        train_y_df = pd.read_csv(self.train_label_file)\n        train_ids_df = pd.read_csv(self.train_ids_file)\n        self.instr_df = pd.read_csv(self.instr_file)\n        \n        self.feature_names = [feature for feature in train_X_df]\n        self.original_feature_names = [feature for feature in train_X_df]\n        self.label_names = [feature for feature in train_y_df]\n        self.id_names = [feature for feature in train_ids_df]\n        \n        # create cross validation data\n        self.cv_X_df = pd.DataFrame(train_X_df)\n        self.cv_y_df = pd.DataFrame(train_y_df)\n        self.cv_ids_df = pd.DataFrame(train_ids_df)\n        \n        # read in the test data if it exists\n        if self.test_data_file != None:\n            self.test_X_df = pd.read_csv(self.test_data_file)\n            self.test_ids_df = pd.read_csv(self.test_ids_file)\n            self.all_X_df = train_X_df.append(self.test_X_df)\n        else:\n            self.test_X_df = None\n            self.test_ids_df = None\n            self.all_X_df = pd.DataFrame(train_X_df)\n        \n        # determine the shape of the input data\n        self.train_X_shape = train_X_df.shape\n        self.train_y_shape = train_y_df.shape\n        self.train_ids_shape = train_ids_df.shape\n        self.instr_shape = self.instr_df.shape\n        self.all_shape = self.all_X_df.shape\n        \n        # get size of test data if it exists\n        if self.test_data_file != None:\n            self.test_X_shape = self.test_X_df.shape\n            self.test_ids_shape = self.test_ids_df.shape\n        else:\n            self.test_X_shape = None\n            self.test_ids_shape = None\n\n        \n    def process(self, shuffle_train_data=False):\n        """Performs the processing on cross validation and train/test data"""\n        \n        # ADD OPTION TO SHUFFLE DATA HERE\n        \n        # processing on all data - remember to include cv_X and all_X for each condition\n        for col in self.original_feature_names:\n            print(col)\n            \n            # determine what to perform at each of the steps\n            col_instr = self.instr_df[col].values\n            col_enc = col_instr[1]\n            col_scl = col_instr[2]\n            col_imp = col_instr[3]\n\n            # impute values\n            # imputed first so that other functions will not use nan values in calculations\n            if col_imp == \'UNIQ\':\n                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n            if col_imp == \'MEAN\':\n                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n                self.all_X_df[col] = MEAN(self.all_X_df[col])\n            if col_imp == \'MODE\':\n                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n                self.all_X_df[col] = MODE(self.all_X_df[col])\n            if col_imp == \'MED\':\n                self.cv_X_df[col] = MED(self.cv_X_df[col])\n                self.all_X_df[col] = MED(self.all_X_df[col])\n            if is_int(col_imp):\n                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n            if col_imp == \'DEL\':\n                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n            \n            \n            # perform encoding of data\n            if col_enc == \'MAP\':\n                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n                self.all_X_df[col] = MAP(self.all_X_df[col])\n            if col_enc == \'OHE\':\n                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n                    feature_names=self.feature_names)\n            if col_enc == \'LOO\':\n                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n                self.all_X_df[col] = LOO(self.all_X_df[col])\n            \n\n            # perform scaling\n            if col_scl == \'NRM1\':\n                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n                self.all_X_df[col] = NRM1(self.all_X_df[col])\n            if col_scl == \'SCL1\':\n                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n                self.all_X_df[col] = SCL1(self.all_X_df[col])\n            if col_scl == \'TRSH\':\n                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n                self.all_X_df[col] = TRSH(self.all_X_df[col])\n\n        \n        # get the values from the dataframes\n        self.cv_X = self.cv_X_df.values\n        self.cv_y = self.cv_y_df.values\n        self.cv_ids = self.cv_ids_df.values\n        \n        all_X = self.all_X_df.values\n        self.train_X = all_X[:self.train_X_shape[0], :]\n        self.train_y = self.cv_y_df.values\n        self.train_ids = self.cv_ids_df.values\n        \n        if self.test_data_file != None:\n            self.test_X = all_X[self.train_X_shape[0]:, :]\n            self.test_ids = self.test_ids_df.values\n        else:\n            self.test_X = None\n            self.test_ids = None\n        \n    def write_data(self, out_dir=\'./processed_data/\'):\n        """Writes all of the data to output files"""\n        \n        # create the output directory if it does not exist\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n            \n        # convert arrays back into DataFrames\n        cv_X_df = pd.DataFrame(self.cv_X,  columns=self.feature_names)\n        cv_y_df = pd.DataFrame(self.cv_y, columns=self.label_names)\n        cv_ids_df = pd.DataFrame(self.cv_ids, columns=self.id_names)\n        train_X_df = pd.DataFrame(self.train_X, columns=self.feature_names)\n        train_y_df = pd.DataFrame(self.train_y, columns=self.label_names)\n        train_ids_df = pd.DataFrame(self.train_ids, columns=self.id_names)\n        if self.test_data_file != None:\n            test_X_df = pd.DataFrame(self.test_X, columns=self.feature_names)\n            test_ids_df = pd.DataFrame(self.test_ids, columns=self.id_names)\n        \n        # write the dataframes to file\n        cv_X_df.to_csv(out_dir+\'cv_X.csv\', index=False)\n        cv_y_df.to_csv(out_dir+\'cv_y.csv\', index=False)\n        cv_ids_df.to_csv(out_dir+\'cv_ids.csv\', index=False)\n        train_X_df.to_csv(out_dir+\'train_X.csv\', index=False)\n        train_y_df.to_csv(out_dir+\'train_y.csv\', index=False)\n        train_ids_df.to_csv(out_dir+\'train_ids.csv\', index=False)\n        if self.test_data_file != None:\n            test_X_df.to_csv(out_dir+\'test_X.csv\', index=False)\n            test_ids_df.to_csv(out_dir+\'test_ids.csv\', index=False)\n        \n    def select_features(self):\n        """Perform features selection / compression algs like PCA."""\n        """These will be implemented once more has been done."""\n        self.feature_names = self.feature_names\n```\n\nUnused variables:\n- `shuffle_train_data`'}, {'reason': 'stop', 'result': "from sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english',\n                        max_df=.1,\n                        max_features=5000)\nX = count.fit_transform(df['review'].values)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# some global variables\n\ndata_filepath = "./data/"\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'intersections = collections.defaultdict(list)\nfor s in subsets:\n    intersections[len(set(subset).intersection(set(s[0])))].append(s)\nmax_subsets = max(intersections.items())```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')/255\ntest_tensors = paths_to_tensor(test_files).astype('float32')/255```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "consumer_key = ''\nconsumer_secret = ''\naccess_token = ''\naccess_secret = ''\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# Split the data into features and target label\nincome_raw = data['income']\nfeatures_raw = data.drop('income', axis = 1)\n\n# Visualize skewed continuous features of original data\nvs.distribution(data)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'class USZIPCodeRepository:\n    CACHE = {}\n\n    def __init__(self, data_url_prefix = \'https://raw.githubusercontent.com/yyu/GeoJSON-US/master\'):\n        self.data_url_prefix = data_url_prefix\n        self.geojson_url_prefix = f\'{data_url_prefix}/perZIPgeojson\'\n\n        self.refresh_zipcode_latlons(f\'{data_url_prefix}/ZIPCodesGazetteer.tsv\')\n        self.refresh_available_zipcodes(f\'{data_url_prefix}/perZIPgeojson/all_zipcodes.txt\')\n\n\n    def refresh_zipcode_latlons(self, url):\n        lines = [ line.decode(\'UTF8\').strip() for line in urllib.request.urlopen(url).readlines() ]\n        tsv = csv.DictReader(lines, delimiter=\'\\t\')\n        self.gazetteer = dict((d[\'GEOID\'], {\'lat\': float(d[\'INTPTLAT\']), \'lon\': float(d[\'INTPTLONG\'])}) for d in tsv)\n\n\n    def refresh_available_zipcodes(self, url):\n        lines = [ zipcode.decode(\'UTF8\').strip() for zipcode in urllib.request.urlopen(url).readlines() ]\n        self.zipcode_list = lines[1:] # ignore the first line\n        self.zipcode_set = set(self.zipcode_list)\n\n\n    def make_url(self, zipcode):\n        return f\'{self.data_url_prefix}/perZIPgeojson/{zipcode[0]}/{zipcode[1]}/{zipcode[2]}/{zipcode}.json\'\n\n\n    def fetch_zipcode(self, zipcode):\n        \'\'\'returns a (dict, err) tuple where err could be a string for error message or None\'\'\'\n\n        url = self.make_url(zipcode)\n\n        if url in USZIPCodeRepository.CACHE:\n            return (USZIPCodeRepository.CACHE[url], None)\n\n        try:\n            s = urllib.request.urlopen(url).read()\n        except urllib.error.URLError as e:\n            return (None, \'failed to get \' + url, \':\', e.reason)\n\n        j = json.loads(s)\n\n        USZIPCodeRepository.CACHE[url] = j\n\n        return (j, None)\n\n\n    def fetch_zipcodes(self, *zipcodes):\n        d = {"type": "FeatureCollection", "features": []}\n\n        available_zipcodes = set(zipcodes) & self.zipcode_set\n\n        for z in available_zipcodes:\n            j, err = self.fetch_zipcode(z)\n\n            if j is not None:\n                d[\'features\'].append(j)\n\n        return d\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'reset_graph()\n\nimport tensorflow as tf\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n\ntraining = tf.placeholder_with_default(False, shape=(), name=\'training\')\n\nhidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")\nbn1_act = tf.nn.elu(hidden1)\n\nhidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")\nbn2_act = tf.nn.elu(hidden2)\n\nlogits_before_bn = tf.layers.dense(bn2_act, n_outputs, name="outputs")\nlogits = tf.layers.batch_normalization(logits_before_bn, training=training,\n                                       momentum=0.9)```\n\nUnused variables:\n- bn1\n- bn2'}, {'reason': 'stop', 'result': "```fig, ax = plt.subplots(figsize=(8, 6))\n\nplot_w = np.arange(K) + 1\n\nax.bar(plot_w - 0.5, trace['w'].mean(axis=0), width=1., lw=0);\n\nax.set_xlim(0.5, K);\nax.set_xlabel('Component');\n\nax.set_ylabel('Posterior expected mixture weight');```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'total_keys = len(profile_dict.keys())'}, {'reason': 'stop', 'result': 'alpha = integral.trapez(u1_z, 0, 0, len(u1_z)-1)\nbeta = integral.trapez(u2_z, 0, 0, len(u2_z)-1)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'sch = algorithms.UncontrolledCharging()```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nlearning_rate = 0.01\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nn_epochs = 40\nbatch_size = 50\n```\n\nUnused variables:\n- `n_epochs`\n- `batch_size`'}, {'reason': 'stop', 'result': 'x_values = None\ny_values = None```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'reload(pccsims)\ncoco = pccsims.pyCoCo(pcc.utils.b(filter_path), pcc.utils.b(coco_root_path))\n\nflux, flux_err = coco.simulate(b"SN2007uy", \n                    z_obs, 0.0, 0.0, 0.0, 3.1, \n                    mjdmax, mjd_to_sim, \n                    filters_to_sim)```\n\nUnused variables:\n- flux, flux_err (commented out)'}, {'reason': 'stop', 'result': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Valid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Valid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'import sympy as sym\nfrom sympy import *\nA, U, S = symbols("A U S")\nalpha_UA, alpha_AU, alpha_US, alpha_SU  = symbols("alpha_UA alpha_AU alpha_US alpha_SU")\n\nmodel_dyn = [\n    alpha_UA*U - alpha_AU*A,\n    alpha_AU*A + alpha_SU*S - (alpha_UA + alpha_US)*U,\n    alpha_US*U - alpha_SU*S,\n    A + U + S - 1 # this equation sets the total population size to 1\n    ]\n\n# steady-state solution\nsol_dyn = solve(model_dyn, A, U, S)\n\n# functions for calculating the proportion of the population in each compartment at \n# steady state, given transition rates between compartments\ndyn_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A] + sol_dyn[S])\nU_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[U])\nA_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A])\nS_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[S])\n\nsol_dyn\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# TODO: Import \'r2_score\'\n\ndef performance_metric(y_true, y_predict):\n    """ Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. """\n    \n    # Return the score\n    return None\n```\n\nUnused variables:\n- `score`'}, {'reason': 'stop', 'result': "# Let's log-tranform this: \nsales_prices_log = np.log1p(sales_price)\ngraph = sns.distplot(sales_prices_log)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "#Select the data for 2009 onwards\nAfter2009 = PM25.sel(time=slice('2009', '2016'))```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'K = 50\nN = sunspot_df.shape[0]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# TODO: Set the epochs, batch_size, and learning_rate with the best parameters from problem 3\nepochs = 5\nbatch_size = 100\nlearning_rate = 0.2\n\n### DON'T MODIFY ANYTHING BELOW ###\n# The accuracy measured against the test set\ntest_accuracy = 0.0\n\nwith tf.Session() as session:\n    \n    session.run(init)\n    batch_count = int(math.ceil(len(train_features)/batch_size))\n\n    for epoch_i in range(epochs):\n        \n        # Progress bar\n        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n        \n        # The training cycle\n        for batch_i in batches_pbar:\n            # Get a batch of training features and labels\n            batch_start = batch_i*batch_size\n            batch_features = train_features[batch_start:batch_start + batch_size]\n            batch_labels = train_labels[batch_start:batch_start + batch_size]\n\n            # Run optimizer\n            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n\n        # Check accuracy against Test data\n        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n\n\nassert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\nprint('Nice Job! Test Accuracy is {}'.format(test_accuracy))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'lights_data = np.concatenate((stern_images, stbd_broad_images_gt_50m, port_broad_images_gt_50m, bow_images_gt_50m))```\n\nUnused variables:\n- stbd_broad_lt_50m_images\n- port_broad_lt_50m_images\n- bow_lights_lt_50_images'}, {'reason': 'stop', 'result': "# Input placeholders\n# N_INPUT -> size of vector representing one image in sequence\n# Inputs shape (batch_size, max_seq_length, vec_size) - time major\ninputs = tf.placeholder(shape=(None, None, N_INPUT),\n                        dtype=tf.float32,\n                        name='inputs')\nlength = tf.placeholder(shape=(None,),\n                        dtype=tf.int32,\n                        name='length')\n# Required for training, not required for application\ntargets = tf.placeholder(shape=(None, None),\n                         dtype=tf.int64,\n                         name='targets')\n# Dropout value\nkeep_prob = tf.placeholder(tf.float32, name='keep_prob')```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'ridge_preds = np.expm1(model_ridge.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\nelastic_preds = np.expm1(model_elastic.predict(X_test))```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "test_image = 'sample_image/ski.jpg'\noriImg = cv.imread(test_image) # B,G,R order\nf = plt.imshow(oriImg[:,:,[2,1,0]]) # reorder it before displaying```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '# Getting the tsv file from the given link provided using request library\nurl="https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"\nresponse = requests.get(url)\n\nwith open(\'image_predictions.tsv\', \'wb\') as file:\n    file.write(response.content)\n    \n#reading the file\nimg_predictions = pd.read_csv(\'image_predictions.tsv\',sep=\'\\t\')```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'T = 1000  # number of samples\nwith tf.name_scope("posterior"):\n    qpi = Empirical(tf.get_variable("qpi/params", [T, K], initializer=tf.constant_initializer(1.0/K)))\n    qz = Empirical(tf.get_variable("qz/params", [T, N], initializer=tf.zeros_initializer(), dtype=tf.int32))```\n\nUnused variables:\n- qmu\n- qsigma'}, {'reason': 'stop', 'result': "if 'p' in globals().keys():\n    # Hyper-parameters passed by the experiment runner.\n    for key, value in p.items():\n        globals()[key] = value\nelse:\n    m = 64  # 64, 128, 512\n    ls = 1\n    ld = 10\n    le = None\n    lg = 1\n    rtol = 1e-5  # 1e-3, 1e-5, 1e-7\n    N_inner = 500\n    N_outer = 50\n    Ngenres, Nclips, Nframes = 10, 100, 644\n    noise_std = 0\n    folder = 'data'\n    filename_audio = 'audio.hdf5'\n    filename_graph = 'graph.hdf5'\n    filename_features = 'features.hdf5'```\n\nUnused variables:\n- Ngenres\n- Nclips\n- Nframes"}, {'reason': 'stop', 'result': 'K = 9\n```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'basic_model = HiddenMarkovModel(name="base-hmm-tagger")\n\ntags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\n\nemission_counts = pair_counts(tags, words)\nstates = {}\nfor tag, word_dict in emission_counts.items():\n    emission_dict = defaultdict(float)\n    for word in word_dict.keys():\n        emission_dict[word] = emission_counts[tag][word] / tag_unigrams[tag] \n    state_emission = DiscreteDistribution(dict(emission_dict))\n    states[tag] = State(state_emission, name=tag)\n    \nbasic_model.add_states(list(states.values()))\n\nfor tag in data.training_set.tagset:\n    state = states[tag]\n    basic_model.add_transition(basic_model.start, state, tag_starts[tag]/len(data.training_set))\n    basic_model.add_transition(state, basic_model.end, tag_ends[tag]/tag_unigrams[tag])\n    for next_tag in data.training_set.tagset:\n        next_state = states[next_tag]\n        basic_model.add_transition(state, next_state, tag_bigrams[(tag, next_tag)]/tag_unigrams[tag])\n\nbasic_model.bake()\n\nassert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n       "Every state in your network should use the name of the associated tag, which must be one of the training set tags."\nassert basic_model.edge_count() == 168, \\\n       ("Your network should have an edge from the start node to each state, one edge between every " +\n        "pair of tags (states), and an edge from each state to the end node.")\nHTML(\'<div class="alert alert-block alert-success">Your HMM network topology looks good!</div>\')'}, {'reason': 'stop', 'result': 'array_1 = band1.ReadAsArray().flatten()\narray_2 = band2.ReadAsArray().flatten()\narray_3 = band3.ReadAsArray().flatten()\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'CURRENT = "current"\nSAVING = "saving"\nACCOUNTTYPES  = [CURRENT, SAVING]\nSUPPORTED_BANKS = {\n    "unicaja": "Unicaja",\n    "cajamar": "Cajamar",\n    "openbank": "Openbank"\n}\nCURRENCY = \'€\'```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# Do EVD to see the projection matrix\ncov = X.T @ X /(X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:,idx]\nproject_X = X @ vec\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(141)\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.subplot(142)\nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\ninverse = np.linalg.inv(vec)\ninverse = inverse - inverse.mean(0, keepdims=True)\nrevert_X = project_X @ inverse\n\nplt.subplot(143)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\ninverse = np.linalg.inv(vec)\ninverse = inverse - inverse*inverse.std(0, keepdims=True)\nrevert_X = project_X @ inverse\n\nplt.subplot(144)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.show()```\n\nUnused variables:\n- `idx`\n- `project_V`\n- `revertedV`"}, {'reason': 'stop', 'result': "df = pd.read_csv('data.csv',\n                 header=None,\n                 usecols=['authors'])\n\n# print df\n```\n\nUnused variables:\n- `year`\n- `journal`\n- `media`\n- `paper_name`"}, {'reason': 'stop', 'result': 'X_test_refined = pd.DataFrame([])\nr_precisions = []\npbar = tqdm(data_test.groupby([\'playlist_pid\']))\nfor pid, df in pbar:\n    p_info = df[playlist_df.columns].iloc[0]\n    labels = y_test.loc[df.index]\n    \n    # Positive Tracks\n    positive_tracks_idx = labels[labels == 1].index\n    positive_tracks = data_test.loc[positive_tracks_idx]\n    sp_positive_tracks = vectorizer.transform(positive_tracks.values)\n    \n    # Negative Tracks\n    negative_tracks_idx = ~np.isin(data_test.index, positive_tracks_idx)\n    negative_tracks = data_test[negative_tracks_idx].drop(\n        playlist_df.columns, axis=1)\n    negative_playlist = np.array([p_info.values] * len(negative_tracks))\n    negative_playlist_samples = np.hstack([negative_tracks, negative_playlist])\n    sp_negative_tracks = vectorizer.transform(negative_playlist_samples)\n    \n    # Test Tracks\n    test_tracks = vstack([sp_negative_tracks, sp_positive_tracks])\n    index_order = negative_tracks.index.append(positive_tracks_idx)\n    \n    # Predict, r_precision\n    y_prob = AdaModel.predict_proba(test_tracks)\n    y_pred = np.argsort(-y_prob[:,1])\n    best_pred = index_order[y_pred]\n    if len(positive_tracks_idx) > 0:\n        r_precisions.append(r_precision(positive_tracks_idx, best_pred))\n    pbar.set_description("{}".format(np.mean(r_precisions)))\n```\n\nUnused variables:\n- `X_test_refined`\n- `r_precisions`'}, {'reason': 'stop', 'result': 'import eda; reload(eda);\nlog_data2 = log_data.loc[[x for x in log_data.index if x not in d_unique_idx.keys()]]\nsamples2 = log_data.loc[[x[0] for x in d_unique_idx.iteritems() if x[1]>1]]\nax = eda.features_boxplot(log_data2, samples2, samples2.index);```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'f1 =  lambda x: 3 - x\nf2 =  lambda x: 4 - 2 * x\nx = np.linspace(0, 2, 100)\nfig, ax = plt.subplots()\nax.plot(x, f1(x) , linewidth= 2)\nax.plot(x, f2(x) , linewidth= 2)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'unique_countries = None\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'print("start")\nregionLinks = linksPerRegion(regionNodes, links)\nprint(\'extracted links for every region\')```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_outputs = 10\n```\n\nUnused variables:\n- n_hidden1\n- n_hidden2'}, {'reason': 'stop', 'result': "from nltk.corpus import conll2002\n\netr = conll2002.chunked_sents('esp.train') # In Spanish\neta = conll2002.chunked_sents('esp.testa') # In Spanish\netb = conll2002.chunked_sents('esp.testb') # In Spanish\n```\n\nUnused variables:\n- `from itertools import chain`\n- `import nltk`\n- `from sklearn.metrics import classification_report, confusion_matrix`\n- `from sklearn.preprocessing import LabelBinarizer`\n- `import sklearn`"}, {'reason': 'stop', 'result': 'def lane_detection(image):\n    gray = grayscale(image)\n    # Define a kernel size and apply Gaussian smoothing\n    blur_gray = gaussian_blur(gray, 5)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 180\n    edges = canny(blur_gray, low_threshold, high_threshold) \n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0,imshape[0]),(450, 320), (490, 320), (imshape[1],imshape[0])]], dtype=np.int32)\n\n    masked_edges = region_of_interest(edges, vertices)\n\n    # Define the Hough transform parameters\n    rho = 2 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 15     # minimum number of votes (intersections in Hough grid cell)\n    min_line_len = 40 #minimum number of pixels making up a line\n    max_line_gap = 20    # maximum gap in pixels between connectable line segments\n\n    # Run Hough on edge detected image\n    line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\n\n    # Create a "color" binary image to combine with line image\n    color_edges = np.dstack((edges, edges, edges)) \n\n    # Draw the lines on the edge image\n    lines_edges = weighted_img(line_image, image, α=0.8, β=1., λ=0.)\n    return lines_edges\n```\n\nUnused variables:\n- kernel_size'}, {'reason': 'stop', 'result': "def draw_week_id(id,start_date='wk_0',end_date='wk_68',figsieze=(16,9)):\n    shop_info.loc[id,start_date:end_date].T.plot(figsize=(16,9))\n    \ndef draw_week_ids(ids,start_date='wk_0',end_date='wk_68',figsieze=(16,9)):\n    shop_info.loc[ids,start_date:end_date].T.plot(figsize=(16,9))\n    \ndef draw_ids(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):\n    if by_week:\n        xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n        shop_info.loc[ids,start_date:end_date].T.plot(figsize=figsize,ax=ax)\n    else: \n        shop_info.loc[ids,start_date:end_date].T.plot(figsize=figsize)\n    plt.show()\n    \ndef draw_ids_avg(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=True,figsize=(70,10)):\n    if by_week:\n        xmajorLocator   = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[ids,start_date:end_date].mean(axis=0).plot(figsize=figsize)\n    plt.show()\n    \ndef draw_ids_diff(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):  \n    if by_week:\n        xmajorLocator   = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    (shop_info.loc[ids[0],start_date:end_date]-shop_info.loc[ids[1],start_date:end_date]).plot(figsize=figsize)\n    \ndef draw_id(id,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):\n    if by_week:\n        xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id,start_date:end_date].plot(legend=True,figsize=figsize)\n```\n\n\nUnused variables: None"}, {'reason': 'stop', 'result': "fig = plt.figure(figsize = (12, 7))\n\nax1 = fig.add_subplot(231)\np = ax1.pcolor(inc,scr, ZU)\nc = ax1.contour(inc,scr, ZU, [0.92,0.94,0.96,0.98], colors=['k','k','k','k'])\nplt.clabel(c, manual = [(0.02,0.25), (0.05,0.25), (0.07,0.15), (0.09,0.05)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax1)\nax1.set_ylabel('Screening Rate (years $^{-1}$)')\nt = ax1.text(0.05, 0.45, 'Uninfected', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax1.set_ylim(0, 0.5)\nax1.set_xlim(0, 0.1)\n\nax2 = fig.add_subplot(232)\np = ax2.pcolor(inc,scr, ZS)\nc = ax2.contour(inc,scr, ZS, (0.001,0.002,0.003), colors='k', manual=True)\nplt.clabel(c, manual = [(0.03,0.15), (0.06,0.15), (0.09,0.15)], fmt='%1.3f')\ncb = fig.colorbar(p, ax=ax2)\nt = ax2.text(0.05, 0.45, 'Infected, Symptomatic', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax2.set_ylim(0, 0.5)\nax2.set_xlim(0, 0.1)\n\nax3 = fig.add_subplot(233)\np = ax3.pcolor(inc,scr, ZA)\nc = ax3.contour(inc,scr, ZA, (0.02,0.04,0.06,0.08), colors='k')\nplt.clabel(c, manual = [(0.02,0.25), (0.05,0.25), (0.07,0.15), (0.09,0.05)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax3)\nt = ax3.text(0.05, 0.45, 'Infected, Asymptomatic', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax3.set_ylim(0, 0.5)\nax3.set_xlim(0, 0.1)\n\nax4 = fig.add_subplot(234)\np = ax4.pcolor(inc,scr, Zprev)\nc = ax4.contour(inc,scr, Zprev, (0.02,0.04,0.06,0.08), colors='k')\nplt.clabel(c, manual = [(0.02,0.25), (0.05,0.25), (0.07,0.15), (0.09, 0.05)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax4)\nax4.set_xlabel('Incidence  (years $^{-1}$)')\nax4.set_ylabel('Screening Rate (years $^{-1}$)')\nt = ax4.text(0.05, 0.45, 'Prevalence', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax4.set_ylim(0, 0.5)\nax4.set_xlim(0, 0.1)\n\nax5 = fig.add_subplot(235)\np = ax5.pcolor(inc,scr, Ztest)\nc = ax5.contour(inc,scr, Ztest, (0.2,0.4), colors='k')\nplt.clabel(c, manual = [(0.045,0.2), (0.045,0.5)], fmt='%1.1f')\ncb = fig.colorbar(p, ax=ax5)\nax5.set_xlabel('Incidence  (years $^{-1}$)')\nt = ax5.text(0.05, 0.45, 'Testing Rate', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax5.set_ylim(0, 0.5)\nax5.set_xlim(0, 0.1)\n\nax6 = fig.add_subplot(236)\np = ax6.pcolor(inc,scr, Zdiag)\nc = ax6.contour(inc,scr, Zdiag, (0.02,0.04,0.06), colors='k')\nplt.clabel(c, manual = [(0.04,0.2), (0.06,0.4), (0.09,0.35)], fmt='%1.2f')\ncb = fig.colorbar(p, ax=ax6)\nax6.set_xlabel('Incidence (years $^{-1}$)')\nt = ax6.text(0.05, 0.45, 'Diagnosis Rate', ha='center', size='large')\nt.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='None'))\nax6.set_ylim(0, 0.5)\nax6.set_xlim(0, 0.1)\n\nplt.show()```"}, {'reason': 'stop', 'result': 'errors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'def generate_line(model, words, seed=None, rhymes=None):\n    syllables_left = 10\n    \n    line= []\n    start_probs = model.startprob_\n    emission_probs = model.emissionprob_\n    transition_probs = model.transmat_\n    \n    start_state = np.random.choice(len(start_probs), p = start_probs)\n    \n    if seed is not None:\n        possible_start_emissions = np.where(rhymes[seed] == 1)\n        probs = np.array(emission_probs[start_state][possible_start_emissions])\n\n        scaled_probs = probs / sum(probs)\n        while True:\n            start_emission = np.random.choice(possible_start_emissions[0], p=scaled_probs)\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1 :\n                break\n    else:\n        while True:\n            start_emission = np.random.choice(len(emission_probs[start_state]), p=emission_probs[start_state])\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1:\n                break\n    \n    line.append(start_emission)\n    start_stress = poetrytools.stress(words[start_emission])\n    syllables_left -= len(start_stress)\n    \n    if len(start_stress) == 1:\n        prev_starting_stress = 1\n    else:\n        prev_starting_stress = int(start_stress[0])\n\n    curr_state = start_state\n    while syllables_left > 0:\n        possible_transitions = transition_probs[curr_state]\n        curr_state = np.random.choice(len(possible_transitions), p=possible_transitions)\n        possible_emissions = emission_probs[curr_state]\n        while True:\n            curr_emission = np.random.choice(len(possible_emissions), p=possible_emissions)\n            curr_stress = poetrytools.stress(words[curr_emission])\n            if len(curr_stress) == 1:\n                prev_starting_stress = 1 - prev_starting_stress\n                syllables_left -= 1\n                break\n            elif len(curr_stress) > syllables_left or int(curr_stress[-1]) == prev_starting_stress:\n                continue\n            else:\n                prev_starting_stress = int(curr_stress[0])\n                syllables_left -= len(curr_stress)\n                break\n        line.append(curr_emission)\n\n    return line\n\ndef convert_line(sample, words):\n    ret = \'\'\n    i = 0\n    for word in reversed(sample):\n        curr_word = words[word]\n        if i == 0 or (curr_word == \'i\'):\n            ret += curr_word.title() + \' \'\n        else:\n            ret += curr_word + \' \'\n        i += 1\n    return ret\n\ndef generate_pair(model, words, rhymes):\n    while True:\n        a_line = generate_line(model, words)\n        seed = a_line[0]\n        if len(np.where(rhymes[seed] == 1)[0]) > 0:\n            b_line = generate_line(model, words, seed, rhymes)\n            return a_line, b_line\n        \ndef generate_rhyming_and_meter_sonnet():\n    sonnet = \'\'\n    a_lines = []\n    b_lines = []\n    \n    for _ in range(4):\n        a_line, b_line = generate_pair(reversed_quatrain_model, quatrain_words, quatrain_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    for i in range(2):\n        sonnet += convert_line(a_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(a_lines[2 * i + 1], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i + 1], quatrain_words) + \'\\n\'\n    \n    a_lines = []\n    b_lines = []\n    \n    for _ in range(2):\n        a_line, b_line = generate_pair(reversed_volta_model, volta_words, volta_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    sonnet += convert_line(a_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(a_lines[1], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[1], volta_words) + \'\\n\'\n    \n    a_line, b_line = generate_pair(reversed_couplet_model, couplet_words, couplet_rhymes)\n    sonnet += convert_line(a_line, couplet_words) + \'\\n\'\n    sonnet += convert_line(b_line, couplet_words) + \'\\n\'\n    \n    return sonnet\n\ndef generate_10_rhyming_and_meter_sonnets():\n    sonnets = \'\'\n    for i in range(10):\n        print(\'Generating Sonnet \' + str(i + 1))\n        sonnets += str(i) + \'\\n\' + generate_rhyming_and_meter_sonnet() + \'\\n\'\n    \n    f = open("project2data/rhyming_and_meter_shakespeare.txt","w")\n    f.write(sonnets)\n    return sonnets\n```\n\n\nUnused variables:\n- last_stress'}, {'reason': 'stop', 'result': '# Train the model\nAdaModel = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                             n_estimators=100, learning_rate=0.05)\nAdaModel = AdaModel.fit(X_train, y_train)\ny_pred = AdaModel.predict(X_train)\n```\n\nUnused variables:\n- `y_pred`'}, {'reason': 'stop', 'result': 'm = sfdmap.SFDMap()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Plot Frequency again\nsign_frequencies = get_frequencies(y_train_augmented, sign_dict)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nind = np.arange(len(sign_dict))\nwidth = 0.8\n\nrects = ax.bar(ind, sign_frequencies.values(), width, align="edge", alpha=0.5)\nax.set_ylabel(\'Frequency\')\nax.set_title(\'Traffic Sign Classes\')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(sign_frequencies.keys(), rotation=90)\nplt.show()```\n\nUnused variables: \n- classes\n- rects'}, {'reason': 'stop', 'result': '# Spase PCA\nimport pywt\n\nclass SPC(object):\n    \n    def __init__(self, number_of_components,max_iter=10, threshold_val=1.5 ):\n        \n        \n        """Initialize the SPC object\n        \n        Positional arguments:\n        number_of_components -- the number of sparse principal components \n        to compute, must be between 1 and p (total number of features)\n        \n        Keyword argument:\n        max_iter -- the number of iterations to perform (default=10)\n        threshold_val -- value of the lambda regularisation \n        parameter (default=10)\n        """\n        self.number_of_components=number_of_components\n        self.max_iter=max_iter\n        self.threshold_val=threshold_val\n    \n    def fit(self, X_):\n        """learn the sparse pc of a data matrix, return sparse estimates\n        of the left and right singular vectors (U and V respectively) \n        as well as the standard principal components loading matrix W\n    \n        Positional arguments:\n        X_ -- training data matrix, as numpy ndarray\n        \n        """ \n        print("computing sparse principal components...")\n        print("computing SVD of data matrix...")\n        U, s, V = np.linalg.svd(X_, full_matrices=True)  \n        cnt = 0\n        self.U = U\n        self.W = V.T\n        def normalize(vector):\n            norm=np.linalg.norm(vector)\n            if norm>0:\n                return vector/norm\n            else:\n                return vector\n        print("starting iterations...")\n        while True:\n           \n            self.V = pywt.threshold(np.dot(U[:self.number_of_components],X_), self.threshold_val)\n            self.U = np.dot(self.V,X_.T)\n            self.U = np.array([normalize(u_i) for u_i in self.U])\n            if cnt%2==0:\n                print("{} out of {} iterations".format(cnt,self.max_iter))\n            cnt += 1\n            if cnt == self.max_iter:\n                self.V = np.array([normalize(v_i) for v_i in self.V])\n                break\n        print("...finish")\n        return self.U, self.V, self.W\n    \n    def transform(self, X_, k=2):\n        X_reduced_spca     = np.dot(X_, np.dot(self.V[:k].T, self.V[:k]))\n        return X_reduced_spca\nmy_spca  = SPC(2,3000,0.1)\nmy_spca.fit(X)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_nolm_aer, label='AER 3.2')\nplt.plot(f/3e10, y_nolm_hitran, label='HITRAN')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables:\n- l"}, {'reason': 'stop', 'result': "sign_name = pd.read_csv('signnames.csv')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': '```python\nm = len(X_train)\nplt.figure(figsize=(11, 4))\nfor subplot, learning_rate in ((121, 1), (122, 0.5)):\n    sample_weights = np.ones(m)\n    for i in range(5):\n        plt.subplot(subplot)\n        svm_clf = SVC(kernel="rbf", C=0.05, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n        y_pred = svm_clf.predict(X_train)\n        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n        plt.title("learning_rate = {}".format(learning_rate), fontsize=16)\n\nplt.subplot(121)\nplt.text(-0.7, -0.65, "1", fontsize=14)\nplt.text(-0.6, -0.10, "2", fontsize=14)\nplt.text(-0.5,  0.10, "3", fontsize=14)\nplt.text(-0.4,  0.55, "4", fontsize=14)\nplt.text(-0.3,  0.90, "5", fontsize=14)\nsave_fig("boosting_plot")\nplt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "for (name, prd) in ('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf):\n    prd.fit(X_train, y_train)```\n\nUnused variables:\n- name"}, {'reason': 'stop', 'result': '### União do Treino e Teste \njoin = pd.concat([train[colunasTeste], test])```\n\nUnused variables:\n- frames'}, {'reason': 'stop', 'result': '# Get the predictions on the entire dataset\nlogits = model.predict(X)\npreds = tf.argmax(logits, axis=1)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# import dependencies\nimport json\nimport pandas as pd\nimport os\nimport csv\nimport numpy\nimport requests\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n\n#set up API URL\napi_key = "a2rOTe8PfBiJBwTOSlRbteARqAEa0s6DsRyoSOOF"\nurl = \'https://api.data.gov/ed/collegescorecard/v1/schools.json?\'\n\nsearch = \'school.degrees_awarded.predominant=3&_fields=id,school.name,latest.cost.tuition.in_state,latest.cost.tuition.out_of_state,school.region_id,latest.earnings.10_yrs_after_entry.median,latest.earnings.6_yrs_after_entry.median,latest.repayment.5_yr_repayment.completers_rate,latest.repayment.7_yr_repayment.completers_rate,latest.repayment.3_yr_repayment.completers_rate\'\n\nsearch_url = url + search + \'&api_key=\' + api_key\n\nresponse = requests.get(search_url)\nresponse_json = response.json()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_nolm_aer, label='AER')\nplt.plot(f/3e10, y_lm, label='AER-LM')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables: `l`"}, {'reason': 'stop', 'result': 'with tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "# Pymc model\ncluster_means = []\nnp.random.seed(45)\nwith pm.Model() as model:\n    pi = pm.Dirichlet('pi', np.ones(K))\n    comp_dist = []\n    mu = []\n    sigma_sq = []\n    cov = []\n    for i in range(K):\n        temp_mean = np.random.randint(low=20, high=230, size=D)\n        mu.append(pm.Normal('mu%i' % i, temp_mean, 20, shape=D))\n        sigma_sq.append(pm.InverseGamma('sigma_sq%i' % i, 1, 1, shape=D))\n        cov.append(tt.nlinalg.alloc_diag(sigma_sq[i]))\n        comp_dist.append(pm.MvNormal.dist(mu=mu[i], cov=cov[i]))      \n        cluster_means.append(temp_mean)\n    xobs = pm.Mixture('x_obs', pi, comp_dist,\n                      observed=X_shared)```\n\nUnused variables:\n- cluster_means\n- pi\n- sigma_sq\n- cov"}, {'reason': 'stop', 'result': 'logreg = LogisticRegression(solver=\'liblinear\')\n\nresult = logreg.fit(X_train,y_train)\n\nprint("Coeffs ",logreg.coef_)\nprint("Intercept ", logreg.intercept_)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Create a decision tree regressor object\n    regressor = DecisionTreeRegressor()\n\n    # Create a dictionary for the parameter \'max_depth\' with a range from 1 to 10\n    params = {\'max_depth\': range(1, 11)}\n\n    # Transform \'performance_metric\' into a scoring function using \'make_scorer\' \n    scoring_fnc = make_scorer(performance_metric)\n\n    # Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values \'regressor\', \'params\', \'scoring_fnc\', and \'cv_sets\' respectively.\n    grid = GridSearchCV(regressor, params, scoring=scoring_fnc, cv=cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_```\n\nUnused variables:\n- `grid`'}, {'reason': 'stop', 'result': "# CREATE DATAFRAME FROM BOOKINGS_DF_EX GROUPED BY arr_port\nBOOKINGS_DF_EX.groupby(['arr_port']).sum()```\n\nUnused variables:\n- BOOKINGS_GROUP_BY_ARR_PORT"}, {'reason': 'stop', 'result': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '#(x,y) values for 4 tiers *note x value doesn\'t change only y does\nx_axis = [x for x in range(1,9)]\ny_axis1 = df0_10k_grouped["earnings_cost_ratio"]\ny_axis2 = df10_18k_grouped["earnings_cost_ratio"]\ny_axis3 = df18_32_grouped["earnings_cost_ratio"]\ny_axis4 = df32_grouped["earnings_cost_ratio"]\n\n#tiered \nlessThan10k = plt.plot(x_axis, y_axis1, \'go--\', linewidth=2, markersize=8, color=\'blue\', label=\'lessThan10k\')\nTenKto18k = plt.plot(x_axis, y_axis2, \'go--\', linewidth=2, markersize=8, color=\'green\', label=\'10Kto18k\')\nEighteenTo32k = plt.plot(x_axis, y_axis3, \'go--\', linewidth=2, markersize=8, color=\'yellow\', label=\'18kTo32k\')\ngreaterThan32k = plt.plot(x_axis, y_axis4, \'go--\', linewidth=2, markersize=8, color=\'red\', label=">32k")\n\nplt.title("Tiered Earnings-Cost ratio per region ($)")\nplt.xlabel("Region")\nplt.ylabel("Tiered Earnings-cost Ratio")\nplt.grid()\nplt.legend(title=["Tiered Tuitions"])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n#save fig\n#plt.savefig("Plots/TieredEarningsCostRatio.png")```\n\nUnused variables:\n- bins\n- bin_names'}, {'reason': 'stop', 'result': 'U = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\nfor state_action, value in q_agent.Q.items():\n    state, action = state_action\n    if U[state] < value:\n        U[state] = value```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'if use_attention is False:\n    with tf.variable_scope("beam_search"):\n        beam_width = 4\n        start_tokens = tf.fill([config.batch_size], dataset.SOS)\n        bm_dec_initial_state = tf.contrib.seq2seq.tile_batch(\n            encoder_state, multiplier=beam_width)\n        bm_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n            cell=decoder_cell,\n            embedding=embedding,\n            start_tokens=start_tokens,\n            initial_state=bm_dec_initial_state,\n            beam_width=beam_width,\n            output_layer=output_proj,\n            end_token=dataset.EOS\n        )\n        bm_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n            bm_decoder, maximum_iterations=config.tgt_maxlen)```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'vectorizer = HashingVectorizer(decode_error=\'ignore\', n_features=2 ** 18,\n                               non_negative=True)\n\n# Iterator over parsed Reuters SGML files.\ndata_stream = stream_reuters_documents()\n\n# We learn a binary classification between the "acq" class and all the others.\n# "acq" was chosen as it is more or less evenly distributed in the Reuters\n# files. For other datasets, one should take care of creating a test set with\n# a realistic portion of positive instances.\nall_classes = np.array([0, 1])\npositive_class = \'acq\'\n\n# Here are some classifiers that support the `partial_fit` method\npartial_fit_classifiers = {\n    \'SGD\': SGDClassifier(),\n    \'Perceptron\': Perceptron(),\n    \'NB Multinomial\': MultinomialNB(alpha=0.01),\n    \'Passive-Aggressive\': PassiveAggressiveClassifier(),\n}\n\n\ndef get_minibatch(doc_iter, size, pos_class=positive_class):\n    """Extract a minibatch of examples, return a tuple X_text, y.\n\n    Note: size is before excluding invalid docs with no topics assigned.\n\n    """\n    data = [(u\'{title}\\n\\n{body}\'.format(**doc), pos_class in doc[\'topics\'])\n            for doc in itertools.islice(doc_iter, size)\n            if doc[\'topics\']]\n    if not len(data):\n        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n    X_text, y = zip(*data)\n    return X_text, np.asarray(y, dtype=int)\n\n\ndef iter_minibatches(doc_iter, minibatch_size):\n    """Generator of minibatches."""\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\n    while len(X_text):\n        yield X_text, y\n        X_text, y = get_minibatch(doc_iter, minibatch_size)\n\n\n# test data statistics\ntest_stats = {\'n_test\': 0, \'n_test_pos\': 0}\n\n# First we hold out a number of examples to estimate accuracy\nn_test_documents = 1000\ntick = time.time()\nX_test_text, y_test = get_minibatch(data_stream, 1000)\nparsing_time = time.time() - tick\ntick = time.time()\nX_test = vectorizer.transform(X_test_text)\nvectorizing_time = time.time() - tick\ntest_stats[\'n_test\'] += len(y_test)\ntest_stats[\'n_test_pos\'] += sum(y_test)\nprint("Test set is %d documents (%d positive)" % (len(y_test), sum(y_test)))\n\n\ndef progress(cls_name, stats):\n    """Report progress information, return a string."""\n    duration = time.time() - stats[\'t0\']\n    s = "%20s classifier : \\t" % cls_name\n    s += "%(n_train)6d train docs (%(n_train_pos)6d positive) " % stats\n    s += "%(n_test)6d test docs (%(n_test_pos)6d positive) " % test_stats\n    s += "accuracy: %(accuracy).3f " % stats\n    s += "in %.2fs (%5d docs/s)" % (duration, stats[\'n_train\'] / duration)\n    return s\n\n\ncls_stats = {}\n\nfor cls_name in partial_fit_classifiers:\n    stats = {\'n_train\': 0, \'n_train_pos\': 0,\n             \'accuracy\': 0.0, \'accuracy_history\': [(0, 0)], \'t0\': time.time(),\n             \'runtime_history\': [(0, 0)], \'total_fit_time\': 0.0}\n    cls_stats[cls_name] = stats\n\nget_minibatch(data_stream, n_test_documents)\n# Discard test set\n\n# We will feed the classifier with mini-batches of 1000 documents; this means\n# we have at most 1000 docs in memory at any time.  The smaller the document\n# batch, the bigger the relative overhead of the partial fit methods.\nminibatch_size = 1000\n\n# Create the data_stream that parses Reuters SGML files and iterates on\n# documents as a stream.\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\ntotal_vect_time = 0.0\n\n# Main loop : iterate on mini-batches of examples\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n\n    tick = time.time()\n    X_train = vectorizer.transform(X_train_text)\n    total_vect_time += time.time() - tick\n\n    for cls_name, cls in partial_fit_classifiers.items():\n        tick = time.time()\n        # update estimator with examples in the current mini-batch\n        cls.partial_fit(X_train, y_train, classes=all_classes)\n\n        # accumulate test accuracy stats\n        cls_stats[cls_name][\'total_fit_time\'] += time.time() - tick\n        cls_stats[cls_name][\'n_train\'] += X_train.shape[0]\n        cls_stats[cls_name][\'n_train_pos\'] += sum(y_train)\n        tick = time.time()\n        cls_stats[cls_name][\'accuracy\'] = cls.score(X_test, y_test)\n        cls_stats[cls_name][\'prediction_time\'] = time.time() - tick\n        acc_history = (cls_stats[cls_name][\'accuracy\'],\n                       cls_stats[cls_name][\'n_train\'])\n        cls_stats[cls_name][\'accuracy_history\'].append(acc_history)\n        run_history = (cls_stats[cls_name][\'accuracy\'],\n                       total_vect_time + cls_stats[cls_name][\'total_fit_time\'])\n        cls_stats[cls_name][\'runtime_history\'].append(run_history)\n\n        if i % 3 == 0:\n            print(progress(cls_name, cls_stats[cls_name]))\n    if i % 3 == 0:\n        print(\'\\n\')```'}, {'reason': 'stop', 'result': "fig, ax = plt.subplots(figsize=(8, 6))\n\nax.hist(old_faithful_df.std_waiting, bins=20, color=blue, lw=0, alpha=0.5);\n\nax.set_xlabel('Standardized waiting time between eruptions');\nax.set_ylabel('Number of eruptions');```\n\nUnused variables:\n- n_bins"}, {'reason': 'stop', 'result': "####### Defining network #######\n# input: state x\n# output: control u\n\ninput_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\nfc1 = tf.layers.dense(inputs=input_layer, units=1, activation=tf.nn.tanh, name='fc1', reuse=tf.AUTO_REUSE)\nu = tf.layers.dense(inputs=fc1, units=1, activation=tf.nn.tanh, name='fc_out', reuse=tf.AUTO_REUSE)\n\n### LOSS FUNCTION ### \nloss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n\nxs = x\nus = u\n\nfor i in range(T):\n    # Dynamics: advancing the system dynamics\n    Ax = tf.matmul(A, x, name='Ax'+str(i))\n    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n\n    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n    \n    fc1 = tf.layers.dense(inputs=tf.transpose(x), units=1, name='fc1', reuse=True)\n    u = tf.layers.dense(inputs=fc1, units=1, name='fc_out', reuse=True)\n    \n    xs = tf.concat([xs, x], 1)\n    us = tf.concat([us, u], 1)\n    \nopt = tf.train.GradientDescentOptimizer(0.0001)\ntrain = opt.minimize(loss)\ngrads_and_vars = opt.compute_gradients(loss)\n```\n\nUnused variables:\n- `input_layer`\n- `fc1`\n- `xs`\n- `us`\n- `grads_and_vars`"}, {'reason': 'stop', 'result': 'ids_zero_morethan1_count_in_last_three_week_but_no_zero_in_last_two_week=[1053, 432, 1464, 1465, 1993, 1109, 1269]```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'sum(1 for completed_process, outcome in outcomes if completed_process.stderr == "")```\n\nUnused variables:\n- `outcome`'}, {'reason': 'stop', 'result': '### Arbol de sklearn ###\narbol = DecisionTreeClassifier(max_depth=3)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '```python\n# finding minimum discount price out of hotel name - checking date - discount code group and fixing data\nmost_checkins["Discount Price"] = most_checkins.groupby([\'Hotel Name\', \'Checkin Date\', \'Discount Code\'])["Discount Price"].transform(\'min\')\nmost_checkins.drop_duplicates(subset=["Hotel Name", "Checkin Date", "Discount Code"], inplace=True)\nmost_checkins.sort_values(by=["Hotel Name", "Checkin Date", "Discount Code"], ascending=True, inplace=True)\nmost_checkins[\'Discount Price\'].replace(sys.maxsize, -1, inplace=True)\n\n# taking only needed data\ncheckin_hotel_discount = most_checkins[["Hotel Name", "Checkin Date", "Discount Code", "Discount Price"]].reset_index()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "# split our dataset into training / testing sets\ntrain_test_split = int(np.ceil(2*len(y)/float(3)))   # set the split point\n\n# partition the training set\nX_train = X[:train_test_split,:]\ny_train = y[:train_test_split]\n\n# keep the last chunk for testing\nX_test = X[train_test_split:,:]\ny_test = y[train_test_split:]\n\n# NOTE: to use keras's RNN LSTM module our input must be reshaped to [samples, window size, stepsize] \nX_train = np.reshape(X_train, (X_train.shape[0], window_size, 1))\nX_test = np.reshape(X_test, (X_test.shape[0], window_size, 1))```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'data_pre = []\nfor e in data:\n    ret, th = cv2.threshold(e, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    kernel = np.ones((3,3), np.uint8)\n    dilation = cv2.dilate(th, kernel, iterations=1)\n    erosion = cv2.erode(dilation, kernel, iterations=1)\n\n    data_pre.append(erosion)```\n\nUnused variables:\n- `ret`'}, {'reason': 'stop', 'result': '# get probabilities for all images\nprobas_df = clf_df.decision_function(act_test)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "```python\n# Set name of MODFLOW exe\n# Assumes executable is in user's path statement\nexe_name = 'mfusg'\nif platform.system() == 'Windows':\n    exe_name += '.exe'\nmfexe = exe_name\n\nmodelpth = os.path.join('data')\nmodelname = 'zaidel'\n\n# Make sure modelpth directory exists\nif not os.path.exists(modelpth):\n    os.makedirs(modelpth)\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'for i in range(1000):\n    try:\n        url = fuzzer()\n        http_program(url)\n        print("Success!")\n    except ValueError:\n        pass\n```\n\nUnused variables:\n- `result`'}, {'reason': 'stop', 'result': '# North Lincolnshire\n# find steady state based on 2012 data\n\ncov_2012 = 0.100807801953\nadpc_2012 = 0.0111652211547\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.173269822929\nadpc_2013 = 0.0216211803756\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nsol_n_lincs = odeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```\n\nUnused variables:\n- cov_2012\n- adpc_2012\n- cov_2013\n- adpc_2013'}, {'reason': 'stop', 'result': 'import time\nimport os\nimport importlib\n\npath="/Users/louis/Google Drive/M.Sc-DIRO-UdeM/IFT6135-Apprentissage de représentations/assignment4/"\nif os.path.isdir(path):\n    os.chdir(path)\nelse:\n    os.chdir("./")\nprint(os.getcwd())\n\nimport matplotlib.pyplot as plt\nfrom scipy.misc import imresize\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom torchvision.utils import save_image\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom PIL import Image\nimport itertools\nimport inception_score \nimport GAN_CelebA\nimportlib.reload(GAN_CelebA)\nimportlib.reload(inception_score)\n\nuse_cuda = torch.cuda.is_available()\ntorch.manual_seed(999)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(999)\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'X, y, accuracy, training_op = tf.get_collection("my_important_ops")```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'fig = plt.figure(figsize=(8,6)) \nx = df_county_data["Household Size"]\ny = df_county_data["Graduation Rate"] \nplt.scatter(x, y, color="g", marker="o", alpha=0.9) \n\n#Calculate and add R2 value\nmask = ~np.isnan(x) & ~np.isnan(y)\n\n#Add regression line\nsns.regplot(df_county_data["Household Size"], \n              df_county_data["Graduation Rate"], color=\'r\',label = "Household Size" )\n\n# Incorporate the other graph properties\nplt.title("Correlation between Graduation rates and Household Size")\nplt.ylabel("Graduation Rate")\nplt.xlabel("House Size") \n\nplt.legend(loc=\'best\')\nplt.grid(True)\nsns.set_style(\'whitegrid\')\nplt.text(3.4, 0.925, "Note:\\nThere is a slight inverse correlation shown in this graph.")\n\nplt.savefig("Images/County_Grad_House_Rates2.png", bbox_inches = "tight")\nplt.show()```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'treino = train\nteste = test\nuniao = join```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def showPrecisionRecallPairByLabel(precision_by_label, recall_by_label, label_encoder, classifier_name, colors):\n    labels = []\n    for i in range(len(precision_by_label)):\n        label = label_encoder.inverse_transform([i])[0]\n        labels.append(label)\n    \n    y_pos = np.arange(len(labels))    \n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n\n    ax1.invert_xaxis()\n    ax1.yaxis.tick_right()\n    \n    ax1.set_yticks(y_pos)\n    ax1.set_yticklabels(labels)\n    \n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels(labels)\n        \n    ax1.barh(y_pos, precision_by_label, color=colors[0] , label="precision")\n    ax2.barh(y_pos, recall_by_label,    color=colors[1],  label=\'recall\')\n\n    ax1.set_title(\'Precision( \' + classifier_name + \')\')\n    ax2.set_title(\'Recall (\' + classifier_name + \')\')\n    \n    plt.grid()\n    plt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "from ipywidgets import interact\nfrom kf_book.book_plots import IntSlider, FloatSlider\n\ndef plot_FPFT(F00, F01, F10, F11, covar):   \n    plt.figure()\n    dt = 1.\n    x = np.array((0, 0.))\n    P = np.array(((1, covar), (covar, 2)))\n    F = np.array(((F00, F01), (F10, F11)))\n    plot_covariance_ellipse(x, P)\n    plot_covariance_ellipse(x, F @ P @ F.T, ec='r')\n    plt.gca().set_aspect('equal')\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n    plt.xlabel('position')\n    plt.ylabel('velocity')\n    plt.show()\n                 \ninteract(plot_FPFT, \n         F00=IntSlider(value=1, min=0, max=2), \n         F01=FloatSlider(value=1, min=0, max=2, description='F01(dt)'),\n         F10=FloatSlider(value=0, min=0, max=2),\n         F11=FloatSlider(value=1, min=0, max=2),\n         covar=FloatSlider(value=0, min=0, max=1));```\n\nUnused variables: None"}, {'reason': 'stop', 'result': 'fig = plt.figure()\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.tight_layout()\n    plt.imshow(example_data[i][0], cmap=\'gray\', interpolation=\'none\')\n    plt.title("Prediction: {}/{}".format(le.inverse_transform(output.data.max(1, keepdim=True)[1][i])[0],\n                                         le.inverse_transform(example_targets[i].view(-1, 1)[0])[0]))\n    plt.xticks([])\n    plt.yticks([])```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'def squared_error(x_values, y_values, m, b, x):\n    pass\n```\n\nUnused variables:\n- `x_values`\n- `y_values`\n- `m`\n- `b`\n- `x`'}, {'reason': 'stop', 'result': "unique, counts = np.unique(y_train, return_counts=True)\nunique_test, counts_test = np.unique(y_test, return_counts=True)\n\nplt.hist(y_train, color = 'b', label = 'train', normed = True, bins=range(n_classes+1))\nplt.hist(y_test, color = 'r', alpha = 0.5, label = 'test', normed = True, bins=range(n_classes+1))\nplt.title('Distribution of the number of images per class \\n for train and test set')\nplt.xlabel('Class')\nplt.ylabel('Percentage of images')\nplt.show()\n```\n\nUnused variables:\n- `counts`\n- `counts_test`"}, {'reason': 'stop', 'result': '#Importing this because multiple deprecation warnings cluttering the output\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\ncontinuous = [\'ConvertedAge\', \'BreedRank\']\ndiscrete = [\n    \'AnimalType\',\n    \'Female\',\n    \'Intact\',\n    \'MixedBreed\',\n    \'Named\',\n    \'TopBreed\',\n    \'PitBull\',\n    \'BlackCat\'\n]\ntarget = \'OutcomeType\'\n\n#For those missing an age, fill with the median age by animal type\ndata["ConvertedAge"] = data.groupby("AnimalType").transform(lambda x: x.fillna(x.median()))\ndata[continuous].describe().T\n\n#Turn categorical variables into binaries\ndata2 = pd.concat([data[target], data[continuous], pd.get_dummies(data[discrete])], axis=1)\n\ndiscrete = [\'AnimalType_Cat\', \'AnimalType_Dog\', \'Female_Female\', \'Female_Male\', \'Female_Unknown\',\n           \'Intact_Intact\', \'Intact_Spayed/Neutered\', \'Intact_Unknown\', \'MixedBreed_Known Breed Combo\',\n           \'MixedBreed_Mixed Breed\', \'MixedBreed_Nonmixed\', \'Named_Named\', \'Named_Unnamed\']\n\n\npredictors = continuous + discrete\ntarget = \'OutcomeType\'\n\n\n# Train/test split on the full dataset\nX = data2[predictors]\ny = data2[[target]]\nX_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)\n\n#Normalize the continuous variables\nss = StandardScaler()\nss.fit(X_train[continuous])   # Compute mean and std of training data\nX_train[continuous] = ss.transform(X_train[continuous])  # Use that mean and std to normalize columns of training data\nX_dev[continuous] = ss.transform(X_dev[continuous]) \n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'train_counter = []\ntest_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]```\n\nUnused variables:\n- train_losses\n- test_losses'}, {'reason': 'stop', 'result': 'def character(S, left_basis=s, right_basis=s, row_symmetry=None):\n    if isinstance(S, dict):\n        return sum(character(V,\n                             left_basis=left_basis, right_basis=right_basis, row_symmetry=row_symmetry) \n                   for V in S.values())\n    else:\n        basis = S.basis()\n        basis_element = basis.values().pop()[0]\n        P = basis_element.parent()\n        n = P.ncols()\n        r = P.nrows()\n        \n        charac = 0\n        if row_symmetry != "permutation":\n            q = PolynomialRing(QQ,\'q\',r).gens()\n\n        for nu in Partitions(n):\n            basis_nu = {}\n            charac_nu = 0\n            # Get the nu_isotypic part of the basis\n            for key, value in basis.iteritems():\n                if Partition(key[1]) == nu:\n                    basis_nu[key[0]] = value\n\n            # Use monomials to compute the character\n            if row_symmetry == "permutation":\n                for deg, b in basis_nu.iteritems():\n                    charac_nu += sum(m(Partition(deg)) for p in b)\n                if charac_nu != 0 :\n                    if left_basis == s :\n                        charac_nu = s(charac_nu).restrict_partition_lengths(r,exact=False)\n                    elif left_basis != m :\n                        charac_nu = left_basis(charac_nu)\n\n            # Or use directly the degrees\n            else:\n                for deg, b in basis_nu.iteritems():\n                    charac_nu += sum(prod(q[i]**deg[i] for i in range(0,len(deg))) for p in b)\n                if charac_nu != 0 :\n                    if left_basis == s :\n                        charac_nu = s.from_polynomial(charac_nu).restrict_partition_lengths(r,exact=False)           \n                    else:\n                        charac_nu = left_basis.from_polynomial(charac_nu)\n\n            # Make the tensor product with s[nu]\n            if charac_nu != 0:\n                charac += tensor([charac_nu, right_basis(s(nu))])\n        return charac\n```\n\nUnused variables:\n- `P`\n- `q`'}, {'reason': 'stop', 'result': 'for n in range(1, nt):\n    rn = rho.copy()\n    for j in range(1, nx):\n        v = (vmax * (1 - rho / rhomax)) * (5 / 18)\n        f1 = v * rho\n        rho[1:] = rn[1:] - dt / dx * (f1[1:] - f1[0:-1])\n        rho[0] = 10\n```\n\nUnused variables:\n- Unused variable `n` in the outer loop\n- Unused variable `j` in the inner loop'}, {'reason': 'stop', 'result': "model={'boxsize':368}\nmodel['stride'] = 8\nparam={}\nparam['scale_search'] = [0.5, 1, 1.5, 2]\nmultiplier = [x * model['boxsize']*1.0 / oriImg.shape[0] for x in param['scale_search']]```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'with open ("mu_test_data/diff_mu_logL=2_1231_model") as model_21:\n    data_cleanup(model_21,temp_model_21)\nwith open ("mu_test_data/diff_mu_logL=2_5001_model") as model_25:\n    data_cleanup(model_25,temp_model_25)\nwith open ("mu_test_data/diff_mu_logL=2_1231_model") as model_27:\n    data_cleanup(model_27,temp_model_27)\nmodel_data_21 = np.array(temp_model_21)\nmodel_data_25 = np.array(temp_model_25)\nmodel_data_27 = np.array(temp_model_27)\n\nwith open ("mu_test_data/diff_mu_logL=2_1231_grads") as grads_21:\n    data_cleanup(grads_21,temp_grads_21)\nwith open ("mu_test_data/diff_mu_logL=2_5001_grads") as grads_25:\n    data_cleanup(grads_25,temp_grads_25)\nwith open ("mu_test_data/diff_mu_logL=2_1231_grads") as grads_27:\n    data_cleanup(grads_27,temp_grads_27)\ngrads_data_21 = np.array(temp_grads_21)\ngrads_data_25 = np.array(temp_grads_25)\ngrads_data_27 = np.array(temp_grads_27)\n\nfocus_model_21 = focus_sync(grads_data_21,model_data_21)\nfocus_model_25 = focus_sync(grads_data_25,model_data_25)\nfocus_model_27 = focus_sync(grads_data_27,model_data_27)\n\ndel_mu_21 = grads_data_21[:,4]\ndel_mu_25 = grads_data_25[:,4]\ndel_mu_27 = grads_data_27[:,4]\n\nrad_ad_21 = focus_model_21[:,10] - focus_model_21[:,11]\nrad_ad_25 = focus_model_25[:,10] - focus_model_25[:,11]\nrad_ad_27 = focus_model_27[:,10] - focus_model_27[:,11]\n\nD_21 = grads_data_21[:,-2]\nD_25 = grads_data_25[:,-2]\nD_27 = grads_data_27[:,-2]\n\nrad_frac_21 = focus_model_21[:,1]\nrad_frac_25 = focus_model_25[:,1]\nrad_frac_27 = focus_model_27[:,1]\n\nmass_frac_21 = grads_data_21[:,0]\nmass_frac_25 = grads_data_25[:,0]\nmass_frac_27 = grads_data_27[:,0]\n\nlogP_21 = focus_model_21[:,2]\nlogP_25 = focus_model_25[:,2]\nlogP_27 = focus_model_27[:,2]\n\nlogT_21 = focus_model_21[:,3]\nlogT_25 = focus_model_25[:,3]\nlogT_27 = focus_model_27[:,3]\n\nlogDens_21 = focus_model_21[:,4]\nlogDens_25 = focus_model_25[:,4]\nlogDens_27 = focus_model_27[:,4]\n\nopac_21 = focus_model_21[:,12]\nopac_25 = focus_model_25[:,12]\nopac_27 = focus_model_27[:,12]\n\nmu_21 = grads_data_21[:,1]\nmu_25 = grads_data_25[:,1]\nmu_27 = grads_data_27[:,1]\n\nmodel_degen_21 = data_focus(mu_21,0.134e1, model_data_21)\nmodel_degen_25 = data_focus(mu_25,0.134e1, model_data_25)\nmodel_degen_27 = data_focus(mu_27,0.134e1, model_data_27)'}, {'reason': 'stop', 'result': 'X_model = X[model_features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X_model, y, test_size=0.2, random_state= 5)\n\nscaler.fit(X_train[numeric_model_features])\nX_train[numeric_model_features] = scaler.transform(X_train[numeric_model_features])\nX_test[numeric_model_features] = scaler.transform(X_test[numeric_model_features])\n\nlogreg.fit(X_train,y_train)\n\ny_pred = logreg.predict(X_test)\n# save model stats\nprediction_probabilities = logreg.predict_proba(X_test)\n\ncross_val_scores = cross_val_score(logreg, X,y, cv=10, scoring=\'accuracy\')\ncross_validation_average = cross_val_scores.mean()\n\nprint("Accuracy:",metrics.accuracy_score(y_test, y_pred))\nprint("Precision:", metrics.precision_score(y_test, y_pred))\nprint("Recall:",metrics.recall_score(y_test, y_pred))\nprint("Log loss= ",log_loss(y_test, prediction_probabilities))\n\nutils.display_confusion_matrix(y_test, y_pred)```\n\nUnused variables:\n- X_train\n- y_train\n- X_test\n- y_test'}, {'reason': 'stop', 'result': 'n_epochs = 20\nbatch_size = 200\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'four_x_squared_plus_four_x_minus_ten = [(4, 2), (4, 1), (-10, 0)]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': 'max_norm_reg = max_norm_regularizer(threshold=1.0)\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_reg, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_reg, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'y_predictE = trainer.predict(finalXt)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'from arcgis import geometry\nfrom arcgis import features\n\ndef create_feature(map1, g):\n    try:\n        pt = geometry.Point(g)\n        feat = features.Feature(geometry=pt, attributes={\'name\': \'name\',\n                                                        \'type\': \'park\',\n                                                        \'surface\': \'dirt\'})\n        feature_layer.edit_features(adds=[feat])\n        print(str(g))\n        map1.draw(g)\n    except:\n        print("Couldn\'t create the feature. Try again, please...")```\n\nUnused variables:\n- oid'}, {'reason': 'stop', 'result': '# Plot the monthly revenues per year\nax = sns.lineplot(y=\'Revenue\', x=df.index.month, hue=df.index.year, data=df, palette="ch:r=-.5,l=.75")```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "'''Read data'''\n\ncfpy_az_df = read_table('data/u-233-cfpy-AZ-thermal.dat')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': 'threshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\ntf.assign(weights, clipped_weights)```\n\nUnused variables:\n- clip_weights'}, {'reason': 'stop', 'result': "```python\n### TODO: Obtain bottleneck features from another pre-trained CNN.\n\nINCEPTION_BNECK = 'bottleneck_features/DogInceptionV3Data.npz'\n\nbottleneck_features = np.load(INCEPTION_BNECK)\ntrain_incp_bn = bottleneck_features['train']\nvalid_incp_bn = bottleneck_features['valid']\ntest_incp_bn  = bottleneck_features['test']```"}, {'reason': 'stop', 'result': 'with tf.name_scope("train"):\n    initial_learning_rate = 0.1\n    decay_steps = 10000\n    decay_rate = 1/10\n    global_step = tf.Variable(0, trainable=False, name="global_step")\n    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n                                               decay_steps, decay_rate)\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n    training_op = optimizer.minimize(loss, global_step=global_step)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'from matplotlib.ticker import MultipleLocator\n\nfilters = ["SDSS_r"]\n\nalpha = 1.0\nxminorticks = 10\n\npcc.utils.setup_plot_defaults()\n\nfig = plt.figure(figsize=[8, 4])\nfig.subplots_adjust(left = 0.1, bottom = 0.13, top = 0.93,\n                right = 0.91, hspace=0, wspace = 0)\n## Label the axes\nxaxis_label_string = r\'$\\textnormal{Time, MJD (days)}$\'\nyaxis_label_string = r\'$\\textnormal{Flux, erg s}^{-1}\\textnormal{\\AA}^{-1}\\textnormal{cm}^{-2}$\'\n\nax1 = fig.add_subplot(111)\naxes_list = [ax1]\n\nfor filter_key in filters:\n    plot_label_string = r\'$\\rm{\' + sn.phot.data_filters[filter_key].filter_name.replace(\'_\', \'\\\\_\') + \'}$\'\n    plot_label_string_fake = r\'$\\rm{\' + sn_fake.phot.data_filters[filter_key].filter_name.replace(\'_\', \'\\\\_\') + \', simulated}$\'\n\n    ax1.errorbar(sn.phot.data[filter_key][\'MJD\'], sn.phot.data[filter_key][\'flux\'],\n                 yerr = sn.phot.data[filter_key][\'flux_err\'],\n                 capsize = 0, fmt = \'x\', color = sn.phot.data_filters[filter_key]._plot_colour,\n                 label = plot_label_string, ecolor = pcc.hex[\'batman\'], mec = pcc.hex["batman"],\n                 alpha = alpha)\n    ax1.fill_between(sn.lcfit.data[filter_key][\'MJD\'], sn.lcfit.data[filter_key][\'flux_upper\'], sn.lcfit.data[filter_key][\'flux_lower\'],\n                     color = pcc.hex["batman"],\n                     alpha = 0.8, zorder = 0)\n    \nax1.errorbar(sn_fake.phot.data[filter_key][\'MJD\'], sn_fake.phot.data[filter_key][\'flux\'],\n         yerr = sn_fake.phot.data[filter_key][\'flux_err\'],\n         capsize = 0, fmt = \'o\', color = pcc.hex[\'r\'],\n         label = plot_label_string_fake, ecolor = pcc.hex[\'batman\'], mec = pcc.hex["batman"],\n         alpha = alpha)\n    \nxminorLocator = MultipleLocator(xminorticks)\nax1.spines[\'top\'].set_visible(True)\nax1.xaxis.set_minor_locator(xminorLocator)\n\nplot_legend = ax1.legend(loc = \'upper right\', scatterpoints = 1, markerfirst = False,\n                  numpoints = 1, frameon = False, bbox_to_anchor=(1., 1.),\n                  fontsize = 12.)\n\nax1.set_ylabel(yaxis_label_string)\nax1.set_xlabel(xaxis_label_string)\n\noutpath = "/Users/berto/projects/LSST/cadence/SN2007uy_consistency_check_SDSS_r"\n\nfig.savefig(outpath + ".png", format = \'png\', dpi=500)\n```\n\nUnused variables:\n- `filters`\n- `alpha`\n- `xminorticks`\n- `axes_list`'}, {'reason': 'stop', 'result': 'googl = data[data.symbol == "GOOGL"]```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '```python\nclass EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, training, mask):\n    attn_output, _ = self.mha(x, x, x, mask)  \n    attn_output = self.dropout1(attn_output, training=training) \n    out1 = self.layernorm1(x + attn_output)  \n    \n    ffn_output = self.ffn(out1) \n    ffn_output = self.dropout2(ffn_output, training=training)  \n    out2 = self.layernorm2(out1 + ffn_output)\n    \n    return out2\n```\n\nUnused variables: `attn`'}, {'reason': 'stop', 'result': '@print_log("Awesome function")\ndef do_something(arg, **other_args):\n    print("Doing something")\n    return\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'sparsity = sparse_codes(Z)'}, {'reason': 'stop', 'result': "RM_D_Nm = sy.Symbol('RM_{D}[Nm]')```\n\nUnused variables:\nNone"}, {'reason': 'stop', 'result': "a = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 0].Invalid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 0].Invalid).tconfint_mean()```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': '#Meta-parameters\nembedding_size = 128\nhidden_size = 64\n\nbatch_size = 64\nepochs = 32\nvalidation_split = 0.2\n\ndataset_cut = -1\nmaxlen = 25```\n\nUnused variables:\n- dropout\n- recurrent_dropout'}, {'reason': 'stop', 'result': "```parametersNaiveBayes = {\n    'priors':priors\n}\n\n(_, random_bayes) = correr_randomized_y_mostrar(\n    GaussianNB(), \n    parametersNaiveBayes, \n    5, \n    5\n)\n\nverTiempo(tiempo_bayes, tiempo_random_bayes)```"}, {'reason': 'stop', 'result': 'X_train, X_test, y_train, y_test = train_test_split(X, y)\nxgb = xgboost.XGBClassifier(n_estimators=200)\nxgb.fit(X_train, y_train)\nprediction = xgb.predict_proba(X_test)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Dudley\n# find steady state based on 2012 data\n\ncov_2012 = 0.0750667240187\nadpc_2012 = 0.0057129570304\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.238873910562\nadpc_2013 = 0.0199612670162\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nsol_dudley = odeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )```\n\nUnused variables:\n- cov_2012\n- adpc_2012\n- cov_2013\n- adpc_2013'}, {'reason': 'stop', 'result': 'import numpy as np\n\n# v1 . v2 = |v1| |v2| cos(a)\n# <=> a = cos-1( (v1.v2) / |v1||v2| )\n\n# 5 degrees tolerance is fine!\n\ndef debug_vectors(v1, v2):\n    print("v1: {0}, v2: {1}".format(v1, v2))\n    print("Angle: {0}".format(v_angle(v1, v2)))\n    print("Perpendicular: {0}". format(v_perpendicular(v1, v2, 4)))\n    print("Parallel: {0}".format(v_parallel(v1, v2, 3)))\n    print("Same Orientation: {0}".format(v_same_orientation(v1, v2)))\n    print("Dot product: {0}\\n".format(np.dot(v1, v2)))\n\ndef debug_all_samples(): \n    for sample in samples[0x10] + samples[0x80]:\n        va = np.array(sample[1])\n        vb = np.array(sample[0])\n        o = np.array(sample[3])\n        s = np.array(sample[2])\n\n        v1 = (va - o) / np.linalg.norm((va - o))\n        v2 = (vb - o) / np.linalg.norm((vb - o))\n\n        debug_vectors(v1, v2)\n\n# vy (1486,68)\n# vx (1638,213)\n# s  (1581,119)\n# o  (1628,69)\n        \n    \ndebug_all_samples()\n\n\nva = np.array([1638, 213]) \nvb = np.array([1486, 68]) \no = np.array([1628, 69])\n\nreal_origin = np.array([0, 0])\ntranslate = real_origin - o\n\not = o + translate\n\nvat = (va + translate)\nvbt = (vb + translate)\n\ndebug_vectors(va - o, vb - o)\ndebug_vectors(vat, vbt)\n\nprint("va: {0}, vb: {1}".format(va,vb))\nprint("vat: {0}, vbt: {1}".format(vat,vbt))```\n\nUnused variables:\n- s'}, {'reason': 'stop', 'result': "```benefits_assesement = [\n    'AssessBenefits1', 'AssessBenefits2', 'AssessBenefits3', 'AssessBenefits4', 'AssessBenefits5', \n    'AssessBenefits6','AssessBenefits7', 'AssessBenefits8', 'AssessBenefits9', 'AssessBenefits10', 'AssessBenefits11'\n]\n\nff = top10_df[benefits_assesement].mode()\nff_p = ff.pivot_table(columns='Country')\nplt.figure(figsize=(14, 8))\nff = pd.Series(index=ff.columns, data=ff.values[0])\nplt.bar(ff.index, ff)\nsns.despine(left=True)\nplt.title('Benefits assessement comparaison', fontsize=21)\n_ = plt.xticks(rotation='vertical')\n```\n\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': 'nb_episodes = 1000\npositions = []\nvelocities = []\nangles = []\nangular_velocities = []\n\nfor episode in range(0, nb_episodes):\n    obs = env.reset()\n    done = False\n    final_score = 0\n\n    while not done:\n        action = env.action_space.sample()\n        obs, reward, done, info = env.step(action)\n        position, velocity, angle, angular_velocity = obs\n        positions.append(position)\n        velocities.append(velocity)\n        angles.append(angle)\n        angular_velocities.append(angular_velocity)        ```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'angles=[]\nmeanI=[]\ndmeanI=[]\n\nfor x in range(2,16):\n    name="Data/A/{}.ASC".format(x*10)\n    angles.append(x*10)\n    data=np.genfromtxt(name,skip_header=16,delimiter="\\t",filling_values=-1,comments="\\"")\n    I=data[183:,1]\n    I*=np.sin(gamma*x*10) # Winkelkorrektur\n    \n    meanI.append(np.mean(I))\n    dmeanI.append(np.std(I))\n\n#meanI=[y/np.max(meanI) for y in meanI] # Normierung auf die größte Intensität\n#\n# WIE SOLL MAN DAS RICHTIG NORMIEREN ???\n# \n# Datapoints for fitting\nstart=9\nstop=-1\n\nfig,ax=plt.subplots(dpi=144)\nax.errorbar(angles, meanI ,label="Data",xerr=1,yerr=dmeanI, marker="",ls="",lw=1)\n\ndef fit_mi_shit(start,stop,a):\n    popt,pcov=cf(formfaktor, angles[start:stop], meanI[start:stop], p0=(200e-9,a))\n    perr = np.sqrt(np.diag(pcov))\n\n    q=np.linspace(angles[start],angles[stop],1000)\n    ax.plot(q, formfaktor(q,*popt) ,label="Fit: a= {:.2f} $\\pm$ {:.2f} nm".format(abs(popt[0])*1e9,abs(perr[0])*1e9))\n\nfit_mi_shit(0,-1,30)\nfit_mi_shit(0,9,30)\nfit_mi_shit(9,-1,30)\n\n"""for i in np.linspace(35e-9,180e-9,2):\n    ax.plot(q, formfaktor(q,i,100) ,label="Testwert: {:.2f} nm".format(i/1e-9,popt[0]*1e9,perr[0]*1e9))\n"""\nq=np.linspace(angles[0],120,1000)\nax.plot(q, formfaktor(q,113e-9,100) ,label="Example: {:.2f} nm".format(113) )\nax.set(xlabel=r"Angle $\\theta \\: [°]$", ylabel="Form factor $P(q)$", title="Formfactor for Data A",yscale="log")\n\nax.legend(frameon=False)\nfig.savefig("Plots/FormA");\n\nanglesA,meanIA,dmeanIA = angles,meanI,dmeanI\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '#TCGA dictionary information\ntcga_dict = open("tcga_dictionaries.txt","r")\ndict_name_index = 0 #Set dictionary index counter to 0\nfor line in tcga_dict:\n    if line.startswith("#"): #If line starts with #, the next line will be a known dictionary\n        dict_name_index += 1\n    elif dict_name_index == 5:\n        eval(line)\n```\n\nUnused variables:\n- code_to_disease'}, {'reason': 'stop', 'result': 'def return_cuckoo_filter_with_specified_load_factor(capacity, finger_print_size=2, load_factor=0.2):\n    c_filter = CuckooFilter(capacity, finger_print_size, bucket_size=2)\n    for i in range(int(capacity*6)):\n        try:\n            item = "".join(random.sample(string.ascii_lowercase, 12))\n            c_filter.add(item)\n        except Exception(\'CuckooFilter has filled up!\'):\n            break\n            \n        if round(c_filter.get_load_factor(), 2) == round(load_factor, 2):\n            return c_filter, c_filter.get_load_factor()\n    raise ValueError\n\ndef return_bloom_filter_with_specified_load_factor(capacity, percent_to_fill=0.2):\n    b_filter = CountingBloomFilter(capacity)\n    for i in range(int(percent_to_fill*capacity)):\n        item = "".join(random.sample(string.ascii_lowercase, 12))\n        b_filter.add(item)\n    \n    return b_filter, percent_to_fill\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': 'n_epochs = 20\nbatch_size = 50\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'def full_test(model, hypothesis, name, plot=False, train_len=95, test_len=10,\n              ex_name=\'test\'):\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(), hypothesis,\n                                         train_len=train_len, test_len=train_len)\n    metric_pearsonr = lambda a, b: stats.pearsonr(a, b)[0]\n    \n    print("Hypothesis: {} (normal)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(class_weight=\'balanced\'),\n                                         hypothesis, train_len=train_len, test_len=test_len)\n    print("Hypothesis: {} (balanced)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))```\n\nUnused variables: `plot`'}, {'reason': 'stop', 'result': '#load\nwith open("data.pkl", "rb") as file:\n    [filtered_sentences, dictionary, tokens] = pickle.load(file)```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# Include the observations, which are Bernoulli\nwith model:\n    obs = pm.Bernoulli("obs", p, observed=occurrences)\n    # To be explained in chapter 3\n    step = pm.Metropolis()\n    trace = pm.sample(18000, step=step)\n    burned_trace = trace[1000:]```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': '# generate predictions for training\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': '# Re-run the model with the Bib numbers as a feature and for the 5K, 10K 15K and 20K split times to predict Half marathon time\n\n### set up data for modeling\nX_Half = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\', \'20K Duration\']]\ny_Half = boston_clean[\'Half Duration\'].values.reshape(-1, 1)\nprint(X_Half.shape, y_Half.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_Half, X_test_Half, y_train_Half, y_test_Half = train_test_split(X_Half, y_Half, random_state=29)\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_Half = LinearRegression()\nmodel_Half.fit(X_train_Half, y_train_Half)\n\n# Make predictions\n\npredictions_Half = model_Half.predict(X_test_Half)\n\n# Plot the residuals\n\nplt.scatter(model_Half.predict(X_train_Half), model_Half.predict(X_train_Half) - y_train_Half, c="blue", label="Training Data")\nplt.scatter(model_Half.predict(X_test_Half), model_Half.predict(X_test_Half) - y_test_Half, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_Half.min(), xmax=y_test_Half.max())\nplt.title("Residual Plot Half Marathon")\nplt.savefig(\'model_Half.png\')\nplt.show()\n```\n\nUnused variables: None'}, {'reason': 'stop', 'result': "```python\npost_pmf_contribs = sp.stats.poisson.pmf(np.atleast_3d(x_plot),\n                                         trace['mu'][:, np.newaxis, :])\npost_pmfs = (trace['w'][:, np.newaxis, :] * post_pmf_contribs).sum(axis=-1)\n\npost_pmf_low, post_pmf_high = np.percentile(post_pmfs, [2.5, 97.5], axis=0)\n```\n\nUnused variables: None"}, {'reason': 'stop', 'result': "s = lv_workspace.get_step_1_object('A')"}, {'reason': 'stop', 'result': '# Find clusters in projected data\ny_kmeans_proj = []\ncenters_kmeans_proj = []\nfor x in X_proj:\n    kmeans = KMeans(n_clusters=5)\n    kmeans.fit(x)\n    centers_kmeans_proj.append(kmeans.cluster_centers_)\n    y_kmeans_proj.append(kmeans.predict(x))```\n\nUnused variables:\n- i'}, {'reason': 'stop', 'result': 'def delta_f(list_of_terms, x_value, delta_x):\n    pass\n```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': "from datetime import datetime\n\nimport time\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nfrom scipy.stats import lognorm\nimport pandas as pd\n\nfrom astropy import stats\nfrom astropy.io import fits\nfrom astropy.time import Time\nimport astropy.units as u\n\nimport matplotlib\n#matplotlib.use('nbagg')\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nconfig = tf.ConfigProto(\n    intra_op_parallelism_threads=5,\n    inter_op_parallelism_threads=5,\n    allow_soft_placement=True,\n    log_device_placement=True,\n    device_count = {'CPU': 5}\n)\nsess = tf.Session(config=config)\n\nnp.random.seed(42)\n\nprint(tf.__version__)```\n\nUnused variables:\n- None"}, {'reason': 'stop', 'result': "# define network parameters\ny = tf.placeholder(tf.int32, shape=(None), name='y')\none_hot_y = tf.one_hot(y, 43)\nx = tf.placeholder(tf.float32, shape=(None, 32, 32, 1), name='x')\nmu, sigma = 0, 0.1\nconv1_params = (5,5,1,6, 1, 'VALID')\nconv2_params = (5, 5, 6, 16, 1, 'VALID')\np1_params = (2, 2, 'VALID')\np2_params = (2, 2, 'VALID')\nfc1_params = 120\nfc2_params = 84\nfc3_params = 43\nhold_prob = tf.placeholder(tf.float32, name='hold_prob')\n\n# create the network\nlenet = LeNet(x, mu, sigma, conv1_params,\n                  conv2_params, p1_params, p2_params, fc1_params,\n                  fc2_params, fc3_params, hold_prob)```\n\nUnused variables:\n- bias_value"}, {'reason': 'stop', 'result': '# boolean\nb1 = True\n\ntype(b1)```\n\nUnused variables:\n- b2'}, {'reason': 'stop', 'result': '# Initialize objects\ngROOT.ProcessLine(\'SimulationManipulation sm("{}",0)\'.format(rspPath))\ngROOT.ProcessLine(\'HistogramOperations ops\')\ngROOT.ProcessLine(\'HistogramWriter writer;\')\ngROOT.ProcessLine(\'lightTables.setBirksParams(1.0,6.90)\')\n\n# Create the bin structures\nrspEbins=np.arange(rspEmin,rspEmax,rspEwidth)\nrspEbins=np.append(rspEbins,rspEmax)\n#print rspEbins\nrspLbins=np.arange(rspLmin,rspLmax,rspLwidth)\nrspLbins=np.append(rspLbins,rspLmax)\n#print rspLbins\ngROOT.ProcessLine(\'const Int_t EBINS = {}; const Int_t LBINS = {};\'.format(len(rspEbins)-1,len(rspLbins)-1))\ngROOT.ProcessLine(\'Double_t eEdges[EBINS + 1] = {}{}{};\'.format("{",", ".join(str(e) for e in rspEbins),"}"))\ngROOT.ProcessLine(\'Double_t lEdges[LBINS + 1] = {}{}{};\'.format("{",", ".join(str(e) for e in rspLbins),"}"))\ngROOT.ProcessLine(\'axis1 = TAxis(EBINS,eEdges);\')\ngROOT.ProcessLine(\'axis2 = TAxis(LBINS,lEdges);\')\n\n# Create the Histogram and output file\ngROOT.ProcessLine(\'TH2* matrix1=sm.getNormalizedResponseMatrix(axis1,axis2)\')\ngROOT.ProcessLine(\'writer.ResponseToHEPROW(matrix1,"EJ309_resp_03_50")\')\n\n# Smear the Response Matrix and Create the .rsp File\nfor detNum, detName in detNames.iteritems():   \n    params = CalibParams(calPath+calNames[detNum])\n\n    gROOT.ProcessLine(\'TH2* smearMatrix{0} = ops.skewedGausSmearMatrix(matrix1, {1}, {2}, {3})\'.format(detNum, params.alpha, params.beta, params.gamma))\n    gROOT.ProcessLine(\'smearMatrix{0}->Draw("colz")\'.format(detNum))\n    gROOT.ProcessLine(\'writer.ResponseToHEPROW(smearMatrix{0},"{0}_smearedResp_03_50")\'.format(detNum))\n\n    pause()'}, {'reason': 'stop', 'result': 'lights_labels = make_target_label_array()```\n\nUnused variables:\nNone'}, {'reason': 'stop', 'result': "X_train_cat = X_train_feature.drop('AnimalType_Cat', 1)\nX_dev_cat = X_dev_feature.drop('AnimalType_Cat', 1)\n\nmodel4 = Sequential([\n    Dense(32, input_shape=(12,)),\n    Dropout(0.1),   \n    Activation('sigmoid'),\n    Dense(5),\n    Activation('softmax'),\n])\n\nmodel4.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel4.fit(np.array(X_train_cat), y_train_hot, epochs=10, batch_size=32)```\n\nUnused variables:\n- X_train_cat (initial assignment)\n- X_dev_cat (initial assignment)"}, {'reason': 'stop', 'result': "plt.plot(f/3e10, y_nolm_aer - y_nolm_aer_36, label='AER 3.2$-$AER 3.6')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()```\n\nUnused variables: `l`"}, {'reason': 'stop', 'result': "```y=tourney_comp_ratings[tourney_comp_ratings['season_t']<= stop_tournament]['game_result']\nX= X.drop(columns=['season_t'])\n\nfeature_list = list(X)\n```\n\nUnused variables:\n- `feature_list`"}, {'reason': 'stop', 'result': '# obtenemos los datos que necesitamos y los ponemos en una matriz de Julia\nvinos = matread("wine.mat")\ndatos = vinos["data"]\n\n# separamos las caracteristicas y los tipos de vino\ncaracteristicas = datos[:,2:end]\ntipo_de_vino = datos[:,1];```\n\nUnused variables:\n- None'}, {'reason': 'stop', 'result': 'best_model = load_model("GRUmodel_128_128")```\n\nUnused variables:\nNone'}]