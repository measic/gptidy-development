{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE_FUNCTIONS = 213\n",
    "SAMPLE_SIZE_VARIABLES = 361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all random sample functions cell content\n",
    "random_sample_functions_cell_content = []\n",
    "for i in range(SAMPLE_SIZE_FUNCTIONS):\n",
    "    with open(f'../renaming/determining_files_rename/random_samples_functions/{i}.py', 'r') as f:\n",
    "        random_sample_functions_cell_content.append(f.read())\n",
    "\n",
    "# read in all random sample variables cell content\n",
    "random_sample_variables_cell_content = []\n",
    "for i in range(SAMPLE_SIZE_VARIABLES):\n",
    "    with open(f'../renaming/determining_files_rename/random_samples_variables/{i}.py', 'r') as f:\n",
    "        random_sample_variables_cell_content.append(f.read())\n",
    "\n",
    "# read in all random sample functions urls\n",
    "random_sample_functions_urls = []\n",
    "with open('../renaming/determining_files_rename/random_samples_functions/urls.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        random_sample_functions_urls.append(line.strip())\n",
    "\n",
    "# read in all random sample variables urls\n",
    "random_sample_variables_urls = []\n",
    "with open('../renaming/determining_files_rename/random_samples_variables/urls.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        random_sample_variables_urls.append(line.strip())\n",
    "\n",
    "# read in all random sample functions original names\n",
    "random_sample_functions_original_names = []\n",
    "with open('../renaming/determining_files_rename/random_samples_functions/original_names.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        random_sample_functions_original_names.append(line.strip())\n",
    "\n",
    "# read in all random sample variables original names\n",
    "random_sample_variables_original_names = []\n",
    "with open('../renaming/determining_files_rename/random_samples_variables/original_names.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        random_sample_variables_original_names.append(line.strip())\n",
    "\n",
    "# read in all random sample functions new names\n",
    "# TODO\n",
    "random_sample_variables_new_names = [None for _ in range(SAMPLE_SIZE_VARIABLES)]\n",
    "\n",
    "# read in all random sample variables new names\n",
    "# TODO\n",
    "random_sample_functions_new_names = [None for _ in range(SAMPLE_SIZE_FUNCTIONS)]\n",
    "\n",
    "# read in all random sample functions original cell ids\n",
    "random_sample_functions_original_cell_ids = []\n",
    "with open('../renaming/determining_files_rename/random_samples_functions/original_cell_ids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        random_sample_functions_original_cell_ids.append(line.strip())\n",
    "\n",
    "# read in all random sample variables original cell ids\n",
    "random_sample_variables_original_cell_ids = []\n",
    "with open('../renaming/determining_files_rename/random_samples_variables/original_cell_ids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        random_sample_variables_original_cell_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of readmes by url\n",
    "readmes_by_url = {}\n",
    "\n",
    "# first we have to read in readmes.txt for both functions and variables\n",
    "with open('../renaming/determining_files_rename/random_samples_functions/readmes.txt', 'r') as f:\n",
    "    random_sample_functions_readmes = eval(f.read())\n",
    "\n",
    "with open('../renaming/determining_files_rename/random_samples_variables/readmes.txt', 'r') as f:\n",
    "    random_sample_variables_readmes = eval(f.read())\n",
    "\n",
    "# then we have to combine them\n",
    "for url, readme in zip(random_sample_functions_urls, random_sample_functions_readmes):\n",
    "    readmes_by_url[url] = readme\n",
    "\n",
    "for url, readme in zip(random_sample_variables_urls, random_sample_variables_readmes):\n",
    "    readmes_by_url[url] = readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions left: 103\n",
      "Number of variables left: 219\n"
     ]
    }
   ],
   "source": [
    "# filter out entries that are too short or too long\n",
    "MIN_LENGTH = 100\n",
    "MAX_LENGTH = 1000\n",
    "\n",
    "filter_ids_functions = []\n",
    "for i, cell_content in enumerate(random_sample_functions_cell_content):\n",
    "    if len(cell_content) < MIN_LENGTH or len(cell_content) > MAX_LENGTH:\n",
    "        filter_ids_functions.append(i)\n",
    "\n",
    "filter_ids_variables = []\n",
    "for i, cell_content in enumerate(random_sample_variables_cell_content):\n",
    "    if len(cell_content) < MIN_LENGTH or len(cell_content) > MAX_LENGTH:\n",
    "        filter_ids_variables.append(i)\n",
    "\n",
    "# filter out entries that are too short or too long\n",
    "random_sample_functions_cell_content = [cell_content for i, cell_content in enumerate(random_sample_functions_cell_content) if i not in filter_ids_functions]\n",
    "random_sample_functions_urls = [url for i, url in enumerate(random_sample_functions_urls) if i not in filter_ids_functions]\n",
    "random_sample_functions_original_names = [original_name for i, original_name in enumerate(random_sample_functions_original_names) if i not in filter_ids_functions]\n",
    "random_sample_functions_new_names = [new_name for i, new_name in enumerate(random_sample_functions_new_names) if i not in filter_ids_functions]\n",
    "random_sample_functions_original_cell_ids = [original_cell_id for i, original_cell_id in enumerate(random_sample_functions_original_cell_ids) if i not in filter_ids_functions]\n",
    "\n",
    "random_sample_variables_cell_content = [cell_content for i, cell_content in enumerate(random_sample_variables_cell_content) if i not in filter_ids_variables]\n",
    "random_sample_variables_urls = [url for i, url in enumerate(random_sample_variables_urls) if i not in filter_ids_variables]\n",
    "random_sample_variables_original_names = [original_name for i, original_name in enumerate(random_sample_variables_original_names) if i not in filter_ids_variables]\n",
    "random_sample_variables_new_names = [new_name for i, new_name in enumerate(random_sample_variables_new_names) if i not in filter_ids_variables]\n",
    "random_sample_variables_original_cell_ids = [original_cell_id for i, original_cell_id in enumerate(random_sample_variables_original_cell_ids) if i not in filter_ids_variables]\n",
    "\n",
    "# print number of cells left for each category\n",
    "print(f'Number of functions left: {len(random_sample_functions_cell_content)}')\n",
    "print(f'Number of variables left: {len(random_sample_variables_cell_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order everything by notebook\n",
    "# within a notebook order by unique cell id\n",
    "\n",
    "notebooks = {}\n",
    "\n",
    "def populate_notebooks(type_considered):\n",
    "    if type_considered == 'variable':\n",
    "        range_size = len(random_sample_variables_cell_content)\n",
    "        random_sample_urls = random_sample_variables_urls\n",
    "        random_sample_cell_content = random_sample_variables_cell_content\n",
    "        random_sample_original_names = random_sample_variables_original_names\n",
    "        random_sample_new_names = random_sample_variables_new_names\n",
    "        random_sample_original_cell_ids = random_sample_variables_original_cell_ids\n",
    "    else:\n",
    "        range_size = len(random_sample_functions_cell_content)\n",
    "        random_sample_urls = random_sample_functions_urls\n",
    "        random_sample_cell_content = random_sample_functions_cell_content\n",
    "        random_sample_original_names = random_sample_functions_original_names\n",
    "        random_sample_new_names = random_sample_functions_new_names\n",
    "        random_sample_original_cell_ids = random_sample_functions_original_cell_ids\n",
    "    \n",
    "    # loop over all cells\n",
    "    for i in range(range_size):\n",
    "        cell_entry = {\n",
    "            'type': type_considered,\n",
    "            'cell_content': random_sample_cell_content[i],\n",
    "            'original_name': random_sample_original_names[i],\n",
    "            'new_name': random_sample_new_names[i],\n",
    "        }\n",
    "\n",
    "        if random_sample_urls[i] in notebooks:\n",
    "            if random_sample_original_cell_ids[i] in notebooks[random_sample_urls[i]]['cells']:\n",
    "                notebooks[random_sample_urls[i]]['cells'][random_sample_original_cell_ids[i]].append(cell_entry)\n",
    "            else:\n",
    "                notebooks[random_sample_urls[i]]['cells'][random_sample_original_cell_ids[i]] = [cell_entry]\n",
    "        else:\n",
    "            notebooks[random_sample_urls[i]] = {\n",
    "                'cells': {\n",
    "                    random_sample_original_cell_ids[i]: [cell_entry]\n",
    "                }\n",
    "            }\n",
    "\n",
    "populate_notebooks('variable')\n",
    "populate_notebooks('function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notebooks: 84\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of notebooks: {len(notebooks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of functions per notebook and number of variables per notebook\n",
    "def count_num():\n",
    "    notebook_counts_type = []\n",
    "    for i, notebook in enumerate(notebooks):\n",
    "        function_count = 0\n",
    "        variable_count = 0\n",
    "        for cell in notebooks[notebook]['cells']:\n",
    "            for cell_entry in notebooks[notebook]['cells'][cell]:\n",
    "                if cell_entry['type'] == 'function':\n",
    "                    function_count += 1\n",
    "                else:\n",
    "                    variable_count += 1\n",
    "        notebook_counts_type.append({\n",
    "            'url': notebook,\n",
    "            'function_count': function_count,\n",
    "            'variable_count': variable_count\n",
    "        })\n",
    "    return notebook_counts_type\n",
    "\n",
    "notebook_counts_type = count_num()\n",
    "\n",
    "# remove notebooks that don't have at least one function and one variable each\n",
    "remove_notebooks = [x for x in notebook_counts_type if x['function_count'] > 0 and x['variable_count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter notebooks by ids in notebook_counts\n",
    "notebooks = {k: v for k, v in notebooks.items() if k in [x['url'] for x in remove_notebooks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notebooks: 37\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of notebooks: {len(notebooks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sorts in ascending order (for reproducibility)\n",
    "\n",
    "# sort the notebooks by number of cells\n",
    "notebooks = {k: v for k, v in sorted(notebooks.items(), key=lambda x: len(x[1]['cells']), reverse=False)}\n",
    "\n",
    "# sort the cells by number of entries\n",
    "for notebook in notebooks:\n",
    "    notebooks[notebook]['cells'] = {k: v for k, v in sorted(notebooks[notebook]['cells'].items(), key=lambda x: len(x[1]), reverse=False)}\n",
    "\n",
    "# sort the entries by length of cell_content\n",
    "for notebook in notebooks:\n",
    "    for cell in notebooks[notebook]['cells']:\n",
    "        notebooks[notebook]['cells'][cell] = sorted(notebooks[notebook]['cells'][cell], key=lambda x: len(x['cell_content']), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://raw.githubusercontent.com/iyerhari5/P1-FindingLanes/f81c6ac12046c824eb616c1e5e0733ca210cabcd/P1.ipynb': {'cells': {'4520': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(image):\\n    gray = grayscale(image)\\n    kernel_size = 5\\n    blur_gray = gaussian_blur(gray, kernel_size)\\n    low_threshold = 60\\n    high_threshold = 100\\n    edges = canny(blur_gray, low_threshold, high_threshold)\\n    imshape = image.shape\\n    vertices = np.array([[(0, imshape[0]), (imshape[1] * 0.48, imshape[0] * 0.6), (imshape[1] * 0.52, imshape[0] * 0.6), (imshape[1], imshape[0])]], dtype=np.int32)\\n    masked_edges = region_of_interest(edges, vertices)\\n    rho = 1\\n    theta = np.pi / 180\\n    threshold = 90\\n    min_line_length = 30\\n    max_line_gap = 30\\n    line_image = np.copy(image) * 0\\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap, vertices)\\n    result = weighted_img(lines, image, α=0.8, β=1.0, λ=0.0)\\n    return result',\n",
       "     'original_name': 'process_image',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'def process_image(image):\\n    gray = grayscale(image)\\n    kernel_size = 5\\n    blur_gray = gaussian_blur(gray, kernel_size)\\n    low_threshold = 60\\n    high_threshold = 100\\n    edges = canny(blur_gray, low_threshold, high_threshold)\\n    imshape = image.shape\\n    variable_def = np.array([[(0, imshape[0]), (imshape[1] * 0.48, imshape[0] * 0.6), (imshape[1] * 0.52, imshape[0] * 0.6), (imshape[1], imshape[0])]], dtype=np.int32)\\n    masked_edges = region_of_interest(edges, variable_def)\\n    rho = 1\\n    theta = np.pi / 180\\n    threshold = 90\\n    min_line_length = 30\\n    max_line_gap = 30\\n    line_image = np.copy(image) * 0\\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap, variable_def)\\n    result = weighted_img(lines, image, α=0.8, β=1.0, λ=0.0)\\n    return result',\n",
       "     'original_name': 'vertices',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/marthtz/CarND-T1P1-LaneLines/484b3430995ff58d10f61799d0c0a346a049c34a/P1.ipynb': {'cells': {'5965': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(image):\\n    gray = grayscale(image)\\n    kernel_size = 5\\n    blur_gray = gaussian_blur(gray, kernel_size)\\n    low_threshold = 50\\n    high_threshold = 150\\n    edges = canny(blur_gray, low_threshold, high_threshold)\\n    imshape = image.shape\\n    xPct = 0.05\\n    yPct = 0.6\\n    xbl = imshape[1] * xPct\\n    xbr = imshape[1] * (1 - xPct)\\n    xtl = imshape[1] * (0.5 - xPct)\\n    xtr = imshape[1] * (0.5 + xPct)\\n    yb = imshape[0]\\n    yt = imshape[0] * yPct\\n    vertices = np.array([[(xbl, yb), (xtl, yt), (xtr, yt), (xbr, yb)]], dtype=np.int32)\\n    masked_image = region_of_interest(edges, vertices)\\n    rho = 2\\n    theta = np.pi / 180\\n    threshold = 15\\n    min_line_len = 20\\n    max_line_gap = 30\\n    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\\n    overlayedImg = weighted_img(line_img, image, 0.8, 1, 0)\\n    return overlayedImg',\n",
       "     'original_name': 'process_image',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'def process_image(image):\\n    gray = grayscale(image)\\n    kernel_size = 5\\n    variable_def = gaussian_blur(gray, kernel_size)\\n    low_threshold = 50\\n    high_threshold = 150\\n    edges = canny(variable_def, low_threshold, high_threshold)\\n    imshape = image.shape\\n    xPct = 0.05\\n    yPct = 0.6\\n    xbl = imshape[1] * xPct\\n    xbr = imshape[1] * (1 - xPct)\\n    xtl = imshape[1] * (0.5 - xPct)\\n    xtr = imshape[1] * (0.5 + xPct)\\n    yb = imshape[0]\\n    yt = imshape[0] * yPct\\n    vertices = np.array([[(xbl, yb), (xtl, yt), (xtr, yt), (xbr, yb)]], dtype=np.int32)\\n    masked_image = region_of_interest(edges, vertices)\\n    rho = 2\\n    theta = np.pi / 180\\n    threshold = 15\\n    min_line_len = 20\\n    max_line_gap = 30\\n    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\\n    overlayedImg = weighted_img(line_img, image, 0.8, 1, 0)\\n    return overlayedImg',\n",
       "     'original_name': 'blur_gray',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'def process_image(image):\\n    gray = grayscale(image)\\n    kernel_size = 5\\n    blur_gray = gaussian_blur(gray, kernel_size)\\n    low_threshold = 50\\n    high_threshold = 150\\n    edges = canny(blur_gray, low_threshold, high_threshold)\\n    imshape = image.shape\\n    xPct = 0.05\\n    variable_def = 0.6\\n    xbl = imshape[1] * xPct\\n    xbr = imshape[1] * (1 - xPct)\\n    xtl = imshape[1] * (0.5 - xPct)\\n    xtr = imshape[1] * (0.5 + xPct)\\n    yb = imshape[0]\\n    yt = imshape[0] * variable_def\\n    vertices = np.array([[(xbl, yb), (xtl, yt), (xtr, yt), (xbr, yb)]], dtype=np.int32)\\n    masked_image = region_of_interest(edges, vertices)\\n    rho = 2\\n    theta = np.pi / 180\\n    threshold = 15\\n    min_line_len = 20\\n    max_line_gap = 30\\n    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\\n    overlayedImg = weighted_img(line_img, image, 0.8, 1, 0)\\n    return overlayedImg',\n",
       "     'original_name': 'yPct',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/ZaraYar/Analyzing-NYC-High-School-Data/88d2fa7974800142157e068fe7a97b7f345bcfeb/Basics.ipynb': {'cells': {'272': [{'type': 'variable',\n",
       "     'cell_content': \"cols = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\\nfor c in cols:\\n    data['sat_results'][c] = pd.to_numeric(data['sat_results'][c], errors='coerce')\\ndata['sat_results']['sat_score'] = data['sat_results'][cols[0]] + data['sat_results'][cols[1]] + data['sat_results'][cols[2]]\\n\\ndef find_lat(loc):\\n    coords = re.findall('\\\\\\\\(.+, .+\\\\\\\\)', loc)\\n    lat = coords[0].split(',')[0].replace('(', '')\\n    return lat\\n\\ndef find_lon(loc):\\n    coords = re.findall('\\\\\\\\(.+, .+\\\\\\\\)', loc)\\n    variable_def = coords[0].split(',')[1].replace(')', '').strip()\\n    return variable_def\\ndata['hs_directory']['lat'] = data['hs_directory']['Location 1'].apply(find_lat)\\ndata['hs_directory']['lon'] = data['hs_directory']['Location 1'].apply(find_lon)\\ndata['hs_directory']['lat'] = pd.to_numeric(data['hs_directory']['lat'], errors='coerce')\\ndata['hs_directory']['lon'] = pd.to_numeric(data['hs_directory']['lon'], errors='coerce')\",\n",
       "     'original_name': 'lon',\n",
       "     'new_name': None}],\n",
       "   '276': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(dbn):\\n    return dbn[0:2]\\ncombined['school_dist'] = combined['DBN'].apply(function_def)\",\n",
       "     'original_name': 'get_first_two_chars',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/IvanBrasilico/ajna_docs/5a7255e8d063505240132ac362a456caf85b3247/notebooks/ContainerDetectionTraining1.ipynb': {'cells': {'3243': [{'type': 'variable',\n",
       "     'cell_content': \"import csv\\nimport datetime\\nimport os\\nimport glob\\nvariable_def = '/home/ivan/Área de trabalho/2017'\\nlista = []\\nfor filename in glob.iglob(variable_def + '**/*/*/*/*stamp.jpg*', recursive=True):\\n    lista.append(filename)\\nprint(lista[:4])\\nprint(len(lista))\\nprint(find_conteiner(lista[1]))\\nprint(find_conteiner(lista[15]))\",\n",
       "     'original_name': 'IMG_PATH',\n",
       "     'new_name': None}],\n",
       "   '3258': [{'type': 'variable',\n",
       "     'cell_content': \"from image_aq.utils.pascal_voc_io import PascalVocWriter\\n\\ndef gera_voc(file):\\n    foldername, filename = os.path.split(file)\\n    img = imageio.imread(file)\\n    imgSize = img.shape\\n    print(foldername, filename, imgSize)\\n    variable_def = PascalVocWriter(foldername, filename, imgSize)\\n    coords = find_conteiner(file)\\n    variable_def.addBndBox(*coords, 'cc', '')\\n    variable_def.save(file[:-4] + '.xml')\\nfile = lista[0]\\nprint(file)\\ngera_voc(file)\\nglob.glob(foldername + '/*')\",\n",
       "     'original_name': 'voc_writer',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': \"from image_aq.utils.pascal_voc_io import PascalVocWriter\\n\\ndef function_def(file):\\n    foldername, filename = os.path.split(file)\\n    img = imageio.imread(file)\\n    imgSize = img.shape\\n    print(foldername, filename, imgSize)\\n    voc_writer = PascalVocWriter(foldername, filename, imgSize)\\n    coords = find_conteiner(file)\\n    voc_writer.addBndBox(*coords, 'cc', '')\\n    voc_writer.save(file[:-4] + '.xml')\\nfile = lista[0]\\nprint(file)\\nfunction_def(file)\\nglob.glob(foldername + '/*')\",\n",
       "     'original_name': 'gera_voc',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/sakshi30/tensorflow/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/examples/udacity/5_word2vec.ipynb': {'cells': {'4788': [{'type': 'variable',\n",
       "     'cell_content': \"def plot(embeddings, labels):\\n    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\\n    pylab.figure(figsize=(15, 15))\\n    for i, label in enumerate(labels):\\n        variable_def, y = embeddings[i, :]\\n        pylab.scatter(variable_def, y)\\n        pylab.annotate(label, xy=(variable_def, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\\n    pylab.show()\\nwords = [reverse_dictionary[i] for i in range(1, num_points + 1)]\\nplot(two_d_embeddings, words)\",\n",
       "     'original_name': 'x',\n",
       "     'new_name': None}],\n",
       "   '4781': [{'type': 'function',\n",
       "     'cell_content': 'url = \\'http://mattmahoney.net/dc/\\'\\n\\ndef function_def(filename, expected_bytes):\\n    \"\"\"Download a file if not present, and make sure it\\'s the right size.\"\"\"\\n    if not os.path.exists(filename):\\n        filename, _ = urlretrieve(url + filename, filename)\\n    statinfo = os.stat(filename)\\n    if statinfo.st_size == expected_bytes:\\n        print(\\'Found and verified %s\\' % filename)\\n    else:\\n        print(statinfo.st_size)\\n        raise Exception(\\'Failed to verify \\' + filename + \\'. Can you get to it with a browser?\\')\\n    return filename\\nfilename = function_def(\\'text8.zip\\', 31344016)',\n",
       "     'original_name': 'maybe_download',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/jangeloni/challenge_amadeus/304529ed07b06c0aca900f5bbe4938aa7a087514/second_exercise.ipynb': {'cells': {'3905': [{'type': 'variable',\n",
       "     'cell_content': \"cols = ['arr_port', 'pax']\\nvariable_def = get_df_cols(BOOKINGS, cols, '^')\\nprint_top_n_arrival_airport(variable_def, 10)\",\n",
       "     'original_name': 'dataframe',\n",
       "     'new_name': None}],\n",
       "   '3919': [{'type': 'function',\n",
       "     'cell_content': 'from GeoBases import GeoBase\\ngeo_o = GeoBase(data=\\'ori_por\\', verbose=False)\\n\\ndef function_def(IATA_code):\\n    \"\"\"\\n    Function to return the name of the airport linked to IATA_code\\n    \\n    @IATA_code : String object which is a IATA_code\\n   \\n    @return    : String object which is the name of the airport\\n  \"\"\"\\n    try:\\n        result = geo_o.get(IATA_code.replace(\\' \\', \\'\\'), \\'name\\')\\n    except KeyError as e:\\n        result = \\'NOT FOUND IATA CODE\\'\\n    return result\\nBOOKINGS_GROUP_BY_ARR_PORT.columns.values\\nnew_df = BOOKINGS_GROUP_BY_ARR_PORT.reset_index()\\nnew_df[\\'airport_name\\'] = new_df[\\'arr_port\\'].apply(lambda x: function_def(x))\\nnew_df',\n",
       "     'original_name': 'get_name',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/lealldiogo/CarND-Traffic-Sign-Classifier-Project/e629044c0b68fd6992a0ff3e582bb2cd63a0acc3/Traffic_Sign_Classifier.ipynb': {'cells': {'5805': [{'type': 'variable',\n",
       "     'cell_content': 'import tensorflow as tf\\nimport numpy as np\\nfrom sklearn.utils import shuffle\\nX_train_normal = np.array(X_train / 255 - 0.5)\\nX_valid_normal = np.array(X_valid / 255 - 0.5)\\nX_test_normal = np.array(X_test / 255 - 0.5)\\nvariable_def = 15\\nBATCH_SIZE = 128',\n",
       "     'original_name': 'EPOCHS',\n",
       "     'new_name': None}],\n",
       "   '5809': [{'type': 'function',\n",
       "     'cell_content': 'correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\\naccuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\\nsaver = tf.train.Saver()\\n\\ndef function_def(X_data, y_data):\\n    num_examples = len(X_data)\\n    total_accuracy = 0\\n    sess = tf.get_default_session()\\n    for offset in range(0, num_examples, BATCH_SIZE):\\n        batch_x, batch_y = (X_data[offset:offset + BATCH_SIZE], y_data[offset:offset + BATCH_SIZE])\\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\\n        total_accuracy += accuracy * len(batch_x)\\n    return total_accuracy / num_examples',\n",
       "     'original_name': 'evaluate',\n",
       "     'new_name': None}],\n",
       "   '5802': [{'type': 'variable',\n",
       "     'cell_content': \"import pickle\\ntraining_file = '../traffic-signs-data/train.p'\\nvalidation_file = '../traffic-signs-data/valid.p'\\ntesting_file = '../traffic-signs-data/test.p'\\nwith open(training_file, mode='rb') as f:\\n    train = pickle.load(f)\\nwith open(validation_file, mode='rb') as f:\\n    valid = pickle.load(f)\\nwith open(testing_file, mode='rb') as f:\\n    test = pickle.load(f)\\nX_train, y_train = (train['features'], train['labels'])\\nX_valid, y_valid = (valid['features'], valid['labels'])\\nvariable_def, y_test = (test['features'], test['labels'])\",\n",
       "     'original_name': 'X_test',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"import pickle\\ntraining_file = '../traffic-signs-data/train.p'\\nvalidation_file = '../traffic-signs-data/valid.p'\\ntesting_file = '../traffic-signs-data/test.p'\\nwith open(training_file, mode='rb') as f:\\n    train = pickle.load(f)\\nwith open(validation_file, mode='rb') as f:\\n    valid = pickle.load(f)\\nwith open(testing_file, mode='rb') as f:\\n    variable_def = pickle.load(f)\\nX_train, y_train = (train['features'], train['labels'])\\nX_valid, y_valid = (valid['features'], valid['labels'])\\nX_test, y_test = (variable_def['features'], variable_def['labels'])\",\n",
       "     'original_name': 'test',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/mgribov/coursera-uw-ml/a98a5034fbb5426bf8ff1cec7955b2e9ec26e1fb/course-4/2_kmeans-with-text-data_blank.ipynb': {'cells': {'2251': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = pairwise_distances(tf_idf, tf_idf[0:3], metric='euclidean')\\ncluster_assignment = np.argmin(variable_def, axis=1)\",\n",
       "     'original_name': 'distances',\n",
       "     'new_name': None}],\n",
       "   '2245': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(data, k, seed=None):\\n    \"\"\"Randomly choose k data points as initial centroids\"\"\"\\n    if seed is not None:\\n        np.random.seed(seed)\\n    n = data.shape[0]\\n    rand_indices = np.random.randint(0, n, k)\\n    centroids = data[rand_indices, :].toarray()\\n    return centroids',\n",
       "     'original_name': 'get_initial_centroids',\n",
       "     'new_name': None}],\n",
       "   '2271': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(data, k, seed=None):\\n    \"\"\"Use k-means++ to initialize a good set of centroids\"\"\"\\n    if seed is not None:\\n        np.random.seed(seed)\\n    centroids = np.zeros((k, data.shape[1]))\\n    idx = np.random.randint(data.shape[0])\\n    centroids[0] = data[idx, :].toarray()\\n    squared_distances = pairwise_distances(data, centroids[0:1], metric=\\'euclidean\\').flatten() ** 2\\n    for i in xrange(1, k):\\n        idx = np.random.choice(data.shape[0], 1, p=squared_distances / sum(squared_distances))\\n        centroids[i] = data[idx, :].toarray()\\n        squared_distances = np.min(pairwise_distances(data, centroids[0:i + 1], metric=\\'euclidean\\') ** 2, axis=1)\\n    return centroids',\n",
       "     'original_name': 'smart_initialize',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/schen57/AI-ML-DL-Playground/a9c9c24b0a659c3b669b05908330903322fd0705/Local%20Python/AI-ML-DL%20Algorithms/LSTM%20Neural%20Networks/Time%20Series%20Forecasting%20with%20the%20Long%20Short-Term%20Memory%20Network%20in%20Python.ipynb': {'cells': {'959': [{'type': 'variable',\n",
       "     'cell_content': \"predictions = list()\\nfor i in range(len(test_scaled)):\\n    variable_def, y = (test_scaled[i, 0:-1], test_scaled[i, -1])\\n    yhat = forecast_lstm(lstm_model, 1, variable_def)\\n    yhat = invert_scale(scaler, variable_def, yhat)\\n    yhat = inverse_difference(raw_values, yhat, len(test_scaled) + 1 - i)\\n    predictions.append(yhat)\\n    expected = raw_values[len(train) + i + 1]\\n    print('Month=%d, Predicted=%f, Expected=%f' % (i + 1, yhat, expected))\\nrmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\\nprint('Test RMSE: %.3f' % rmse)\\npyplot.plot(raw_values[-12:])\\npyplot.plot(predictions)\\npyplot.show()\",\n",
       "     'original_name': 'X',\n",
       "     'new_name': None}],\n",
       "   '940': [{'type': 'variable',\n",
       "     'cell_content': 'from sklearn.preprocessing import MinMaxScaler\\nvariable_def = MinMaxScaler(feature_range=(-1, 1))\\nvariable_def',\n",
       "     'original_name': 'scaler',\n",
       "     'new_name': None}],\n",
       "   '953': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(train, batch_size, nb_epoch, neurons):\\n    X, y = (train[:, 0:-1], train[:, -1])\\n    X = X.reshape(X.shape[0], 1, X.shape[1])\\n    model = Sequential()\\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\\n    model.add(Dense(1))\\n    model.compile(loss='mean_squared_error', optimizer='adam')\\n    for i in range(nb_epoch):\\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\\n        model.reset_states()\\n    return model\",\n",
       "     'original_name': 'fit_lstm',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/jpbarto/ml_lifecycle_lab/cc2c1a47f290cd415de6e5c8d930dee99ea18e57/03%20Hyperparameter%20tuning.ipynb': {'cells': {'46': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(df, horizon, inplace=False):\\n    n_df = df\\n    if not inplace:\\n        n_df = df.copy()\\n    for offset in range(1, horizon + 1):\\n        min_price = n_df['MinPrice'].shift(offset).fillna(method='bfill')\\n        max_price = n_df['MaxPrice'].shift(offset).fillna(method='bfill')\\n        start_price = n_df['StartPrice'].shift(offset).fillna(method='bfill')\\n        end_price = n_df['EndPrice'].shift(offset).fillna(method='bfill')\\n        trade_vol = n_df['TradedVolume'].shift(offset).fillna(method='bfill')\\n        num_trades = n_df['NumberOfTrades'].shift(offset).fillna(method='bfill')\\n        n_df['h{}_MinPrice'.format(offset)] = min_price\\n        n_df['h{}_MaxPrice'.format(offset)] = max_price\\n        n_df['h{}_StartPrice'.format(offset)] = start_price\\n        n_df['h{}_EndPrice'.format(offset)] = end_price\\n        n_df['h{}_TradeVolume'.format(offset)] = trade_vol\\n        n_df['h{}_NumberOfTrades'.format(offset)] = num_trades\\n    return n_df\",\n",
       "     'original_name': 'create_xgb_features',\n",
       "     'new_name': None}],\n",
       "   '47': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(dates):\\n    unprocessed_df = read_s3_csv(dates)\\n    print('Loaded CSV data set from S3')\\n    cleaned_df = clean_data(unprocessed_df, inplace=True)\\n    print('Cleaned CSV data set')\\n    xgb_data = create_xgb_features(cleaned_df, 5, inplace=True)\\n    xgb_data['NextMaxPrice'] = create_xgb_target(xgb_data)\\n    print('Engineered CSV data set')\\n    train_data, validate_data = train_test_split(xgb_data, train_size=0.8, test_size=0.2, shuffle=True)\\n    cols = list(train_data.columns.values)\\n    cols.remove('NextMaxPrice')\\n    cols = ['NextMaxPrice'] + cols\\n    train_data = pd.get_dummies(train_data[cols])\\n    validate_data = pd.get_dummies(validate_data[cols])\\n    print('Data split for training purposes')\\n    return (train_data, validate_data)\",\n",
       "     'original_name': 'engineer_date_range',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def engineer_date_range(dates):\\n    unprocessed_df = read_s3_csv(dates)\\n    print('Loaded CSV data set from S3')\\n    cleaned_df = clean_data(unprocessed_df, inplace=True)\\n    print('Cleaned CSV data set')\\n    variable_def = create_xgb_features(cleaned_df, 5, inplace=True)\\n    variable_def['NextMaxPrice'] = create_xgb_target(variable_def)\\n    print('Engineered CSV data set')\\n    train_data, validate_data = train_test_split(variable_def, train_size=0.8, test_size=0.2, shuffle=True)\\n    cols = list(train_data.columns.values)\\n    cols.remove('NextMaxPrice')\\n    cols = ['NextMaxPrice'] + cols\\n    train_data = pd.get_dummies(train_data[cols])\\n    validate_data = pd.get_dummies(validate_data[cols])\\n    print('Data split for training purposes')\\n    return (train_data, validate_data)\",\n",
       "     'original_name': 'xgb_data',\n",
       "     'new_name': None}],\n",
       "   '42': [{'type': 'variable',\n",
       "     'cell_content': \"def build_index(non_empty_days, from_time, to_time):\\n    date_ranges = []\\n    for date in non_empty_days:\\n        yyyy, mm, dd = date.split('-')\\n        from_hour, from_min = from_time.split(':')\\n        to_hour, to_min = to_time.split(':')\\n        variable_def = datetime(int(yyyy), int(mm), int(dd), int(from_hour), int(from_min), 0)\\n        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour), int(to_min), 0)\\n        date_ranges.append(pd.DataFrame({'OrganizedDateTime': pd.date_range(variable_def, t2, freq='1Min').values}))\\n    agg = pd.concat(date_ranges, axis=0)\\n    agg.index = agg['OrganizedDateTime']\\n    return agg\",\n",
       "     'original_name': 't1',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def build_index(non_empty_days, from_time, to_time):\\n    date_ranges = []\\n    for date in non_empty_days:\\n        yyyy, mm, dd = date.split('-')\\n        from_hour, from_min = from_time.split(':')\\n        to_hour, to_min = to_time.split(':')\\n        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour), int(from_min), 0)\\n        variable_def = datetime(int(yyyy), int(mm), int(dd), int(to_hour), int(to_min), 0)\\n        date_ranges.append(pd.DataFrame({'OrganizedDateTime': pd.date_range(t1, variable_def, freq='1Min').values}))\\n    agg = pd.concat(date_ranges, axis=0)\\n    agg.index = agg['OrganizedDateTime']\\n    return agg\",\n",
       "     'original_name': 't2',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def build_index(non_empty_days, from_time, to_time):\\n    date_ranges = []\\n    for date in non_empty_days:\\n        yyyy, mm, variable_def = date.split('-')\\n        from_hour, from_min = from_time.split(':')\\n        to_hour, to_min = to_time.split(':')\\n        t1 = datetime(int(yyyy), int(mm), int(variable_def), int(from_hour), int(from_min), 0)\\n        t2 = datetime(int(yyyy), int(mm), int(variable_def), int(to_hour), int(to_min), 0)\\n        date_ranges.append(pd.DataFrame({'OrganizedDateTime': pd.date_range(t1, t2, freq='1Min').values}))\\n    agg = pd.concat(date_ranges, axis=0)\\n    agg.index = agg['OrganizedDateTime']\\n    return agg\",\n",
       "     'original_name': 'dd',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/jingz8804/CarND-P01-Lane-Lines/b3d61e0ac63d71630c397ef1ea4d472798829054/P1.ipynb': {'cells': {'4618': [{'type': 'variable',\n",
       "     'cell_content': \"output_directory = 'test_images_output'\\nif not os.path.exists(output_directory):\\n    os.makedirs(output_directory)\\nvariable_def = mpimg.imread('test_images/solidYellowLeft.jpg')\\nprint('This image is:', type(variable_def), 'with dimensions:', variable_def.shape)\\nplt.imshow(variable_def)\",\n",
       "     'original_name': 'image',\n",
       "     'new_name': None}],\n",
       "   '4621': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = mpimg.imread('test_images/solidWhiteCurve.jpg')\\nlane_detected = lane_detection(variable_def)\\nplt.imshow(lane_detected)\",\n",
       "     'original_name': 'image',\n",
       "     'new_name': None}],\n",
       "   '4619': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(image):\\n    gray = grayscale(image)\\n    kernel_size = 5\\n    blur_gray = gaussian_blur(gray, 5)\\n    low_threshold = 60\\n    high_threshold = 180\\n    edges = canny(blur_gray, low_threshold, high_threshold)\\n    imshape = image.shape\\n    vertices = np.array([[(0, imshape[0]), (450, 320), (490, 320), (imshape[1], imshape[0])]], dtype=np.int32)\\n    masked_edges = region_of_interest(edges, vertices)\\n    rho = 2\\n    theta = np.pi / 180\\n    threshold = 15\\n    min_line_len = 40\\n    max_line_gap = 20\\n    line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\\n    color_edges = np.dstack((edges, edges, edges))\\n    lines_edges = weighted_img(line_image, image, α=0.8, β=1.0, λ=0.0)\\n    return lines_edges',\n",
       "     'original_name': 'lane_detection',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'def lane_detection(image):\\n    gray = grayscale(image)\\n    variable_def = 5\\n    blur_gray = gaussian_blur(gray, 5)\\n    low_threshold = 60\\n    high_threshold = 180\\n    edges = canny(blur_gray, low_threshold, high_threshold)\\n    imshape = image.shape\\n    vertices = np.array([[(0, imshape[0]), (450, 320), (490, 320), (imshape[1], imshape[0])]], dtype=np.int32)\\n    masked_edges = region_of_interest(edges, vertices)\\n    rho = 2\\n    theta = np.pi / 180\\n    threshold = 15\\n    min_line_len = 40\\n    max_line_gap = 20\\n    line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\\n    color_edges = np.dstack((edges, edges, edges))\\n    lines_edges = weighted_img(line_image, image, α=0.8, β=1.0, λ=0.0)\\n    return lines_edges',\n",
       "     'original_name': 'kernel_size',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/ucaiado/Customer_Segments/e6bd7f64f3253eea9e63504bcb4853d762468bc1/customer_segments.ipynb': {'cells': {'4877': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = np.log(data.copy())\\nlog_data2 = np.log(data2.copy())\\nlog_samples = np.log(samples.copy())\\npd.scatter_matrix(log_data2, alpha=0.3, figsize=(14, 8), diagonal='kde')\",\n",
       "     'original_name': 'log_data',\n",
       "     'new_name': None}],\n",
       "   '4880': [{'type': 'variable',\n",
       "     'cell_content': 'import eda\\nreload(eda)\\nvariable_def = log_data.loc[[x for x in log_data.index if x not in d_unique_idx.keys()]]\\nsamples2 = log_data.loc[[x[0] for x in d_unique_idx.iteritems() if x[1] > 1]]\\nax = eda.features_boxplot(variable_def, samples2, samples2.index)',\n",
       "     'original_name': 'log_data2',\n",
       "     'new_name': None}],\n",
       "   '4874': [{'type': 'function',\n",
       "     'cell_content': 'from sklearn.cross_validation import train_test_split\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.metrics import r2_score\\n\\ndef function_def(X, y):\\n    \"\"\" \\n    Shuffles and splits data into 75% training and 25% testing subsets,\\n    then returns the training and testing subsets.\\n    :param X: numpy array. Features\\n    :param y: numpy array. Labels\\n    \"\"\"\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\\n    return (X_train, y_train, X_test, y_test)\\nl_score = []\\nfor s_col in data.columns:\\n    new_data = data.copy()\\n    y = new_data[s_col]\\n    X = new_data.drop([s_col], axis=1)\\n    X_train, y_train, X_test, y_test = function_def(X, y)\\n    regressor = DecisionTreeRegressor(random_state=0)\\n    regressor.fit(X_train, y_train)\\n    score = r2_score(regressor.predict(X_test), y_test)\\n    l_score.append(score)\\ndf_r2 = pd.DataFrame(l_score, index=data.columns, columns=[\\'Score\\'])\\ndf_r2.index.names = [\\'PREDICTED\\']\\ndf_r2',\n",
       "     'original_name': 'shuffle_split_data',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/juditacs/morph-segmentation/cc2e1148b8aa6312f73c04cb9cf492ef5b1f14fc/notebooks/sandbox/seq2seq_attention.ipynb': {'cells': {'3744': [{'type': 'variable',\n",
       "     'cell_content': \"if use_toy_data:\\n    input_fn = 'toy_input.txt'\\n    with open(input_fn, 'w') as f:\\n        f.write('a b c\\\\td e f d e f\\\\n')\\n        f.write('d e f\\\\ta b c a b c\\\\n')\\nelse:\\n    DATA_DIR = '../../data/'\\n    input_fn = 'instrumental.full.train'\\n    input_fn = os.path.join(DATA_DIR, input_fn)\\n    if not os.path.exists(input_fn):\\n        import urllib\\n        variable_def = urllib.request.URLopener()\\n        variable_def.retrieve('http://sandbox.mokk.bme.hu/~judit/resources/instrumental.full.train', input_fn)\",\n",
       "     'original_name': 'u',\n",
       "     'new_name': None}],\n",
       "   '3743': [{'type': 'variable',\n",
       "     'cell_content': \"PROJECT_DIR = '../../'\\nvariable_def = False\\nLOG_DIR = 'logs'\\nif variable_def:\\n    batch_size = 8\\n    embedding_dim = 5\\n    cell_size = 32\\n    max_len = 6\\nelse:\\n    batch_size = 64\\n    embedding_dim = 20\\n    cell_size = 128\\n    max_len = 33\\nuse_attention = True\\nuse_bidirectional_encoder = True\\nis_time_major = True\",\n",
       "     'original_name': 'use_toy_data',\n",
       "     'new_name': None}],\n",
       "   '3746': [{'type': 'function',\n",
       "     'cell_content': \"class Config(object):\\n    default_fn = os.path.join(PROJECT_DIR, 'config', 'seq2seq', 'default.yaml')\\n\\n    @staticmethod\\n    def load_defaults(fn=default_fn):\\n        with open(fn) as f:\\n            return yaml.load(f)\\n\\n    @classmethod\\n    def function_def(cls, fn):\\n        params = yaml.load(fn)\\n        return cls(**params)\\n\\n    def __init__(self, **kwargs):\\n        defaults = Config.load_defaults()\\n        for param, val in defaults.items():\\n            setattr(self, param, val)\\n        for param, val in kwargs.items():\\n            setattr(self, param, val)\\nconfig = Config(src_maxlen=30, tgt_maxlen=33)\\ndataset = Dataset(input_fn, config)\",\n",
       "     'original_name': 'from_yaml',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/CFerraren/PyBank/7e4811e564e8fb049978d8123bba30651a74a590/2-EDA.ipynb': {'cells': {'189': [{'type': 'variable',\n",
       "     'cell_content': \"QHrev = dfq.Revenue.max()[3]\\nQHLoss = dfq.Revenue.min()[3]\\nQmax = max(dfq.idxmax())\\nvariable_def = min(dfq.idxmin())\\nprint('Highest Grossing Quarterly Revenue of $%.0f was observed on the %s %s-quarter.' % (QHrev, Qmax[0], Qmax[1]))\\nprint('Biggest Quarterly Loss of $%.00f was observed on the %s %s-quarter.' % (QHLoss, variable_def[0], variable_def[1]))\",\n",
       "     'original_name': 'Qmin',\n",
       "     'new_name': None}],\n",
       "   '188': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = df.pivot_table(index=[df.index.year, df.index.quarter], aggfunc=(np.mean, np.sum, min, max)).rename_axis(['year', 'quarter'])\\nvariable_def.style.applymap(color_negative_red).apply(highlight_max).apply(highlight_min)\",\n",
       "     'original_name': 'dfq',\n",
       "     'new_name': None}],\n",
       "   '171': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(s):\\n    \"\"\"\\n    highlight the minimum in a Series pink.\\n    \"\"\"\\n    is_min = s == s.min()\\n    return [\\'background-color: pink\\' if v else \\'\\' for v in is_min]',\n",
       "     'original_name': 'highlight_min',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/hananiel/gcptraining/d21f17f513cf0317986c3230205908fb5f5d2154/blogs/babyweight/babyweight.ipynb': {'cells': {'1929': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(column_name):\\n    sql = '\\\\nSELECT\\\\n  {0},\\\\n  COUNT(1) AS num_babies,\\\\n  AVG(weight_pounds) AS avg_wt\\\\nFROM\\\\n  publicdata.samples.natality\\\\nWHERE\\\\n  year > 2000\\\\nGROUP BY\\\\n  {0}\\\\n    '.format(column_name)\\n    return bq.Query(sql).execute().result().to_dataframe()\",\n",
       "     'original_name': 'get_distinct_values',\n",
       "     'new_name': None}],\n",
       "   '1939': [{'type': 'function',\n",
       "     'cell_content': \"def function_def():\\n    feature_placeholders = {'is_male': tf.placeholder(tf.string, [None]), 'mother_age': tf.placeholder(tf.float32, [None]), 'plurality': tf.placeholder(tf.string, [None]), 'gestation_weeks': tf.placeholder(tf.float32, [None])}\\n    features = {key: tf.expand_dims(tensor, -1) for key, tensor in feature_placeholders.items()}\\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\",\n",
       "     'original_name': 'serving_input_fn',\n",
       "     'new_name': None}],\n",
       "   '1938': [{'type': 'variable',\n",
       "     'cell_content': \"def get_wide_deep():\\n    is_male, mother_age, plurality, variable_def = [tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False', 'Unknown']), tf.feature_column.numeric_column('mother_age'), tf.feature_column.categorical_column_with_vocabulary_list('plurality', ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)', 'Multiple(2+)']), tf.feature_column.numeric_column('gestation_weeks')]\\n    age_buckets = tf.feature_column.bucketized_column(mother_age, boundaries=np.arange(15, 45, 1).tolist())\\n    gestation_buckets = tf.feature_column.bucketized_column(variable_def, boundaries=np.arange(17, 47, 1).tolist())\\n    wide = [is_male, plurality, age_buckets, gestation_buckets]\\n    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\\n    embed = tf.feature_column.embedding_column(crossed, 3)\\n    deep = [mother_age, variable_def, embed]\\n    return (wide, deep)\",\n",
       "     'original_name': 'gestation_weeks',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def get_wide_deep():\\n    variable_def, mother_age, plurality, gestation_weeks = [tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False', 'Unknown']), tf.feature_column.numeric_column('mother_age'), tf.feature_column.categorical_column_with_vocabulary_list('plurality', ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)', 'Multiple(2+)']), tf.feature_column.numeric_column('gestation_weeks')]\\n    age_buckets = tf.feature_column.bucketized_column(mother_age, boundaries=np.arange(15, 45, 1).tolist())\\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, boundaries=np.arange(17, 47, 1).tolist())\\n    wide = [variable_def, plurality, age_buckets, gestation_buckets]\\n    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\\n    embed = tf.feature_column.embedding_column(crossed, 3)\\n    deep = [mother_age, gestation_weeks, embed]\\n    return (wide, deep)\",\n",
       "     'original_name': 'is_male',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/tychovdo/Char-RNN-Understanding/80146b831b1dbfded1663ff574a7cd23150a3ad4/DiagnosticClassifier.ipynb': {'cells': {'513': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = torch.load('models/linux_3x512_0d3_lstm_200l_40000E.model').cuda()\\nmodel_gru = torch.load('models/linux_3x512_0d3_gru_200l_40000E.model').cuda()\\nprint('Perplexity LSTM:', 2 ** np.mean([test_model(variable_def, 'data/linux/test.txt') for _ in range(1)]))\\nprint('Perplexity GRU: ', 2 ** np.mean([test_model(model_gru, 'data/linux/test.txt') for _ in range(1)]))\",\n",
       "     'original_name': 'model_lstm',\n",
       "     'new_name': None}],\n",
       "   '524': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(model, N, text_len=500):\\n    texts, hiddens, hyps = ([], [], [])\\n    for i in range(N):\\n        text, hidden = generate(model, '\\\\n\\\\n', text_len, 0.8, True)\\n        hidden = hidden.reshape(hidden.shape[0], -1)\\n        hyp = hypothesis_inlinecounter(text)\\n        hiddens.append(hidden)\\n        hyps.append(hyp)\\n        texts.append(text)\\n    return (''.join(texts), np.concatenate(hyps), np.concatenate(hiddens))\\ntrain_texts, train_hyps, train_hiddens = function_def(model_gru, 500)\\ntest_texts, test_hyps, test_hiddens = function_def(model_gru, 2)\",\n",
       "     'original_name': 'gen_hyp_data',\n",
       "     'new_name': None}],\n",
       "   '522': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(text):\\n    hyp = np.concatenate([np.linspace(1, -1, len(x) + 1) for x in text.split('\\\\n')])[:-1]\\n    return hyp\\n\\ndef hypothesis_capswords(text):\\n    hyp = np.concatenate([np.full(len(x) + 1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x) + 1, -1) for x in text.split('\\\\n')])[:-1]\\n    return hyp\\n\\ndef hypothesis_pos(text, pos_tag):\\n    hyp = text.replace('1', '0')\\n    for word, tag in pynlpir.segment(text):\\n        if tag == pos_tag:\\n            hyp = hyp.replace(word, '1' * len(word), 1)\\n        else:\\n            hyp = hyp.replace(word, '0' * len(word), 1)\\n    hyp = [1 if x == '1' else -1 for x in re.sub('[^1]', '0', hyp)]\\n    return hyp\\n\\ndef hypothesis_verbs(text):\\n    return hypothesis_pos(text, 'verb')\\n\\ndef hypothesis_nouns(text):\\n    return hypothesis_pos(text, 'noun')\",\n",
       "     'original_name': 'hypothesis_inlinecounter',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': \"def hypothesis_inlinecounter(text):\\n    hyp = np.concatenate([np.linspace(1, -1, len(x) + 1) for x in text.split('\\\\n')])[:-1]\\n    return hyp\\n\\ndef function_def(text):\\n    hyp = np.concatenate([np.full(len(x) + 1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x) + 1, -1) for x in text.split('\\\\n')])[:-1]\\n    return hyp\\n\\ndef hypothesis_pos(text, pos_tag):\\n    hyp = text.replace('1', '0')\\n    for word, tag in pynlpir.segment(text):\\n        if tag == pos_tag:\\n            hyp = hyp.replace(word, '1' * len(word), 1)\\n        else:\\n            hyp = hyp.replace(word, '0' * len(word), 1)\\n    hyp = [1 if x == '1' else -1 for x in re.sub('[^1]', '0', hyp)]\\n    return hyp\\n\\ndef hypothesis_verbs(text):\\n    return hypothesis_pos(text, 'verb')\\n\\ndef hypothesis_nouns(text):\\n    return hypothesis_pos(text, 'noun')\",\n",
       "     'original_name': 'hypothesis_capswords',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': \"def hypothesis_inlinecounter(text):\\n    hyp = np.concatenate([np.linspace(1, -1, len(x) + 1) for x in text.split('\\\\n')])[:-1]\\n    return hyp\\n\\ndef hypothesis_capswords(text):\\n    hyp = np.concatenate([np.full(len(x) + 1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x) + 1, -1) for x in text.split('\\\\n')])[:-1]\\n    return hyp\\n\\ndef hypothesis_pos(text, pos_tag):\\n    hyp = text.replace('1', '0')\\n    for word, tag in pynlpir.segment(text):\\n        if tag == pos_tag:\\n            hyp = hyp.replace(word, '1' * len(word), 1)\\n        else:\\n            hyp = hyp.replace(word, '0' * len(word), 1)\\n    hyp = [1 if x == '1' else -1 for x in re.sub('[^1]', '0', hyp)]\\n    return hyp\\n\\ndef function_def(text):\\n    return hypothesis_pos(text, 'verb')\\n\\ndef hypothesis_nouns(text):\\n    return hypothesis_pos(text, 'noun')\",\n",
       "     'original_name': 'hypothesis_verbs',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/kaiszybiak/RoboND-Rover-Project/0421efbbb610d9f15414a899accf50d49413cc39/code/Rover_Project_Test_Notebook.ipynb': {'cells': {'2079': [{'type': 'variable',\n",
       "     'cell_content': \"def perspect_transform(img, src, dst):\\n    M = cv2.getPerspectiveTransform(src, dst)\\n    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))\\n    mask = cv2.warpPerspective(np.ones_like(img[:, :, 0]), M, (img.shape[1], img.shape[0]))\\n    return (warped, mask)\\ndst_size = 5\\nbottom_offset = 6\\nvariable_def = np.float32([[14, 140], [301, 140], [200, 96], [118, 96]])\\ndestination = np.float32([[image.shape[1] / 2 - dst_size, image.shape[0] - bottom_offset], [image.shape[1] / 2 + dst_size, image.shape[0] - bottom_offset], [image.shape[1] / 2 + dst_size, image.shape[0] - 2 * dst_size - bottom_offset], [image.shape[1] / 2 - dst_size, image.shape[0] - 2 * dst_size - bottom_offset]])\\nwarped, mask = perspect_transform(grid_img, variable_def, destination)\\nfig = plt.figure(figsize=(12, 3))\\nplt.subplot(121)\\nplt.imshow(warped)\\nplt.subplot(122)\\nplt.imshow(mask, cmap='gray')\",\n",
       "     'original_name': 'source',\n",
       "     'new_name': None}],\n",
       "   '2077': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = '../test_dataset/IMG/*'\\nimg_list = glob.glob(variable_def)\\nidx = np.random.randint(0, len(img_list) - 1)\\nimage = mpimg.imread(img_list[idx])\\nplt.imshow(image)\",\n",
       "     'original_name': 'path',\n",
       "     'new_name': None}],\n",
       "   '2080': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(img, rgb_thresh=(160, 160, 160)):\\n    color_select = np.zeros_like(img[:, :, 0])\\n    above_thresh = (img[:, :, 0] > rgb_thresh[0]) & (img[:, :, 1] > rgb_thresh[1]) & (img[:, :, 2] > rgb_thresh[2])\\n    color_select[above_thresh] = 1\\n    return color_select\\nthreshed = function_def(warped)\\nplt.imshow(threshed, cmap='gray')\",\n",
       "     'original_name': 'color_thresh',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/hfoffani/tensorflow-dl/e1f2947e31b5d94c0c834ffecbc4714529463f54/4_convolutions.ipynb': {'cells': {'224': [{'type': 'variable',\n",
       "     'cell_content': \"image_size = 28\\nnum_labels = 10\\nvariable_def = 1\\n\\ndef reformat(dataset, labels):\\n    dataset = dataset.reshape((-1, image_size, image_size, variable_def)).astype(np.float32)\\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\\n    return (dataset, labels)\\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\\nprint('Training set', train_dataset.shape, train_labels.shape)\\nprint('Validation set', valid_dataset.shape, valid_labels.shape)\\nprint('Test set', test_dataset.shape, test_labels.shape)\",\n",
       "     'original_name': 'num_channels',\n",
       "     'new_name': None}],\n",
       "   '230': [{'type': 'function',\n",
       "     'cell_content': \"import datetime\\n\\ndef function_def(start, n, total):\\n    now = datetime.datetime.now()\\n    diff = now - start\\n    secs = (total - n) * 1.0 * diff.seconds / (n + 1)\\n    ends = now + datetime.timedelta(seconds=secs)\\n    return ends.strftime('%H:%M:%S')\",\n",
       "     'original_name': 'eta',\n",
       "     'new_name': None}],\n",
       "   '225': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(predictions, labels):\\n    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]',\n",
       "     'original_name': 'accuracy',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/AgremE/Hand_Signs_Recognizer/9416b82f855f4aa0e75bfdc4cdf08861363f246b/asl_recognizer.ipynb': {'cells': {'1872': [{'type': 'variable',\n",
       "     'cell_content': 'from asl_test_model_selectors import TestSelectors\\nvariable_def = unittest.TestLoader().loadTestsFromModule(TestSelectors())\\nunittest.TextTestRunner().run(variable_def)',\n",
       "     'original_name': 'suite',\n",
       "     'new_name': None}],\n",
       "   '1860': [{'type': 'function',\n",
       "     'cell_content': \"import unittest\\n\\nclass TestFeatures(unittest.TestCase):\\n\\n    def function_def(self):\\n        sample = asl.df.ix[98, 1][features_ground].tolist()\\n        self.assertEqual(sample, [9, 113, -12, 119])\\n\\n    def test_features_norm(self):\\n        sample = asl.df.ix[98, 1][features_norm].tolist()\\n        np.testing.assert_almost_equal(sample, [1.153, 1.663, -0.891, 0.742], 3)\\n\\n    def test_features_polar(self):\\n        sample = asl.df.ix[98, 1][features_polar].tolist()\\n        np.testing.assert_almost_equal(sample, [113.3578, 0.0794, 119.603, -0.1005], 3)\\n\\n    def test_features_delta(self):\\n        sample = asl.df.ix[98, 0][features_delta].tolist()\\n        self.assertEqual(sample, [0, 0, 0, 0])\\n        sample = asl.df.ix[98, 18][features_delta].tolist()\\n        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], 'Sample value found was {}'.format(sample))\\nsuite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\\nunittest.TextTestRunner().run(suite)\",\n",
       "     'original_name': 'test_features_ground',\n",
       "     'new_name': None}],\n",
       "   '1871': [{'type': 'variable',\n",
       "     'cell_content': \"from my_model_selectors import SelectorDIC\\ntraining = asl.build_training(features_ground)\\nsequences = training.get_all_sequences()\\nXlengths = training.get_all_Xlengths()\\nfor word in words_to_train:\\n    variable_def = timeit.default_timer()\\n    model = SelectorDIC(sequences, Xlengths, word, min_n_components=2, max_n_components=15, random_state=14).select()\\n    end = timeit.default_timer() - variable_def\\n    if model is not None:\\n        print('Training complete for {} with {} states with time {} seconds'.format(word, model.n_components, end))\\n    else:\\n        print('Training failed for {}'.format(word))\",\n",
       "     'original_name': 'start',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"from my_model_selectors import SelectorDIC\\ntraining = asl.build_training(features_ground)\\nsequences = training.get_all_sequences()\\nXlengths = training.get_all_Xlengths()\\nfor word in words_to_train:\\n    start = timeit.default_timer()\\n    model = SelectorDIC(sequences, Xlengths, word, min_n_components=2, max_n_components=15, random_state=14).select()\\n    variable_def = timeit.default_timer() - start\\n    if model is not None:\\n        print('Training complete for {} with {} states with time {} seconds'.format(word, model.n_components, variable_def))\\n    else:\\n        print('Training failed for {}'.format(word))\",\n",
       "     'original_name': 'end',\n",
       "     'new_name': None}],\n",
       "   '1863': [{'type': 'variable',\n",
       "     'cell_content': \"my_testword = 'CHOCOLATE'\\nvariable_def, logL = train_a_word(my_testword, 3, features_ground)\\nshow_model_stats(my_testword, variable_def)\\nprint('logL = {}'.format(logL))\",\n",
       "     'original_name': 'model',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"my_testword = 'CHOCOLATE'\\nmodel, variable_def = train_a_word(my_testword, 3, features_ground)\\nshow_model_stats(my_testword, model)\\nprint('logL = {}'.format(variable_def))\",\n",
       "     'original_name': 'logL',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/mdeff/dlaudio/0b214ac787ba8763cc660ae0c1bac9079e0e1e58/audio_features.ipynb': {'cells': {'1897': [{'type': 'variable',\n",
       "     'cell_content': \"N = Ngenres * Nclips * Nframes * 2\\nsizeX = N * n / 2.0 ** 20\\nvariable_def = N * m / 2.0 ** 20\\nsizeD = n * m / 2.0 ** 10\\nsizeE = m * n / 2.0 ** 10\\nprint('Size X: {:.1f} M --> {:.1f} MiB'.format(sizeX, sizeX * 4))\\nprint('Size Z: {:.1f} M --> {:.1f} MiB'.format(variable_def, variable_def * 4))\\nprint('Size D: {:.1f} k --> {:.1f} kiB'.format(sizeD, sizeD * 4))\\nprint('Size E: {:.1f} k --> {:.1f} kiB'.format(sizeE, sizeE * 4))\",\n",
       "     'original_name': 'sizeZ',\n",
       "     'new_name': None}],\n",
       "   '1896': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = os.path.join(folder, filename_graph)\\nwith h5py.File(variable_def, 'r') as graph:\\n    print('Attributes:')\\n    for attr in graph.attrs:\\n        print('  {} = {}'.format(attr, graph.attrs[attr]))\\n    print('Datasets:')\\n    for dname, dset in graph.items():\\n        print('  {:10}: {:10}, {}'.format(dname, dset.shape, dset.dtype))\\n    pars = []\\n    for par in ('data', 'indices', 'indptr', 'shape'):\\n        pars.append(graph.get('L_' + par))\\n    L = scipy.sparse.csr_matrix(tuple(pars[:3]), shape=pars[3])\\nif L.shape != (X.shape[0], X.shape[0]):\\n    raise ValueError('Graph size does not correspond to data size.')\",\n",
       "     'original_name': 'filename',\n",
       "     'new_name': None}],\n",
       "   '1892': [{'type': 'variable',\n",
       "     'cell_content': \"if 'p' in globals().keys():\\n    for key, value in p.items():\\n        globals()[key] = value\\nelse:\\n    m = 64\\n    ls = 1\\n    ld = 10\\n    le = None\\n    lg = 1\\n    rtol = 1e-05\\n    N_inner = 500\\n    N_outer = 50\\n    Ngenres, Nclips, Nframes = (10, 100, 644)\\n    noise_std = 0\\n    folder = 'data'\\n    filename_audio = 'audio.hdf5'\\n    filename_graph = 'graph.hdf5'\\n    variable_def = 'features.hdf5'\",\n",
       "     'original_name': 'filename_features',\n",
       "     'new_name': None}],\n",
       "   '1894': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(X, name=\\'Dataset\\'):\\n    \"\"\"Print dataset size and dimensionality\"\"\"\\n    print(\\'{}:\\\\n  size: N={:,} x n={} -> {:,} floats\\\\n  dim: {:,} features per clip\\\\n  shape: {}\\'.format(name, np.prod(X.shape[:-1]), X.shape[-1], np.prod(X.shape), np.prod(X.shape[2:]), X.shape))',\n",
       "     'original_name': 'datinfo',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/roei60/Data-Science-Hotels/9bed0b5800e31b47da4455f604eb8349f093f0a5/Clustering.ipynb': {'cells': {'381': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def['Hotel_Count'] = variable_def.groupby('Hotel Name')['Hotel Name'].transform('count')\\ndescending_hotels = variable_def.sort_values(by=['Hotel_Count'], ascending=False).reset_index()\\ndf_hotels = descending_hotels['Hotel Name'].unique()[:150]\\nmost_common_hotels = descending_hotels[descending_hotels['Hotel Name'].isin(df_hotels)]\",\n",
       "     'original_name': 'df',\n",
       "     'new_name': None}],\n",
       "   '380': [{'type': 'function',\n",
       "     'cell_content': \"import numpy as np\\nimport pandas as pd\\nfrom pandas import Series, DataFrame\\ndf = pd.read_csv('hotels_data.csv')\\nfrom datetime import datetime\\nfrom dateutil.parser import parse\\n\\ndef function_def(date_str):\\n    return datetime.strptime(date_str, '%m/%d/%Y %H:%M')\\ndf['DayDiff'] = DataFrame([function_def(val) for val in df['Checkin Date']]) - DataFrame([function_def(val) for val in df['Snapshot Date']])\\ndf['WeekDay'] = DataFrame([function_def(val).weekday() for val in df['Checkin Date']])\\ndf['DiscountDiff'] = df['Original Price'] - df['Discount Price']\\ndf['DiscountPerc'] = df['DiscountDiff'] / df['Original Price'] * 100\\ndf\",\n",
       "     'original_name': 'get_datetime',\n",
       "     'new_name': None}],\n",
       "   '385': [{'type': 'function',\n",
       "     'cell_content': \"discount_filtered = checkin_hotel_discount[checkin_hotel_discount['Discount Price'] > -1]\\n\\ndef function_def(x):\\n    diff = max(x) - min(x)\\n    if diff == 0:\\n        return 0\\n    else:\\n        return round((x - min(x)) / (max(x) - min(x)) * 100)\\ndiscount_filtered_grouped = discount_filtered.groupby('Hotel Name')['Discount Price']\\ndiscount_filtered['Normal'] = discount_filtered_grouped.transform(function_def)\\ndiscount_synth = checkin_hotel_discount[checkin_hotel_discount['Discount Price'] == -1]\\ndiscount_synth['Normal'] = -1\\nnormal_dataFrame = discount_synth.append(discount_filtered)\\nnormal_dataFrame.sort_values(by=['Hotel Name', 'Checkin Date', 'Discount Code'], ascending=True, inplace=True)\\nnormal_dataFrame\",\n",
       "     'original_name': 'normalize_data',\n",
       "     'new_name': None}],\n",
       "   '383': [{'type': 'variable',\n",
       "     'cell_content': \"unique_hotels_names = most_checkins['Hotel Name'].unique()\\nunique_checkins = most_checkins['Checkin Date'].unique()\\nvariable_def = [1, 2, 3, 4]\\nimport itertools\\nimport sys\\ncombs = []\\nfor x in unique_hotels_names:\\n    for y in unique_checkins:\\n        for z in variable_def:\\n            combs.append([x, y, z, sys.maxsize])\\nnew_df = DataFrame.from_records(combs, columns=['Hotel Name', 'Checkin Date', 'Discount Code', 'Discount Price'])\\nmost_checkins = most_checkins.append(new_df)\",\n",
       "     'original_name': 'unique_discount_code',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"unique_hotels_names = most_checkins['Hotel Name'].unique()\\nunique_checkins = most_checkins['Checkin Date'].unique()\\nunique_discount_code = [1, 2, 3, 4]\\nimport itertools\\nimport sys\\ncombs = []\\nfor x in unique_hotels_names:\\n    for y in unique_checkins:\\n        for z in unique_discount_code:\\n            combs.append([x, y, z, sys.maxsize])\\nvariable_def = DataFrame.from_records(combs, columns=['Hotel Name', 'Checkin Date', 'Discount Code', 'Discount Price'])\\nmost_checkins = most_checkins.append(variable_def)\",\n",
       "     'original_name': 'new_df',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/zzjf/tensorflow-ios-simple-project/f143790323781e87b45abb96e641de3c9531e617/tensorflow/examples/udacity/6_lstm.ipynb': {'cells': {'5508': [{'type': 'variable',\n",
       "     'cell_content': 'valid_size = 1000\\nvalid_text = text[:valid_size]\\ntrain_text = text[valid_size:]\\nvariable_def = len(train_text)\\nprint(variable_def, train_text[:64])\\nprint(valid_size, valid_text[:64])',\n",
       "     'original_name': 'train_size',\n",
       "     'new_name': None}],\n",
       "   '5511': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(predictions, labels):\\n    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\\n    predictions[predictions < 1e-10] = 1e-10\\n    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\\n\\ndef sample_distribution(distribution):\\n    \"\"\"Sample one element from a distribution assumed to be an array of normalized\\n  probabilities.\\n  \"\"\"\\n    r = random.uniform(0, 1)\\n    s = 0\\n    for i in range(len(distribution)):\\n        s += distribution[i]\\n        if s >= r:\\n            return i\\n    return len(distribution) - 1\\n\\ndef sample(prediction):\\n    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\\n    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\\n    p[0, sample_distribution(prediction[0])] = 1.0\\n    return p\\n\\ndef random_distribution():\\n    \"\"\"Generate a random column of probabilities.\"\"\"\\n    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\\n    return b / np.sum(b, 1)[:, None]',\n",
       "     'original_name': 'logprob',\n",
       "     'new_name': None}],\n",
       "   '5507': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(filename):\\n    with zipfile.ZipFile(filename) as f:\\n        name = f.namelist()[0]\\n        data = tf.compat.as_str(f.read(name))\\n    return data\\ntext = function_def(filename)\\nprint('Data size %d' % len(text))\",\n",
       "     'original_name': 'read_data',\n",
       "     'new_name': None}],\n",
       "   '5506': [{'type': 'function',\n",
       "     'cell_content': 'url = \\'http://mattmahoney.net/dc/\\'\\n\\ndef function_def(filename, expected_bytes):\\n    \"\"\"Download a file if not present, and make sure it\\'s the right size.\"\"\"\\n    if not os.path.exists(filename):\\n        filename, _ = urlretrieve(url + filename, filename)\\n    statinfo = os.stat(filename)\\n    if statinfo.st_size == expected_bytes:\\n        print(\\'Found and verified %s\\' % filename)\\n    else:\\n        print(statinfo.st_size)\\n        raise Exception(\\'Failed to verify \\' + filename + \\'. Can you get to it with a browser?\\')\\n    return filename\\nfilename = function_def(\\'text8.zip\\', 31344016)',\n",
       "     'original_name': 'maybe_download',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'url = \\'http://mattmahoney.net/dc/\\'\\n\\ndef maybe_download(filename, expected_bytes):\\n    \"\"\"Download a file if not present, and make sure it\\'s the right size.\"\"\"\\n    if not os.path.exists(filename):\\n        filename, _ = urlretrieve(url + filename, filename)\\n    variable_def = os.stat(filename)\\n    if variable_def.st_size == expected_bytes:\\n        print(\\'Found and verified %s\\' % filename)\\n    else:\\n        print(variable_def.st_size)\\n        raise Exception(\\'Failed to verify \\' + filename + \\'. Can you get to it with a browser?\\')\\n    return filename\\nfilename = maybe_download(\\'text8.zip\\', 31344016)',\n",
       "     'original_name': 'statinfo',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/RobFirth/pycoco/1c9be662b6dd67f8be5423568bb3ecbff2979492/notebooks/CoCo_SN2007uy_Test.ipynb': {'cells': {'5593': [{'type': 'variable',\n",
       "     'cell_content': \"verbose = False\\nfor i, f in enumerate(variable_def):\\n    variable_def[i] = f.replace(b'SDSS', b'LSST').replace(b'BessellV', b'LSST_g')\\nif verbose:\\n    print(mjdmax)\\n    print(mjd_to_sim)\\n    print(variable_def)\",\n",
       "     'original_name': 'filters_to_sim',\n",
       "     'new_name': None}],\n",
       "   '5596': [{'type': 'variable',\n",
       "     'cell_content': \"reload(pccsims)\\ncoco = pccsims.pyCoCo(pcc.utils.b(filter_path), pcc.utils.b(coco_root_path))\\nflux, variable_def = coco.simulate(b'SN2007uy', z_obs, 0.0, 0.0, 0.0, 3.1, mjdmax, mjd_to_sim, filters_to_sim)\",\n",
       "     'original_name': 'flux_err',\n",
       "     'new_name': None}],\n",
       "   '5629': [{'type': 'function',\n",
       "     'cell_content': 'from astropy import units as u\\nfrom astropy.coordinates import SkyCoord\\nfrom astropy.table import Table\\n\\ndef function_def(filename=\\'sncoordinates.list\\'):\\n    \"\"\"\\n\\n    \"\"\"\\n    path = os.path.abspath(os.path.join(pcc.__path__[0], os.path.pardir, filename))\\n    coordtable = Table.read(path, format=\\'ascii.commented_header\\')\\n    return coordtable',\n",
       "     'original_name': 'load_coords',\n",
       "     'new_name': None}],\n",
       "   '5580': [{'type': 'function',\n",
       "     'cell_content': \"def convert_column_string_encoding(column):\\n    column = Column([pcc.utils.b(x) for x in column.data], name=column.name)\\n    return column\\n\\ndef function_def(sn):\\n    v = sn.lcfit.spline['BessellV']\\n    mjd_spline = np.arange(np.nanmin(sn.phot.data['BessellV']['MJD']), np.nanmax(sn.phot.data['BessellV']['MJD']), 0.001)\\n    w = np.where(v(mjd_spline) == np.nanmax(v(mjd_spline)))\\n    mjdmax = mjd_spline[w]\\n    return mjdmax\",\n",
       "     'original_name': 'get_mjdmax_BessellV',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/cmaumet/nipype_tutorial/aba7bda10326689d721b5a99985df044852f50aa/notebooks/introduction_python.ipynb': {'cells': {'3462': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = []\\nvariable_def.append('A')\\nvariable_def.append('d')\\nvariable_def.append('d')\\nprint(variable_def)\",\n",
       "     'original_name': 'l',\n",
       "     'new_name': None}],\n",
       "   '3524': [{'type': 'function',\n",
       "     'cell_content': 'def function_def():\\n    try:\\n        10 / 0\\n    except ZeroDivisionError:\\n        print(\\'Oops, invalid.\\')\\n    else:\\n        pass\\n    finally:\\n        print(\"We\\'re done with that.\")\\nfunction_def()',\n",
       "     'original_name': 'some_function',\n",
       "     'new_name': None}],\n",
       "   '3501': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(x, p=2, debug=False):\\n    if debug:\\n        print('evaluating myfunc for x = ' + str(x) + ' using exponent p = ' + str(p))\\n    return x ** p\",\n",
       "     'original_name': 'myfunc',\n",
       "     'new_name': None}],\n",
       "   '3506': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(arg1, **kwargs):\\n    print('kwargs is now a dictionary...\\\\nType: %s\\\\nContent: %s\\\\n' % (type(kwargs), kwargs))\\n    print('Formal arg:', arg1)\\n    for key in kwargs:\\n        print('another keyword arg: %s: %s' % (key, kwargs[key]))\\nfunction_def(arg1=1, myarg2='two', myarg3=3)\",\n",
       "     'original_name': 'kwargs_func',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/gkbharathy/Py_ML_Book_Raschka/ef6c60f81e20a3717a18812f88d3cfd6d264a1bd/code/ch08/ch08.ipynb': {'cells': {'2091': [{'type': 'variable',\n",
       "     'cell_content': \"import pyprind\\nimport pandas as pd\\nimport os\\nbasepath = 'aclImdb'\\nlabels = {'pos': 1, 'neg': 0}\\nvariable_def = pyprind.ProgBar(50000)\\ndf = pd.DataFrame()\\nfor s in ('test', 'train'):\\n    for l in ('pos', 'neg'):\\n        path = os.path.join(basepath, s, l)\\n        for file in sorted(os.listdir(path)):\\n            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\\n                txt = infile.read()\\n            df = df.append([[txt, labels[l]]], ignore_index=True)\\n            variable_def.update()\\ndf.columns = ['review', 'sentiment']\",\n",
       "     'original_name': 'pbar',\n",
       "     'new_name': None}],\n",
       "   '2130': [{'type': 'variable',\n",
       "     'cell_content': 'import pyprind\\npbar = pyprind.ProgBar(45)\\nclasses = np.array([0, 1])\\nfor _ in range(45):\\n    variable_def, y_train = get_minibatch(doc_stream, size=1000)\\n    if not variable_def:\\n        break\\n    variable_def = vect.transform(variable_def)\\n    clf.partial_fit(variable_def, y_train, classes=classes)\\n    pbar.update()',\n",
       "     'original_name': 'X_train',\n",
       "     'new_name': None}],\n",
       "   '2089': [{'type': 'variable',\n",
       "     'cell_content': \"import os\\nimport sys\\nimport tarfile\\nimport time\\nvariable_def = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\\ntarget = 'aclImdb_v1.tar.gz'\\n\\ndef reporthook(count, block_size, total_size):\\n    global start_time\\n    if count == 0:\\n        start_time = time.time()\\n        return\\n    duration = time.time() - start_time\\n    progress_size = int(count * block_size)\\n    speed = progress_size / (1024.0 ** 2 * duration)\\n    percent = count * block_size * 100.0 / total_size\\n    sys.stdout.write('\\\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed' % (percent, progress_size / 1024.0 ** 2, speed, duration))\\n    sys.stdout.flush()\\nif not os.path.isdir('aclImdb') and (not os.path.isfile('aclImdb_v1.tar.gz')):\\n    if sys.version_info < (3, 0):\\n        import urllib\\n        urllib.urlretrieve(variable_def, target, reporthook)\\n    else:\\n        import urllib.request\\n        urllib.request.urlretrieve(variable_def, target, reporthook)\",\n",
       "     'original_name': 'source',\n",
       "     'new_name': None}],\n",
       "   '2115': [{'type': 'variable',\n",
       "     'cell_content': \"from sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.model_selection import GridSearchCV\\ntfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\\nparam_grid = [{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, {'vect__ngram_range': [(1, 1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}]\\nvariable_def = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\\ngs_lr_tfidf = GridSearchCV(variable_def, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\",\n",
       "     'original_name': 'lr_tfidf',\n",
       "     'new_name': None}],\n",
       "   '2105': [{'type': 'function',\n",
       "     'cell_content': \"import re\\n\\ndef function_def(text):\\n    text = re.sub('<[^>]*>', '', text)\\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\\\\\\\)|\\\\\\\\(|D|P)', text)\\n    text = re.sub('[\\\\\\\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\\n    return text\",\n",
       "     'original_name': 'preprocessor',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/odebeir/info-h-500-501/e42c5ef51dd2553e03effbc124ec3a35ac1383fe/05-Morphomathematics/01-Operators.ipynb': {'cells': {'66': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = B12_a[:, -1::-1]\\nHoM = hit_or_miss(X, variable_def)\\nplt.figure(figsize=[10, 10])\\nplt.subplot(1, 2, 1)\\nplt.imshow(X, interpolation='nearest')\\nplt.subplot(1, 2, 2)\\nplt.imshow(X, interpolation='nearest', alpha=0.8)\\nplt.imshow(HoM, interpolation='nearest', alpha=0.5)\",\n",
       "     'original_name': 'B12_b',\n",
       "     'new_name': None}],\n",
       "   '63': [{'type': 'variable',\n",
       "     'cell_content': \"from skimage.io import imread\\nfrom skimage import feature\\nvariable_def = imread('https://upload.wikimedia.org/wikipedia/commons/5/5f/MRI_EGC_sagittal.png')[-1::-1, :, :]\\ncanny = feature.canny(variable_def[:, :, 0], low_threshold=0.1 * 255, high_threshold=0.4 * 255) * 255\\nplt.figure(figsize=[10, 10])\\nplt.imshow(canny[-1::-1, :])\",\n",
       "     'original_name': 'ct',\n",
       "     'new_name': None}],\n",
       "   '58': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = X[:, -1::-1][-1::-1, :]\\nplt.imshow(variable_def, interpolation='nearest', cmap=plt.cm.gray)\",\n",
       "     'original_name': 'Xs',\n",
       "     'new_name': None}],\n",
       "   '60': [{'type': 'variable',\n",
       "     'cell_content': \"from skimage.morphology import disk, erosion, dilation, square\\nB = square(3)\\nvariable_def = dilation(X, selem=B)\\nX_ero = erosion(X, selem=B)\\nplt.figure(figsize=[10, 5])\\nplt.subplot(1, 3, 1)\\nplt.imshow(X, interpolation='nearest', cmap=plt.cm.gray)\\nplt.title('$X$')\\nplt.subplot(1, 3, 2)\\nplt.imshow(variable_def, interpolation='nearest', cmap=plt.cm.gray)\\nplt.title('dilation of $X$ by $B$')\\nplt.subplot(1, 3, 3)\\nplt.imshow(X_ero, interpolation='nearest', cmap=plt.cm.gray)\\nplt.title('erosion of $X$ by $B$')\\nplt.figure(figsize=[6, 6])\\nplt.imshow(X, interpolation='nearest', cmap=plt.cm.gray, alpha=0.3)\\nplt.imshow(X_ero, interpolation='nearest', cmap=plt.cm.gray, alpha=0.3)\\nplt.imshow(variable_def, interpolation='nearest', cmap=plt.cm.gray, alpha=0.3)\",\n",
       "     'original_name': 'X_dil',\n",
       "     'new_name': None}],\n",
       "   '65': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(X, B12):\\n    B1 = B12 == 1\\n    B2 = B12 == 0\\n    r = np.logical_and(erosion(X, B1), erosion(1 - X, B2))\\n    return r\\nX = (imread('http://homepages.ulb.ac.be/~odebeir/data/man.tif') > 0)[:, :, 0].astype(np.uint8)\\nB12_a = np.array([[2, 1, 2], [0, 1, 1], [0, 0, 2]])\\nHoM = function_def(X, B12_a)\\nplt.figure(figsize=[10, 10])\\nplt.subplot(1, 2, 1)\\nplt.imshow(X, interpolation='nearest')\\nplt.subplot(1, 2, 2)\\nplt.imshow(X, interpolation='nearest', alpha=0.8)\\nplt.imshow(HoM, interpolation='nearest', alpha=0.5)\",\n",
       "     'original_name': 'hit_or_miss',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/epfml/ML_course/c8bb9be4ad1acc4bc217eb6852c5cc37271b385d/labs/ex03/template/ex03.ipynb': {'cells': {'2869': [{'type': 'variable',\n",
       "     'cell_content': 'variable_def = 56\\ndegree = 7\\nsplit_ratio = 0.5\\nridge_regression_demo(x, y, degree, split_ratio, variable_def)',\n",
       "     'original_name': 'seed',\n",
       "     'new_name': None}],\n",
       "   '2868': [{'type': 'variable',\n",
       "     'cell_content': 'def ridge_regression_demo(x, y, degree, ratio, seed):\\n    \"\"\"ridge regression demo.\"\"\"\\n    lambdas = np.logspace(-5, 0, 15)\\n    raise NotImplementedError\\n    raise NotImplementedError\\n    variable_def = []\\n    rmse_te = []\\n    for ind, lambda_ in enumerate(lambdas):\\n        print(\\'proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\\'.format(p=ratio, d=degree, l=lambda_, tr=variable_def[ind], te=rmse_te[ind]))\\n    plot_train_test(variable_def, rmse_te, lambdas, degree)\\n    raise NotImplementedError',\n",
       "     'original_name': 'rmse_tr',\n",
       "     'new_name': None}],\n",
       "   '2864': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(x, y, ratio, seed=1):\\n    \"\"\"\\n    split the dataset based on the split ratio. If ratio is 0.8 \\n    you will have 80% of your data set dedicated to training \\n    and the rest dedicated to testing\\n    \"\"\"\\n    np.random.seed(seed)\\n    raise NotImplementedError',\n",
       "     'original_name': 'split_data',\n",
       "     'new_name': None}],\n",
       "   '2865': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(x, y, degree, ratio, seed):\\n    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\\n    raise NotImplementedError\\n    raise NotImplementedError\\n    raise NotImplementedError\\n    raise NotImplementedError\\n    print(\\'proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\\'.format(p=ratio, d=degree, tr=rmse_tr, te=rmse_te))',\n",
       "     'original_name': 'train_test_split_demo',\n",
       "     'new_name': None}],\n",
       "   '2862': [{'type': 'function',\n",
       "     'cell_content': 'from plots import *\\n\\ndef function_def():\\n    \"\"\"Constructing the polynomial basis function expansion of the data,\\n       and then running least squares regression.\"\"\"\\n    degrees = [1, 3, 7, 12]\\n    num_row = 2\\n    num_col = 2\\n    f, axs = plt.subplots(num_row, num_col)\\n    for ind, degree in enumerate(degrees):\\n        raise NotImplementedError\\n        raise NotImplementedError\\n        print(\\'Processing {i}th experiment, degree={d}, rmse={loss}\\'.format(i=ind + 1, d=degree, loss=rmse))\\n        plot_fitted_curve(y, x, weights, degree, axs[ind // num_col][ind % num_col])\\n    plt.tight_layout()\\n    plt.savefig(\\'visualize_polynomial_regression\\')\\n    plt.show()',\n",
       "     'original_name': 'polynomial_regression',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/sandeeppaulraj/artificial-intelligence/1af2e6f2908f5f898b8373703a72a41cd0548ec9/Projects/4_HMM%20Tagger/HMM%20Tagger.ipynb': {'cells': {'5649': [{'type': 'variable',\n",
       "     'cell_content': 'variable_def = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\\nprint(type(variable_def))\\nprint(type(words))',\n",
       "     'original_name': 'tags',\n",
       "     'new_name': None}],\n",
       "   '5663': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(sequences):\\n    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\\n    that counts the number of occurrences where that value is at the beginning of\\n    a sequence.\\n    \\n    For example, if 8093 sequences start with NOUN, then you should return a\\n    dictionary such that your_starting_counts[NOUN] == 8093\\n    \"\"\"\\n    d4 = defaultdict(int)\\n    for i in sequences:\\n        d4[i[0]] += 1\\n    return d4\\ntag_starts = function_def(data.training_set.Y)\\nprint(tag_starts)\\nassert len(tag_starts) == 12, \\'Uh oh. There should be 12 tags in your dictionary.\\'\\nassert min(tag_starts, key=tag_starts.get) == \\'X\\', \"Hmmm...\\'X\\' is expected to be the least common starting bigram.\"\\nassert max(tag_starts, key=tag_starts.get) == \\'DET\\', \"Hmmm...\\'DET\\' is expected to be the most common starting bigram.\"\\nHTML(\\'<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>\\')',\n",
       "     'original_name': 'starting_counts',\n",
       "     'new_name': None}],\n",
       "   '5664': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(sequences):\\n    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\\n    that counts the number of occurrences where that value is at the end of\\n    a sequence.\\n    \\n    For example, if 18 sequences end with DET, then you should return a\\n    dictionary such that your_starting_counts[DET] == 18\\n    \"\"\"\\n    d5 = defaultdict(int)\\n    for i in sequences:\\n        d5[i[-1]] += 1\\n    return d5\\ntag_ends = function_def(data.training_set.Y)\\nassert len(tag_ends) == 12, \\'Uh oh. There should be 12 tags in your dictionary.\\'\\nassert min(tag_ends, key=tag_ends.get) in [\\'X\\', \\'CONJ\\'], \"Hmmm...\\'X\\' or \\'CONJ\\' should be the least common ending bigram.\"\\nassert max(tag_ends, key=tag_ends.get) == \\'.\\', \"Hmmm...\\'.\\' is expected to be the most common ending bigram.\"\\nHTML(\\'<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>\\')',\n",
       "     'original_name': 'ending_counts',\n",
       "     'new_name': None}],\n",
       "   '5656': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(X, Y, model):\\n    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\\n    in the input X and comparing the prediction with the true labels in Y.\\n    \\n    The X should be an array whose first dimension is the number of sentences to test,\\n    and each element of the array should be an iterable of the words in the sequence.\\n    The arrays X and Y should have the exact same shape.\\n    \\n    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\\n    Y = [(), (), ...]\\n    \"\"\"\\n    correct = total_predictions = 0\\n    for observations, actual_tags in zip(X, Y):\\n        try:\\n            most_likely_tags = simplify_decoding(observations, model)\\n            correct += sum((p == t for p, t in zip(most_likely_tags, actual_tags)))\\n        except:\\n            pass\\n        total_predictions += len(observations)\\n    return correct / total_predictions',\n",
       "     'original_name': 'accuracy',\n",
       "     'new_name': None}],\n",
       "   '5654': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(sequence):\\n    \"\"\"Return a copy of the input sequence where each unknown word is replaced\\n    by the literal string value \\'nan\\'. Pomegranate will ignore these values\\n    during computation.\\n    \"\"\"\\n    return [w if w in data.training_set.vocab else \\'nan\\' for w in sequence]\\n\\ndef simplify_decoding(X, model):\\n    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\\n    _, state_path = model.viterbi(function_def(X))\\n    return [state[1].name for state in state_path[1:-1]]',\n",
       "     'original_name': 'replace_unknown',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': 'def replace_unknown(sequence):\\n    \"\"\"Return a copy of the input sequence where each unknown word is replaced\\n    by the literal string value \\'nan\\'. Pomegranate will ignore these values\\n    during computation.\\n    \"\"\"\\n    return [w if w in data.training_set.vocab else \\'nan\\' for w in sequence]\\n\\ndef function_def(X, model):\\n    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\\n    _, state_path = model.viterbi(replace_unknown(X))\\n    return [state[1].name for state in state_path[1:-1]]',\n",
       "     'original_name': 'simplify_decoding',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/annalisasheehan/masters/5bcaa18aebed6b8e2d44644082c29fd16aeedbb7/RasterstatsFromXArray.ipynb': {'cells': {'1542': [{'type': 'variable',\n",
       "     'cell_content': \"for f in tqdm(feats):\\n    rasterized_image = features.rasterize([(shape(f['geometry']), 1)], out_shape=out_shape, transform=new_aff, fill=0, all_touched=True)\\n    region = data.where(rasterized_image == 1)\\n    res = region.stack(allpoints=['x', 'y']).mean(dim='allpoints').to_dataframe(name=f['properties']['LSOA11CD'])\\n    dfs.append(res)\\nvariable_def = pd.concat(dfs, axis=1)\",\n",
       "     'original_name': 'stats',\n",
       "     'new_name': None}],\n",
       "   '1563': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = pd.read_csv('D:\\\\\\\\Annies_Dissertation\\\\\\\\Analysis\\\\\\\\Regression\\\\\\\\Validation\\\\\\\\Monthly_PM25_LSOA_Validation.csv', parse_dates=['time'])\",\n",
       "     'original_name': 'Pixel',\n",
       "     'new_name': None}],\n",
       "   '1536': [{'type': 'variable',\n",
       "     'cell_content': 'c, _, _, f = window_bounds(((x_start, 5000), (y_start, 5000)), orig_aff)\\na, b, _, d, variable_def, _, _, _, _ = tuple(orig_aff)\\nnew_aff = rasterio.Affine(a, b, c, d, variable_def, f)',\n",
       "     'original_name': 'e',\n",
       "     'new_name': None}],\n",
       "   '1584': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(window, affine):\\n    (row_start, row_stop), (col_start, col_stop) = window\\n    w, s = (col_start, row_stop) * affine\\n    e, n = (col_stop, row_start) * affine\\n    return (w, s, e, n)',\n",
       "     'original_name': 'window_bounds',\n",
       "     'new_name': None}],\n",
       "   '1535': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(window, affine):\\n    (row_start, row_stop), (col_start, col_stop) = window\\n    w, s = (col_start, row_stop) * affine\\n    e, n = (col_stop, row_start) * affine\\n    return (w, s, e, n)',\n",
       "     'original_name': 'window_bounds',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/loganyc1934/traffic-sign-classifier/4f8fd3ac1bbcb433ccd46f4bacf9119690d1406f/Traffic_Sign_Classifier.ipynb': {'cells': {'5407': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = get_frequencies(y_train_augmented, sign_dict)\\nfig, ax = plt.subplots(figsize=(15, 10))\\nclasses = list(sign_dict.values())\\nind = np.arange(len(classes))\\nwidth = 0.8\\nrects = ax.bar(ind, variable_def.values(), width, align='edge', alpha=0.5)\\nax.set_ylabel('Frequency')\\nax.set_title('Traffic Sign Classes')\\nax.set_xticks(ind + width / 2)\\nax.set_xticklabels(variable_def.keys(), rotation=90)\\nplt.show()\",\n",
       "     'original_name': 'sign_frequencies',\n",
       "     'new_name': None}],\n",
       "   '5422': [{'type': 'variable',\n",
       "     'cell_content': \"my_labels = [16, 33, 11, 38, 35, 17]\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    saver = tf.train.import_meta_graph('./lenet.meta')\\n    saver.restore(sess, './lenet')\\n    variable_def = [normalize(rgb2gray(img)) for img in my_images]\\n    my_accuracy = evaluate(variable_def, my_labels)\\n    print('Test Set Accuracy = {:.3f}'.format(my_accuracy))\",\n",
       "     'original_name': 'my_images_normalized',\n",
       "     'new_name': None}],\n",
       "   '5416': [{'type': 'function',\n",
       "     'cell_content': 'correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\\naccuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\\nsaver = tf.train.Saver()\\n\\ndef function_def(X_data, y_data):\\n    num_examples = len(X_data)\\n    total_accuracy = 0\\n    sess = tf.get_default_session()\\n    for offset in range(0, num_examples, BATCH_SIZE):\\n        batch_x, batch_y = (X_data[offset:offset + BATCH_SIZE], y_data[offset:offset + BATCH_SIZE])\\n        batch_x = np.expand_dims(batch_x, axis=3)\\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\\n        total_accuracy += accuracy * len(batch_x)\\n    return total_accuracy / num_examples',\n",
       "     'original_name': 'evaluate',\n",
       "     'new_name': None}],\n",
       "   '5408': [{'type': 'variable',\n",
       "     'cell_content': \"X_train, y_train = shuffle(X_train_augmented, y_train_augmented)\\n\\ndef get_random_img(X_data, y_label):\\n    index = random.randint(0, len(X_data))\\n    image = X_data[index].squeeze()\\n    return (image, y_label[index], index)\\n\\ndef plot_imgs(X_data, y_label):\\n    f, axarr = plt.subplots(3, 3, figsize=(16, 16))\\n    variable_def = []\\n    for i in range(9):\\n        image, label, index = get_random_img(X_data, y_label)\\n        variable_def.append(index)\\n        label_str = str(label)\\n        axarr[i // 3, i % 3].imshow(image, cmap='gray')\\n        axarr[i // 3, i % 3].set_title(label_str + ': ' + sign_dict[label_str])\\n        plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\\n        plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\\n    return variable_def\\nrand_img_indices = plot_imgs(X_train, y_train)\\nprint(rand_img_indices)\",\n",
       "     'original_name': 'rand_indices',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': \"X_train, y_train = shuffle(X_train_augmented, y_train_augmented)\\n\\ndef get_random_img(X_data, y_label):\\n    index = random.randint(0, len(X_data))\\n    image = X_data[index].squeeze()\\n    return (image, y_label[index], index)\\n\\ndef function_def(X_data, y_label):\\n    f, axarr = plt.subplots(3, 3, figsize=(16, 16))\\n    rand_indices = []\\n    for i in range(9):\\n        image, label, index = get_random_img(X_data, y_label)\\n        rand_indices.append(index)\\n        label_str = str(label)\\n        axarr[i // 3, i % 3].imshow(image, cmap='gray')\\n        axarr[i // 3, i % 3].set_title(label_str + ': ' + sign_dict[label_str])\\n        plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\\n        plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\\n    return rand_indices\\nrand_img_indices = function_def(X_train, y_train)\\nprint(rand_img_indices)\",\n",
       "     'original_name': 'plot_imgs',\n",
       "     'new_name': None}],\n",
       "   '5410': [{'type': 'function',\n",
       "     'cell_content': \"def rgb2gray(rgb):\\n    gray_img = cv.cvtColor(rgb, cv.COLOR_BGR2GRAY)\\n    return np.asarray(gray_img)\\n\\ndef function_def(img):\\n    return cv.normalize(img, img, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\\nimg_sums = [sum(sum(rgb2gray(img))) for img in X_train]\\nmin_ind = np.argmin(img_sums)\\nprint(min_ind)\\nf, axarr = plt.subplots(1, 2, figsize=(8, 4))\\naxarr[0].imshow(X_train[min_ind], cmap='gray')\\naxarr[0].set_title('Before normalization')\\nnormalized_gray = function_def(X_train[min_ind])\\naxarr[1].imshow(normalized_gray, cmap='gray')\\naxarr[1].set_title('After normalization')\\nX_train = [rgb2gray(img) for img in X_train]\\nX_train = [function_def(img) for img in X_train]\\nrand_img_indices = plot_imgs(X_train, y_train)\\nprint(rand_img_indices)\",\n",
       "     'original_name': 'normalize',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': \"def function_def(rgb):\\n    gray_img = cv.cvtColor(rgb, cv.COLOR_BGR2GRAY)\\n    return np.asarray(gray_img)\\n\\ndef normalize(img):\\n    return cv.normalize(img, img, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\\nimg_sums = [sum(sum(function_def(img))) for img in X_train]\\nmin_ind = np.argmin(img_sums)\\nprint(min_ind)\\nf, axarr = plt.subplots(1, 2, figsize=(8, 4))\\naxarr[0].imshow(X_train[min_ind], cmap='gray')\\naxarr[0].set_title('Before normalization')\\nnormalized_gray = normalize(X_train[min_ind])\\naxarr[1].imshow(normalized_gray, cmap='gray')\\naxarr[1].set_title('After normalization')\\nX_train = [function_def(img) for img in X_train]\\nX_train = [normalize(img) for img in X_train]\\nrand_img_indices = plot_imgs(X_train, y_train)\\nprint(rand_img_indices)\",\n",
       "     'original_name': 'rgb2gray',\n",
       "     'new_name': None}],\n",
       "   '5417': [{'type': 'function',\n",
       "     'cell_content': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\\n\\ndef function_def(nparray):\\n    return [x for x in nparray]\\n\\ndef cv_split(X, y):\\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\\n    X_np = np.asarray(X)\\n    y_np = np.asarray(y)\\n    for train_index, valid_index in sss.split(X_np, y_np):\\n        X_train, X_valid = (function_def(X_np[train_index]), function_def(X_np[valid_index]))\\n        y_train, y_valid = (function_def(y_np[train_index]), function_def(y_np[valid_index]))\\n    train = (X_train, y_train)\\n    valid = (X_valid, y_valid)\\n    return (train, valid)',\n",
       "     'original_name': 'nparray_to_list',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\\n\\ndef nparray_to_list(nparray):\\n    return [x for x in nparray]\\n\\ndef function_def(X, y):\\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\\n    X_np = np.asarray(X)\\n    y_np = np.asarray(y)\\n    for train_index, valid_index in sss.split(X_np, y_np):\\n        X_train, X_valid = (nparray_to_list(X_np[train_index]), nparray_to_list(X_np[valid_index]))\\n        y_train, y_valid = (nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index]))\\n    train = (X_train, y_train)\\n    valid = (X_valid, y_valid)\\n    return (train, valid)',\n",
       "     'original_name': 'cv_split',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\\n\\ndef nparray_to_list(nparray):\\n    return [x for x in nparray]\\n\\ndef cv_split(X, y):\\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\\n    X_np = np.asarray(X)\\n    y_np = np.asarray(y)\\n    for train_index, valid_index in sss.split(X_np, y_np):\\n        X_train, X_valid = (nparray_to_list(X_np[train_index]), nparray_to_list(X_np[valid_index]))\\n        variable_def, y_valid = (nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index]))\\n    train = (X_train, variable_def)\\n    valid = (X_valid, y_valid)\\n    return (train, valid)',\n",
       "     'original_name': 'y_train',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\\n\\ndef nparray_to_list(nparray):\\n    return [x for x in nparray]\\n\\ndef cv_split(X, y):\\n    variable_def = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\\n    X_np = np.asarray(X)\\n    y_np = np.asarray(y)\\n    for train_index, valid_index in variable_def.split(X_np, y_np):\\n        X_train, X_valid = (nparray_to_list(X_np[train_index]), nparray_to_list(X_np[valid_index]))\\n        y_train, y_valid = (nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index]))\\n    train = (X_train, y_train)\\n    valid = (X_valid, y_valid)\\n    return (train, valid)',\n",
       "     'original_name': 'sss',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/jutanke/pak/8515a6eaa599ec7165c250c1b2dc1767da8b0fdf/samples/MARCOnI.ipynb': {'cells': {'3878': [{'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Kickbox']\\ncamera = 0\\nvariable_def = 0\\nfig = plt.figure(figsize=(16, 8))\\nax = fig.add_subplot(111)\\nim = X[camera, variable_def]\\nax.imshow(im)\\nAnnotations_for_cam = Annotations[camera]\\nAnnot_on_frame_cam = Annotations_for_cam[variable_def]\\nCOLORS = ['red', 'yellow']\\nfor i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\\n    head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n    ax.plot(head_x, head_y, color=COLORS[i])\\n    for jx, jy, visible in joints:\\n        plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nplt.axis('off')\\nplt.show()\",\n",
       "     'original_name': 'frame',\n",
       "     'new_name': None}],\n",
       "   '3880': [{'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Soccer2']\\ncamera = 0\\nframe = 220\\nfig = plt.figure(figsize=(16, 8))\\nax = fig.add_subplot(111)\\nvariable_def = X[camera, frame]\\nax.imshow(variable_def)\\nAnnotations_for_cam = Annotations[camera]\\nAnnot_on_frame_cam = Annotations_for_cam[frame]\\nCOLORS = ['red', 'yellow']\\nfor i, annot in enumerate(Annot_on_frame_cam):\\n    if annot is not None:\\n        (tl, br), joints = annot\\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n        ax.plot(head_x, head_y, color=COLORS[i])\\n        for jx, jy, visible in joints:\\n            plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nplt.axis('off')\\nplt.show()\",\n",
       "     'original_name': 'im',\n",
       "     'new_name': None}],\n",
       "   '3885': [{'type': 'variable',\n",
       "     'cell_content': \"fig = plt.figure(figsize=(16, 5))\\nax = fig.add_subplot(121)\\nax.axis('off')\\nay = fig.add_subplot(122)\\nay.axis('off')\\nP1 = Ps[0]\\nP2 = Ps[1]\\nK1 = Ks[0]\\nK2 = Ks[1]\\nRt1 = Rts[0]\\nRt2 = Rts[1]\\nannot1 = Annotations[0][0]\\nannot2 = Annotations[1][0]\\nindv_left = annot1[0]\\nvariable_def = annot2[1]\\nlefthand_left = indv_left[1][0]\\nlefthand_right = variable_def[1][0]\\nax.imshow(X[0, 0])\\nax.scatter(lefthand_left[0], lefthand_left[1], color='red')\\nay.imshow(X[1, 0])\\nay.scatter(lefthand_right[0], lefthand_right[1], color='red')\\nplt.show()\",\n",
       "     'original_name': 'indv_right',\n",
       "     'new_name': None}],\n",
       "   '3884': [{'type': 'function',\n",
       "     'cell_content': \"X, Annotations = marconi['Soccer']\\nfig = plt.figure(figsize=(16, 4))\\n\\ndef function_def(ax, camera, frame):\\n    ax.set_title('Camera ' + str(camera) + ' at frame ' + str(frame))\\n    im = X[camera, frame]\\n    ax.imshow(im)\\n    Annotations_for_cam = Annotations[camera]\\n    Annot_on_frame_cam = Annotations_for_cam[frame]\\n    COLORS = ['red', 'yellow']\\n    for i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n        ax.plot(head_x, head_y, color=COLORS[i])\\n        for jx, jy, visible in joints:\\n            ax.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nfunction_def(fig.add_subplot(121), 0, 0)\\nfunction_def(fig.add_subplot(122), 1, 0)\\nplt.axis('off')\\nplt.show()\\nPs, Ks, Rts = marconi.get_calibration('Soccer', split_intrinsic_extrinsic=True)\",\n",
       "     'original_name': 'get_annot_img',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Soccer']\\nfig = plt.figure(figsize=(16, 4))\\n\\ndef get_annot_img(ax, camera, frame):\\n    ax.set_title('Camera ' + str(camera) + ' at frame ' + str(frame))\\n    variable_def = X[camera, frame]\\n    ax.imshow(variable_def)\\n    Annotations_for_cam = Annotations[camera]\\n    Annot_on_frame_cam = Annotations_for_cam[frame]\\n    COLORS = ['red', 'yellow']\\n    for i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n        ax.plot(head_x, head_y, color=COLORS[i])\\n        for jx, jy, visible in joints:\\n            ax.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nget_annot_img(fig.add_subplot(121), 0, 0)\\nget_annot_img(fig.add_subplot(122), 1, 0)\\nplt.axis('off')\\nplt.show()\\nPs, Ks, Rts = marconi.get_calibration('Soccer', split_intrinsic_extrinsic=True)\",\n",
       "     'original_name': 'im',\n",
       "     'new_name': None}],\n",
       "   '3877': [{'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Soccer']\\ncamera = 3\\nframe = 79\\nfig = plt.figure(figsize=(16, 8))\\nax = fig.add_subplot(111)\\nim = X[camera, frame]\\nax.imshow(im)\\nAnnotations_for_cam = Annotations[camera]\\nAnnot_on_frame_cam = Annotations_for_cam[frame]\\nCOLORS = ['red', 'yellow']\\nfor i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\\n    head_x, variable_def = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n    ax.plot(head_x, variable_def, color=COLORS[i])\\n    for jx, jy, visible in joints:\\n        plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nplt.axis('off')\\nplt.show()\",\n",
       "     'original_name': 'head_y',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Soccer']\\ncamera = 3\\nframe = 79\\nfig = plt.figure(figsize=(16, 8))\\nax = fig.add_subplot(111)\\nim = X[camera, frame]\\nax.imshow(im)\\nAnnotations_for_cam = Annotations[camera]\\nAnnot_on_frame_cam = Annotations_for_cam[frame]\\nvariable_def = ['red', 'yellow']\\nfor i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\\n    head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n    ax.plot(head_x, head_y, color=variable_def[i])\\n    for jx, jy, visible in joints:\\n        plt.scatter(jx, jy, color=variable_def[i], alpha=1 if visible == 1 else 0.4)\\nplt.axis('off')\\nplt.show()\",\n",
       "     'original_name': 'COLORS',\n",
       "     'new_name': None}],\n",
       "   '3882': [{'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Walk1']\\ncamera = 0\\nframe = 220\\nfig = plt.figure(figsize=(16, 8))\\nax = fig.add_subplot(111)\\nim = X[camera, frame]\\nax.imshow(im)\\nvariable_def = Annotations[camera]\\nAnnot_on_frame_cam = variable_def[frame]\\nCOLORS = ['red', 'yellow']\\nfor i, annot in enumerate(Annot_on_frame_cam):\\n    if annot is not None:\\n        (tl, br), joints = annot\\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n        ax.plot(head_x, head_y, color=COLORS[i])\\n        for jx, jy, visible in joints:\\n            plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nplt.axis('off')\\nplt.show()\",\n",
       "     'original_name': 'Annotations_for_cam',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"X, Annotations = marconi['Walk1']\\ncamera = 0\\nframe = 220\\nfig = plt.figure(figsize=(16, 8))\\nax = fig.add_subplot(111)\\nim = X[camera, frame]\\nax.imshow(im)\\nAnnotations_for_cam = Annotations[camera]\\nAnnot_on_frame_cam = Annotations_for_cam[frame]\\nCOLORS = ['red', 'yellow']\\nfor i, annot in enumerate(Annot_on_frame_cam):\\n    if annot is not None:\\n        (tl, br), joints = annot\\n        head_x, variable_def = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\\n        ax.plot(head_x, variable_def, color=COLORS[i])\\n        for jx, jy, visible in joints:\\n            plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\\nplt.axis('off')\\nplt.show()\",\n",
       "     'original_name': 'head_y',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/VictorZuanazzi/MachineLearningLab2/7e2d5938d6fe101eb4d3ed897a7409b1e644a8ed/12325724_12372773_lab2.ipynb': {'cells': {'5893': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(data, num_cols, targets=None, shape=(28, 28)):\\n    num_digits = data.shape[0]\\n    num_rows = int(num_digits / num_cols)\\n    for i in range(num_digits):\\n        plt.subplot(num_rows, num_cols, i + 1)\\n        plt.imshow(data[i].reshape(shape), interpolation='none', cmap='Greys')\\n        if targets is not None:\\n            plt.title(int(targets[i]))\\n        plt.colorbar()\\n        plt.axis('off')\\n    plt.tight_layout()\\n    plt.show()\\nfunction_def(x_train[0:40000:5000], num_cols=4, targets=t_train[0:40000:5000])\",\n",
       "     'original_name': 'plot_digits',\n",
       "     'new_name': None}],\n",
       "   '5894': [{'type': 'function',\n",
       "     'cell_content': 'from scipy.special import logsumexp\\nimport numpy as np\\n\\ndef function_def(a, num_classes):\\n    return np.eye(num_classes)[a.reshape(-1)]\\n\\ndef logprob(x, w, b):\\n    ln_q = np.matmul(x, w) + b\\n    ln_Z = logsumexp(ln_q)\\n    ln_p = ln_q - ln_Z\\n    return (ln_q, ln_Z, ln_p)\\n\\ndef logreg_gradient(x, t, w, b):\\n    num_classes = len(b)\\n    ln_q, ln_Z, ln_p = logprob(x, w, b)\\n    t_oh = function_def(t, num_classes)\\n    delta = t_oh - np.exp(ln_q) / np.exp(ln_Z)\\n    dL_db = delta\\n    dL_dw = np.matmul(x.T, delta)\\n    logp = ln_p\\n    return (logp[:, t].squeeze(), dL_dw, dL_db.squeeze())',\n",
       "     'original_name': 'one_hot',\n",
       "     'new_name': None}],\n",
       "   '5909': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(x_train, t_train, x_valid, t_valid):\\n    epochs = 10\\n    lrs = [0.01, 0.001, 0.0001]\\n    Vs = [[], [], []]\\n    logp_train = [[], [], []]\\n    logp_valid = [[], [], []]\\n    for i, lr in enumerate(lrs):\\n        W, b, V, a = init_params(28 * 28, 10, 20)\\n        logp_t = eval_mean_logp(x_train, t_train, W, b, V, a)\\n        logp_v = eval_mean_logp(x_valid, t_valid, W, b, V, a)\\n        logp_train[i].append(logp_t)\\n        logp_valid[i].append(logp_v)\\n        Vs[i].append(V)\\n        print(f'lr: {lr}')\\n        for epoch in range(1, epochs + 1):\\n            print(f'epoch: {epoch}', end='\\\\r')\\n            logp_t, W, b, V, a = mlp_sgd_iter(x_train, t_train, W, b, V, a, lr)\\n            logp_v = eval_mean_logp(x_valid, t_valid, W, b, V, a)\\n            Vs[i].append(V)\\n            logp_train[i].append(logp_t)\\n            logp_valid[i].append(logp_v)\\n    return (logp_train, logp_valid, Vs)\\nlogp_train, logp_valid, Vs = function_def(x_train, t_train, x_valid, t_valid)\",\n",
       "     'original_name': 'mlp_test',\n",
       "     'new_name': None}],\n",
       "   '5899': [{'type': 'variable',\n",
       "     'cell_content': 'def sgd_iter(x_train, t_train, W, b):\\n    variable_def = np.arange(len(x_train))\\n    np.random.shuffle(variable_def)\\n    lr = 0.0001\\n    logp = np.zeros(len(x_train))\\n    for i in variable_def:\\n        x = x_train[i:i + 1]\\n        t = t_train[i]\\n        logp[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\\n        W = W + lr * grad_w\\n        b = b + lr * grad_b\\n    logp_train = logp.mean()\\n    return (logp_train, W, b)',\n",
       "     'original_name': 'indices',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'def sgd_iter(x_train, t_train, W, b):\\n    indices = np.arange(len(x_train))\\n    np.random.shuffle(indices)\\n    lr = 0.0001\\n    variable_def = np.zeros(len(x_train))\\n    for i in indices:\\n        x = x_train[i:i + 1]\\n        t = t_train[i]\\n        variable_def[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\\n        W = W + lr * grad_w\\n        b = b + lr * grad_b\\n    logp_train = variable_def.mean()\\n    return (logp_train, W, b)',\n",
       "     'original_name': 'logp',\n",
       "     'new_name': None}],\n",
       "   '5912': [{'type': 'variable',\n",
       "     'cell_content': \"import numpy as np\\nfrom scipy.special import expit\\nn = 100\\nxs = np.linspace(-3, 3, n)\\nReLu = np.maximum(xs, 0)\\nd_ReLu = np.concatenate((np.zeros(int(n / 2)), np.ones(int(n / 2))))\\ntanh = np.tanh(xs)\\nvariable_def = 1 - tanh ** 2\\nsig = expit(xs)\\nd_sig = sig * (1 - sig)\\nplt.figure(figsize=(20, 5))\\nplt.subplot(1, 3, 1)\\nplt.plot(xs, ReLu, label='ReLu')\\nplt.plot(xs, d_ReLu, label='d_Relu')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title('ReLu(x) Plot')\\nplt.ylim(-1.1, 1.1)\\nplt.legend()\\nplt.subplot(1, 3, 2)\\nplt.plot(xs, tanh, label='tanh')\\nplt.plot(xs, variable_def, label='d_tanh')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title('tanh(x) Plot')\\nplt.ylim(-1.1, 1.1)\\nplt.legend()\\nplt.subplot(1, 3, 3)\\nplt.plot(xs, sig, label='sigmoid')\\nplt.plot(xs, d_sig, label='d_sigmoid')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title('Sigmoid(x) Plot')\\nplt.ylim(-1.1, 1.1)\\nplt.legend()\\nplt.suptitle('Activation functions')\\nplt.show()\",\n",
       "     'original_name': 'd_tanh',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"import numpy as np\\nfrom scipy.special import expit\\nn = 100\\nxs = np.linspace(-3, 3, n)\\nReLu = np.maximum(xs, 0)\\nd_ReLu = np.concatenate((np.zeros(int(n / 2)), np.ones(int(n / 2))))\\nvariable_def = np.tanh(xs)\\nd_tanh = 1 - variable_def ** 2\\nsig = expit(xs)\\nd_sig = sig * (1 - sig)\\nplt.figure(figsize=(20, 5))\\nplt.subplot(1, 3, 1)\\nplt.plot(xs, ReLu, label='ReLu')\\nplt.plot(xs, d_ReLu, label='d_Relu')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title('ReLu(x) Plot')\\nplt.ylim(-1.1, 1.1)\\nplt.legend()\\nplt.subplot(1, 3, 2)\\nplt.plot(xs, variable_def, label='tanh')\\nplt.plot(xs, d_tanh, label='d_tanh')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title('tanh(x) Plot')\\nplt.ylim(-1.1, 1.1)\\nplt.legend()\\nplt.subplot(1, 3, 3)\\nplt.plot(xs, sig, label='sigmoid')\\nplt.plot(xs, d_sig, label='d_sigmoid')\\nplt.xlabel('x')\\nplt.ylabel('y')\\nplt.title('Sigmoid(x) Plot')\\nplt.ylim(-1.1, 1.1)\\nplt.legend()\\nplt.suptitle('Activation functions')\\nplt.show()\",\n",
       "     'original_name': 'tanh',\n",
       "     'new_name': None}],\n",
       "   '5902': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(x_train, t_train, x_valid, t_valid, w, b):\\n    Lts = []\\n    Lvs = []\\n    epsilon = 0.01\\n    while (len(Lts) < 2 or abs(Lts[-1] - Lts[-2]) > epsilon) and len(Lts) < 25:\\n        L_train, w, b = sgd_iter(x_train, t_train, w, b)\\n        logp = []\\n        for x, t in zip(x_valid, t_valid):\\n            _, _, logp_valid = logprob(x, w, b)\\n            logp.append(logp_valid[t])\\n        L_valid = np.array(logp).mean()\\n        Lts.append(L_train)\\n        Lvs.append(L_valid)\\n    x = np.arange(len(Lts))\\n    plt.plot(x, Lts, label='L_train')\\n    plt.plot(x, Lvs, label='L_validation')\\n    plt.legend()\\n    plt.show()\\n    return (w, b)\\nnp.random.seed(1243)\\nw = np.zeros((28 * 28, 10))\\nb = np.zeros(10)\\nw, b = function_def(x_train, t_train, x_valid, t_valid, w, b)\",\n",
       "     'original_name': 'test_sgd',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def test_sgd(x_train, t_train, x_valid, t_valid, w, b):\\n    Lts = []\\n    variable_def = []\\n    epsilon = 0.01\\n    while (len(Lts) < 2 or abs(Lts[-1] - Lts[-2]) > epsilon) and len(Lts) < 25:\\n        L_train, w, b = sgd_iter(x_train, t_train, w, b)\\n        logp = []\\n        for x, t in zip(x_valid, t_valid):\\n            _, _, logp_valid = logprob(x, w, b)\\n            logp.append(logp_valid[t])\\n        L_valid = np.array(logp).mean()\\n        Lts.append(L_train)\\n        variable_def.append(L_valid)\\n    x = np.arange(len(Lts))\\n    plt.plot(x, Lts, label='L_train')\\n    plt.plot(x, variable_def, label='L_validation')\\n    plt.legend()\\n    plt.show()\\n    return (w, b)\\nnp.random.seed(1243)\\nw = np.zeros((28 * 28, 10))\\nb = np.zeros(10)\\nw, b = test_sgd(x_train, t_train, x_valid, t_valid, w, b)\",\n",
       "     'original_name': 'Lvs',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/laventura/AIND_dog_breed_classifier/cabcf3de693f3750aa59eab91a79064a5f292092/dog_app.ipynb': {'cells': {'4669': [{'type': 'variable',\n",
       "     'cell_content': \"K.clear_session()\\nvariable_def = Sequential()\\nvariable_def.add(GlobalAveragePooling2D(input_shape=train_incp_bn.shape[1:]))\\nvariable_def.add(Activation('relu'))\\nvariable_def.add(Dense(1024, activation='relu'))\\nvariable_def.add(Dropout(0.5))\\nvariable_def.add(Dense(512, activation='relu'))\\nvariable_def.add(Dropout(0.5))\\nvariable_def.add(Dense(NUM_CLASSES, activation='softmax'))\\nvariable_def.summary()\",\n",
       "     'original_name': 'inception_bneck',\n",
       "     'new_name': None}],\n",
       "   '4678': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(img_path):\\n    \"\"\" Using the given image (in img_path), returns either:\\n        - Dog breed (if it\\'s a dog)\\n        - Dog breed that resembles a human (if it\\'s a human face)\\n        \\n        Uses the transfer-learned CNN model from Step 5\\n    \"\"\"\\n    print(\\'.\\' * 60)\\n    print(\\'Given image:\\', img_path)\\n    human_found = face_detector(img_path)\\n    print(\\'Found human:\\', human_found)\\n    breed, chance = detect_dog_breed(img_path, inception_bneck, use_bottleneck=True, img_H=229, img_W=229)\\n    print()\\n    print(\\'Image is dog breed: {} ({:.2f}% prob)\\'.format(breed, chance))\\n    print(\\'🐶 Woof!\\') if not human_found else print(\\'Hellooo, 🐱👩🏻👦🏻👧🏻 animal 🤔\\')\\n    print(\\'=\\' * 60)',\n",
       "     'original_name': 'whose_a_good_doggy',\n",
       "     'new_name': None}],\n",
       "   '4641': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(img_path, cascade):\\n    \"\"\" Alternate implementation of Face Detector - using given Haar Cascade file\\n        :img_path - path to image file\\n        :cascade - cascade algo to use\\n        :returns - True if face found, else False\\n    \"\"\"\\n    img = cv2.imread(img_path)\\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    faces = cascade.detectMultiScale(img_gray)\\n    return len(faces) > 0',\n",
       "     'original_name': 'detect_face',\n",
       "     'new_name': None}],\n",
       "   '4679': [{'type': 'function',\n",
       "     'cell_content': \"import matplotlib.image as mpimg\\n\\ndef function_def(img_path):\\n    img = mpimg.imread(img_path)\\n    fig = plt.figure()\\n    plt.subplot()\\n    plt.imshow(img)\\n    plt.axis('off')\\n    plt.plot()\\n    plt.show()\",\n",
       "     'original_name': 'disp_image',\n",
       "     'new_name': None}],\n",
       "   '4647': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(img_path):\\n    prediction = ResNet50_predict_labels(img_path)\\n    return (prediction <= 268) & (prediction >= 151)',\n",
       "     'original_name': 'dog_detector',\n",
       "     'new_name': None}],\n",
       "   '4637': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(img_path):\\n    img = cv2.imread(img_path)\\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    faces = face_cascade.detectMultiScale(gray)\\n    return len(faces) > 0',\n",
       "     'original_name': 'face_detector',\n",
       "     'new_name': None}],\n",
       "   '4645': [{'type': 'function',\n",
       "     'cell_content': 'from keras.preprocessing import image\\nfrom tqdm import tqdm\\n\\ndef path_to_tensor(img_path, height=224, width=224):\\n    \"\"\" Loads RGB image as PIL.Image.Image type of given Height x Width dimensions\\n    \"\"\"\\n    img = image.load_img(img_path, target_size=(height, width))\\n    x = image.img_to_array(img)\\n    return np.expand_dims(x, axis=0)\\n\\ndef function_def(img_paths):\\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\\n    return np.vstack(list_of_tensors)',\n",
       "     'original_name': 'paths_to_tensor',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/188xuhe/handson-ml/8f07ff0d3082d89e8d81474674ee436fa3bcf6a2/13_convolutional_neural_networks.ipynb': {'cells': {'129': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = len(flower_classes)\\nwith tf.name_scope('new_output_layer'):\\n    flower_logits = tf.layers.dense(prelogits, variable_def, name='flower_logits')\\n    Y_proba = tf.nn.softmax(flower_logits, name='Y_proba')\",\n",
       "     'original_name': 'n_outputs',\n",
       "     'new_name': None}],\n",
       "   '146': [{'type': 'variable',\n",
       "     'cell_content': \"n_epochs = 10\\nbatch_size = 40\\nn_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\\nwith tf.Session() as sess:\\n    init.run()\\n    inception_saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\\n    for epoch in range(n_epochs):\\n        print('Epoch', epoch, end='')\\n        for iteration in range(n_iterations_per_epoch):\\n            print('.', end='')\\n            variable_def, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size)\\n            sess.run(training_op, feed_dict={X: variable_def, y: y_batch, training: True})\\n        acc_train = accuracy.eval(feed_dict={X: variable_def, y: y_batch})\\n        print('  Train accuracy:', acc_train)\\n        save_path = saver.save(sess, './my_flowers_model')\",\n",
       "     'original_name': 'X_batch',\n",
       "     'new_name': None}],\n",
       "   '91': [{'type': 'variable',\n",
       "     'cell_content': \"n_epochs = 10\\nbatch_size = 100\\nwith tf.Session() as sess:\\n    init.run()\\n    for epoch in range(n_epochs):\\n        for iteration in range(mnist.train.num_examples // batch_size):\\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\\n        print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)\\n        variable_def = saver.save(sess, './my_mnist_model')\",\n",
       "     'original_name': 'save_path',\n",
       "     'new_name': None}],\n",
       "   '87': [{'type': 'variable',\n",
       "     'cell_content': \"X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\\nmax_pool = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\\nwith tf.Session() as sess:\\n    variable_def = sess.run(max_pool, feed_dict={X: dataset})\\nplt.imshow(variable_def[0].astype(np.uint8))\\nplt.show()\",\n",
       "     'original_name': 'output',\n",
       "     'new_name': None}],\n",
       "   '110': [{'type': 'function',\n",
       "     'cell_content': \"import sys\\nimport tarfile\\nfrom six.moves import urllib\\nFLOWERS_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\\nFLOWERS_PATH = os.path.join('datasets', 'flowers')\\n\\ndef function_def(url=FLOWERS_URL, path=FLOWERS_PATH):\\n    if os.path.exists(FLOWERS_PATH):\\n        return\\n    os.makedirs(path, exist_ok=True)\\n    tgz_path = os.path.join(path, 'flower_photos.tgz')\\n    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\\n    flowers_tgz = tarfile.open(tgz_path)\\n    flowers_tgz.extractall(path=path)\\n    flowers_tgz.close()\\n    os.remove(tgz_path)\",\n",
       "     'original_name': 'fetch_flowers',\n",
       "     'new_name': None}],\n",
       "   '99': [{'type': 'variable',\n",
       "     'cell_content': \"import sys\\nimport tarfile\\nfrom six.moves import urllib\\nTF_MODELS_URL = 'http://download.tensorflow.org/models'\\nINCEPTION_V3_URL = TF_MODELS_URL + '/inception_v3_2016_08_28.tar.gz'\\nvariable_def = os.path.join('datasets', 'inception')\\nINCEPTION_V3_CHECKPOINT_PATH = os.path.join(variable_def, 'inception_v3.ckpt')\\n\\ndef download_progress(count, block_size, total_size):\\n    percent = count * block_size * 100 // total_size\\n    sys.stdout.write('\\\\rDownloading: {}%'.format(percent))\\n    sys.stdout.flush()\\n\\ndef fetch_pretrained_inception_v3(url=INCEPTION_V3_URL, path=variable_def):\\n    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\\n        return\\n    os.makedirs(path, exist_ok=True)\\n    tgz_path = os.path.join(path, 'inception_v3.tgz')\\n    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\\n    inception_tgz = tarfile.open(tgz_path)\\n    inception_tgz.extractall(path=path)\\n    inception_tgz.close()\\n    os.remove(tgz_path)\",\n",
       "     'original_name': 'INCEPTION_PATH',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"import sys\\nimport tarfile\\nfrom six.moves import urllib\\nTF_MODELS_URL = 'http://download.tensorflow.org/models'\\nINCEPTION_V3_URL = TF_MODELS_URL + '/inception_v3_2016_08_28.tar.gz'\\nINCEPTION_PATH = os.path.join('datasets', 'inception')\\nINCEPTION_V3_CHECKPOINT_PATH = os.path.join(INCEPTION_PATH, 'inception_v3.ckpt')\\n\\ndef download_progress(count, block_size, total_size):\\n    percent = count * block_size * 100 // total_size\\n    sys.stdout.write('\\\\rDownloading: {}%'.format(percent))\\n    sys.stdout.flush()\\n\\ndef fetch_pretrained_inception_v3(url=INCEPTION_V3_URL, path=INCEPTION_PATH):\\n    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\\n        return\\n    os.makedirs(path, exist_ok=True)\\n    variable_def = os.path.join(path, 'inception_v3.tgz')\\n    urllib.request.urlretrieve(url, variable_def, reporthook=download_progress)\\n    inception_tgz = tarfile.open(variable_def)\\n    inception_tgz.extractall(path=path)\\n    inception_tgz.close()\\n    os.remove(variable_def)\",\n",
       "     'original_name': 'tgz_path',\n",
       "     'new_name': None}],\n",
       "   '101': [{'type': 'function',\n",
       "     'cell_content': \"import re\\nCLASS_NAME_REGEX = re.compile('^n\\\\\\\\d+\\\\\\\\s+(.*)\\\\\\\\s*$', re.M | re.U)\\n\\ndef function_def():\\n    with open(os.path.join('datasets', 'inception', 'imagenet_class_names.txt'), 'rb') as f:\\n        content = f.read().decode('utf-8')\\n        return CLASS_NAME_REGEX.findall(content)\",\n",
       "     'original_name': 'load_class_names',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"import re\\nCLASS_NAME_REGEX = re.compile('^n\\\\\\\\d+\\\\\\\\s+(.*)\\\\\\\\s*$', re.M | re.U)\\n\\ndef load_class_names():\\n    with open(os.path.join('datasets', 'inception', 'imagenet_class_names.txt'), 'rb') as f:\\n        variable_def = f.read().decode('utf-8')\\n        return CLASS_NAME_REGEX.findall(variable_def)\",\n",
       "     'original_name': 'content',\n",
       "     'new_name': None}],\n",
       "   '94': [{'type': 'function',\n",
       "     'cell_content': \"def get_model_params():\\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\\n\\ndef function_def(model_params):\\n    gvar_names = list(model_params.keys())\\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\",\n",
       "     'original_name': 'restore_model_params',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': \"def function_def():\\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\\n\\ndef restore_model_params(model_params):\\n    gvar_names = list(model_params.keys())\\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\",\n",
       "     'original_name': 'get_model_params',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def get_model_params():\\n    variable_def = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\\n    return {gvar.op.name: value for gvar, value in zip(variable_def, tf.get_default_session().run(variable_def))}\\n\\ndef restore_model_params(model_params):\\n    gvar_names = list(model_params.keys())\\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\",\n",
       "     'original_name': 'gvars',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/maneesh-chouksey/cnn-dog-classifier/9725c0eff841c33980cf55bfdf535138343b9b9c/dog_app.ipynb': {'cells': {'4371': [{'type': 'variable',\n",
       "     'cell_content': \"bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\\ntrain_VGG16 = bottleneck_features['train']\\nvalid_VGG16 = bottleneck_features['valid']\\nvariable_def = bottleneck_features['test']\",\n",
       "     'original_name': 'test_VGG16',\n",
       "     'new_name': None}],\n",
       "   '4355': [{'type': 'variable',\n",
       "     'cell_content': \"import random\\nrandom.seed(8675309)\\nvariable_def = np.array(glob('lfw/*/*'))\\nrandom.shuffle(variable_def)\\nprint('There are %d total human images.' % len(variable_def))\",\n",
       "     'original_name': 'human_files',\n",
       "     'new_name': None}],\n",
       "   '4365': [{'type': 'variable',\n",
       "     'cell_content': \"from PIL import ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES = True\\ntrain_tensors = paths_to_tensor(train_files).astype('float32') / 255\\nvariable_def = paths_to_tensor(valid_files).astype('float32') / 255\\ntest_tensors = paths_to_tensor(test_files).astype('float32') / 255\",\n",
       "     'original_name': 'valid_tensors',\n",
       "     'new_name': None}],\n",
       "   '4368': [{'type': 'variable',\n",
       "     'cell_content': \"from keras.callbacks import ModelCheckpoint\\nepochs = ...\\nvariable_def = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)\\nmodel.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), epochs=epochs, batch_size=20, callbacks=[variable_def], verbose=1)\",\n",
       "     'original_name': 'checkpointer',\n",
       "     'new_name': None}],\n",
       "   '4361': [{'type': 'function',\n",
       "     'cell_content': 'from keras.preprocessing import image\\nfrom tqdm import tqdm\\n\\ndef path_to_tensor(img_path):\\n    img = image.load_img(img_path, target_size=(224, 224))\\n    x = image.img_to_array(img)\\n    return np.expand_dims(x, axis=0)\\n\\ndef function_def(img_paths):\\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\\n    return np.vstack(list_of_tensors)',\n",
       "     'original_name': 'paths_to_tensor',\n",
       "     'new_name': None}],\n",
       "   '4377': [{'type': 'function',\n",
       "     'cell_content': 'from extract_bottleneck_features import *\\n\\ndef function_def(img_path):\\n    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\\n    predicted_vector = VGG16_model.predict(bottleneck_feature)\\n    return dog_names[np.argmax(predicted_vector)]',\n",
       "     'original_name': 'VGG16_predict_breed',\n",
       "     'new_name': None}],\n",
       "   '4362': [{'type': 'function',\n",
       "     'cell_content': 'from keras.applications.resnet50 import preprocess_input, decode_predictions\\n\\ndef function_def(img_path):\\n    img = preprocess_input(path_to_tensor(img_path))\\n    return np.argmax(ResNet50_model.predict(img))',\n",
       "     'original_name': 'ResNet50_predict_labels',\n",
       "     'new_name': None}],\n",
       "   '4354': [{'type': 'function',\n",
       "     'cell_content': \"from sklearn.datasets import load_files\\nfrom keras.utils import np_utils\\nimport numpy as np\\nfrom glob import glob\\n\\ndef function_def(path):\\n    data = load_files(path)\\n    dog_files = np.array(data['filenames'])\\n    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\\n    return (dog_files, dog_targets)\\ntrain_files, train_targets = function_def('dogImages/train')\\nvalid_files, valid_targets = function_def('dogImages/valid')\\ntest_files, test_targets = function_def('dogImages/test')\\ndog_names = [item[20:-1] for item in sorted(glob('dogImages/train/*/'))]\\nprint('There are %d total dog categories.' % len(dog_names))\\nprint('There are %s total dog images.\\\\n' % len(np.hstack([train_files, valid_files, test_files])))\\nprint('There are %d training dog images.' % len(train_files))\\nprint('There are %d validation dog images.' % len(valid_files))\\nprint('There are %d test dog images.' % len(test_files))\",\n",
       "     'original_name': 'load_dataset',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"from sklearn.datasets import load_files\\nfrom keras.utils import np_utils\\nimport numpy as np\\nfrom glob import glob\\n\\ndef load_dataset(path):\\n    data = load_files(path)\\n    dog_files = np.array(data['filenames'])\\n    variable_def = np_utils.to_categorical(np.array(data['target']), 133)\\n    return (dog_files, variable_def)\\ntrain_files, train_targets = load_dataset('dogImages/train')\\nvalid_files, valid_targets = load_dataset('dogImages/valid')\\ntest_files, test_targets = load_dataset('dogImages/test')\\ndog_names = [item[20:-1] for item in sorted(glob('dogImages/train/*/'))]\\nprint('There are %d total dog categories.' % len(dog_names))\\nprint('There are %s total dog images.\\\\n' % len(np.hstack([train_files, valid_files, test_files])))\\nprint('There are %d training dog images.' % len(train_files))\\nprint('There are %d validation dog images.' % len(valid_files))\\nprint('There are %d test dog images.' % len(test_files))\",\n",
       "     'original_name': 'dog_targets',\n",
       "     'new_name': None}],\n",
       "   '4357': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(img_path):\\n    img = cv2.imread(img_path)\\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    faces = face_cascade.detectMultiScale(gray)\\n    return len(faces) > 0',\n",
       "     'original_name': 'face_detector',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': 'def face_detector(img_path):\\n    variable_def = cv2.imread(img_path)\\n    gray = cv2.cvtColor(variable_def, cv2.COLOR_BGR2GRAY)\\n    faces = face_cascade.detectMultiScale(gray)\\n    return len(faces) > 0',\n",
       "     'original_name': 'img',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/davide-belli/machine-learning-2-labs-hws/3453076bd80220dc42a87874a2c45c5f83a32af4/lab2/lab2.ipynb': {'cells': {'4983': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = Variable(name='X', num_states=2)\\nX_prior = Factor(name='p(X)', f=np.array([0.95, 0.05]), neighbours=[variable_def])\\nZ = Variable(name='Z', num_states=2)\\nZ_prior = Factor(name='p(Z)', f=np.array([0.8, 0.2]), neighbours=[Z])\\nY = Variable(name='Y', num_states=2)\\nf_Y_cond = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\\nY_cond = Factor(name='p(Y |X, Z)', f=np.array(f_Y_cond), neighbours=[Y, variable_def, Z])\",\n",
       "     'original_name': 'X',\n",
       "     'new_name': None}],\n",
       "   '4989': [{'type': 'variable',\n",
       "     'cell_content': 'def calc_other_neighbour_msg_prod(sender, receiver):\\n    variable_def = get_neighbour_messages(sender, receiver)\\n    return np.multiply.reduce(np.ix_(*variable_def))',\n",
       "     'original_name': 'vectors',\n",
       "     'new_name': None}],\n",
       "   '5002': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(node_list):\\n    for n in node_list:\\n        while len(n.pending) > 0:\\n            f = next(iter(n.pending))\\n            n.send_sp_msg(f)\\n    for n in reversed(node_list):\\n        while len(n.pending) > 0:\\n            f = next(iter(n.pending))\\n            n.send_sp_msg(f)',\n",
       "     'original_name': 'sum_product',\n",
       "     'new_name': None}],\n",
       "   '4995': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(self, factor):\\n    assert isinstance(factor, Factor), 'Variable can only send messages to factor!'\\n    assert can_send_message(self, factor), 'Cannot send message!'\\n    out_msg = calc_sum_product_variable_to_factor_msg(self, factor)\\n    factor.receive_msg(self, out_msg)\\n    self.pending.discard(factor)\\nVariable.send_sp_msg = function_def\",\n",
       "     'original_name': 'variable_send_sp_msg',\n",
       "     'new_name': None}],\n",
       "   '4988': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(sender, receiver):\\n    messages = []\\n    for n in sender.neighbours:\\n        if n is not receiver:\\n            messages.append(sender.in_msgs[n])\\n    return messages',\n",
       "     'original_name': 'get_neighbour_messages',\n",
       "     'new_name': None}],\n",
       "   '4992': [{'type': 'function',\n",
       "     'cell_content': \"def function_def(self, variable):\\n    assert isinstance(variable, Variable), 'Factor can only send messages to variable!'\\n    assert can_send_message(self, variable), 'Cannot send message!'\\n    out_msg = calc_sum_product_factor_to_variable_msg(self, variable)\\n    variable.receive_msg(self, out_msg)\\n    self.pending.discard(variable)\\nFactor.send_sp_msg = function_def\",\n",
       "     'original_name': 'factor_send_sp_msg',\n",
       "     'new_name': None}],\n",
       "   '5020': [{'type': 'variable',\n",
       "     'cell_content': \"nodes = [ST, F, C, W, f_I, f_ST, f_F, f_C, f_W, I, B, f_B, S, f_S]\\nfor n in nodes:\\n    n.reset()\\nC.pending.add(f_C)\\nW.pending.add(f_W)\\nf_I.pending.add(I)\\nf_S.pending.add(S)\\nST.pending.add(f_ST)\\nF.pending.add(f_F)\\nmax_sum(nodes)\\nI_ulm = I.unnormalized_log_marginal()\\nS_ulm = S.unnormalized_log_marginal()\\nvariable_def = ST.unnormalized_log_marginal()\\nF_ulm = F.unnormalized_log_marginal()\\nB_ulm = B.unnormalized_log_marginal()\\nC_ulm = C.unnormalized_log_marginal()\\nW_ulm = W.unnormalized_log_marginal()\\nprint('I', I_ulm)\\nprint('S', S_ulm)\\nprint('ST', variable_def)\\nprint('F', F_ulm)\\nprint('B', B_ulm)\\nprint('C', C_ulm)\\nprint('W', W_ulm)\",\n",
       "     'original_name': 'ST_ulm',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"nodes = [ST, F, C, W, f_I, f_ST, f_F, f_C, f_W, I, B, f_B, S, f_S]\\nfor n in nodes:\\n    n.reset()\\nC.pending.add(f_C)\\nW.pending.add(f_W)\\nf_I.pending.add(I)\\nf_S.pending.add(S)\\nST.pending.add(f_ST)\\nF.pending.add(f_F)\\nmax_sum(nodes)\\nI_ulm = I.unnormalized_log_marginal()\\nS_ulm = S.unnormalized_log_marginal()\\nST_ulm = ST.unnormalized_log_marginal()\\nF_ulm = F.unnormalized_log_marginal()\\nvariable_def = B.unnormalized_log_marginal()\\nC_ulm = C.unnormalized_log_marginal()\\nW_ulm = W.unnormalized_log_marginal()\\nprint('I', I_ulm)\\nprint('S', S_ulm)\\nprint('ST', ST_ulm)\\nprint('F', F_ulm)\\nprint('B', variable_def)\\nprint('C', C_ulm)\\nprint('W', W_ulm)\",\n",
       "     'original_name': 'B_ulm',\n",
       "     'new_name': None}],\n",
       "   '5017': [{'type': 'variable',\n",
       "     'cell_content': \"I = Variable(name='I', num_states=2)\\nS = Variable(name='S', num_states=2)\\nST = Variable(name='ST', num_states=2)\\nF = Variable(name='F', num_states=2)\\nB = Variable(name='B', num_states=2)\\nC = Variable(name='C', num_states=2)\\nW = Variable(name='W', num_states=2)\\nf_I = Factor(name='p(I)', f=np.array([0.95, 0.05]), neighbours=[I])\\nf_S = Factor(name='p(S)', f=np.array([0.8, 0.2]), neighbours=[S])\\nprob_ST = [[0.999, 0.7], [0.001, 0.3]]\\nf_ST = Factor(name='p(ST |I)', f=np.array(prob_ST), neighbours=[ST, I])\\nprob_F = [[0.95, 0.1], [0.05, 0.9]]\\nf_F = Factor(name='p(F |I)', f=np.array(prob_F), neighbours=[F, I])\\nprob_B = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\\nf_B = Factor(name='p(B |I, S)', f=np.array(prob_B), neighbours=[B, I, S])\\nprob_C = [[0.93, 0.2], [0.07, 0.8]]\\nf_C = Factor(name='p(C |B)', f=np.array(prob_C), neighbours=[C, B])\\nprob_W = [[0.999, 0.4], [0.001, 0.6]]\\nvariable_def = Factor(name='p(W |B)', f=np.array(prob_W), neighbours=[W, B])\",\n",
       "     'original_name': 'f_W',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"I = Variable(name='I', num_states=2)\\nS = Variable(name='S', num_states=2)\\nST = Variable(name='ST', num_states=2)\\nF = Variable(name='F', num_states=2)\\nB = Variable(name='B', num_states=2)\\nC = Variable(name='C', num_states=2)\\nW = Variable(name='W', num_states=2)\\nf_I = Factor(name='p(I)', f=np.array([0.95, 0.05]), neighbours=[I])\\nf_S = Factor(name='p(S)', f=np.array([0.8, 0.2]), neighbours=[S])\\nprob_ST = [[0.999, 0.7], [0.001, 0.3]]\\nf_ST = Factor(name='p(ST |I)', f=np.array(prob_ST), neighbours=[ST, I])\\nvariable_def = [[0.95, 0.1], [0.05, 0.9]]\\nf_F = Factor(name='p(F |I)', f=np.array(variable_def), neighbours=[F, I])\\nprob_B = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\\nf_B = Factor(name='p(B |I, S)', f=np.array(prob_B), neighbours=[B, I, S])\\nprob_C = [[0.93, 0.2], [0.07, 0.8]]\\nf_C = Factor(name='p(C |B)', f=np.array(prob_C), neighbours=[C, B])\\nprob_W = [[0.999, 0.4], [0.001, 0.6]]\\nf_W = Factor(name='p(W |B)', f=np.array(prob_W), neighbours=[W, B])\",\n",
       "     'original_name': 'prob_F',\n",
       "     'new_name': None}],\n",
       "   '5010': [{'type': 'function',\n",
       "     'cell_content': \"def calc_max_sum_variable_to_factor_msg(variable, factor):\\n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\\n    if len(neighbour_msg_prod) > 0:\\n        message = np.sum(np.array(neighbour_msg_prod), axis=0)\\n    else:\\n        message = np.zeros(variable.num_states)\\n    message += np.log(variable.observed_state)\\n    return message\\n\\ndef function_def(self, factor):\\n    assert isinstance(factor, Factor), 'Variable can only send messages to factor!'\\n    assert can_send_message(self, factor), 'Cannot send message!'\\n    out_msg = calc_max_sum_variable_to_factor_msg(self, factor)\\n    factor.receive_msg(self, out_msg)\\n    self.pending.discard(factor)\\nVariable.send_ms_msg = function_def\",\n",
       "     'original_name': 'variable_send_ms_msg',\n",
       "     'new_name': None},\n",
       "    {'type': 'variable',\n",
       "     'cell_content': \"def calc_max_sum_variable_to_factor_msg(variable, factor):\\n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\\n    if len(neighbour_msg_prod) > 0:\\n        variable_def = np.sum(np.array(neighbour_msg_prod), axis=0)\\n    else:\\n        variable_def = np.zeros(variable.num_states)\\n    variable_def += np.log(variable.observed_state)\\n    return variable_def\\n\\ndef variable_send_ms_msg(self, factor):\\n    assert isinstance(factor, Factor), 'Variable can only send messages to factor!'\\n    assert can_send_message(self, factor), 'Cannot send message!'\\n    out_msg = calc_max_sum_variable_to_factor_msg(self, factor)\\n    factor.receive_msg(self, out_msg)\\n    self.pending.discard(factor)\\nVariable.send_ms_msg = variable_send_ms_msg\",\n",
       "     'original_name': 'message',\n",
       "     'new_name': None}]}},\n",
       " 'https://raw.githubusercontent.com/s8rhdobi/fuzzingbook/3de737d330f30132565ac26fe85b37db299f8c49/notebooks/MutationFuzzer.ipynb': {'cells': {'3313': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = ['Hello World']\\ncgi_runner = FunctionCoverageRunner(cgi_decode)\\nm = MutationCoverageFuzzer(variable_def)\\nresults = m.runs(cgi_runner, 10000)\",\n",
       "     'original_name': 'seed',\n",
       "     'new_name': None}],\n",
       "   '3310': [{'type': 'variable',\n",
       "     'cell_content': \"variable_def = 'http://www.google.com/search?q=fuzzing'\\nmutation_fuzzer = MutationCoverageFuzzer(seed=[variable_def])\\nmutation_fuzzer.runs(http_runner, trials=10000)\\nmutation_fuzzer.population\",\n",
       "     'original_name': 'seed_input',\n",
       "     'new_name': None}],\n",
       "   '3275': [{'type': 'variable',\n",
       "     'cell_content': 'hours_until_success = seconds_until_success / 3600\\nvariable_def = hours_until_success / 24\\nyears_until_success = variable_def / 365.25\\nyears_until_success',\n",
       "     'original_name': 'days_until_success',\n",
       "     'new_name': None}],\n",
       "   '3281': [{'type': 'variable',\n",
       "     'cell_content': 'def flip_random_character(s):\\n    \"\"\"Returns s with a random bit flipped in a random position\"\"\"\\n    if s == \\'\\':\\n        return s\\n    pos = random.randint(0, len(s) - 1)\\n    c = s[pos]\\n    bit = 1 << random.randint(0, 6)\\n    variable_def = chr(ord(c) ^ bit)\\n    return s[:pos] + variable_def + s[pos + 1:]',\n",
       "     'original_name': 'new_c',\n",
       "     'new_name': None}],\n",
       "   '3297': [{'type': 'function',\n",
       "     'cell_content': 'class MutationFuzzer(MutationFuzzer):\\n\\n    def function_def(self):\\n        candidate = random.choice(self.population)\\n        trials = random.randint(self.min_mutations, self.max_mutations)\\n        for i in range(trials):\\n            candidate = self.mutate(candidate)\\n        return candidate',\n",
       "     'original_name': 'create_candidate',\n",
       "     'new_name': None}],\n",
       "   '3277': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(s):\\n    \"\"\"Returns s with a random character deleted\"\"\"\\n    if s == \\'\\':\\n        return s\\n    pos = random.randint(0, len(s) - 1)\\n    return s[:pos] + s[pos + 1:]',\n",
       "     'original_name': 'delete_random_character',\n",
       "     'new_name': None}],\n",
       "   '3285': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(url):\\n    try:\\n        result = http_program(url)\\n        return True\\n    except ValueError:\\n        return False',\n",
       "     'original_name': 'is_valid_url',\n",
       "     'new_name': None}],\n",
       "   '3306': [{'type': 'function',\n",
       "     'cell_content': 'class FunctionCoverageRunner(FunctionRunner):\\n\\n    def run_function(self, inp):\\n        with Coverage() as cov:\\n            try:\\n                result = super().run_function(inp)\\n            except Exception as exc:\\n                self._coverage = cov.coverage()\\n                raise exc\\n        self._coverage = cov.coverage()\\n        return result\\n\\n    def function_def(self):\\n        return self._coverage',\n",
       "     'original_name': 'coverage',\n",
       "     'new_name': None}],\n",
       "   '3283': [{'type': 'function',\n",
       "     'cell_content': 'def function_def(s):\\n    \"\"\"Return s with a random mutation applied\"\"\"\\n    mutators = [delete_random_character, insert_random_character, flip_random_character]\\n    mutator = random.choice(mutators)\\n    return mutator(s)',\n",
       "     'original_name': 'mutate',\n",
       "     'new_name': None}],\n",
       "   '3309': [{'type': 'function',\n",
       "     'cell_content': 'class MutationCoverageFuzzer(MutationFuzzer):\\n\\n    def function_def(self):\\n        super().reset()\\n        self.coverages_seen = set()\\n        self.population = []\\n\\n    def run(self, runner):\\n        \"\"\"Run function(inp) while tracking coverage.\\n           If we reach new coverage,\\n           add inp to population and its coverage to population_coverage\\n        \"\"\"\\n        result, outcome = super().run(runner)\\n        new_coverage = frozenset(runner.coverage())\\n        if outcome == Runner.PASS and new_coverage not in self.coverages_seen:\\n            self.population.append(self.inp)\\n            self.coverages_seen.add(new_coverage)\\n        return result',\n",
       "     'original_name': 'reset',\n",
       "     'new_name': None},\n",
       "    {'type': 'function',\n",
       "     'cell_content': 'class MutationCoverageFuzzer(MutationFuzzer):\\n\\n    def reset(self):\\n        super().reset()\\n        self.coverages_seen = set()\\n        self.population = []\\n\\n    def function_def(self, runner):\\n        \"\"\"Run function(inp) while tracking coverage.\\n           If we reach new coverage,\\n           add inp to population and its coverage to population_coverage\\n        \"\"\"\\n        result, outcome = super().run(runner)\\n        new_coverage = frozenset(runner.coverage())\\n        if outcome == Runner.PASS and new_coverage not in self.coverages_seen:\\n            self.population.append(self.inp)\\n            self.coverages_seen.add(new_coverage)\\n        return result',\n",
       "     'original_name': 'run',\n",
       "     'new_name': None}]}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 87\n",
      "Number of variables: 107\n"
     ]
    }
   ],
   "source": [
    "# count number of functions and variables\n",
    "function_count = 0\n",
    "variable_count = 0\n",
    "\n",
    "for notebook in notebooks:\n",
    "    for cell in notebooks[notebook]['cells']:\n",
    "        for cell_entry in notebooks[notebook]['cells'][cell]:\n",
    "            if cell_entry['type'] == 'function':\n",
    "                function_count += 1\n",
    "            else:\n",
    "                variable_count += 1\n",
    "\n",
    "print(f'Number of functions: {function_count}')\n",
    "print(f'Number of variables: {variable_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_small_functions = 0\n",
    "count_large_functions = 0\n",
    "count_small_variables = 0\n",
    "count_large_variables = 0\n",
    "CHANGE_THRESHOLD = 7\n",
    "MIDDLE_THRESHOLD = (MIN_LENGTH + MAX_LENGTH) // 2\n",
    "\n",
    "samples = []\n",
    "\n",
    "for i, notebook in enumerate(notebooks):\n",
    "    function_entry = None\n",
    "    variable_entry = None\n",
    "\n",
    "    for cell in notebooks[notebook]['cells']:\n",
    "        for cell_entry in notebooks[notebook]['cells'][cell]:\n",
    "            if cell_entry['type'] == 'function' and function_entry is None:\n",
    "                if count_small_functions < CHANGE_THRESHOLD and len(cell_entry['cell_content']) <= MIDDLE_THRESHOLD:\n",
    "                    function_entry = (cell, 'small', cell_entry)\n",
    "                    break\n",
    "                elif count_large_functions < CHANGE_THRESHOLD and len(cell_entry['cell_content']) > MIDDLE_THRESHOLD:\n",
    "                    function_entry = (cell, 'large', cell_entry)\n",
    "                    break\n",
    "            elif cell_entry['type'] == 'variable' and variable_entry is None:\n",
    "                if count_small_variables < CHANGE_THRESHOLD and len(cell_entry['cell_content']) <= MIDDLE_THRESHOLD:\n",
    "                    variable_entry = (cell, 'small', cell_entry)\n",
    "                    break\n",
    "                elif count_large_variables < CHANGE_THRESHOLD and len(cell_entry['cell_content']) > MIDDLE_THRESHOLD:\n",
    "                    variable_entry = (cell, 'large', cell_entry)\n",
    "                    break\n",
    "    if function_entry is not None and variable_entry is not None and function_entry[0] != variable_entry[0]:\n",
    "        if function_entry[1] == 'small':\n",
    "            count_small_functions += 1\n",
    "        else:\n",
    "            count_large_functions += 1\n",
    "        \n",
    "        if variable_entry[1] == 'small':\n",
    "            count_small_variables += 1\n",
    "        else:\n",
    "            count_large_variables += 1\n",
    "\n",
    "        samples.append({\n",
    "            'url': notebook,\n",
    "            'function_cell': function_entry,\n",
    "            'variable_cell': variable_entry,\n",
    "        })\n",
    "        print(f'Notebook {i}: {function_entry} {variable_entry}')\n",
    "\n",
    "print(f'Number of samples: {len(samples)}')\n",
    "print(f'\\tNumber of small functions: {count_small_functions}')\n",
    "print(f'\\tNumber of large functions: {count_large_functions}')\n",
    "print(f'\\tNumber of small variables: {count_small_variables}')\n",
    "print(f'\\tNumber of large variables: {count_large_variables}')\n",
    "        \n",
    "for sample in samples:\n",
    "    print(sample['url'])\n",
    "    print(readmes_by_url[sample['url']])\n",
    "    print(len(sample['function_cell'][2]['cell_content']))\n",
    "    print(len(sample['variable_cell'][2]['cell_content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
