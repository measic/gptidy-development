[{'identified': "- The function arguments in the 'show_plot' function call are not separated by commas", 'updated_code': 'show_plot([make_scatter(data_x, data_y, text)], make_layout(500, 500, shapes=[shape]))'}, {'identified': "- The comment should start with a single '#' instead of '##'", 'updated_code': '# For each row calculate Lambda Prime\nfor i in range(len(data)):\n    N = 0.0\n    for x in range(len(data.iloc[i]))[4:last]:\n        if data.iloc[i][x] > 0:\n            N += data.iloc[i][x]\n    \n    array = data.iloc[i][4:last]\n    num = 0.0\n    for y in array:\n        num += (y * (y - 1))\n    lam = num / (N * (N - 1))\n    data.loc[i, "lam\'"] = lam'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- Comment should have a space after the '#' symbol", 'updated_code': '# Apply the function to transform data:\ncr = transform_data(df, a=conf_alpha, a_0=alpha_0, b_0=beta_0)\nprint(cr.shape)\nprint(cr.head())'}, {'identified': None, 'updated_code': None}, {'identified': '- Multiple imports should be on separate lines', 'updated_code': 'import zipfile\nfrom urllib import request\nimport os\nimport sys\n\n\npath_set14 = (r"https://github.com/titu1994/Super-Resolution-using-"\n              r"Generative-Adversarial-Networks/releases/download/v0.1/Set14.zip")\nfilename = "Set14.zip"\n\n\ndef _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\rDownloading %s %.2f%%\' % (filename,\n                                                  float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n\n\nif not os.path.exists("tests/set14/set14"):\n    print("Downloading Set14 images")\n    filehandler = request.urlretrieve(path_set14, reporthook=_progress)\n    zf = zipfile.ZipFile(filehandler)\n    print()\n    print("Extracting images")\n    uncompress_size = sum((file.file_size for file in zf.infolist()))\n    extracted_size = 0\n    for file_item in zf.infolist():\n        extracted_size += file_item.file_size\n        sys.stdout.write(\'\\rExtracting %.2f%%\' % (float(extracted_size * 100 / uncompress_size)))\n        sys.stdout.flush()\n        zf.extract(file_item, "tests/set14")\n\nprint()\nprint("Set14 is all set!!")'}, {'identified': None, 'updated_code': None}, {'identified': "- The comment block uses '#' for the comment lines but it should use '# ' (with a space after '#') for consistency", 'updated_code': 'nb_images = len(tf.io.gfile.glob(GCS_PATTERN))\nprint("Pattern matches {} images.".format(nb_images))\n\n# YOUR CODE GOES HERE\n#\n# display_9_images_from_dataset(dataset)'}, {'identified': 'None', 'updated_code': "# Find x and y coordinates from Easting and Northing values for the LSOA\na = PM25.attrs['affine']\na = rasterio.Affine.from_gdal(*a)\n~a * (439040, 115775)"}, {'identified': None, 'updated_code': None}, {'identified': '- The comments should be on separate lines before the corresponding code', 'updated_code': '## Define color list\ncolors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\n\n## Define index for iterating through color list\nindex = 0\n\n## For each river segment\nfor segment in segments:\n    ## Subset df to non-zero values for the current river segment\n    segDF = data.loc[data[\'RiverSeg\'] == segment]\n    segDF = segDF[segDF["lam\'"] > 0]\n\n    ## Sort based on year\n    segDF = segDF.sort_values(\'Year\')\n    ## Define x,y for plotting\n    x = segDF["Year"]\n    y = segDF["N21"]\n    ## Change name of y to Riv Seg for legend\n    y.name = segment\n\n    ## Build graph...\n    ## Plot segment x vs y\n    plt.plot(x, y, colors[index])\n    ## Locate legend\n    plt.legend(loc=(1.05, 0.2))\n    ## Advance color index\n    index += 1\n\n## Update title\nplt.title("Hill N21, " + river + " River Segments")\n## Label x axis\nplt.xlabel(\'Year\')\n## Label y axis\nplt.ylabel("N21")\n## Force x axis to integer values, increment by 1 year\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\n## Rotate year labels 90 degrees\nplt.xticks(rotation=90)\n\n## Save figure\nplt.savefig(output + "\\\\" + river + "_Hill_N21.png", bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\n## Display figure\nplt.show()'}, {'identified': '- The import statement should be at the top of the file, before any other code', 'updated_code': 'import matplotlib.patches as patches\nfrom matplotlib import pyplot as plt\n\nfig = plt.figure(figsize=(15, 10))\n\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    plt.scatter(X[i][:, 0], X[i][:, 1], c=y_kmeans_proj[i], s=100, cmap=\'viridis\', marker=\'.\')\n    centers = centers_kmeans_proj[i]\n    plt.scatter(centers, np.ones(centers.shape)*25, c=\'black\', s=200, alpha=0.5, marker=\'o\')\n    plt.xticks([])\n    plt.yticks([])\n    current_axis = plt.gca()\n    for c in centers:\n        current_axis.add_patch(patches.Rectangle((c - 13, 0), 26, 50, color="red", fill=False))\n\nplt.show()'}, {'identified': '- The semicolons at the end of the lines are not necessary in Python', 'updated_code': 'raveled.reshape(255, 255, 3)\nreshaped = raveled.reshape(255, 255, 3)'}, {'identified': "- The imports 'set_figsize' and 'figsize' are on the same line and should be on separate lines", 'updated_code': 'from kf_book.book_plots import set_figsize\nfrom kf_book.nonlinear_plots import figsize, plot_gaussians\n\nP = np.diag([3., 1.])\nnp.random.seed(3)\nMs, Ps = kf_book.run(count=25, R=10, Q=0.01, P=P, do_plot=False)\nwith figsize(x=9, y=5):\n    plot_gaussians(Ms[::7], Ps[::7], (-5, 25), (-5, 5), 75)'}, {'identified': 'None', 'updated_code': 'from typing import Any, List\nfrom dataclasses import dataclass, field\nimport numpy as np\n\n@dataclass\nclass Clustering:\n    """Base class of clustering, offering basic and common variables and operations for clustering."""\n    data: Any  # array-like (List, pd.Series, np.ndarray((N, L))) data\n    names: List[str] = None  # of each data; displayed in plots\n    N: int = field(init=False)  # number of data; = len(data) or data.shape[0]\n    L: int = field(init=False, default=None)  # number of features; = data.shape[1]\n    assignments: np.ndarray = field(init=False)  # cluster assignment for each data; int type of length N\n    s_dist_mat: np.ndarray = field(init=False, default=None)  # square distance matrix\n    c_dist_mat: np.ndarray = field(init=False, default=None)  # condensed distance matrix\n    cache: dict = field(default_factory=dict)  # store large intermediate data'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- There is an unnecessary triple hash ('###') at the beginning of the code", 'updated_code': 'import random\nimport matplotlib.pyplot as plt\nfrom prompts import new_prompt\n\n\ndef plot_imgs(X_data, y_label):\n    """\n    Plot a grid of random images with corresponding labels.\n    \n    Args:\n    X_data: numpy array of input images\n    y_label: numpy array of corresponding labels\n    \n    Returns:\n    rand_indices: list of randomly selected image indices\n    """\n    f, axarr = plt.subplots(3, 3, figsize=(16, 16))\n    rand_indices = []\n    for i in range(9):\n        image, label, index = get_random_img(X_data, y_label)\n        rand_indices.append(index)\n        label_str = str(label)\n        axarr[i // 3, i % 3].imshow(image, cmap="gray")\n        axarr[i // 3, i % 3].set_title(label_str + ": " + sign_dict[label_str])\n        # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n        plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n        plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n    return rand_indices\n\n\nX_train, y_train = shuffle(X_train_augmented, y_train_augmented)\n\ndef get_random_img(X_data, y_label):\n    index = random.randint(0, len(X_data))\n    image = X_data[index].squeeze()\n    return image, y_label[index], index\n\n\nrand_img_indices = plot_imgs(X_train, y_train)\nprint(rand_img_indices)'}, {'identified': "- The import statement 'import time' should be at the top of the code, before any variable assignments", 'updated_code': "import time\nk = 10\nheterogeneity = {}\n\nstart = time.time()\nfor seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n    initial_centroids = get_initial_centroids(tf_idf, k, seed)\n    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n                                           record_heterogeneity=None, verbose=False)\n    # To save time, compute heterogeneity only once in the end\n    heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n    print(np.bincount(cluster_assignment))\n    sys.stdout.flush()\nend = time.time()\nprint(end - start)"}, {'identified': '- Inconsistent indentation within the try-except block', 'updated_code': '%%time\ntry:\n    subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n    print(f"載入已建立的字典： {en_vocab_file}")\nexcept:\n    print("沒有已建立的字典，從頭建立。")\n    subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n        (en.numpy() for en, _ in train_examples),\n        target_vocab_size=2**13\n    )  # 有需要可以調整字典大小\n\n    # 將字典檔案存下以方便下次 warmstart\n    subword_encoder_en.save_to_file(en_vocab_file)\n\nprint(f"字典大小：{subword_encoder_en.vocab_size}")\nprint(f"前 10 個 subwords：{subword_encoder_en.subwords[:10]}")\nprint()'}, {'identified': "- The comments should have a space after the '#' symbol", 'updated_code': "# Selecting certain numerical features\nx_select = X_Num_Cov[space]\n\n# Dropping the missing values from the feature space\nx_select = x_select.dropna()\n\n# Creating the output space\nY = x_select['reviews_per_month']\nx_select = x_select.drop(['reviews_per_month'], axis=1)\n\nprint(x_select.info())\n\nx_select.head()"}, {'identified': "- The assignment operator '=' should have whitespace around it", 'updated_code': 'Image(filename=pathname + DATAppDzNtrk)'}, {'identified': '- There is no space after the comma following \'b"SN2007uy"\'', 'updated_code': 'flux, flux_err = coco.simulate(b"SN2007uy",\n                               z_obs, 0.0, 0.0, 0.0, 3.1,\n                               mjdmax, mjd_to_sim,\n                               filters_to_sim)'}, {'identified': None, 'updated_code': None}, {'identified': '- Statements are not separated by blank lines', 'updated_code': 'import matplotlib.pyplot as plt\nimport numpy as np\n\nbarwidth = 0.75\n\nfig, ax = plt.subplots(figsize=(9, 7))\n\nrects1 = ax.bar(0.5, SkyPresence.mean(), barwidth, color=sns.xkcd_rgb[\'green\'], yerr=SkyPresenceSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects2 = ax.bar(1.5, ColorScheme.mean(), barwidth, color=(0.3, 0.9, 0.3), yerr=ColorSchemeSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects3 = ax.bar(2.5, TreeFreq.mean(), barwidth, color=(0.15, 1, 0.15), yerr=TreeFreqSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects4 = ax.bar(4, ImageType.mean(), barwidth, yerr=ImageTypeSEM, ecolor=\'k\', edgecolor=sns.xkcd_rgb[\'green\'], linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\nrects5 = ax.bar(5, FeatureType.mean(), barwidth, yerr=FeatureTypeSEM, ecolor=\'k\', edgecolor=(0.3, 0.9, 0.3), linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\nrects6 = ax.bar(6, LightType.mean(), barwidth, yerr=LightTypeSEM, ecolor=\'k\', edgecolor=(0.15, 1, 0.15), linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\n\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None)\n\nax.set_ylim(0, 100)\nax.set_xlim(0, 7.5)\nax.set_xticklabels((\'SP\', \'CS\', \'TF\', \'IT\', \'FT\', \'LT\'))\nax.set_xticks([0.5 + barwidth/2, 1.5 + barwidth/2, 2.5 + barwidth/2, 4 + barwidth/2, 5 + barwidth/2, 6 + barwidth/2])\nax.set_yticks(np.arange(0, 101, 10))\n\nplt.title(\'Q2: Rate the Frequency at Which These Perceptual Categories\\nPredicted an Easy/Hard Color-Word Trial\', fontsize=18, fontweight="bold")\nplt.ylabel(\'<-- Less Likely      More Likely -->\', fontsize=17, fontweight="bold")\nplt.xlabel(\'S-C Phase                 S-CT Phase\', fontsize=17, fontweight="bold")\n\nsns.despine()\n\nplt.show()'}, {'identified': "- The comment should start with a single '#' instead of '##'", 'updated_code': '# add new column\ndata["N10\'"] = ""'}, {'identified': None, 'updated_code': None}, {'identified': "- The comment should start with a space after the '#' character", 'updated_code': '# RT Validity Effect\nstats.ttest_rel(RTanalysis.Valid, RTanalysis.Invalid)'}, {'identified': '- The import statement is not on a separate line', 'updated_code': "# Import data\nPixel = pd.read_csv(r'D:\\Annies_Dissertation\\Analysis\\Regression\\Validation\\\n                     Monthly_PM25_LSOA_Validation.csv', parse_dates=['time'])"}, {'identified': "- The comment should have a space after the '#' character", 'updated_code': '# get activations for testing Density Forest\nact_test = get_activations_batch(model_unet, -2, data_test_overlap.im_patches, 20, verbose=True)\n\n# remove test activations overlap\nact_test = remove_overlap(data_test.imgs, act_test, patch_size=64, stride=32)\nact_test = np.concatenate(np.concatenate(act_test))'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- Comments should start with a single '#' character, not '##'", 'updated_code': "# Bayesian prior parameters\nalpha_0 = 1\nbeta_0 = 1\n\n# Set parameters for simulating data\nstart_date = as.Date('2017-01-01')  # start date of tests\ntest_duration = 60  # length of tests in days\nnum_tests = 3  # how many test variants excluding default\ncounts = 10000  # total view in each group\nprob_list = [0.02, 0.04, 0.025, 0.02]  # true proportion of Test0, Test1, Test2, Test3\nConf_alpha = 0.1  # Confidence level"}, {'identified': "- The keyword arguments in the function call 'ekos.get_unique_id_for_alias' should have spaces around the equal signs", 'updated_code': "subset_uuid = ekos.get_unique_id_for_alias(workspace_alias=workspace_alias, subset_alias='A')\nprint(w.get_subset_list(), subset_uuid)\n\nf1 = w.get_data_filter_object(subset=subset_uuid, step=1) \nprint(f1.include_list_filter)\n\nw.apply_data_filter(subset=subset_uuid, step=1)\n\ndf_step1 = w.get_filtered_data(step=1, subset=subset_uuid)\n# print(df_step1.columns)\n# df_step1[['SDATE', 'YEAR', 'MONTH', 'POSITION', 'VISS_EU_CD', 'WATER_TYPE_AREA', 'DEPH', 'MNDEP', 'MXDEP','BQIm']].dropna(subset=['BQIm'])"}, {'identified': '- The code is missing import statements', 'updated_code': 'from sympy import vandermonde, Partition\nfrom sympy.polys.specialpolys import vandermonde\nfrom sympy.polys.monomials import monomial_mul\n\nv = vandermonde(Partition([3, 1, 1]))\ndeg_v = v.degree()\ngenerators = {v.multidegree(): [v]}\noperators = monomial_mul.partial_derivatives(v.parent())\nw1 = Subspace(generators=generators, operators=operators, add_degrees=add_degree)\n\npolarization_operators = polarization_operators(2, max_deg=deg_v)\nw2 = PolarizedSpace(IsotypicComponent(w1, 5), polarization_operators)\nCharacter(w2)'}, {'identified': "- The comment '### Test, test, test' should be a single-line comment using '#'", 'updated_code': '# Test, test, test\n\nX_prior.reset()\nX.reset()\nY_cond.reset()\n\nX_prior.send_sp_msg(X)\nX.set_observed(0)\nX.send_sp_msg(Y_cond)\nassert np.allclose(list(Y_cond.in_msgs.values()), [9.5e-01, 5.0e-12])'}, {'identified': '- There are unnecessary extra spaces around the equals sign in the commented out line', 'updated_code': '# Depth 10 is usually plenty of depth for most datasets, but you never know\nhyper_params = {\'max_depth\': range(1, 30, 2)}\n# hyper_params = {\'max_depth\': [4, 6, 8, 12, 16, 20]}  # Faster for larger datasets\n\n# Build initial GBM Model\ngbm_grid = H2OGradientBoostingEstimator(\n    # More trees is better if the learning rate is small enough \n    # Here, use "more than enough" trees - we have early stopping\n    ntrees=10000,\n    # Smaller learning rate is better\n    # Since we have learning_rate_annealing, we can afford to start with a bigger learning rate\n    learn_rate=0.05,\n    # Learning rate annealing: learning_rate shrinks by 1% after every tree \n    # (use 1.00 to disable, but then lower the learning_rate)\n    learn_rate_annealing=0.99,\n    # Sample 80% of rows per tree\n    sample_rate=0.8,\n    # Sample 80% of columns per split\n    col_sample_rate=0.8,\n    # Fix a random number generator seed for reproducibility\n    seed=1234,\n    # Score every 10 trees to make early stopping reproducible \n    # (it depends on the scoring interval)\n    score_tree_interval=10,\n    # Early stopping once the validation AUC doesn\'t improve by at least 0.01% for 5 consecutive scoring events\n    stopping_rounds=5,\n    stopping_metric="AUC",\n    stopping_tolerance=1e-4)\n\n# Build grid search with previously made GBM and hyper parameters\ngrid = H2OGridSearch(gbm_grid, hyper_params,\n                     grid_id=\'depth_grid\',\n                     search_criteria={\'strategy\': "Cartesian"})\n\n# Train grid search\ngrid.train(x=predictors, \n           y=response,\n           training_frame=train,\n           validation_frame=valid)'}, {'identified': "- The comment should start with a single '#', not '###'", 'updated_code': "# Não.\n\ntest.groupby(['ParentesIrmao']).size()"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- There should be two blank lines between the imports and the first line of code', 'updated_code': "from sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering\n\npca = PCA(n_components=2).fit(data)\npca_2d = pca.transform(data)\n\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster.fit_predict(pca_2d)\n\nplt.figure(figsize=(10, 7))\nplt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=cluster.labels_, cmap='rainbow')"}, {'identified': "- The import statement for 'plt' is missing", 'updated_code': 'import matplotlib.pyplot as plt\n\n# plot\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 15))\ny_pos = np.arange(len(feature_names))\nax.barh(feature_names, top50importance, align="center", color="mediumspringgreen")\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\nax.grid(alpha=0)\nax.set_xlabel("Importance", fontsize=20)\nax.set_ylabel("Features", fontsize=20)\nax.yaxis.set_tick_params(labelsize=12)\nax.xaxis.set_tick_params(labelsize=15)\n\nfig.savefig("Feature Importance.png", bbox_inches="tight")'}, {'identified': "- There is no space after the comma in the variable assignment for 'data, labels'", 'updated_code': 'data, labels = vectorize_docs(\n    artist="linkin-park",  # specify artist\n    albums=["hybrid-theory"],  # specify album(s)\n    keep_album=False,  # option to use the album name as a delimiter\n    titlify=True,  # converts song title to original format\n)\n\nsong_to_search = "Crawling"\nfor i, song in enumerate(labels):\n    if song == song_to_search:\n        display(Markdown("**Song name: **" + labels[i])) \n        display(Markdown("**Lyrics:**")) \n        print(data[i][:1000] + "...")\n        sample_lyrics = data[i]\n        break'}, {'identified': None, 'updated_code': None}, {'identified': '- The function definition and its arguments should not exceed 79 characters per line', 'updated_code': "from prompts import new_prompt\n\n\ndef character_quotient(M, N, n, r, left_basis=s, right_basis=s):\n    # a corriger\n    b_tot = M.basis()\n    b_ideal = N.basis()\n    charac = 0\n    q = PolynomialRing(QQ, 'q', r).gens()\n\n    for nu in Partitions(n):\n        basis_nu_tot = {}\n        basis_nu_ideal = {}\n        charac_nu = 0\n\n        # Get the nu_isotypic part of the bases\n        for k, v in b_tot.items():\n            if Partition(k[1]) == nu:\n                basis_nu_tot[k[0]] = v\n        for k, v in b_ideal.items():\n            if Partition(k[1]) == nu:\n                basis_nu_ideal[k[0]] = v\n\n        # Use the degrees to compute the character\n        for deg, b in basis_nu_tot.items():\n            charac_nu += sum(prod(q[i] ** deg[i] for i in range(0, len(deg))) for p in b)\n        for deg, b in basis_nu_ideal.items():\n            charac_nu -= sum(prod(q[i] ** deg[i] for i in range(0, len(deg))) for p in b)\n        if charac_nu != 0:\n            if left_basis == s:\n                charac_nu = s.from_polynomial(charac_nu).restrict_partition_lengths(r, exact=False)\n            else:\n                charac_nu = left_basis.from_polynomial(charac_nu)\n\n            # Make the tensor product with s[nu]\n            charac += tensor([charac_nu, right_basis(s(nu))])\n\n    return charac"}, {'identified': '- There should be two blank lines between the imports and the first variable assignment not one', 'updated_code': 'N_test = 100\nsigma = 0.2\nbeta = 1.0 / pow(sigma, 2)\nx_test = np.linspace(-1, 1, N_test)\ny_test = true_mean_function(x_test)\n\nthetas = np.array([(1., 4., 0., 0.),\n                   (9., 4., 0., 0.),\n                   (1., 64., 0., 0.),\n                   (1., 0.25, 0., 0.),\n                   (1., 4., 10., 0.),\n                   (1., 4., 0., 5.)])\nn_train = 10\nx_train = np.random.uniform(-1, 1, n_train)\ny_train = true_mean_function(x_train)\nt_train = add_noise(y_train, sigma)\n\nfor idx, theta in enumerate(thetas):\n    mean_test, covar_test, _ = gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C=None)\n    lp, _, _ = gp_log_likelihood(x_train, t_train, theta, beta, C=None, invC=None)\n    plt.subplot(2, 3, idx + 1).title.set_text(f"theta: {theta}, lp = {lp:.2f}")\n    gp_plot(x_test, y_test, mean_test, covar_test, x_train, t_train, theta, beta)\n\nplt.suptitle(f"{n_train} data points")\nplt.show()'}, {'identified': None, 'updated_code': None}, {'identified': '- The import statement for `GaussianMixture` should be on a separate line from the comment', 'updated_code': '# TODO: Apply your clustering algorithm of choice to the reduced data\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\n# TODO: for loop\nx = 3\n\nclusterer = GaussianMixture(n_components=x)\nclusterer.fit(reduced_data)\n\n# TODO: Predict the cluster for each data point\npreds = clusterer.predict(reduced_data)\n\n# TODO: Find the cluster centers\ncenters = clusterer.means_\n\n# TODO: Predict the cluster for each transformed sample data point\nsample_preds = clusterer.predict(pca_samples)\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\nscore = silhouette_score(reduced_data, preds)\n\nprint(x, score)'}, {'identified': "- The import statement for 'pyplot' is missing", 'updated_code': "import matplotlib.pyplot as plt\n\n# Line plot\nseries.plot()\nplt.xticks(rotation=90)\nplt.show()\n\n# The reason this chart doesn't match up with what's provided is because Python\n# understands time now and there are missing data for several points across multiple dates.\n# I guess I will have to try differently to reproduce this."}, {'identified': None, 'updated_code': None}, {'identified': "- The function name 'sharedNormSpanningSystem' violates the PEP 8 naming convention for functions. Function names should be lowercase with words separated by underscores.", 'updated_code': 'shared_norm_spanning_system(spanNorm[V3(1, 0, 0), V3(0, 1, 0), V3(0, 0, 1)], spanNorm[V3(1, 0, 0), V3(0, 1, 0)] :: Norm ℝ³)'}, {'identified': "- The imports 'os' and 'sys' should not be on the same line", 'updated_code': "import os\nimport sys\nfrom ROOT import gROOT\n\npath = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/16MeVTa/Unfold/NSD/'\nos.chdir(path)\nprint('Currently working in: \\n {}'.format(os.getcwd()))"}, {'identified': '- The comment should start with a capital letter and end with a period', 'updated_code': '# We can already add the values in our dataframe that won\'t lead to an address.\nuniversity_canton_dict[\'Nicht zuteilbar - NA\'] = {\'long_name\': \'N/A\', \'short_name\': \'N/A\'}  # It means "Not Available" in German!\ninstitution_canton_dict[\'NaN\'] = {\'long_name\': \'N/A\', \'short_name\': \'N/A\'}\ninstitution_canton_dict[\'nan\'] = {\'long_name\': \'N/A\', \'short_name\': \'N/A\'}'}, {'identified': "- The function name 'update_nueron' should be 'update_neuron' to match PEP 8 naming conventions", 'updated_code': "def update_neuron(w, x, label):\n    '''\n    Update McCulloch + Pitts Neuron using Rosenblatt's Perceptron Algorithms\n    w = neuron weights\n    x = (n dimensional numpy array example)\n    label = binary label\n    '''\n\n    if neuron(x, w) == 0 and label == 1:\n        w = w + x\n        print('incorrect!, New weights = ' + str(w))\n\n    elif neuron(x, w) == 1 and label == 0:\n        w = w - x\n        print('incorrect!, New weights = ' + str(w))\n\n    else:\n        print('correct!')\n\n    return w"}, {'identified': None, 'updated_code': None}, {'identified': "- The dictionary keys in the variable 'cutPt' should be separated from their values by a space", 'updated_code': 'cutPt = {0: 0.28, 2: 0.3, 4: 0.29}\nfor detNum, detName in detNames.items():\n    gROOT.ProcessLine(\'AmBe{}->cd()\'.format(detNum))\n    # gROOT.ProcessLine(\'eventTree->Draw("(m_amplitude-m_shape)/m_shape:m_shape>>(1250,0,35000,1024,0,1)","(m_amplitude-m_shape)/m_amplitude<{}","colz")\'.format(cutPt[detNum]))\n    pause()'}, {'identified': None, 'updated_code': None}, {'identified': '- The line is too long and should be split into multiple lines', 'updated_code': "filename = 'gs://sarasmaster/kopari/shopify/products/products_1.json'\ndataset_id = 'kopari'\ntable_name = 'shopify_products'\ndelimiter_type = 'NEWLINE_DELIMITED_JSON'\nload_type = 'WRITE_TRUNCATE'\nload_file_to_bigquery(filename, dataset_id, table_name, delimiter_type, load_type, None)"}, {'identified': None, 'updated_code': None}, {'identified': "- The closing square bracket in the indexing operation is missing after 'y[i%X.shape[0]]'", 'updated_code': 'for i in range(25):\n    w = update_nueron(w, X[i % X.shape[0], :], y[i % X.shape[0]])'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- The comment should have a space after the '#' symbol", 'updated_code': "# Quarterly Revenues across multi-year\ndfqq = df.pivot_table(index=df.index.quarter, aggfunc=(np.mean, np.sum, min, max)).rename_axis('quarter')\ndfqq.style \\\n    .applymap(color_negative_red) \\\n    .apply(highlight_max) \\\n    .apply(highlight_min)"}, {'identified': '- Comments should be in English, not Chinese', 'updated_code': '# Hyperparameters\nnum_layers = 1\nd_model = 4\nnum_heads = 2\ndff = 8\n\n# + 2 for <start> & <end> token\ninput_vocab_size = subword_encoder_en.vocab_size + 2\noutput_vocab_size = subword_encoder_zh.vocab_size + 2\n\n# The key point. Use the previous word to predict the next Chinese character during training\ntar_inp = tar[:, :-1]\ntar_real = tar[:, 1:]\n\n# Masks for source/target language. Note that `combined_mask` combines both masks for the target language\ninp_padding_mask = create_padding_mask(inp)\ntar_padding_mask = create_padding_mask(tar_inp)\nlook_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\ncombined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n\n# Initialize our first transformer\ntransformer = Transformer(num_layers, d_model, num_heads, dff, \n                          input_vocab_size, output_vocab_size)\n\n# Pass the English and Chinese sequences to the transformer to get the predicted next Chinese characters\npredictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n                                        combined_mask, inp_padding_mask)\n\nprint("tar:", tar)\nprint("-" * 20)\nprint("tar_inp:", tar_inp)\nprint("-" * 20)\nprint("tar_real:", tar_real)\nprint("-" * 20)\nprint("predictions:", predictions)'}, {'identified': '- There is no space after the colon in the dictionary comprehension', 'updated_code': "# create flow variables for each couple of nodes\n# x(i,j) is the flow going out of node i to node j\nx = {(i, j): tm.continuous_var(name='x_{0}_{1}'.format(i, j)) for i in source for j in target}\n\n# each arc comes with a cost. Minimize all costed flows\ntm.minimize(\n    tm.sum(\n        x[i, j] * costs.get((i, j), 0)\n        for i in source for j in target\n    )\n)\n\n    tm.print_information()"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The import statement should be on a separate line', 'updated_code': 'from matplotlib.ticker import MultipleLocator\nimport matplotlib.pyplot as plt\n\n# filters = ["BessellV"]\nfilters = ["SDSS_r"]\n\nalpha = 1.0\nxminorticks = 10\n\npcc.utils.setup_plot_defaults()\n\nfig = plt.figure(figsize=[8, 4])\nfig.subplots_adjust(left=0.1, bottom=0.13, top=0.93,\n                    right=0.91, hspace=0, wspace=0)\n\n## Label the axes\nxaxis_label_string = r\'$\\textnormal{Time, MJD (days)}$\'\nyaxis_label_string = r\'$\\textnormal{Flux, erg s}^{-1}\\textnormal{\\AA}^{-1}\\textnormal{cm}^{-2}$\'\n\nax1 = fig.add_subplot(111)\naxes_list = [ax1]\n\nfor filter_key in filters:\n    plot_label_string = r\'$\\rm{\' + sn.phot.data_filters[filter_key].filter_name.replace(\'_\', \'\\\\_\') + \'}$\'\n    plot_label_string_fake = r\'$\\rm{\' + sn_fake.phot.data_filters[filter_key].filter_name.replace(\'_\', \'\\\\_\') + \', simulated}$\'\n\n    ax1.errorbar(sn.phot.data[filter_key][\'MJD\'], sn.phot.data[filter_key][\'flux\'],\n                 yerr=sn.phot.data[filter_key][\'flux_err\'],\n                 capsize=0, fmt=\'x\', color=sn.phot.data_filters[filter_key]._plot_colour,\n                 label=plot_label_string, ecolor=pcc.hex[\'batman\'], mec=pcc.hex["batman"],\n                 alpha=alpha)\n    ax1.fill_between(sn.lcfit.data[filter_key][\'MJD\'], sn.lcfit.data[filter_key][\'flux_upper\'], sn.lcfit.data[filter_key][\'flux_lower\'],\n                     color=pcc.hex["batman"],\n                     alpha=0.8, zorder=0)\n\nax1.errorbar(sn_fake.phot.data[filter_key][\'MJD\'], sn_fake.phot.data[filter_key][\'flux\'],\n             yerr=sn_fake.phot.data[filter_key][\'flux_err\'],\n             # capsize = 0, fmt = \'o\', color = sn_fake.phot.data_filters[filter_key]._plot_colour,\n             capsize=0, fmt=\'o\', color=pcc.hex[\'r\'],\n             label=plot_label_string_fake, ecolor=pcc.hex[\'batman\'], mec=pcc.hex["batman"],\n             alpha=alpha)\n\nxminorLocator = MultipleLocator(xminorticks)\nax1.spines[\'top\'].set_visible(True)\nax1.xaxis.set_minor_locator(xminorLocator)\n\nplot_legend = ax1.legend(loc=\'upper right\', scatterpoints=1, markerfirst=False,\n                         numpoints=1, frameon=False, bbox_to_anchor=(1., 1.),\n                         fontsize=12.)\n\nax1.set_ylabel(yaxis_label_string)\nax1.set_xlabel(xaxis_label_string)\n\noutpath = "/Users/berto/projects/LSST/cadence/SN2007uy_consistency_check_SDSS_r"\n\nfig.savefig(outpath + ".png", format=\'png\', dpi=500)'}, {'identified': '- There should be whitespace before and after the comma in the function call', 'updated_code': "from IPython.display import IFrame\n\nIFrame(\n    ('https://docs.google.com/presentation/d/1urkX-nRsD8VJvcOnJsjmCy0Jpv752Ssn5Pphg2sMC-0/'\n     'embed?start=false&loop=false&delayms=3000'),\n    800,\n    600\n)"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The comments are not aligned with the code they refer to', 'updated_code': 'white_output = \'test_videos_output/solidWhiteRight.mp4\'\nleftline = [(0, 0, 0, 0)]\nrightline = [(0, 0, 0, 0)]\n\n# To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n# To do so add .subclip(start_second,end_second) to the end of the line below\n# Where start_second and end_second are integer values representing the start and end of the subclip\n# You may also uncomment the following line for a subclip of the first 5 seconds\n# clip1 = VideoFileClip("test_videos/solidWhiteRight.mp4").subclip(0,5)\nclip1 = VideoFileClip("test_videos/solidWhiteRight.mp4")\nwhite_clip = clip1.fl_image(process_image)  # NOTE: this function expects color images!!\nwhite_clip.write_videofile(white_output, audio=False)'}, {'identified': "- The class name 'ValueEstimator' should be in CamelCase", 'updated_code': 'class ValueEstimator():\n    """\n    Value Function approximator. \n    """\n    \n    def __init__(self, learning_rate=0.1, scope="value_estimator"):\n        \n        with tf.variable_scope(scope):\n            self.state = tf.placeholder(tf.float32, [400], "state")\n            self.target = tf.placeholder(dtype=tf.float32, name="target")\n\n            # This is just linear classifier\n            self.output_layer = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n\n            self.value_estimate = tf.squeeze(self.output_layer)\n            self.loss = tf.squared_difference(self.value_estimate, self.target)\n\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            self.train_op = self.optimizer.minimize(\n                self.loss, global_step=tf.contrib.framework.get_global_step())        \n    \n    def predict(self, state, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        return sess.run(self.value_estimate, { self.state: state })\n\n    def update(self, state, target, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        feed_dict = { self.state: state, self.target: target }\n        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n        return loss'}, {'identified': "- Comments should start with a single '#', not '##'", 'updated_code': '# TODO: Execute your algorithm from Step 6 on\n# at least 6 images on your computer.\n# Feel free to use as many code cells as needed.\n\ndef detect_image(img_path):\n    disp_image(img_path)\n\n    # Corrected function name to lowercase with underscores\n    whose_a_good_doggy(img_path)'}, {'identified': None, 'updated_code': None}, {'identified': '- The import statement is missing', 'updated_code': "import data  # Add missing import statement\n\n# Some simple testing code and such\ndataset = 'mnist'\ntrain_data = data_filepath + dataset + '_data_train.csv'\ntrain_labels = data_filepath + dataset + '_labels_train.csv'\ntrain_ids = data_filepath + dataset + '_ids_train.csv'\ntest_data = data_filepath + dataset + '_data_test.csv'\ntest_ids = data_filepath + dataset + '_ids_test.csv'\ndescription = data_filepath + dataset + '_feature_descriptions.csv'\n\nproc = Preprocessor(train_data_file=train_data,\n                    train_label_file=train_labels,\n                    train_ids_file=train_ids,\n                    test_data_file=test_data,\n                    test_ids_file=test_ids,\n                    instr_file=description)\n\nproc.read_data()\n\nproc.process()\n\n# Doesn't do anything yet, hasn't been implemented\nproc.select_features()\n\n# Data is written to output directory\n# Any existing data is overwritten\nproc.write_data()"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The variable names contain underscores, but should use lowercase letters and words should be separated by underscores', 'updated_code': "from prompts import new_prompt\n\nuniao_dropna_slice_nome = uniao_dropna['Nome'].str.split(',', expand=True)\nuniao_dropna['Primeiro Nome'] = uniao_dropna_slice_nome[0]\nuniao_dropna_nome_slice = uniao_dropna_slice_nome[1].str.split('.', expand=True, n=1)\nuniao_dropna_complemento_slice = uniao_dropna_nome_slice[1].str.split('(', expand=True)\nuniao_dropna['Saudação'] = uniao_dropna_nome_slice[0]\nuniao_dropna['Sobrenome'] = uniao_dropna_complemento_slice[0]\nuniao_dropna['Complemento'] = uniao_dropna_complemento_slice[1]"}, {'identified': None, 'updated_code': None}, {'identified': '- There should be a space after the comma in the print statement', 'updated_code': 'for x in range(4):  # By default, range starts at 0\n    print(x),'}, {'identified': "- The function name 'forecast_lstm' should be lowercase with words separated by underscores", 'updated_code': '# make a one-step forecast\ndef forecast_lstm(model, batch_size, X):\n    X = X.reshape(1, 1, len(X))\n    yhat = model.predict(X, batch_size=batch_size)\n    return yhat[0, 0]\n\n# load the dataset\nseries = read_csv(filename, header=0, parse_dates=[0], index_col=0, squeeze=True)\n\n# transform data to be stationary\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\ntrain, test = supervised_values[0:-12], supervised_values[-12:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)'}, {'identified': '- The comments are using triple hashes (###) instead of a single hash (#)', 'updated_code': '# Define your architecture here.\n# Feel free to use as many code cells as needed.\n\nEPOCHS = 20\nBATCH_SIZE = 150\nrate = 0.002'}, {'identified': None, 'updated_code': None}, {'identified': '- Comments should be preceded by a single space', 'updated_code': '# 函数注解\n# python3.5引入\n# 对函数的参数类型注解\n# 对函数的返回值类型注解\n# 只对函数参数做一个辅助的说明，并不对函数参数进行类型检查\n# 提供给第三方工具，做代码分析，发现隐藏bug\n# 函数注解的信息，保存在__annotations__属性中\n\n# 变量注解\n# python3.6引入\ni: int = 3'}, {'identified': None, 'updated_code': None}, {'identified': "- No space before and after the '=' operator in the 'figsize' parameter", 'updated_code': "fig = plt.figure(figsize=(15, 15))\n\ngr = gridspec.GridSpec(1, 6)\nfor i in range(num_own_examples):\n    probas = proba[i]\n    highest_class = signnames[str(np.argmax(probas, axis=0))]\n    plt.subplot(1, 6, i+1)\n    plt.imshow(X2[i])\n    plt.axis('off')\n    plt.text(0, 1, highest_class, color='k', backgroundcolor='y')\nfig.suptitle('In yellow: prediction made by the model', x=0.5, y=0.6, fontsize=20)\nplt.tight_layout()\nplt.show()"}, {'identified': None, 'updated_code': None}, {'identified': '- The line is too long and should be split into multiple lines for better readability', 'updated_code': 'from prompts import new_prompt\nimport matplotlib.pyplot as plt\n\nformula = ("accuracy ~ C(subject, Treatment(0)) "\n           "+ C(complexity, Treatment(3)) * C(model, Treatment(1))")\nlm = ols(formula, df)\nfit = lm.fit()\nqqplot(fit.resid)\nprint(fit.summary())\n\nprint(\'\\nThe accuracy of the classifier depends on the subject, model type (deep network versus logistic regression), \'\n      \'and task complexity (CV versus consonant versus {vowel, location, degree}). \'\n      \'(ANOVA with subject, model type, task complexity, and model-task complexity interaction, \'\n      \'f-value: {}, p: {}). \'.format(fit.fvalue, fit.f_pvalue) +\n      \'Within this ANOVA, all treatment coefficients were significant \'\n      \'at p<.001 with Subject 1, CV task, and logistic regression as the reference treatment.\')\n\nfor table in fit.summary().tables:\n    print(table.as_latex_tabular())\n\nplt.show()'}, {'identified': '- The comments are written in Spanish, which is not a violation of PEP 8 conventions but may be inconsistent with code written in English.', 'updated_code': '# obtenemos los datos que necesitamos y los ponemos en una matriz de Julia\nvinos = matread("wine.mat")\ndatos = vinos["data"]\n\n# separamos las caracteristicas y los tipos de vino\ncaracteristicas = datos[:, 2:end]\ntipo_de_vino = datos[:, 1]'}, {'identified': "- There is no import statement for 'figsize' and 'plt'", 'updated_code': 'import matplotlib.pyplot as plt\n\n\nfigsize(12.5, 4)\n\nplt.scatter(alpha_samples, beta_samples, alpha=0.1)\nplt.title("Why does the plot look like this?")\nplt.xlabel(r"$\\alpha$")\nplt.ylabel(r"$\\beta$")'}, {'identified': None, 'updated_code': None}, {'identified': "- The comment should start with a space after the '#' symbol", 'updated_code': '# Similarly we can create for loop to calculate the entries of mdat\n# some data that corresponds to each cat. e.g. age\ndata = np.array([4, 14, 6, 11, 3, 14, 8, 17, 17, 12, 10, 18])\n# type of cat (of 3 types)\ncat = np.array([1, 3, 2, 1, 2, 2, 3, 1, 3, 2, 3, 1])\n\nprint(data[cat == 1])\n\nfor i in range(0, 3):\n    # note the +1 because our indexing is base 0 but the categories are 1, 2, 3 (no 0)\n    mdat[i] = np.mean(data[cat == i + 1])\n\nprint(mdat)'}, {'identified': 'None', 'updated_code': None}, {'identified': "- The function name 'calc_sum_product_variable_to_factor_msg' violates the naming convention, it should be 'calc_sum_product_variable_to_factor_message'", 'updated_code': 'def calc_sum_product_variable_to_factor_message(variable, factor):\n\n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n\n    if len(neighbour_msg_prod) > 0:\n        message = np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        message = np.ones(variable.num_states)\n\n    message = (message *\n               variable.observed_state)\n\n    return message'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- Multiple variable assignments are on the same line and should be separated onto separate lines', 'updated_code': 'import matplotlib.pyplot as plt\n\nux, uy, uz = lectura("vx.dat", "vy.dat", "vz.dat")\nx, y, z = lectura("x.dat", "y.dat", "z.dat")\n\ngrafica("vx.dat", "vy.dat", "x.dat", "y.dat", 32)\nplt.savefig("velocidad")\n\ngrafica("Bx.dat", "Bz.dat", "x.dat", "y.dat", 32)\nplt.savefig("Magentico")'}, {'identified': None, 'updated_code': None}, {'identified': "- The import statement '%matplotlib inline' should be on a separate line", 'updated_code': "import matplotlib.pyplot as plt\n\n%matplotlib inline\n\ndef solve_and_plot(nsys):\n    fig = plt.figure(figsize=(12, 4))\n    ax_out = plt.subplot(1, 2, 1, xscale='log', yscale='log')\n    ax_err = plt.subplot(1, 2, 2, xscale='log')\n    ax_err.set_yscale('symlog', linthreshy=1e-14)\n    xres, extra = nsys.solve_and_plot_series(\n        c0, c0+K, NH3_varied, NH3_idx, 'scipy', \n        plot_kwargs=dict(ax=ax_out), plot_residuals_kwargs=dict(ax=ax_err))\n    for ax in (ax_out, ax_err):\n        ax.set_xlabel('[NH3]0 / M')\n    ax_out.set_ylabel('Concentration / M')\n    ax_out.legend(loc='best')\n    ax_err.set_ylabel('Residuals')\n    \n    avg_nfev = np.average([nfo['nfev'] for nfo in extra['info']])\n    avg_njev = np.average([nfo['njev'] for nfo in extra['info']])\n    success = np.average([int(nfo['success']) for nfo in extra['info']])\n    \n    return {'avg_nfev': avg_nfev, 'avg_njev': avg_njev, 'success': success}\n\nsolve_and_plot(neqsys)"}, {'identified': None, 'updated_code': None}, {'identified': "- There is an unnecessary triple '#' character at the beginning of the comment", 'updated_code': '# print out the first 1000 characters of the raw text to get a sense of what we need to throw out\ntext[:1000]'}, {'identified': None, 'updated_code': None}, {'identified': '- The import statement should be placed at the top of the file, before any other statements', 'updated_code': 'import time\nimport matplotlib.pyplot as plt\n\n%pylab inline\nplt.rcParams["figure.figsize"] = [9, 5]\n\nstart = time.time()'}, {'identified': "- The import statement for 'plt' is missing", 'updated_code': 'import matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots()\n\ntick_locations = [value for value in x_axis]\n\nplt.xticks(tick_locations, county, rotation=90)\n\ngrad_rate = df_county_data["Graduation Rate"]\ncounty = df_county_data["County Name"]\npov_rate = df_county_data["Poverty Rate"]\nt = np.arange(len(county))\nax1.plot(t, pov_rate, \'b-\')\nax1.set_xlabel(\'counties\')\n# Make the y-axis label, ticks and tick labels match the line color.\nax1.set_ylabel(\'Poverty Rate\', color=\'b\')\nax1.tick_params(\'y\', colors=\'b\')\n\nplt.title("High School Graduation Rates and Poverty Rates by County")\n\nax2 = ax1.twinx()\n\nax2.plot(t,grad_rate, \'r*\')\nax2.set_ylabel(\'Graduation Rate\', color=\'r\')\nax2.tick_params(\'y\', colors=\'r\')\nzoom = 5\nw, h = fig.get_size_inches()\nfig.set_size_inches(w * zoom, h * zoom/2)\n\nplt.savefig("Images/County_Grad_Poverty_Rates2.png", bbox_inches="tight")\nplt.show()'}, {'identified': None, 'updated_code': None}, {'identified': "- There are two hash symbols '##' at the beginning of the line, which should be replaced with one hash symbol '#'", 'updated_code': '# fit the linear regression model\nlin_mod = lm(y ~ x, data=train)'}, {'identified': None, 'updated_code': None}, {'identified': "- The import statement for 'math' should be at the top of the file, not within the function", 'updated_code': 'import math\nimport pandas as pd\nimport numpy as np\n\n\ndef clean_election_data():\n    \'\'\'\n    Function to clean election data \n    \'\'\'\n    \n    # read in dirty data \n    df = pd.read_csv("2014_election_results.csv")\n    df_clean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\n    for i in range(len(df_clean)):\n        row = df_clean.loc[i]  \n        row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n        if pd.isnull(row["CANDIDATE NAME"]) or row["CANDIDATE NAME"] == \'Scattered\':\n            if pd.isnull(row["CANDIDATE NAME (Last)"]) or row["CANDIDATE NAME (Last)"] == \'Scattered\':\n                row["CANDIDATE NAME"] = "UNKNOWN" \n            else:\n                row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"]\n    \n    df_clean = df_clean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\n    return df_clean'}, {'identified': '- The dictionary keys should be aligned', 'updated_code': "tuned_parameters = [\n    {'kernel': ['rbf'],\n     'nu': [1e-4, 1e-3, 1e-2, 1e-1, 5e-1],\n    },\n    {'kernel': ['poly'],\n     'degree': np.arange(1, 4),\n     'nu': [1e-4, 1e-3, 1e-2, 1e-1, 5e-1],\n     'max_iter': [10000]\n    }\n]\n\n# do parameter search\nps_svm = ParameterSearch(svm.OneClassSVM, tuned_parameters, act_train_svm, act_train_all,\n                         pred_f_tr.flatten(), scorer_roc_probas_svm, n_iter=5,\n                         verbosity=11, n_jobs=-1, subsample_train=.01, subsample_test=.001)\nps_svm.fit()"}, {'identified': None, 'updated_code': None}, {'identified': '- The semicolon at the end of the line should be removed', 'updated_code': 'pm.traceplot(trace, lines={"ℓ": ℓ_true, "η": η_true})'}, {'identified': "- The assignment operator '=' should have whitespace around it", 'updated_code': 'Image(filename=pathname + average_pile_up_mult_collision_frac)'}, {'identified': None, 'updated_code': None}, {'identified': '- Inconsistent indentation', 'updated_code': "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 10))\n\nplt.subplot(2, 1, 1)  # パレットを2行1列に分割し、1行目に以下のグラフをプロットする\nplt.plot(u1_z)\nplt.xlabel('time [ms]')\nplt.ylabel('u1_z [deg/sec]')\nplt.grid()\n\nplt.subplot(2, 1, 2)  # パレットを2行1列に分割し、2行目に以下のグラフをプロットする\nplt.plot(u2_z)\nplt.xlabel('time [ms]')\nplt.ylabel('u2_z [deg/sec]')\nplt.grid()"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The import statements should be separated onto separate lines', 'updated_code': 'from prompts import new_prompt\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\naccuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n\ndef get_positive_class_probabilities(arr):\n    arr_aux = []\n    for entry in arr:\n        arr_aux.append(entry[1])\n    return arr_aux\n\n\ndef get_accuracy(y_pred, y_eval_np):\n    return np.mean(y_pred == y_eval_np)\n\n\ndef show_prediction_accuracy(y_pred, y_eval_np, x_eval_np):\n    print("Predicciones sobre el test set:\\n {}".format(y_pred))\n    print("Score sobre el test set: {:.2f}".format(np.mean(y_pred == y_eval_np)))  # A mano\n    print("Score sobre el test set: {:.2f}".format(arbol.score(x_eval_np, y_eval_np)))  # Usando el método score.\n\n\n# Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train = []\naccuracy_validation = []\nroc_train = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    # print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n\n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n\n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n\n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test))\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train))\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n\n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n\ndf = pd.DataFrame(index=range(1, 6))\ndf.index.name = "Permutación"\n\ndf["Accuracy (training)"] = accuracy_train  # cambiar por accuracies_training\ndf["Accuracy (validación)"] = accuracy_validation  # cambiar por accuracies_validation\ndf["AUC ROC (training)"] = roc_train  # cambiar por aucs_training\ndf["AUC ROC (validación)"] = roc_validation  # cambiar por aucs_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()'}, {'identified': None, 'updated_code': None}, {'identified': '- The print statement should be enclosed in parentheses', 'updated_code': 'print("Coeficiente de Clustering C para la componente gigante: " +\n      str(nx.average_clustering(gig_comp_graph)))'}, {'identified': '- The import statement should be on a separate line from other code', 'updated_code': 'from IPython.core.display import HTML\n\n\ndef css_styling():\n    styles = open("../styles/custom.css", "r").read()\n    return HTML(styles)\n\n\ncss_styling()'}, {'identified': '- The import statements are missing', 'updated_code': 'import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_stacked_scores(athlete):\n    plt.figure(figsize=(30, 10))\n    \n    x = np.arange(len(athlete))\n    \n    athlete[\'additional_index\'] = x\n    scores = [\n        athlete.climbing_scores,\n        athlete.yoga_scores,\n        athlete.tech_scores,\n        athlete.power_scores,\n        athlete.gym_scores,\n        athlete.arc_scores,\n        athlete.hang_scores,\n    ]\n    \n    labels = [\'Climbing\', \'Yoga\', \'Technique\', \'Power\', \'Gymnastics\', \'ARC\', \'Hangboarding\']\n    colors = [\'cornflowerblue\', \'darkturquoise\', \'mediumorchid\', \'red\', \'gold\', \'gray\', \'lightgreen\']\n\n    y = np.vstack(scores)\n    plt.stackplot(x, y, labels=labels, colors=colors)\n    plt.plot(x, athlete.scores, marker=\'o\', color=\'lightgray\')\n\n    for index, row in athlete.iterrows():\n        if row.notes:\n            plt.annotate(row.notes, xy=(row.additional_index, row.scores), fontsize=14, fontweight=\'bold\')\n\n    plt.legend(loc=2)\n    plt.show()\n    \n\ndef plot_scores(athlete):\n    """\n    Plot the following scores:\n    \n    - Hangboard\n    - Climbing\n    - Gymnastics \n    - Technical Scores\n    \n    """\n    fig, axes = plt.subplots()\n    ax1 = fig.add_subplot(221)\n    ax2 = fig.add_subplot(222)\n    ax3 = fig.add_subplot(223)\n    ax4 = fig.add_subplot(224)\n    \n    ax1.set_title(\'Hangboarding\')\n    ax2.set_title(\'Gymnastics\')\n    ax3.set_title(\'Climbing\')\n    ax4.set_title(\'Technique\')\n    \n    athlete_hang_scores = athlete[athlete.hang > 0]\n    athlete_gym_scores = athlete[athlete.gym_scores > 0]\n    athlete_climbing_scores = athlete[athlete.climbing_scores > 0]\n    athlete_tech_scores = athlete[athlete.tech_scores > 0]\n    \n    athlete_hang_scores.hang_scores.plot(ax=ax1, figsize=(20, 10), marker=\'o\', color=\'green\')\n    athlete_gym_scores.gym_scores.plot(ax=ax2, figsize=(20, 10), marker=\'o\', color=\'gold\')\n    athlete_climbing_scores.climbing_scores.plot(ax=ax3, figsize=(20, 10), marker=\'o\', color=\'cornflowerblue\')\n    athlete_tech_scores.tech_scores.plot(ax=ax4, figsize=(20, 10), marker=\'o\', color=\'mediumorchid\')\n    \n    plt.show()'}, {'identified': None, 'updated_code': None}, {'identified': '- In the line `for s in states:`, the colon should have whitespace before it', 'updated_code': "def value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4):\n    \n    # 1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    \n    while delta > epsilon:\n\n        delta = 0\n        \n        Q = {0: {0: 0,   # state 0, action 0\n                 1: 0},  # state 0, action 1\n             1: {2: 0}}  # state 1, action 2\n        for s in states:\n            v = 0\n            for a in actions[s]:\n                for n in next_states:\n                \n                    # Bellman's optimality update rule\n                    Q[s][a] += P[n, s, a] * (R[s, a] + gamma * V_k[n])\n\n            # This step replaces the policy improvement step (gets the maximal value)\n            V_kplus1[s] = max(Q[s].items(), key=operator.itemgetter(1))[1]\n            \n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n    \n    # Updates the policy to be greedy with respect to the value function\n    for s in states:\n        PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n    \n    return V_k, k, PI"}, {'identified': None, 'updated_code': None}, {'identified': "- The comment '#ignore' should not be there as it is not a valid Python comment", 'updated_code': 'import os\nfrom google.colab import drive\nfrom IPython.display import clear_output\n\n# google drive sync\nsave_to_gdrive = True\ndataset_in_gdrive = False  # set to True for speed up without training\n\nif save_to_gdrive:\n    drive.mount(\'/content/gdrive\')\n    output_dir = os.path.join("/content/gdrive/My Drive", output_dir)\n\nen_vocab_file = os.path.join(output_dir, "en_vocab")\nzh_vocab_file = os.path.join(output_dir, "zh_vocab")\ncheckpoint_path = os.path.join(output_dir, "checkpoints")\nlog_dir = os.path.join(output_dir, \'logs\')\n\nif dataset_in_gdrive:\n    download_dir = os.path.join(output_dir, "tensorflow-datasets/downloads")\nelse:\n    download_dir = "tensorflow-datasets/downloads"\n\n# print("Save result to", output_dir)\nclear_output()'}, {'identified': None, 'updated_code': None}, {'identified': '- The exception class in the except statement should be surrounded by parentheses', 'updated_code': 'try:\n    point[0] = 20\nexcept (TypeError) as er:\n    print("TypeError:", er)\nelse:\n    raise'}, {'identified': None, 'updated_code': None}, {'identified': "- The function name 'Freq_plot' should be lowercase and separated by underscores", 'updated_code': 'def freq_plot(CR, num_tests=2, Bayes=True):\n    # if Bayes==True, plot Bayesian estimate and Credible Interval \n    if Bayes:\n        column = \'Post_mean\'\n        LL = \'Cred_LL\'\n        UL = \'Cred_UL\'\n        title = "Bayesian: Posterior Mean and Credible Interval of Proportion Over Time"\n    if not Bayes:\n        column = \'CRate\'\n        LL = \'Conf_LL\'\n        UL = \'Conf_UL\'\n        title = "Frequentist: Mean and Confidence Interval of Proportion Over Time"\n\n    # Set plot color\n    cbPalette = ["#009E73", "#0072B2", "#E69F00", "#D55E00", "#CC79A7", "#F0E442", "#56B4E9", "#999999"]\n    fill_colors = makeTransparent(cbPalette)\n\n    # Plot settings\n    min_val = min(quantile(CR$Cred_LL, 0.01), quantile(CR$Conf_LL, 0.01))\n    max_val = max(quantile(CR$Cred_UL, 0.99), quantile(CR$Conf_UL, 0.01))\n    max_days = quantile(CR$Day, 0.8)\n\n    data = CR[CR$Test_group == 0, ]\n    p <- plot(data$Day, data[column], type="l", lwd=3, col="red", lty=1, ylim=c(min_val, max_val),\n              main=title,\n              xlab=\'Days after tests start\', ylab=\'Proportion\')\n    polygon(c(data$Day, rev(data$Day)), c(data[LL], rev(data[UL])),\n            col=rgb(1, 0, 0, 0.1), border=NA)\n\n    abline(h=0)\n\n    for k in 1:num_tests:\n        data = CR[CR$Test_group == k, ]\n        lines(data$Day, data[column], type="l", lwd=3, col=cbPalette[k], lty=k+1)\n        polygon(c(data$Day, rev(data$Day)), c(data[LL], rev(data[UL])),\n                col=fill_colors[k], border=NA)\n\n    legend_list = []\n    for k in 1:num_tests:\n        legend_list.append(paste0("Test ", k))\n    legend(max_days, max_val, legend=legend_list,\n           col=c("red", cbPalette[2:k]), lty=1:(k+1), cex=0.8, title="Test group")'}, {'identified': '- There is an unnecessary blank line after the import statements.', 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The comment block contains an unnecessary blank line before the first comment line', 'updated_code': '# ignore\n# Copyright 2019 Meng Lee @ leemeng.tw\n# Licensed under the Apache License, Version 2.0 (the "License");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an "AS IS" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- The comment should start with a space after the '#' symbol", 'updated_code': "# Some error checking we could have used in the earlier lesson\nif len(cat) != len(data):\n    print('ERROR: Data and categories are not the same length')\n    sys.exit(1)  # This means exit with a return code indicating a problem.\nelse:\n    print('No problem with array lengths.')"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- There should be a space after the '#' comment symbol", 'updated_code': '# load the best model LSTM\nLSTM = load_model("LSTMmodel_" + str(embedding_size) + \'_\' + str(hidden_size))'}, {'identified': '- The lines are too long and should be split into multiple lines', 'updated_code': 'gROOT.ProcessLine(\'\'\'std::vector<double> params0 =\n    sim0->findGammaCalibrationConstants()\'\'\')\ngROOT.ProcessLine(\'\'\'sim0->writeStateToDisk("CalibParams_4.txt")\'\'\')'}, {'identified': "- The function name 'outputFeatureMap' should be in lowercase with underscores separating words", 'updated_code': '### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef output_feature_map(image_input, tf_activation, activation_min=-1, activation_max=-1, plt_num=1):\n    # Here make sure to preprocess your image_input in a way your network expects\n    # with size, normalization, etc. if needed\n    # image_input =\n    # Note: x should be the same name as your network\'s tensorflow data placeholder variable\n    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n    activation = tf_activation.eval(session=sess, feed_dict={x: image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15, 15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6, 8, featuremap + 1)  # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap))  # displays the feature map number\n        if activation_min != -1 and activation_max != -1:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", vmin=activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min != -1:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", cmap="gray")'}, {'identified': "- The comment should have a space after the '#' character", 'updated_code': "# We save the dictionary of cantons associated with universities\n# Thus we won't need to make requests that have already been made to Google Maps next time we run this notebook!\nwith open('university_canton_dict.json', 'w') as fp:\n    json.dump(university_canton_dict, fp, indent=4)\n\nuniversity_canton_dict"}, {'identified': '- Variable names should be lowercase and words should be separated by underscores', 'updated_code': "### Treino - DROPNA\ntreino_dropna_slice_nome = treino_dropna['Nome'].str.split(',', expand=True)\ntreino_dropna['Primeiro Nome'] = treino_dropna_slice_nome[0]\ntreino_dropna_nome_slice = treino_dropna_slice_nome[1].str.split('.', expand=True, n=1)\ntreino_dropna_complemento_slice = treino_dropna_nome_slice[1].str.split('(', expand=True)\ntreino_dropna['Saudação'] = treino_dropna_nome_slice[0]\ntreino_dropna['Sobrenome'] = treino_dropna_complemento_slice[0]\ntreino_dropna['Complemento'] = treino_dropna_complemento_slice[1]"}, {'identified': None, 'updated_code': None}, {'identified': '- The lambda function is not surrounded by spaces', 'updated_code': "null_info_dataframe = reduce(lambda left, right: pandas.merge(left, right, on='index'), \n                             [percent_null,\n                              percent_filled,\n                              filled_count_series,\n                              null_count_series])"}, {'identified': "- The import statement for 'numpy' is missing", 'updated_code': "import numpy as np\n\nmatrix_1 = np.array([[4, 2, 1, 3, 5]])\nmatrix_2 = np.array([[4], [2], [1], [3], [5]])\nmatrix_3 = np.dot(matrix_1, matrix_2)\nmatrices_1 = [matrix_1, matrix_2, matrix_3]\nnames_1 = ['matrix_1', 'matrix_2', 'matrix_3']\n\nvisualize_multiplication(matrices_1, names_1)"}, {'identified': '- The semicolon at the end of the line should be replaced with a line break', 'updated_code': 'fig = plt.figure(figsize=(12, 5))\nax = fig.gca()\nax.plot(x, invlogit(f_true), \'dodgerblue\', lw=3, label="True rate")\nax.plot(x, y + np.random.randn(n) * 0.01, \'ko\', ms=3, label="Observed data")\nax.set_xlabel("X")\nax.set_ylabel("y")\nplt.legend()'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The comment on line 4 should start with a capital letter', 'updated_code': "# Here's the number of streets away:\nnum_streets_away = abs(42 - 34)\n\n# Compute the number of avenues away in a similar way:\n# Compute the number of avenues away in a similar way:\n# YOUR CODE HERE\nraise NotImplementedError()\n\nstreet_length_m = 80\navenue_length_m = 274\n\n# Now we compute the total distance Chunhua must walk.\nmanhattan_distance = street_length_m * num_streets_away + avenue_length_m * num_avenues_away\n\n# We've included this line so that you see the distance\n# you've computed when you run this cell. You don't need\n# to change it, but you can if you want.\nmanhattan_distance"}, {'identified': 'None', 'updated_code': '# display first 5 rows of data frame with new index\ndata.iloc[0:5]'}, {'identified': None, 'updated_code': None}, {'identified': "- The list comprehension in the first line is missing whitespace around the 'for' and 'in' keywords", 'updated_code': 'most_freq_words = [w for w, freq in sorted_list[:200]]\nidx_most_freq = [tokens[i] for i in most_freq_words]'}, {'identified': "- There is no space after the '#' character in the comment", 'updated_code': '# If we use the ordinary mean() function, we get "nan" meaning "can\'t calculate on this array".\nprint(np.mean(data2))'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The commented out line is not indented correctly', 'updated_code': '# corrs = [full(iteration)[1,0] for iteration in itervars]\ncorrs[:10]'}, {'identified': None, 'updated_code': None}, {'identified': 'None', 'updated_code': "boulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,\n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n\n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n\n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n\n    # yoga\n    'yoga': boulder_6b / 5,\n\n    # ARC\n    'arc': boulder_6b_plus / 5,\n\n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n\n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%': boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n\n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n\n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n\n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n\n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit']\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+']\n\n\ndef calc_score(row):\n    s = [row[key] * scores_translator[key] for key in scores_translator]\n    return sum(s)\n\n\ndef calc_hangboard_scores(row):\n    s = [row[key] * scores_translator[key] for key in hangboard]\n    return sum(s)\n\n\ndef calc_gym_scores(row):\n    s = [row[key] * scores_translator[key] for key in gymnastics]\n    return sum(s)\n\n\ndef calc_climbing_scores(row):\n    s = [row[key] * scores_translator[key] for key in climbing]\n    return sum(s)\n\n\ndef calc_arc_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['arc']]\n    return sum(s)\n\n\ndef calc_technique_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['tech', 'visual']]\n    return sum(s)\n\n\ndef calc_power_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['power', 'speed', 'speed+']]\n    return sum(s)\n\n\ndef calc_yoga_scores(row):\n    s = [row[key] * scores_translator[key] for key in ['yoga']]\n    return sum(s)"}, {'identified': "- There is no space between the percentage sign and 'pylab' in the first line", 'updated_code': '% pylab inline\n\n# Training Data\nX = np.array([[4, 1],\n              [1, 2],\n              [5, 1],\n              [3, 2],\n              [5, 2],\n              [4, 3]])'}, {'identified': '- The with statement is not indented by four spaces', 'updated_code': "num_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    average_loss = 0\n\n    for step in range(num_steps):\n        batch_data, batch_labels = generate_batch(\n            batch_size, num_skips, skip_window)\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += l\n\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss = average_loss / 2000\n\n            # The average loss is an estimate of the loss over the last 2000 batches.\n            print('Average loss at step %d: %f' % (step, average_loss))\n            average_loss = 0\n\n        # note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log = 'Nearest to %s:' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = '%s %s,' % (log, close_word)\n                print(log)\n\n    final_embeddings = normalized_embeddings.eval()"}, {'identified': "- Comments should have a space after the '#' character", 'updated_code': '# graficamos la longitud de pétalo para el primer tipo de flores\n\nfigure(1)\nscatter(0.1 * randn(50, 1), caracteristicas[1:50, 1])\nscatter(1 + 0.1 * randn(50, 1), caracteristicas[51:100, 1])\nscatter(2 + 0.1 * randn(50, 1), caracteristicas[101:150, 1])\nylabel("Longitud del Pétalo (cm)")\n\n# graficamos la anchura del pétalo\n\nfigure(2)\n# comment for the second figure\n\n# graficamos la longitud del sépalo\n\nfigure(3)\n# comment for the third figure\n\n# graficamos la anchura del sépalo\n\nfigure(4)'}, {'identified': '- The import statement should be at the top of the file, before any other code', 'updated_code': 'import numpy as np\n\n# 2D matrix is multiplied elementwise by the scalar A. The 2nd line and indent is for clarity, not required.\nE = np.array([[1, 2, 3],\n              [4, 5, 6]])\nA = 10\nJ = E * A\nprint("J:", (J))'}, {'identified': None, 'updated_code': None}, {'identified': '- The code block inside the first plt.subplot is not indented with 4 spaces', 'updated_code': "sparse_data = my_spca.transform(X)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(121)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nnew_pc_cen = sparse_data - sparse_data.mean(0, keepdims=True)\ncov = new_pc_cen.T @ new_pc_cen / (new_pc_cen.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\n\nplt.subplot(122)\nplt.scatter(new_pc[y == 0, 0], new_pc[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(new_pc[y == 1, 0], new_pc[y == 1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.005, head_length=0.005, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.005, head_length=0.005, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.show()"}, {'identified': None, 'updated_code': None}, {'identified': "- Comments should have a space after the '#' character", 'updated_code': '# graficamos los datos nuevamente\nscatter(x[1:50, 1], x[1:50, 2], alpha=0.25, color="b")\nscatter(x[51:100, 1], x[51:100, 2], alpha=0.25, color="r")\nscatter(x[101:150, 1], x[101:150, 2], alpha=0.25, color="g")\n\nxlabel("Longitud del Pétalo (cm)")\nylabel("Anchura del Pétalo (cm)")\ngrid("on")\n\n\n# obtenemos los vectores con las características promedio para cada una de las clases de flores\nprom_1 = mean(x[1:50, :], 1)\nprom_2 = mean(x[51:100, :], 1)\nprom_3 = mean(x[101:150, :], 1)\n\n# graficamos los vectores\nquiver(prom_1[1, 1], prom_1[1, 2], angles="xy", scale_units="xy", scale=1, color="b")\nquiver(prom_2[1, 1], prom_2[1, 2], angles="xy", scale_units="xy", scale=1, color="r")\nquiver(prom_3[1, 1], prom_3[1, 2], angles="xy", scale_units="xy", scale=1, color="g")'}, {'identified': '- The commented lines are not properly indented', 'updated_code': "# train.drop(['sentiment','seven_days'],axis=1,inplace=True)\n# test.drop(['sentiment','seven_days'],axis=1,inplace=True)\n\nresult_test = []\nresult_train = []\ntot = 0\n\nfor string in ['share', 'comment', 'zan', 'content_len', '链接', '//@', '@', '#', '【', '《', '\\[']:\n    temp = []\n    for i in test[string + '_histogram']:\n        if isinstance(i, int):\n            temp.append(np.zeros(shape=8))\n            tot += 1\n        else:\n            temp.append(i[0])\n    result_test.append(np.asarray(temp))\n    temp = []\n    for i in train[string + '_histogram']:\n        temp.append(i[0])\n    result_train.append(np.asarray(temp))\n\n    train.drop(string + '_histogram', axis=1, inplace=True)\n    test.drop(string + '_histogram', axis=1, inplace=True)\n\ntrain.drop(['pid', 'uid'], inplace=True, axis=1)\ntest.drop(['pid', 'uid'], inplace=True, axis=1)\n\ntrain_y = train[['share', 'comment', 'zan']].values\ntrain.drop(['share', 'comme·nt', 'zan'], axis=1, inplace=True)\ntrain_x = train.values\ntest_x = test.values\n\nfor i in result_train:\n    train_x = np.c_[train_x, i]\n\nfor i in result_test:\n    test_x = np.c_[test_x, i]\n\nnp.save('processed_data/train3_np', train_x)\nnp.save('processed_data/test3_np', test_x)\nnp.save('processed_data/target3_np', train_y)"}, {'identified': '- The import statements should be at the top of the code, before the comments', 'updated_code': 'import sklearn\nimport pandas as pd\nfrom sklearn import linear_model\nfrom sklearn import cross_validation\nfrom sklearn.metrics import mean_squared_error\n\nerrors = []\nk_folds = 10\n\nfor s in getListOfFiles(\'Data/\'):\n    data = getListFromAFile("Data/" + s)\n    kf = cross_validation.KFold(n=len(data[0]), n_folds=k_folds, shuffle=False, random_state=None)\n    error = 0\n    regression = linear_model.LinearRegression()\n    \n    for train_index, test_index in kf:\n        X_train, X_test = data[0][train_index], data[0][test_index]\n        y_train, y_test = data[1][train_index], data[1][test_index]\n        regression.fit(X_train.reshape(-1, 1), y_train)\n        error = error + mean_squared_error(y_test, regression.predict(X_test.reshape(-1, 1)))\n    \n    error = error / k_folds\n    createPlots(data[0].reshape(-1, 1), data[1], xlabel="x-axis -->", ylabel="y-axis -->", title="s:" + s + "|degree:linear reegression", plotterRef=regression.predict)\n    errors.append({\'file\': s, \'mse\': error})\n\nprint(errors)\ndf = pd.DataFrame(errors)\nprint(df)'}, {'identified': '- There are unnecessary blank lines and comments that should be removed', 'updated_code': "import sys\nimport math\nimport logging\nfrom collections import defaultdict\n\nlog_probability = 0\ny_predict = []\ny_actual = []\nfirst_round = True\n\nfor sent in test_sents:\n    log_probability = 0\n    first_round = True\n    \n    for i in range(len(sent)):\n        word = sent[i][0]\n        \n        if word in counter.count_xy:\n            max_probability = 0\n            \n            for label in list(counter.count_xy[word]):\n                emission = float(counter.count_xy[word][label]) / float(counter.count_y[label])\n                \n                if first_round:\n                    y_2 = '*'\n                    y_1 = '*'\n                    first_round = False\n                \n                bigram = y_2 + ' ' + y_1\n                trigram = y_2 + ' ' + y_1 + ' ' + label\n                parameter = 0.0000000001\n                \n                if trigram in counter.trigram_counts:\n                    parameter = float(counter.trigram_counts[trigram]) / float(counter.bigram_counts[bigram])\n                \n                probability = parameter * emission\n                \n                if probability > max_probability:\n                    max_probability = probability\n                    arg_max = label\n            \n            log_probability = log_probability + math.log(max_probability)\n            y_actual.append(sent[i][2])\n            y_predict.append(arg_max)\n            y_2 = y_1\n            y_1 = arg_max\n        \n        else:\n            y_predict.append('O')\n            y_actual.append(sent[i][2])"}, {'identified': '- The use of dollar sign `$` for accessing variables violates PEP 8 conventions', 'updated_code': 'scale = sd(train.y)\ncenter = mean(train.y)\ntrain.y = (train.y - center) / scale\ntest.y = (test.y - center) / scale'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The class docstring should be surrounded by triple double quotes (""" instead of \'\'\')', 'updated_code': 'class ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {\'a\': [1, 5, 2, 5, 2, 8],\n               \'b\': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key=\'a\')\n    >> data[\'a\'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        # if self.key == \'playlist_pid\':\n        #     from IPython.core.debugger import set_trace; set_trace()\n        return data_dict[:, [self.key]].astype(np.int64)\n\n    def get_feature_names(self):\n        return [dataset.columns[self.key]]'}, {'identified': '- The import statement should be on a separate line after the comment', 'updated_code': '# TODO: Implement SelectorDIC in module my_model_selectors.py\nfrom my_model_selectors import SelectorDIC\n\ntraining = asl.build_training(features_ground)  # Experiment here with different feature sets defined in part 1\nsequences = training.get_all_sequences()\nXlengths = training.get_all_Xlengths()\nfor word in words_to_train:\n    start = timeit.default_timer()\n    model = SelectorDIC(sequences, Xlengths, word,\n                        min_n_components=2, max_n_components=15, random_state=14).select()\n    end = timeit.default_timer() - start\n    if model is not None:\n        print("Training complete for {} with {} states with time {} seconds".format(word, model.n_components, end))\n    else:\n        print("Training failed for {}".format(word))'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- There is no code provided after the assignment operator '=' for the variable 'W'", 'updated_code': 'W = []  # Your code here to arrange the average vectors into a matrix called W'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- Comment should start with a capital letter and end with a period', 'updated_code': "# X_numeric has duplicate columns. The code below removes the duplicate columns.\n_, i = np.unique(x_numeric.columns, return_index=True)\nx_num_cov = x_numeric.iloc[:, i]\n\nx_num_cov.to_csv('Numerical_FS.csv')\n\nx_num_cov.shape"}, {'identified': "- The comment after the line with 'obs' is not indented with four spaces", 'updated_code': '# include the observations, which are Bernoulli\nwith model:\n    obs = pm.Bernoulli("obs", p, observed=occurrences)\n    # to be explained in chapter 3\n    step = pm.Metropolis()\n    trace = pm.sample(18000, step=step)\n    burned_trace = trace[1000:]'}, {'identified': '- The import statement should be on a separate line', 'updated_code': "from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer(\n    use_idf=True,\n    norm='l2',\n    smooth_idf=True\n)\nprint(tfidf.fit_transform(count.fit_transform(docs))\n      .toarray())"}, {'identified': None, 'updated_code': None}, {'identified': "- The function 'marginal' is not indented with 4 spaces", 'updated_code': 'def marginal(self):\n    marginal = np.prod(np.array(list(self.in_msgs.values())), axis=0)\n    marginal *= self.observed_state\n    marginal /= np.sum(marginal)\n    return marginal\n\n\nVariable.marginal = marginal'}, {'identified': None, 'updated_code': None}, {'identified': '- The line is too long and should be split into multiple lines', 'updated_code': "a = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 0].Invalid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 0].Invalid).tconfint_mean()"}, {'identified': '- The command should be written on a single line', 'updated_code': '% bash\ngrep "^def" babyweight/trainer/model.py'}, {'identified': None, 'updated_code': None}, {'identified': '- There is inconsistent indentation in the code comments', 'updated_code': "# DATAFRAME STRUCTURE\nBOOKINGS_DF_STRUCT = pd.read_csv(BOOKINGS, error_bad_lines=False,\n                                encoding='UTF8', sep='^', nrows=1)\n\n# Dataframe using suggested columns\nBOOKINGS_DF_EX = pd.read_csv(BOOKINGS, error_bad_lines=False,\n                            encoding='UTF8', sep='^', usecols=['arr_port', 'pax'])"}, {'identified': None, 'updated_code': None}, {'identified': "- There are unnecessary and inconsistent uses of multiple consecutive hash characters ('#')", 'updated_code': '# Test test test\n# Message from X_prior to X\nX_prior.reset()\nX.reset()\n\nX_prior.send_ms_msg(X)\nassert np.allclose(list(X.in_msgs.values()), [-0.05129329, -2.99573227])\n\n# Message from Z_prior to Z\nZ_prior.reset()\nZ.reset()\n\nZ_prior.send_ms_msg(Z)\nassert np.allclose(list(Z.in_msgs.values()), [-0.22314355, -1.60943791])\n\n# Message from Y_cond to Y\nY_cond.reset()\nY.reset()\n\nY_cond.receive_msg(X, X_prior.f)  # Simulating that Y_cond received all necessary messages from X\nY_cond.receive_msg(Z, Z_prior.f)  # Simulating that Y_cond received all necessary messages from Z\n\nY_cond.send_ms_msg(Y)\nassert np.allclose(list(Y.in_msgs.values()), [1.74989999, 0.79332506])'}, {'identified': "- The variable names 'MC_fraction' and 'DATA_fraction' should be lowercase with words separated by underscores", 'updated_code': "mc_fraction = 'pileUpFilterEfficiency_MC.png'\ndata_fraction = 'pileUpFilterEfficiency_DATA.png'\nImage(filename=pathname + mc_fraction)"}, {'identified': None, 'updated_code': None}, {'identified': 'None', 'updated_code': "asl.df['left-x-mean'] = asl.df['speaker'].map(df_means['left-x'])\nasl.df.head()"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- There should be whitespace around the assignment operator '='", 'updated_code': 'ot_model_strict = wot.ot.OTModel(adata, epsilon=0.05, lambda1=3, lambda2=50)\ntmap_anno_strict = ot_model_strict.compute_transport_map(7, 7.5)'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- The import statement for 'np' is missing", 'updated_code': "import numpy as np\n\nsw_inv = np.linalg.inv(sw)\nidentity_matrix = np.matmul(sw, sw_inv)\n\nprint('Sw:')\nprint(sw)\n\nprint('Inverted Sw:')\nprint(sw_inv)\n\nprint('Identity matrix')\nprint(identity_matrix)"}, {'identified': "- The comment line should start with a single '#' not '##'", 'updated_code': '# display first 5 rows of data frame with new index\ndata.iloc[0:5]'}, {'identified': None, 'updated_code': None}, {'identified': '- The import statement should be placed at the top of the file, before any other code', 'updated_code': 'from collections import defaultdict, namedtuple\nfrom prompts import new_prompt\nfrom IPython.display import HTML\n\n\ntags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\n\n# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\n\nmfc_table = defaultdict(list)\nFakeState = namedtuple("FakeState", "name")\n\n\nclass MFCTagger:\n    # NOTE: You should not need to modify this class or any of its methods\n    missing = FakeState(name="<MISSING>")\n\n    def __init__(self, table):\n        self.table = defaultdict(lambda: MFCTagger.missing)\n        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n\n    def viterbi(self, seq):\n        """This method simplifies predictions by matching the Pomegranate viterbi() interface"""\n        return 0., list(enumerate(["<start>"] + [self.table[w] for w in seq] + ["<end>"]))\n\n\n# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n# the same as the emission probabilities) and use it to fill the mfc_table\n\nword_counts = pair_counts(words, tags)\n\nfor key, val in word_counts.items():\n    mfc_table[key] = val.most_common(1)[0][0]\n\n\n# DO NOT MODIFY BELOW THIS LINE\nmfc_model = MFCTagger(mfc_table)  # Create a Most Frequent Class tagger instance\n\nassert len(mfc_table) == len(data.training_set.vocab), ""\nassert all(k in data.training_set.vocab for k in mfc_table.keys()), ""\nassert sum(int(k not in mfc_table) for k in data.testing_set.vocab) == 5521, ""\nHTML(\'<div class="alert alert-block alert-success">Your MFC tagger has all the correct words!</div>\')'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The commented out line is indented with a tab instead of spaces', 'updated_code': '# i = 0\ndraw_rule_and_reg_id(ids_cos_sim_high_3_wk[i])\ni += 1'}, {'identified': None, 'updated_code': None}, {'identified': "- The comments should use a single '#' character instead of triple '#' characters", 'updated_code': '# Test test test\n# Simulate a single forward pass\n\nX_prior.reset()\nX.reset()\nZ_prior.reset()\nZ.reset()\nY_cond.reset()\nY.reset()\n\nX_prior.send_sp_msg(X)\nZ_prior.send_sp_msg(Z)\nX.send_sp_msg(Y_cond)\nZ.send_sp_msg(Y_cond)\nY_cond.send_sp_msg(Y)\n\nassert np.allclose(X.marginal(), [0.95, 0.05])\nassert np.allclose(Z.marginal(), [0.8, 0.2])\nassert np.allclose(Y.marginal(), [0.821024, 0.178976])'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The comment on the first line should not be indented', 'updated_code': "# split our dataset into training / testing sets\ntrain_test_split = int(np.ceil(2 * len(y) / float(3)))  # set the split point\n\n# partition the training set\nX_train = X[:train_test_split, :]\ny_train = y[:train_test_split]\n\n# keep the last chunk for testing\nX_test = X[train_test_split:, :]\ny_test = y[train_test_split:]\n\n# NOTE: to use keras's RNN LSTM module our input must be reshaped to [samples, window size, stepsize]\nX_train = np.asarray(np.reshape(X_train, (X_train.shape[0], window_size, 1)))\nX_test = np.asarray(np.reshape(X_test, (X_test.shape[0], window_size, 1)))"}, {'identified': None, 'updated_code': None}, {'identified': 'None', 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The comment should start with a capital letter', 'updated_code': "'''Total sum of independent FP yield per 100 fissions'''\n\nprint('Total sum = ', round(sum(fpy.yield_percent for fpy in cfpy_az), 1))"}, {'identified': "- There should be a space after the '#' character for the comment", 'updated_code': '# print the zeroth element\nprint(a_list[3])'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- The comment should start with a single '#', not '###'", 'updated_code': '# Test test test\n\nnodes = [X_prior, X, Z_prior, Z, Y_cond, Y]\nfor n in nodes:\n    n.reset()\n\nX_prior.pending.add(X)\nZ_prior.pending.add(Z)\nY.pending.add(Y_cond)\n\nsum_product(nodes)\nassert np.allclose(Y.marginal(), [0.821024, 0.178976])'}, {'identified': '- There should be a space after each comma in the plt.plot() function calls', 'updated_code': 'X = np.linspace(-1, 1, 256)\nplt.plot(X + 0.5, -Hart(X, 1)[1] + 0.5, "k-", label="H = 1")\n#plt.plot(X + 0.5, -Hart(X, 10)[1] + 0.5, "b-", label="H = 10")\n#plt.plot(X + 0.5, -Hart(X, 0.1)[1] + 0.5, "r-", label="H = 0.1")\nplt.xlabel("z")\nplt.ylabel("$B_{x}$")\nplt.title("Campo magnetico Hartmann")\nplt.grid(True)\nplt.legend()\nplt.savefig("Magnetico_Hartmann.png")'}, {'identified': '- There should be a space after each comma in the sns.barplot() function', 'updated_code': "ax = sns.barplot(x=0, y=1, hue=2, data=degree_df)\nax.set_xlabel('Grado de nodo')\nax.set_ylabel('Cantidad de nodos con ese grado')\nax.set_title('Analisis sacando 2% distinguido')\n\nplt.show()"}, {'identified': None, 'updated_code': None}, {'identified': "- The function name 'can_send_message' should be in lowercase with words separated by underscores", 'updated_code': 'def can_send_message(sender, receiver):\n    for n in sender.neighbours:\n        if n is not receiver and n not in sender.in_msgs:\n            return False\n    \n    return True\n\n\n# Do the results make sense?\nprint(can_send_message(X, X_prior))\nprint(can_send_message(X_prior, X))'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- Imports should be organized in separate lines, with each import on its own line', 'updated_code': 'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport tensorflow\nimport keras\nimport itertools\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nfrom sklearn.feature_extraction import FeatureHasher, DictVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\n\nfrom bokeh.charts import Bar, output_file, show, output_notebook\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom IPython.core.display import Image, display\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\n\noutput_notebook()\n\n\n%matplotlib inline'}, {'identified': '- There should be a space after the opening parenthesis of the print statement', 'updated_code': 'print(np.linalg.inv(A))'}, {'identified': '- The code should be split into multiple lines for readability', 'updated_code': "import sms\nimport pandas as pd\n\na = RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid).tconfint_mean()"}, {'identified': '- There are too many blank lines before and after the import statements', 'updated_code': "# TODO: create required RNN model\n# import keras network libraries\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n# given - fix random seed - so we can all reproduce the same results on our default time series\nnp.random.seed(0)\n\n# TODO: implement build_part1_RNN in my_answers.py\nfrom my_answers import build_part1_RNN\nmodel = build_part1_RNN(window_size)\n\n# build model using keras documentation recommended optimizer initialization\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# compile the model\nmodel.compile(loss='mean_squared_error', optimizer=optimizer)"}, {'identified': '- There are no blank lines between the import statements and the first line of code', 'updated_code': "import matplotlib.pyplot as plt\nfrom prompts import new_prompt\n\n\n# Dynamic critic of grandiant descent between Discriminator and Generator.\nfilename = 'GANDeconv_t9999_h100_train2_ep36'\nhidden_dim = 100\n\nG, D, train_hist = GAN_CelebA.load_checkpoint(filename, hidden_dim, use_cuda=use_cuda)\nepoch_num = len(train_hist['D_losses'])\nGAN_CelebA.show_result(G, D, epoch_num, hidden_dim, show=True, save=True, path='figures/' + filename + '.pdf', use_cuda=use_cuda)\n\nplt.plot(range(0, epoch_num), train_hist['D_losses'], label='D_loss')\nplt.plot(range(0, epoch_num), train_hist['G_losses'], label='G_loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('DCGAN dynamic, total time: ' + str(int(train_hist['total_ptime'][-1] / 60)) + ' minutes')\nplt.legend()\nplt.savefig('figures/' + filename + '_Loss.pdf')\nplt.show()"}, {'identified': None, 'updated_code': None}, {'identified': "- Imports 'widgets', 'json', 'time', 'Thread', and 'validate' should not be on the same line", 'updated_code': None}, {'identified': "- There should be whitespace around the '=' operator in the variable assignment 'y_test_predict'", 'updated_code': 'y_test_predict = clf.predict(X_test)\nnp.size(y_test_predict)'}, {'identified': None, 'updated_code': None}, {'identified': '- Imports should be on separate lines', 'updated_code': 'import os\nimport sys\nfrom ROOT import gROOT\nimport numpy as np\n\nsys.path.insert(0, os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Support\'))\nsys.path.insert(0, os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Unfolding\'))\nfrom Utilities import pause\nfrom Root import CalibParams\n\noutPath = "/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/Inputs/"\nrspPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Simulated/PHS/ResponseMatrices/simSideResponse20Mil.root\'\ncalPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/CalibData/\'\n\nos.chdir(outPath)\nprint(\'Currently working in: \\n {}\'.format(os.getcwd()))\n\ndetNames = {\n    0: \'Det0\'}  # , 2: \'Det45\', 4: \'Det90\'}\ncalNames = {\n    0: \'CalibParams_0.txt\'}  # , 2: \'CalibParams_2.txt\', 4: \'CalibParams_4.txt\'}'}, {'identified': "- There should be whitespace before and after the assignment operator for the variable 'save_file'", 'updated_code': "save_file = './train_model_best.ckpt'\nsaver = tf.train.Saver()\n\nwith tf.Session() as session:\n    \n    saver.restore(session, save_file)\n    \n    feed_dict = {\n        tf_train_dataset: X2_norm,\n        tf_keep_prob: 1\n    }\n    \n    proba = session.run(train_prediction, feed_dict)"}, {'identified': '- There should be two blank lines between the function definition and the code inside the function', 'updated_code': "def prepare_analysis(df):\n    \n    acc_2 = pd.DataFrame(np.transpose(df))\n\n    acc_2.columns = ['Acc_train', 'Beta', 'Learning_Rate', 'Learning_Decay', 'Acc_valid']\n    acc_2['Group'] = acc_2['Beta'] + acc_2['Learning_Rate'] + acc_2['Learning_Decay']\n    \n    return acc_2\n\n\nacc = prepare_analysis(all_acc)\nacc['counter'] = acc.groupby(['Beta', 'Learning_Rate', 'Learning_Decay']).cumcount() + 1\nacc = acc.sort_values(['Beta', 'Learning_Rate', 'Learning_Decay', 'counter'])"}, {'identified': None, 'updated_code': None}, {'identified': "- The function 'plot' should be called from the 'pyplot' module, not just 'pyplot'", 'updated_code': "pyplot.plot(x, rho, color='blue', ls='--', lw=3)\npyplot.ylim(0, 52)"}, {'identified': 'None', 'updated_code': 'import numpy as np\n# Set up parameters:\nk_size = 3\nvertex_ratio_h = .45\nvertex_ratio_v = .60\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here:\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline(image, \n                                   k_size=k_size,\n                                   low_thresh=low_thresh,\n                                   high_thresh=high_thresh,\n                                   L2gradient=L2gradient,\n                                   rho=rho,\n                                   theta=theta,\n                                   min_votes=min_votes,\n                                   min_line_len=min_line_len,\n                                   max_line_gap=max_line_gap,\n                                   angle=angle,\n                                   angle_thresh=angle_threshold,\n                                   debug=False)\n    return result'}, {'identified': "- The comment above the import statements should have a space after the '#' character", 'updated_code': '# Import \'GridSearchCV\', \'make_scorer\', and any other necessary libraries\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nk = 999\n# Initialize the classifier\nclf = RandomForestClassifier()\n\n# Create the parameters list you wish to tune\nparameters = {\'n_estimators\': [10, 50, 150, 300],\n              \'criterion\': [\'gini\', \'entropy\'],\n              \'max_features\': [\'auto\', \'sqrt\', \'log2\', None],\n              \'random_state\': [k]}\n\n# Make an fbeta_score scoring object\nscorer = make_scorer(fbeta_score, beta=0.5)\n\n# Perform grid search on the classifier using \'scorer\' as the scoring method\ngrid_obj = GridSearchCV(clf, parameters, scorer)\n\n# Fit the grid search object to the training data and find the optimal parameters\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and optimized model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-after scores\nprint("Unoptimized model\\n------")\nprint("Accuracy score on testing data: {:.4f}".format(accuracy_score(y_test, predictions)))\nprint("F-score on testing data: {:.4f}".format(fbeta_score(y_test, predictions, beta=0.5)))\nprint("\\nOptimized Model\\n------")\nprint("Final accuracy score on the testing data: {:.4f}".format(accuracy_score(y_test, best_predictions)))\nprint("Final F-score on the testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta=0.5)))'}, {'identified': "- The variables 'x_test' and 'y_test' are assigned values in the same line, violating the convention of one assignment per line", 'updated_code': "import numpy as np\n\nx_test, y_test = msig.generate()\ny_hat, *args = model2.predict(x_test, batch_size=batch_size)\nmodel2.reset_states()\ny_pred = np.argmax(y_hat, axis=-1)\nprint('x_test', x_test.shape, '{:>9.4f} {:>9.4f}'.format(np.min(x_test), np.max(x_test)))\nprint('y_test', y_test.shape)\nprint('y_hat ', y_hat.shape, '{:>9.4f} {:>9.4f}'.format(np.min(y_hat), np.max(y_hat)))\nprint('y_pred', y_pred.shape, '{} {}'.format(np.min(y_pred), np.max(y_pred)))\nfor i, arg in enumerate(args):\n    print(i, arg.shape, '{:>9.4f} {:>9.4f}'.format(np.min(arg), np.max(arg)))"}, {'identified': "- The comments should start with a space after the '#' symbol", 'updated_code': '# TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it\n# get an appropriately sized chunk of characters from the text\nstart_inds = []\n\n# save output\noutput_file = open(\'text_gen_output/RNN_large_textdata_output.txt\', \'w\')  # create an output file to write to\n\n# load weights\nmodel.load_weights(\'model_weights/best_RNN_large_textdata_weights.hdf5\')\nfor s in start_inds:\n    start_index = s\n    input_chars = text[start_index: start_index + window_size]\n\n    # use the prediction function\n    predict_input = predict_next_chars(model, input_chars, num_to_predict=100)\n\n    # print out input characters\n    line = \'-------------------\' + \'\\n\'\n    print(line)\n    output_file.write(line)\n\n    input_line = \'input chars = \' + \'\\n\' +  input_chars + \'"\' + \'\\n\'\n    print(input_line)\n    output_file.write(input_line)\n\n    # print out predicted characters\n    predict_line = \'predicted chars = \' + \'\\n\' +  predict_input + \'"\' + \'\\n\'\n    print(predict_line)\n    output_file.write(predict_line)\noutput_file.close()'}, {'identified': None, 'updated_code': None}, {'identified': '- There are inconsistent uses of indentation throughout the code', 'updated_code': "def LeNetTrafficSign(x, n_classes):\n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n\n    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n    # MODIFIED: Layer 1: Convolutional. Input = 32x32x3 (rgb). Output = 28x28x6. shape (5,5,1,6)->(5,5,3,6)\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n\n    # Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Layer 3: Convolutional. Output = 1x1x400.\n    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 400), mean=mu, stddev=sigma))\n    conv3_b = tf.Variable(tf.zeros(400))\n    conv3 = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='VALID') + conv3_b\n\n    # Activation.\n    conv3 = tf.nn.relu(conv3)\n\n    # Flatten. Input = 5x5x16. Output = 400.\n    fc00 = flatten(conv2)\n    # Flatten. Input = 1x1x400. Output = 400.\n    fc01 = flatten(conv3)\n\n    # Concatenate the two convs, Output = 800\n    fc0 = tf.concat([fc01, fc00], 1)\n\n    fc0 = tf.nn.dropout(fc0, keep_prob)\n\n    # Layer 4: Fully Connected. Input = 800. Output = 400.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(800, 400), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(400))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n\n    # Activation.\n    fc1 = tf.nn.relu(fc1)\n\n    # Dropout, to prevent overfitting\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n\n    # Layer 5: Fully Connected. Input = 400. Output = n_classes, which is 43.\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(400, n_classes), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc1, fc2_W) + fc2_b\n\n    return logits"}, {'identified': None, 'updated_code': None}, {'identified': '- There are unnecessary empty lines between the comments', 'updated_code': '# Train your model here.\n# Feel free to use as many code cells as needed.'}, {'identified': "- The variable name 'params_list' should be 'params_list' according to PEP 8 variable naming conventions", 'updated_code': 'params_list = []\nfor key, value in best_model.params.items():\n    params_list.append(str(key) + " = " + str(value[\'actual\']))'}, {'identified': "- The variable name 'parametersDecisionTree' should be written in lowercase with underscores between words", 'updated_code': "parameters_decision_tree = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': range(1, 51)\n}\n\n(tiempo_decision_tree, grid_decision_tree) = correr_y_mostrar(\n    DecisionTreeClassifier(),\n    parameters_decision_tree,\n    5,\n    5\n)\n\nparameters_decision_tree_2 = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': range(1, 51),\n    'min_samples_split': range(2, 30)\n}\n\n(tiempo_decision_tree_2, grid_decision_tree_2) = correr_y_mostrar(\n    DecisionTreeClassifier(),\n    parameters_decision_tree_2,\n    5,\n    5\n)"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- The comments 'ignore' and '# 讀取之前建立的 emb layer 來保證每次的 emb_inp, emb_tar 結果都一樣' should not be indented", 'updated_code': '#ignore\n# 讀取之前建立的 emb layer 來保證每次的 emb_inp, emb_tar 結果都一樣\nd_model = 4\nvocab_size_en = subword_encoder_en.vocab_size + 2\nvocab_size_zh = subword_encoder_zh.vocab_size + 2\n\nemb_en_model_path = os.path.join(output_dir, "demo_emb_en_model.h5")\nemb_zh_model_path = os.path.join(output_dir, "demo_emb_zh_model.h5")\n\n# en\nif not os.path.exists(emb_en_model_path):\n    demo_emb_en_model = tf.keras.Sequential()\n    demo_emb_en_model.add(tf.keras.layers.Embedding(vocab_size_en, d_model))\n    demo_emb_en_model.save(emb_en_model_path)\n    embedding_layer_en = demo_emb_en_model\nelse:\n    embedding_layer_en = tf.keras.models.load_model(emb_en_model_path)\n\n# zh\nif not os.path.exists(emb_zh_model_path):\n    demo_emb_zh_model = tf.keras.Sequential()\n    demo_emb_zh_model.add(tf.keras.layers.Embedding(vocab_size_zh, d_model))\n    demo_emb_zh_model.save(emb_zh_model_path)\n    embedding_layer_zh = demo_emb_zh_model\nelse:\n    embedding_layer_zh = tf.keras.models.load_model(emb_zh_model_path)\n\nclear_output()'}, {'identified': '- Several lines are commented out in the code. Commented code should be removed or uncommented.', 'updated_code': "t_min_max = (vsig.timestamps[0], vsig.timestamps[-1])\nlayer = '1'\nval_arrays = np.load(os.path.join(vsig.out_dir, 'valid_hidden_layer_' + layer + '_output.npy'))\nn_generations, _, n_neurons = val_arrays.shape\nncols = 1\nnrows = n_neurons // ncols\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3))\n\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes#[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(\n            vsig.timestamps[vsig.window_size-1:], \n            vsig.mixed_signal[vsig.window_size-1:], \n            marker='o', \n            c=y_pred_colors, \n            cmap=plt.get_cmap('coolwarm'), \n            vmin=-1, \n            vmax=1\n        )\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n        \n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, 'output', g + 1))\n    plt.show()"}, {'identified': "- Variable 'X_season' and 'y_season' should have spaces around the assignment operator '='", 'updated_code': "X_season = feature_copy[model_features]\n\ny_season = tourney_comp_ratings[tourney_comp_ratings['season_t'] == test_year]['game_result']\nX_season.shape"}, {'identified': '- There should be a blank line between the import statements and the first code block', 'updated_code': 'import numpy as np\nimport matplotlib.pyplot as plt\nfrom prompts import new_prompt\n\n\n# Hyper-params\ngamma = 0.95\nepsilon = 1e-4\n\n# The GRIDWORLD\nworld_size = 5\nterminal_states = [(0, 0), (world_size - 1, world_size - 1), (world_size - 2, world_size - 3), (2, world_size - int(world_size / 2))]\nactions, states, nextState = create_gridworld(world_size, terminal_states)\n\n# Initializations\nV_init = np.zeros((world_size, world_size), dtype=np.float)    # V(s) ... our value function estimate for PI\nPI_init = np.random.randint(low=0, high=4, size=(world_size, world_size), dtype=np.int)     # PI(s) ... our greedy policy\n\nprint("INITIALIZATION")\nprint("Initial value function V is filled with zeros whereas initial policy is random")\nprint("\\nV = \\n", np.round(V_init))\nprint("\\nPI = ")\nprint_policy(PI_init, terminal_states)\n\nPolIt_results = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon)\n\nprint("\\n\\nRESULTS FOR POLICY ITERATION -------------")\nprint("Policy found in {} iterations, where each policy evaluation lasted for k = {}".format(len(PolIt_results[1]), PolIt_results[1]))\nprint("\\nV = \\n", np.round(PolIt_results[0]))\nprint("\\nPI = ")\nprint_policy(PolIt_results[2], terminal_states)\n\nValIt_results = value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon)\n\nprint("\\n\\nRESULTS FOR VALUE ITERATION -------------")\nprint("Policy found in {} iterations".format(ValIt_results[1]))\nprint("\\nV = \\n", np.round(ValIt_results[0]))\nprint("\\nPI = ")\nprint_policy(ValIt_results[2], terminal_states)\n\nM_PolIt_results = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon, modified=True)\n\nprint("\\n\\nRESULTS FOR MODIFIED POLICY ITERATION -------------")\nprint("Policy found in {} iterations, where each policy evaluation lasted for k = {}".format(len(M_PolIt_results[1]), M_PolIt_results[1]))\nprint("\\nV = \\n", np.round(M_PolIt_results[0]))\nprint("\\nPI = ")\nprint_policy(M_PolIt_results[2], terminal_states)\n\nprint("\\n\\nEFFECT OF GAMMA ON CONVERGENCE SPEED")\n\nlogg = {"policy_iteration": [], "value_iteration": [], "M_policy_iteration": []}\n# For different values of gamma\ngammas = [0.50, 0.75, 0.90, 0.95]\nfor g in gammas:\n\n    # Run Policy Iteration\n    _, PolIt_k, _ = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, g, epsilon)\n    logg[\'policy_iteration\'].append(sum(PolIt_k))\n\n    # Run Value Iteration\n    _, ValIt_k, _ = value_iteration(V_init, PI_init, world_size, states, actions, nextState, g, epsilon)\n    logg[\'value_iteration\'].append(ValIt_k)\n\n    # Run Modified Policy Iteration\n    _, M_PolIt_k, _ = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, g, epsilon, modified=True)\n    logg[\'M_policy_iteration\'].append(sum(M_PolIt_k))\n\nplt.figure(figsize=(10, 4))\nplt.plot(gammas, logg[\'policy_iteration\'], label="Policy Iteration")\nplt.plot(gammas, logg[\'value_iteration\'], label="Value Iteration")\nplt.plot(gammas, logg[\'M_policy_iteration\'], label="Modified Policy Iteration")\nplt.title(\'Effect of gamma on convergence speed\', fontweight=\'bold\')\nplt.xlabel(\'Gamma\')\nplt.ylabel(\'Number of full prediction sweeps\')\nplt.legend(loc=\'best\')\nplt.show()'}, {'identified': '- The import statement and the assignment statement are not separated by a blank line', 'updated_code': 'from filterpy.common import Saver\n\npos_vel_filter = pos_vel_filter([0, .1], R=R, P=P, Q=Q, dt=1.)\nsaver = Saver(pos_vel_filter)\n\nfor i in range(1, 6):\n    pos_vel_filter.predict()\n    pos_vel_filter.update([i])\n    saver.save()  # save the current state'}, {'identified': '- There are unnecessary comments on the first two lines', 'updated_code': '# indeedians = from indeedemployeesnapshot yesterday today\n# group by ldap, full_name\n\n    #%%iql2\n    # indeedians = from indeedemployeesnapshot yesterday today\n    # group by ldap, full_name'}, {'identified': None, 'updated_code': None}, {'identified': "- The comment above the function 'parser' is not indented properly", 'updated_code': '# Load dataset and writing the custom parser\ndef parser(x):\n    return datetime.strptime(x, \'%m/%d/%y\')\n\n# Man gotta figure out the strptime function to see how it actually works\n# Date_parser always get called to pass a function understand the dates\nseries = read_csv("/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv",\n                  header=0,\n                  parse_dates=[0],\n                  index_col=0,\n                  squeeze=True,\n                  date_parser=parser)\nseries.head()'}, {'identified': None, 'updated_code': None}, {'identified': "- Multiple lines are under-indented inside the function 'optimize_theta'", 'updated_code': "def optimize_theta(x, y, kernel, params_0=[0.1, 0.1], sigma_n=0.1):\n    def log_pY(theta):\n        K = np.matrix(kernel(x, x, theta, sigma_n))\n        f, y_giv_f = find_f(K, y)\n        W = util.calculate_W(f, y)\n        inv_k = np.linalg.inv(K)\n        log_k = np.log(np.linalg.det(K) * np.linalg.det(inv_k + W))\n        Y_giv_f = np.prod(y_giv_f)\n        output = 0.5 * np.matmul(np.matmul(f.T, inv_k), f)\n        output += 0.5 * log_k\n        output -= np.log(Y_giv_f)\n        return output\n\n    res = minimize(log_pY, params_0, method='nelder-mead', options={'xtol': 1e-8, 'disp': False})\n\n    return list(res.x) + [sigma_n]"}, {'identified': None, 'updated_code': None}, {'identified': "- The comments should start with a single space after the '#' character", 'updated_code': '# -------------Part 2----------------\n\n# Overall mean & covariance\ntrain_data = np.vstack((c1.train, c2.train))\ntest_data = np.vstack((c1.test, c2.test))\n# mu_est = [np.mean(train_data[:,0]), np.mean(train_data[:,1])]\ncov_est = np.cov(test_data.T)   # Transpose data to get correct covariance'}, {'identified': "- There should be a space before and after the '=' operator in the 'plt.figure' line", 'updated_code': 'img_61060 = plt.imread(test_path + "61060_1/pdi_heat.jpg")\nplt.figure(figsize=(15, 15))\nplt.imshow(img_61060);'}, {'identified': None, 'updated_code': None}, {'identified': '- There are unnecessary triple quotes at the beginning and end of the code block', 'updated_code': "with open('tweet_json.txt', 'a', encoding='utf8') as f:\n    for tweet_id in twitter_archive['tweet_id']:\n        try:\n            tweet = api.get_status(tweet_id, tweet_mode='extended')\n            json.dump(tweet._json, f)\n            f.write('\\n')\n        except Exception:\n            continue"}, {'identified': None, 'updated_code': None}, {'identified': '- The line is too long and should be split into multiple lines', 'updated_code': 'firstClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nfirstClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nsecondClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                    dfTitanic.loc[(dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nsecondClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                    dfTitanic.loc[(dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nthirdClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nthirdClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\n\nprint("First Class Male Fraction: {0:.4f} \\t First Class Female Fraction {1:.4f}\\n"\n      "Second Class Male Fraction: {2:.4f} \\t Second Class Female Fraction {3:.4f}\\n"\n      "Third Class Male Fraction: {4:.4f} \\t Third Class Female Fraction {5:.4f}".format(firstClassMRate["Survived"],\n                                                                                           firstClassFRate["Survived"],\n                                                                                           secondClassMRate["Survived"],\n                                                                                           secondClassFRate["Survived"],\n                                                                                           thirdClassMRate["Survived"],\n                                                                                           thirdClassFRate["Survived"]))'}, {'identified': "- Import statement is missing for 'pd' and 'sns' libraries", 'updated_code': 'import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplot_missed_predictions_df = missed_predictions[[\'game_index\', \'features\', \'model_features\']]\n\nplot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars=\'game_index\',\n                                     var_name=\'Features Supporting Outcome\')\n\nm_plot = sns.barplot(x=\'game_index\', y=\'value\', hue=\'Features Supporting Outcome\',\n                     data=plot_missed_predictions_df)\n\nplt.title("Percentage Of Features Consistent With Game Outcomes")\nplt.ylabel(\'Percentage\')\nplt.xlabel(\'Missed Prediction Game Index\')\nm_plot.figure.set_size_inches(20, 6)'}, {'identified': '- The imports should be separated into groups with a blank line between each group', 'updated_code': "import os\nimport pickle\nimport random\nimport subprocess\nfrom datetime import timedelta\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom pandas import DataFrame\nfrom tqdm import tqdm\n\nimport gcsfs\nfrom dask.distributed import Client, LocalCluster\nfrom dask_kubernetes import KubeCluster as Cluster\n\nfrom mcmc import dist, smc\nfrom mcmc_utils import dist_map, get_likelihood_logp, get_prior_logp\nfrom misc import get_label_tree, get_mask, get_masks, get_peq_from_df, get_pet, get_precipitation, gcs_get_dir, startswith_label\nfrom models import gr4hh\nfrom virtual_stations import get_waterlevel\n\nsys.path.append('../python')\n\nis_pangeo_data = False  # True if in Pangeo binder, False if in laptop\nif is_pangeo_data:\n    from dask_kubernetes import KubeCluster as Cluster\n    n_workers = 10\nelse:\n    from dask.distributed import LocalCluster as Cluster\n    n_workers = 4\n\n%matplotlib inline"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- The comments should start with a space after the '#' character", 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The function definition lacks a blank line before it', 'updated_code': "import numpy as np\n\n\ndef marginalize(P, dim):\n    return np.sum(P, axis=tuple(set(range(P.ndim)) - {dim}))\n\n\n# Let's try it\ntest_P = np.random.rand(2, 3, 4)\ntest_P = test_P / test_P.sum()  # Normalize for proper distribution\n\n# Do the marginal distributions look like you expect?\nprint(marginalize(test_P, 0))\nprint(marginalize(test_P, 1))\nprint(marginalize(test_P, 2))"}, {'identified': "- The comment '### Resposta: Sim' is not properly formatted as a comment", 'updated_code': "# Resposta: Sim\nmesmo_numero_passagem = train.groupby(['Sobreviventes', 'PassagemNumero']).size()\nmesmo_numero_passagem"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- The dot operator is not surrounded by whitespace', 'updated_code': "a = RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid).tconfint_mean()"}, {'identified': "- The line after the 'with' statement should be indented with 4 spaces, not 3", 'updated_code': 'with tf.Session(graph=graph) as sess:\n    # saver.restore(sess, "./model/lr_0.003")\n    saver.restore(sess, tf.train.latest_checkpoint(\'./model\'))\n    result = sess.run(tf.argmax(y_pred, 1), feed_dict={x: test_images_np_gray, hold_prob: 1.0})'}, {'identified': None, 'updated_code': None}, {'identified': "- There is no whitespace around the '=' operator in the 'kind' argument of the plot function", 'updated_code': "train[colunasGraficos].plot(kind='box', subplots=True, layout=(3, 3), sharex=False, sharey=False)\nplt.show()"}, {'identified': '- There are inconsistent uses of whitespace around operators and parentheses', 'updated_code': 'def formfaktor(winkel, a, amp=1):\n    # winkel = winkel + offset\n    q = 4 * np.pi * n / wavelen * np.sin(winkel * gamma / 2)\n    return 9 * (np.sin(q * a) - (q * a) * np.cos(q * a)) ** 2 / (q * a) ** 6 * amp\n\n\ndef formfaktorQ(q, a, amp=1):\n    return 9 * (np.sin(q * a) - (q * a) * np.cos(q * a)) ** 2 / (q * a) ** 6 * amp\n\n\n# Funktion funktioniert für yscale="log" bei ca. a=500nm'}, {'identified': None, 'updated_code': None}, {'identified': "- There is an unnecessary comment marker '###' at the beginning of the line", 'updated_code': 'from sklearn.tree import DecisionTreeClassifier\n\narbol = DecisionTreeClassifier(max_depth=3)'}, {'identified': '- There should be a space after the comma in the range function', 'updated_code': "for day_number in range(1, 8):  # 1~7\n    shop_info.loc[:, 'wave_model2_' + str(day_number)] = 0.0\n\n\ndef get_avg_from_stable_period(shop_id, n):\n    stable_periods = shop_info.loc[shop_id, 'stable_period_' + str(n)]\n    day_list = []\n    for stable_period in stable_periods:\n        start_date = week_to_date(stable_period[0])[0]\n        end_date = week_to_date(stable_period[1])[1]\n        day_list.extend(dateRange(start_date, end_date))\n\n    if len(day_list) % 7 != 0:\n        print('ERROR')\n\n    for day_number in range(1, 8):  # 1~7\n        prediction_date = '2016-11-' + '%02d' % day_number\n        prediction_date = pd.to_datetime(prediction_date).date()\n        training_days = [day for day in day_list if\n                         (prediction_date - pd.to_datetime(day).date()).days % 7 == 0]\n\n        shop_info.loc[shop_id, 'wave_model2_' + str(day_number)] = shop_info.loc[shop_id, training_days].mean()\n\n\nfor shop_id in shop_ids_stable_period_1:\n    get_avg_from_stable_period(shop_id, 1)\n\nfor shop_id in shop_ids_stable_period_2:\n    get_avg_from_stable_period(shop_id, 2)\n\nfor shop_id in shop_ids_stable_period_3:\n    get_avg_from_stable_period(shop_id, 3)"}, {'identified': "- The import statement for 'plt' is missing", 'updated_code': "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 60))\nfor i in range(1, 97):\n    plt.subplot(24, 4, i)\n    im = np.array((imgs[i][0], imgs[i][0], imgs[i][0]))\n    im = im.swapaxes(1, 2)\n    im = im.swapaxes(0, 2)\n    bbox = bboxes[i] \n    # print(bbox)\n    # print(im.shape)\n    # print(im)\n    # print(im[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[0])+4])\n    # im[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[0])+5, 0] = 0\n    # im[int(bbox[1]):int(bbox[1])+5, int(bbox[0]):int(bbox[2]), 0] = 0\n    # im[int(bbox[1]):int(bbox[3]), int(bbox[2])-5:int(bbox[2]), 0] = 0\n    # im[int(bbox[3])-5:int(bbox[3]), int(bbox[0]):int(bbox[2]), 0] = 0\n    # print(im[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[0])+4])\n    # im[0:200, 0:200] = 0\n    plt.imshow(im, cmap='gray')\n    # plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none'))\n    plt.annotate(imgs[i][1], (0, 0))"}, {'identified': None, 'updated_code': None}, {'identified': 'None', 'updated_code': None}, {'identified': '- The line continuation in the first line is not aligned with the opening parenthesis', 'updated_code': 'start_day = (pd.to_datetime(\'2016-11-01\').date() - datetime.timedelta(14)).strftime("%Y-%m-%d")\nfor id in ids_zero_morethan1_count_in_last_three_week_but_no_zero_in_last_two_week:\n    for day_number in range(1, 8):  # 1~7\n        predict_day = \'2016-11-\' + \'%02d\' % day_number\n        predict_day = pd.to_datetime(predict_day).date()\n        day_list = [day for day in dateRange(start_day, \'2016-10-31\') if (predict_day - pd.to_datetime(day).date()).days % 7 == 0]\n        shop_info.loc[id, \'wave_model1_\' + str(day_number)] = shop_info.loc[id, day_list].mean()\n\n    if id in ids_stable_period_1_2_3:\n        for i in range(1, 8):\n            shop_info.loc[id, \'wave_model_total_\' + str(i)] = 0.5 * shop_info.loc[id, \'wave_model1_\' + str(i)] + 0.5 * shop_info.loc[id, \'wave_model2_\' + str(i)]\n    else:\n        for i in range(1, 8):\n            shop_info.loc[id, \'wave_model_total_\' + str(i)] = shop_info.loc[id, \'wave_model1_\' + str(i)]'}, {'identified': '- The colon in the if statement is not followed by a space', 'updated_code': "def cal_wave_range(x):\n    sales_count = sorted(x['2016-10-11':'2016-10-31'].tolist())\n    if sum(sales_count[18:]) == 0:\n        return 0\n    return (sum(sales_count[18:]) - sum(sales_count[:3])) / float(sum(sales_count[18:]))\n\nshop_info['wave_range'] = shop_info.apply(lambda x: cal_wave_range(x), axis=1)"}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': "- There should be whitespace around the assignment operator '='", 'updated_code': "save_file = './train_model_best.ckpt'\nsaver = tf.train.Saver()\n\nwith tf.Session() as session:\n    saver.restore(session, save_file)\n    feed_dict = {tf_train_dataset: X2_norm, tf_keep_prob: 1}\n    logits = session.run(logits, feed_dict)\n    predicts = session.run(tf.nn.top_k(logits, k=5, sorted=True))\n    probability = session.run(tf.nn.softmax(predicts[0]))"}, {'identified': '- The function definition lacks two blank lines before and after the docstring', 'updated_code': 'def add(x, y):\n    """\n    :param x: int\n    :param y: int\n    :return: int\n    """\n    return x + y\n\nprint(help(add))'}, {'identified': "- The import statement for 'plt' is missing", 'updated_code': 'import matplotlib.pyplot as plt\n\ndef get_class_image(X_train, y_train, class_label):\n    for i in range(len(y_train)):\n        if y_train[i] == class_label:\n            return X_train[i]\n    return False\n\n\ndef show_class_images():\n    fig, axes = plt.subplots(6, 8, figsize=(15, 10))\n    fig.subplots_adjust(hspace=.8, wspace=.05)\n    axes = axes.ravel()\n\n    for i in range(43):\n        class_image = get_class_image(X_train, y_train, i)\n        axes[i].imshow(class_image, cmap="gray")\n        axes[i].set_title(str(i))\n\n\nshow_class_images()'}, {'identified': '- There should be a space after the comma in the figsize argument', 'updated_code': "plt.figure(figsize=(14, 8))\nsns.countplot(data=df,\n              x='Country',\n              hue='SkipMeals',\n              palette='Paired',\n              order=df['Country'].value_counts()[:10].index)\nsns.despine(left=True)"}, {'identified': "- The keyword argument in the function call 'seaborn.set' should have whitespace around the '=' operator", 'updated_code': 'seaborn.set(rc={"figure.figsize": (13, 10)})\nseaborn.set_style("whitegrid")'}, {'identified': "- There should be whitespace around the '=' operator in the 'plot_lc' function call", 'updated_code': 'sn.plot_lc(["BessellV"], multiplot=False)\nplt.scatter(p.data["BessellV"]["MJD"], p.data["BessellV"]["flux"], label="Synthetic Bessell V")'}, {'identified': '- Import statements should be on separate lines', 'updated_code': '# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport renders as rs\nfrom IPython.display import display  # Allows the use of display() for DataFrames\n\n# Show matplotlib plots inline (nicely formatted in the notebook)\n%matplotlib inline\n\n# Load the wholesale customers dataset\ntry:\n    data = pd.read_csv("customers.csv")\n    data.drop([\'Region\', \'Channel\'], axis=1, inplace=True)\n    print("Wholesale customers dataset has {} samples with {} features each.".format(*data.shape))\nexcept FileNotFoundError:\n    print("Dataset could not be loaded. Is the dataset missing?")'}, {'identified': "- The comment should start with a space after the '#' symbol", 'updated_code': '# ignore\n!pip install pysnooper\n\nimport pysnooper\n\nclear_output()'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- Imports should be grouped in the following order: standard library imports, related third-party imports, local application/library specific imports', 'updated_code': 'import sys\n\nprint(sys.executable)\nprint(sys.path)\n\nfrom padma.models.bbox.bbox import find_conteiner'}, {'identified': "- The comment should use '#' instead of '###'", 'updated_code': "# set up data for modeling\nX_5k = boston_clean[['Bib', 'Age', 'Official Time Duration', 'F', 'M', 'Temp (F)']]\ny_5k = boston_clean['5K Duration'].values.reshape(-1, 1)\nprint(X_5k.shape, y_5k.shape)"}, {'identified': '- The comments are not indented properly.', 'updated_code': '# Decoder 裡頭會有 N 個 DecoderLayer，\n# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        # 3 個 sub-layers 的主角們\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        # 定義每個 sub-layer 用的 LayerNorm\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        # 定義每個 sub-layer 用的 Dropout\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, enc_output, training,\n           combined_mask, inp_padding_mask):\n        # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n        # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n        # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n        # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n\n        # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n        # 我們同時需要 look ahead mask 以及輸出序列的 padding mask\n        # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n        # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n        return out3, attn_weights_block1, attn_weights_block2'}, {'identified': None, 'updated_code': None}, {'identified': "- The parameters in the function call should have whitespace around the '=' operator", 'updated_code': "run(track=trk, zs=zs, R=var, Q=0.02, P=1.0, plot_P=True,\n    title='$P=1\\, m^2')"}, {'identified': None, 'updated_code': None}, {'identified': '- Function names should be lowercase with words separated by underscores', 'updated_code': '# Functie voor het checken van de locatie\ndef check_locatie(postcode):\n    result = wervingsgebieden.loc[(wervingsgebieden.PostcodeStart <= postcode) & (wervingsgebieden.PostcodeEind >= postcode), [\'Locatie\']]\n\n    if result.empty:\n        return \'\'\n    else:\n        return result[\'Locatie\'].iloc[0]\n\n\n# Copy-paste voor de categorie\ndef check_categorie(postcode):\n    result = wervingsgebieden.loc[(wervingsgebieden.PostcodeStart <= postcode) & (wervingsgebieden.PostcodeEind >= postcode), [\'Categorie\']]\n\n    if result.empty:\n        return \'\'\n    elif result.Categorie.count() > 1:\n        return \'Meerdere\'\n    else:\n        return result[\'Categorie\'].iloc[0]\n\n\n# Voeg kolommen toe aan datasource\ndatasource["Locatie"] = datasource["PC4_LEERL"].apply(check_locatie)\ndatasource["Categorie"] = datasource["PC4_LEERL"].apply(check_categorie)'}, {'identified': "- There should be whitespace around the '=' operator in the assignment for 'save_file'", 'updated_code': 'save_file = \'./train_model_best.ckpt\'\nsaver = tf.train.Saver()\n\nwith tf.Session() as session:\n    saver.restore(session, save_file)\n    \n    _, test_accuracy = evaluate(X_test, y_test, b=BETA, lr=LEARNING_RATE)\n    print("Test Accuracy = {:.3f}".format(test_accuracy))'}, {'identified': "- Comments should have a space after the '#' symbol", 'updated_code': '# Valor total - bruto: 891\n# Valor total - Sem Nulos: 712\n\ntrain_dropna = train.dropna(subset=colunasSemCabine)\ntrain_dropna.describe()'}, {'identified': "- The comment lines should start with a single '#' instead of '##'", 'updated_code': "# Load symbol and parameters, the parameters can be downloaded from the following link.\n# including the realtimePose-symbol.json and realtimePose-0000.params\n# <https://drive.google.com/drive/folders/0BzffphMuhDDMV0RZVGhtQWlmS1U?usp=sharing>\noutput_prefix = 'realtimePose'\nsym, arg_params, aux_params = mx.model.load_checkpoint(output_prefix, 0)"}, {'identified': "- The list 'job_assesement' is not formatted properly. There should be a comma and a space after each item in the list.", 'updated_code': "job_assessment = [\n    'AssessJob1', 'AssessJob2', 'AssessJob3', 'AssessJob4', 'AssessJob5',\n    'AssessJob6', 'AssessJob7', 'AssessJob8', 'AssessJob9', 'AssessJob10'\n]\nff = top10_df.groupby('Country').mean()[job_assessment].reset_index()\nff_p = ff.pivot_table(columns='Country')\nplt.figure(figsize=(14, 8))\nfor country in top_10_list:\n    plt.plot(ff_p[country], label=country)\nplt.legend()\nsns.despine(left=True)\nplt.title('Job assessment comparison by country', fontsize=21)"}, {'identified': "- Function name 'Freq_plot' should be lowercase and separated by underscores", 'updated_code': 'freq_plot(cr=cr, num_tests=3, bayes=True)'}, {'identified': '- The variable names are not following lowercase_with_underscores convention', 'updated_code': "from prompts import new_prompt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\nparameters_decision_tree = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': randint(1, 200)\n}\n\ntiempo_random_decision_tree, random_decision = correr_randomized_y_mostrar(\n    DecisionTreeClassifier(),\n    parameters_decision_tree,\n    5,\n    5,\n    100\n)\n\nver_tiempo(tiempo_decision_tree, tiempo_random_decision_tree)\n\nparameters_decision_tree_2 = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': randint(1, 200),\n    'min_samples_split': uniform(0, 1)\n}\n\ntiempo_random_decision_tree_2, random_decision_tree_2 = correr_randomized_y_mostrar(\n    DecisionTreeClassifier(),\n    parameters_decision_tree_2,\n    5,\n    5,\n    500\n)\n\nver_tiempo(tiempo_decision_tree_2, tiempo_random_decision_tree_2)"}, {'identified': '- The line of code is too long and should be split into multiple lines', 'updated_code': "plt.matshow(doripa[['total', 'total6', 'scores']]\n            .corr())"}, {'identified': None, 'updated_code': None}, {'identified': "- The import statement is missing for the 'np' module", 'updated_code': 'import numpy as np\nimport matplotlib.pyplot as plt\n\n# Plot graph density for each threshold.\nthresh_array = []\ndensity_distr = []\n\nfor i in np.arange(0.1, 1, 0.1):\n    density_array = []\n    for j in range(1, int(num_examples / 5)):\n        corr_mat = corr_tensor[j * 5, :, :].copy()\n        corr_mat[(corr_mat > -1 * i) & (corr_mat < i)] = 0\n        G, density = make_graph(corr_mat, nodes, \'signed\')\n        \n        density_array.append(density)\n    \n    density_distr.append(density_array)\n\n\nplt.boxplot(density_distr)\nplt.ylabel("Graph Density")\nplt.xlabel("Correlation Threshold (10^-1)")\nplt.title("Density vs Threshold")\nplt.show()'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': '- Imports should be separated by a blank line', 'updated_code': "import p5_util\n\nis_score_dumped = True\n\nif is_score_dumped:\n    filename = './data/dict_cls_score.dump'\n    dict_cls_score = p5_util.object_load(filename)\nelse:\n    dict_cls_score = dict()\n\ndict_cls_score"}, {'identified': '- There should be two blank lines between the imports and the first function, not three', 'updated_code': "from tensorflow.contrib.layers import flatten\n\n\ndef LeNet(x):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # SOLUTION: Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    \n    # SOLUTION: Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n    fc0 = flatten(conv2)\n    \n    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(120))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n    \n    # SOLUTION: Activation.\n    fc1 = tf.nn.relu(fc1)\n\n    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(120, 84), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(84))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    \n    # SOLUTION: Activation.\n    fc2 = tf.nn.relu(fc2)\n\n    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(84, 43), mean"}, {'identified': None, 'updated_code': None}, {'identified': "- The import for 'plt' is missing", 'updated_code': 'import matplotlib.pyplot as plt\n\nselem = square(3)\nct_close = erosion(dilation(canny, selem=selem), selem=selem)\nplt.figure(figsize=[10, 10])\nplt.imshow(ct_close[-1::-1, :])'}, {'identified': '- The comment should start with a capital letter', 'updated_code': "# Produce a scatter matrix without outliers... Distribution appears more normal\npd.scatter_matrix(good_data, alpha=0.3, figsize=(14, 8), diagonal='kde')"}, {'identified': "- The comment '#일부' should have a space after the '#' symbol", 'updated_code': "spent = []\n# 일부\nfor i in range(1, 3):\n    start = time.time()\n    model = Word2Vec(corpora[i], **params_tag[i])\n    spent.append('Elapsed time: ' + str(time.time() - start) + ' sec' + ' [' + name_model[i] + ']')\n    model.wv.save(name_model[i])\nprint(spent)"}, {'identified': '- The line length of some of the plot statements is too long and should be split into multiple lines', 'updated_code': "plt.figure(figsize=(20, 20))\n\nplt.subplot(4, 1, 1)  # パレットを4行1列に分割し，1行目に以下のグラフをプロットする\nplt.plot(u1_z[:14000], c='blue')\nplt.plot(u2_z[:14000], c='red')\nplt.title('up stairs')\nplt.xlabel('time[ms]')\nplt.ylabel('angular velocity [deg/sec]')\nplt.ylim([-350, 350])\nplt.legend(['z1_z', 'z2_z'])\nplt.grid()  # プロット領域にグリッド線をつける\n\nplt.subplot(4, 1, 2)  # パレットを4行1列に分割し，2行目に以下のグラフをプロットする\nplt.plot(np.arange(14000, 28000), u1_z[14000:28000], c='blue')\nplt.plot(np.arange(14000, 28000), u2_z[14000:28000], c='red')\nplt.title('down stairs')\nplt.xlabel('time[ms]')\nplt.ylabel('angular velocity [deg/sec]')\nplt.ylim([-350, 350])\nplt.legend(['z1_z', 'z2_z'])\nplt.grid()\n\nplt.subplot(4, 1, 3)  # パレットを4行1列に分割し，3行目に以下のグラフをプロットする\nplt.plot(np.arange(51000, 63000), u1_z[51000:63000], c='blue')\nplt.plot(np.arange(51000, 63000), u2_z[51000:63000], c='red')\nplt.title('up stairs')\nplt.xlabel('time[ms]')\nplt.ylabel('angular velocity [deg/sec]')\nplt.ylim([-350, 350])\nplt.legend(['z1_z', 'z2_z'])\nplt.grid()\n\nplt.subplot(4, 1, 4)  # パレットを4行1列に分割し，4行目に以下のグラフをプロットする\nplt.plot(np.arange(63000, 75000), u1_z[63000:75000], c='blue')\nplt.plot(np.arange(63000, 75000), u2_z[63000:75000], c='red')\nplt.title('down stairs')\nplt.xlabel('time[ms]')\nplt.ylabel('angular velocity [deg/sec]')\nplt.ylim([-350, 350])\nplt.legend(['z1_z', 'z2_z'])\nplt.grid()"}, {'identified': None, 'updated_code': None}, {'identified': "- There should be a space before and after the '.' operator", 'updated_code': '### Join\njoin . isnull().sum()'}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}, {'identified': None, 'updated_code': None}]