[{'reason': 'stop', 'result': '```python\nshow_plot(make_scatter(data_x, data_y, text), make_layout(500, 500, shapes=shape))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Apply the function to transform data:\nCR = transform_data(DF, a=Conf_alpha, a_0=alpha_0, b_0=beta_0)\ndim(CR)\nhead(CR)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Find x and y coordinates from Easting and Northing values for the LSOA\na = PM25.attrs['affine']\na = rasterio.Affine.from_gdal(*a)\n~a * (439040, 115775)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n##  define color list\ncolors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\n\n##  define index for iterating through color list\nindex = 0\n\n##  for each river segment\nfor segment in segments:\n    ##  subset df to non-zero values for the current river segment\n    seg_df = data.loc[data[\'RiverSeg\'] == segment]\n    seg_df = seg_df[seg_df["lam\'"] > 0]\n\n    ## sort based on year\n    seg_df = seg_df.sort_values(\'Year\')\n    ## define x, y for plotting\n    x = seg_df["Year"]\n    y = seg_df["N21"]\n    ## change name of y to Riv Seg for legend\n    y.name = segment\n    ##  build graph...\n    ##  plot segment x vs y\n    plt.plot(x, y, colors[index])\n    ##  locate legend\n    plt.legend(loc=(1.05, 0.2))\n    ##  advance color index\n    index += 1\n\n##  update title\nplt.title("Hill N21, " + river + " River Segments")\n##  label x axis\nplt.xlabel(\'Year\')\n##  label y axis\nplt.ylabel("N21")\n##  force x axis to integer values, increment by 1 year\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\n##  rotate year labels 90 degrees\nplt.xticks(rotation=90)\n\n##  save figure\nplt.savefig(output + "\\\\" + river + "_Hill_N21.png", bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\n##  display figure\nplt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n### Data exploration visualization code goes here.\nX_train, y_train = shuffle(X_train_augmented, y_train_augmented)\n\ndef get_random_img(X_data, y_label):\n    index = random.randint(0, len(X_data))\n    image = X_data[index].squeeze()\n    return image, y_label[index], index\n\ndef plot_imgs(X_data, y_label):\n    f, axarr = plt.subplots(3, 3, figsize=(16, 16))\n    rand_indices = []\n    for i in range(9):\n        image, label, index = get_random_img(X_data, y_label)\n        rand_indices.append(index)\n        label_str = str(label)\n        axarr[i // 3, i % 3].imshow(image, cmap="gray")\n        axarr[i // 3, i % 3].set_title(label_str + ": " + sign_dict[label_str])\n        # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n        plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n        plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n    return rand_indices\n\nrand_img_indices = plot_imgs(X_train, y_train)\nprint(rand_img_indices)\n```'}, {'reason': 'stop', 'result': "```python\nk = 10\nheterogeneity = {}\nimport time\nstart = time.time()\nfor seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n  initial_centroids = get_initial_centroids(tf_idf, k, seed)\n  centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n                                         record_heterogeneity=None, verbose=False)\n  # To save time, compute heterogeneity only once in the end\n  heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n  print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n  print(np.bincount(cluster_assignment))\n  sys.stdout.flush()\nend = time.time()\nprint(end - start)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nImage(filename=pathname + "DATAppDzNtrk")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nbarwidth = 0.75\nfig, ax = plt.subplots(figsize=(9, 7))\nrects1 = ax.bar(0.5, SkyPresence.mean(), barwidth, color=sns.xkcd_rgb[\'green\'], yerr=SkyPresenceSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects2 = ax.bar(1.5, ColorScheme.mean(), barwidth, color=(0.3, 0.9, 0.3), yerr=ColorSchemeSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects3 = ax.bar(2.5, TreeFreq.mean(), barwidth, color=(0.15, 1, 0.15), yerr=TreeFreqSEM, ecolor=\'k\', error_kw=dict(lw=3))\nrects4 = ax.bar(4, ImageType.mean(), barwidth, yerr=ImageTypeSEM, ecolor=\'k\', edgecolor=sns.xkcd_rgb[\'green\'], linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\nrects5 = ax.bar(5, FeatureType.mean(), barwidth, yerr=FeatureTypeSEM, ecolor=\'k\', edgecolor=(0.3, 0.9, 0.3), linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\nrects6 = ax.bar(6, LightType.mean(), barwidth, yerr=LightTypeSEM, ecolor=\'k\', edgecolor=(0.15, 1, 0.15), linewidth=2, facecolor=\'none\', error_kw=dict(lw=3))\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None)\nax.set_ylim(0, 100)\nax.set_xlim(0, 7.5)\nax.set_xticklabels((\'SP\', \'CS\', \'TF\', \'IT\', \'FT\', \'LT\'))\nax.set_xticks([0.5 + barwidth/2, 1.5 + barwidth/2, 2.5 + barwidth/2, 4 + barwidth/2, 5 + barwidth/2, 6 + barwidth/2])\nax.set_yticks(np.arange(0, 101, 10))\nplt.title(\'Q2: Rate the Frequency at Which These Perceptual Categories\\nPredicted an Easy/Hard Color-Word Trial\', fontsize=18, fontweight="bold")\nplt.ylabel(\'<-- Less Likely      More Likely -->\', fontsize=17, fontweight="bold")\nplt.xlabel(\'S-C Phase                 S-CT Phase\', fontsize=17, fontweight="bold")\nsns.despine()\n\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\n# add new column\ndata["N10\'"] = ""\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# RT Validity Effect\nstats.ttest_rel(RTanalysis.Valid, RTanalysis.Invalid)\n```'}, {'reason': 'stop', 'result': "```python\n# Import data\nPixel = pd.read_csv(r'D:/Annies_Dissertation/Analysis/Regression/Validation/Monthly_PM25_LSOA_Validation.csv', parse_dates=['time'])\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nsubset_uuid = ekos.get_unique_id_for_alias(workspace_alias, subset_alias='A')\nprint(w.get_subset_list(), subset_uuid)\n\nf1 = w.get_data_filter_object(subset=subset_uuid, step=1)\nprint(f1.include_list_filter)\n\nw.apply_data_filter(subset=subset_uuid, step=1)\n\ndf_step1 = w.get_filtered_data(step=1, subset=subset_uuid)\n# print(df_step1.columns)\n# df_step1[['SDATE', 'YEAR', 'MONTH', 'POSITION', 'VISS_EU_CD', 'WATER_TYPE_AREA', 'DEPH', 'MNDEP', 'MXDEP','BQIm']].dropna(subset=['BQIm'])\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Depth 10 is usually plenty of depth for most datasets, but you never know\nhyper_params = {\'max_depth\' : range(1,30,2)}\n#hyper_params = {max_depth = [4,6,8,12,16,20]} ##faster for larger datasets\n\n# Build initial GBM Model\ngbm_grid = H2OGradientBoostingEstimator(\n    # more trees is better if the learning rate is small enough \n    # here, use "more than enough" trees - we have early stopping\n    ntrees=10000,\n    # smaller learning rate is better\n    # since we have learning_rate_annealing, we can afford to start with a \n    # bigger learning rate\n    learn_rate=0.05,\n    # learning rate annealing: learning_rate shrinks by 1% after every tree \n    # (use 1.00 to disable, but then lower the learning_rate)\n    learn_rate_annealing=0.99,\n    # sample 80% of rows per tree\n    sample_rate=0.8,\n    # sample 80% of columns per split\n    col_sample_rate=0.8,\n    # fix a random number generator seed for reproducibility\n    seed=1234,\n    # score every 10 trees to make early stopping reproducible \n    # (it depends on the scoring interval)\n    score_tree_interval=10, \n    # early stopping once the validation AUC doesn\'t improve by at least 0.01% for \n    # 5 consecutive scoring events\n    stopping_rounds=5,\n    stopping_metric="AUC",\n    stopping_tolerance=1e-4\n)\n\n# Build grid search with previously made GBM and hyper parameters\ngrid = H2OGridSearch(\n    gbm_grid,\n    hyper_params,\n    grid_id=\'depth_grid\',\n    search_criteria={\'strategy\': "Cartesian"}\n)\n\n# Train grid search\ngrid.train(\n    x=predictors, \n    y=response,\n    training_frame=train,\n    validation_frame=valid\n)\n```'}, {'reason': 'stop', 'result': "```python\n# Não.\n    test.groupby(['ParentesIrmao']).size()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ndata, labels = vectorize_docs(\n    artist="linkin-park",  # specify artist\n    albums=["hybrid-theory"],  # specify album(s)\n    keep_album=False,  # option to use the album name as a delimiter\n    titlify=True  # converts song title to original format\n)\n\nsong_to_search = "Crawling"\nfor i, song in enumerate(labels):\n    if song == song_to_search:\n        display(Markdown("**Song name: **" + labels[i]))\n        display(Markdown("**Lyrics:**"))\n        print(data[i][:1000] + "...")\n        sample_lyrics = data[i]\n        break\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\ndef character_quotient(M, N, n, r, left_basis=s, right_basis=s):  # a corriger\n    b_tot = M.basis()\n    b_ideal = N.basis()\n    charac = 0\n    q = PolynomialRing(QQ, 'q', r).gens()\n\n    for nu in Partitions(n):\n        basis_nu_tot = {}\n        basis_nu_ideal = {}\n        charac_nu = 0\n        # Get the nu_isotypic part of the bases\n        for key, value in b_tot.items():\n            if Partition(key[1]) == nu:\n                basis_nu_tot[key[0]] = value\n        for key, value in b_ideal.items():\n            if Partition(key[1]) == nu:\n                basis_nu_ideal[key[0]] = value\n\n        # Use the degrees to compute the character\n        for deg, b in basis_nu_tot.items():\n            charac_nu += sum(prod(q[i] ** deg[i] for i in range(0, len(deg))) for p in b)\n        for deg, b in basis_nu_ideal.items():\n            charac_nu -= sum(prod(q[i] ** deg[i] for i in range(0, len(deg))) for p in b)\n        if charac_nu != 0:\n            if left_basis == s:\n                charac_nu = s.from_polynomial(charac_nu).restrict_partition_lengths(r, exact=False)\n            else:\n                charac_nu = left_basis.from_polynomial(charac_nu)\n            # Make the tensor product with s[nu]\n            charac += tensor([charac_nu, right_basis(s(nu))])\n\n    return charac\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# TODO: Apply your clustering algorithm of choice to the reduced data\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\n# todo: for loop\nnum_clusters = 3\nclusterer = GaussianMixture(n_components=num_clusters)\nclusterer.fit(reduced_data)\n\n# TODO: Predict the cluster for each data point\npreds = clusterer.predict(reduced_data)\n\n# TODO: Find the cluster centers\ncenters = clusterer.means_\n\n# TODO: Predict the cluster for each transformed sample data point\nsample_preds = clusterer.predict(pca_samples)\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\nsilhouette_coefficient = silhouette_score(reduced_data, preds)\n\nprint(num_clusters, silhouette_coefficient)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nshared_norm_spanning_system(span_norm[V3 1 0 0, V3 0 1 0, V3 0 0 1], span_norm[V3 1 0 0, V3 0 1 0] :: Norm ℝ³)\n```'}, {'reason': 'stop', 'result': "```python\nimport os\nimport sys\nfrom ROOT import gROOT\n\npath = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/16MeVTa/Unfold/NSD/'\nos.chdir(path)\nprint('Currently working in: \\n {}'.format(os.getcwd()))\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ncutPt = {0: 0.28, 2: 0.3, 4: 0.29}\nfor detNum, detName in detNames.items():\n    gROOT.ProcessLine(\'AmBe{}->cd()\'.format(detNum))\n    # gROOT.ProcessLine(\'eventTree->Draw("(m_amplitude-m_shape)/m_shape:m_shape>>(1250,0,35000,1024,0,1)","(m_amplitude-m_shape)/m_amplitude<{}","colz")\'.format(cutPt[detNum]))\n    pause()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfor i in range(25):\n    w = update_nueron(w, X[i % X.shape[0], :], y[i % X.shape[0]])\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Quarterly Revenues across multi-year\ndfqq = df.pivot_table(index=df.index.quarter, aggfunc=(np.mean, np.sum, min, max)).rename_axis('quarter')\ndfqq.style.applymap(color_negative_red).apply(highlight_max).apply(highlight_min)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfrom matplotlib.ticker import MultipleLocator\n\n# filters = ["BessellV"]\nfilters = ["SDSS_r"]\n\nalpha = 1.0\nxminorticks = 10\n\npcc.utils.setup_plot_defaults()\n\nfig = plt.figure(figsize=[8, 4])\nfig.subplots_adjust(left=0.1, bottom=0.13, top=0.93,\n                    right=0.91, hspace=0, wspace=0)\n## Label the axes\nxaxis_label_string = r\'$\\textnormal{Time, MJD (days)}$\'\nyaxis_label_string = r\'$\\textnormal{Flux, erg s}^{-1}\\textnormal{\\AA}^{-1}\\textnormal{cm}^{-2}$\'\n\nax1 = fig.add_subplot(111)\naxes_list = [ax1]\n\nfor filter_key in filters:\n    plot_label_string = r\'$\\rm{\' + sn.phot.data_filters[filter_key].filter_name.replace(\'_\', \'\\\\_\') + \'}$\'\n    plot_label_string_fake = r\'$\\rm{\' + sn_fake.phot.data_filters[filter_key].filter_name.replace(\'_\', \'\\\\_\') + \', simulated}$\'\n\n    ax1.errorbar(sn.phot.data[filter_key][\'MJD\'], sn.phot.data[filter_key][\'flux\'],\n                 yerr=sn.phot.data[filter_key][\'flux_err\'],\n                 capsize=0, fmt=\'x\', color=sn.phot.data_filters[filter_key]._plot_colour,\n                 label=plot_label_string, ecolor=pcc.hex[\'batman\'], mec=pcc.hex["batman"],\n                 alpha=alpha)\n    ax1.fill_between(sn.lcfit.data[filter_key][\'MJD\'], sn.lcfit.data[filter_key][\'flux_upper\'], sn.lcfit.data[filter_key][\'flux_lower\'],\n                     color=pcc.hex["batman"],\n                     alpha=0.8, zorder=0)\n\nax1.errorbar(sn_fake.phot.data[filter_key][\'MJD\'], sn_fake.phot.data[filter_key][\'flux\'],\n              yerr=sn_fake.phot.data[filter_key][\'flux_err\'],\n              # capsize = 0, fmt = \'o\', color = sn_fake.phot.data_filters[filter_key]._plot_colour,\n              capsize=0, fmt=\'o\', color=pcc.hex[\'r\'],\n              label=plot_label_string_fake, ecolor=pcc.hex[\'batman\'], mec=pcc.hex["batman"],\n              alpha=alpha)\n\nxminorLocator = MultipleLocator(xminorticks)\nax1.spines[\'top\'].set_visible(True)\nax1.xaxis.set_minor_locator(xminorLocator)\n\nplot_legend = ax1.legend(loc=\'upper right\', scatterpoints=1, markerfirst=False,\n                         numpoints=1, frameon=False, bbox_to_anchor=(1., 1.),\n                         fontsize=12.)\n\nax1.set_ylabel(yaxis_label_string)\nax1.set_xlabel(xaxis_label_string)\n\noutpath = "/Users/berto/projects/LSST/cadence/SN2007uy_consistency_check_SDSS_r"\n\nfig.savefig(outpath + ".png", format=\'png\', dpi=500)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# TODO: Execute your algorithm from Step 6 on\n# at least 6 images on your computer.\n# Feel free to use as many code cells as needed.\n\n\ndef detect_image(img_path):\n    disp_image(img_path)\n    whose_a_good_doggy(img_path)\n\n\n# Import or define the functions 'disp_image' and 'whose_a_good_doggy' before using them\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# some simple testing code and such\ndataset = 'mnist'\ntrain_data = data_filepath + dataset + '_data_train.csv'\ntrain_labels = data_filepath + dataset + '_labels_train.csv'\ntrain_ids = data_filepath + dataset + '_ids_train.csv'\ntest_data = data_filepath + dataset + '_data_test.csv'\ntest_ids = data_filepath + dataset + '_ids_test.csv'\ndescription = data_filepath + dataset + '_feature_descriptions.csv'\n\nproc = Preprocessor(train_data_file=train_data,\n                    train_label_file=train_labels,\n                    train_ids_file=train_ids,\n                    test_data_file=test_data,\n                    test_ids_file=test_ids,\n                    instr_file=description)\n\nproc.read_data()\n\nproc.process()\n\n# doesn't do anything yet, hasn't been implemented\nproc.select_features()\n\n# data is written to output directory\n# any existing data is overwritten\nproc.write_data()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n### União\nuniao_dropna_slice_nome = uniao_dropna['Nome'].str.split(',', expand=True)\nuniao_dropna['Primeiro Nome'] = uniao_dropna_slice_nome[0]\nuniao_dropna_nome_slice = uniao_dropna_slice_nome[1].str.split('.', expand=True, n=1)\nuniao_dropna_complemento_slice = uniao_dropna_nome_slice[1].str.split('(', expand=True)\nuniao_dropna['Saudação'] = uniao_dropna_nome_slice[0]\nuniao_dropna['Sobrenome'] = uniao_dropna_complemento_slice[0]\nuniao_dropna['Complemento'] = uniao_dropna_complemento_slice[1]\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfor x in range(4):  # by default range start at 0\n    print(x)\n```'}, {'reason': 'stop', 'result': '```python\n# make a one-step forecast\n\ndef forecast_lstm(model, batch_size, X):\n    X = X.reshape(1, 1, len(X))\n    yhat = model.predict(X, batch_size=batch_size)\n    return yhat[0, 0]\n\n\n# load the dataset\nseries = read_csv(filename, header=0, parse_dates=[0], index_col=0, squeeze=True)\n\n# transform data to be stationary\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\ntrain, test = supervised_values[0:-12], supervised_values[-12:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)\n```'}, {'reason': 'stop', 'result': '```python\n# Define your architecture here.\n# Feel free to use as many code cells as needed.\n\n\nEPOCHS = 20\nBATCH_SIZE = 150\nrate = 0.002\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# 函数注解\n# python3.5引入\n# 对函数的参数类型注解\n# 对函数的返回值类型注解\n# 只对函数参数做一个铺助的说明，并不对函数参数进行类型检查\n# 提供给第三方工具，做代码分析，发现隐藏bug\n# 函数注解的信息，保存在__annotations__属性中\n# 变量注解\n# python3.6引入\ni: int = 3\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport numpy as np\n\nfig = plt.figure(figsize=(15, 15))\n\ngr = gridspec.GridSpec(1, 6)\nfor i in range(num_own_examples):\n    probas = proba[i]\n    highest_class = signnames[str(np.argmax(probas, axis=0))]\n    plt.subplot(1, 6, i+1)\n    plt.imshow(X2[i])\n    plt.axis('off')\n    plt.text(0, 1, highest_class, color='k', backgroundcolor='y')\nfig.suptitle('In yellow: prediction made by the model', x=0.5, y=0.6, fontsize=20)\nplt.tight_layout()\n\n\nplt.show()    \n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nformula = ("accuracy ~ C(subject, Treatment(0)) + "\n           "C(complexity, Treatment(3)) * "\n           "C(model, Treatment(1))")\nlm = ols(formula, df)\nfit = lm.fit()\nqqplot(fit.resid)\nprint(fit.summary())\nprint(\'\\nThe accuracy of the classifier depends on the subject, \' +\n      \'model type (deep network versus logistic regression), \' +\n      \'and task complexity (CV versus consonant versus {vowel, location, degree}) \' +\n      \'(ANOVA with subject, model type, task complexity, and model-task complexity interaction, \' +\n      \'f-value: {}, p: {}). \'.format(fit.fvalue, fit.f_pvalue) +\n      \'Within this ANOVA, all treatment coefficients were significant \' +\n      \'at p<.001 with Subject 1, CV task, and logistic regression as the reference treatment.\')\nfor table in fit.summary().tables:\n    print(table.as_latex_tabular())\nplt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Write all helper functions here\nfrom scipy.special import logsumexp\nfrom scipy.special import expit\nimport numpy as np\n\n\ndef one_hot(a, num_classes):\n    return np.eye(num_classes)[a.reshape(-1)]\n\n\ndef mlp_logprob(x, W, b, V, a):\n    h = expit(np.matmul(x, V) + a)\n\n    ln_q = np.matmul(h, W) + b\n    ln_Z = logsumexp(ln_q)\n    ln_p = ln_q - ln_Z\n\n    return ln_p, ln_q, ln_Z, h\n\n\ndef mlp_gradient(x, t, W, b, V, a):\n    num_classes = len(b)\n\n    ln_p, ln_q, ln_Z, h = mlp_logprob(x, W, b, V, a)\n    t_oh = one_hot(t, num_classes)\n\n    delta_q = t_oh - np.exp(ln_q) / np.exp(ln_Z)\n    delta_h = np.matmul(delta_q, W.T)\n\n    dL_db = delta_q\n    dL_dW = np.matmul(h.T, delta_q)\n    dL_da = delta_h * h * (1 - h)\n    dL_dV = np.matmul(x.T, dL_da)\n\n    return ln_p[:, t].squeeze(), dL_dW, dL_db.squeeze(), dL_dV, dL_da.squeeze()\n\n\ndef init_params(input_size, num_classes, hidden_units):\n    W = np.random.normal(size=(hidden_units, num_classes), scale=0.1)\n    b = np.zeros(num_classes)\n    V = np.random.normal(size=(input_size, hidden_units), scale=0.1)\n    a = np.zeros(hidden_units)\n    return W, b, V, a\n\n\ndef mlp_sgd_iter(x_train, t_train, W, b, V, a, lr):\n\n    # every day I am shufflin`\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n\n    logp = np.zeros(len(x_train))\n    for i in indices:\n        x = x_train[i:i + 1]\n        t = t_train[i]\n        logp[i], grad_W, grad_b, grad_V, grad_a = mlp_gradient(x, t, W, b, V, a)\n        # grad ascent\n        W = W + lr * grad_W\n        b = b + lr * grad_b\n        V = V + lr * grad_V\n        a = a + lr * grad_a\n\n    logp_train = logp.mean()\n    return logp_train, W, b, V, a\n\n\ndef eval_mean_logp(xs, ts, W, b, V, a):\n    logps = []\n    for x, t in zip(xs, ts):\n        logp, _, _, _ = mlp_logprob(x, W, b, V, a)\n        logps.append(logp[t].squeeze())\n    return mean(logps)\n\n\n# It's always good to check your gradient implementations with finite difference checking:\n# Scipy provides the check_grad function, which requires flat input variables.\n# So we write two helper functions that provide the gradient and output with 'flat' weights:\nfrom scipy.optimize import check_grad\n\nnp.random.seed(123)\n# scalar, 10 X 768  matrix, 10 X 1 vector\nW = np.random.normal(size=(20, 10), scale=0.001)\nb = np.zeros((10,))\nV = np.random.normal(size=(28 * 28, 20), scale=0.001)\na = np.zeros(20)\n\n\ndef func(w):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], w.reshape(20, 10), b, V, a)\n    return logpt\n\n\ndef grad(w):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], w.reshape(20, 10), b, V, a)\n    return grad_W.flatten()\n\n\nfinite_diff_error = check_grad(func, grad, W.flatten())\nprint('Finite difference error grad_W:', finite_diff_error)\nassert finite_diff_error < 1e-3, 'Your gradient computation for W seems off'\n\n\ndef func(b):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], W, b, V, a)\n    return logpt\n\n\ndef grad(b):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], W, b, V, a)\n    return grad_b.flatten()\n\n\nfinite_diff_error = check_grad(func, grad, b)\nprint('Finite difference error grad_b:', finite_diff_error)\nassert finite_diff_error < 1e-3, 'Your gradient computation for b seems off'\n\n\ndef func(v):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], W, b, v.reshape(28 * 28, 20), a)\n    return logpt\n\n\ndef grad(v):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], W, b, v.reshape(28 * 28, 20), a)\n    return grad_V.flatten()\n\n\nfinite_diff_error = check_grad(func, grad, V.flatten())\nprint('Finite difference error grad_V:', finite_diff_error)\nassert finite_diff_error < 1e-3, 'Your gradient computation for V seems off'\n\n\ndef func(a):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], W, b, V, a)\n    return logpt\n\n\ndef grad(a):\n    logpt, grad_W, grad_b, grad_V, grad_a = mlp_gradient(x_train[0:1, :], t_train[0:1], W, b, V, a)\n    return grad_a.flatten()\n\n\nfinite_diff_error = check_grad(func, grad, a.flatten())\nprint('Finite difference error grad_a:', finite_diff_error)\nassert finite_diff_error < 1e-3, 'Your gradient computation for a seems off'\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nux, uy, uz = Lectura("vx.dat", "vy.dat", "vz.dat")\nx, y, z = Lectura("x.dat", "y.dat", "z.dat")\n\nGrafica("vx.dat", "vy.dat", "x.dat", "y.dat", 32)\nplt.savefig("velocidad")\n\nGrafica("Bx.dat", "Bz.dat", "x.dat", "y.dat", 32)\nplt.savefig("Magentico")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n%matplotlib inline\n\ndef solve_and_plot(nsys):\n    fig = plt.figure(figsize=(12, 4))\n    ax_out = plt.subplot(1, 2, 1, xscale='log', yscale='log')\n    ax_err = plt.subplot(1, 2, 2, xscale='log')\n    ax_err.set_yscale('symlog', linthreshy=1e-14)\n    xres, extra = nsys.solve_and_plot_series(\n        c0, c0+K, NH3_varied, NH3_idx, 'scipy', \n        plot_kwargs=dict(ax=ax_out), plot_residuals_kwargs=dict(ax=ax_err))\n    for ax in (ax_out, ax_err):\n        ax.set_xlabel('[NH3]0 / M')\n    ax_out.set_ylabel('Concentration / M')\n    ax_out.legend(loc='best')\n    ax_err.set_ylabel('Residuals')\n    \n    avg_nfev = np.average([nfo['nfev'] for nfo in extra['info']])\n    avg_njev = np.average([nfo['njev'] for nfo in extra['info']])\n    success = np.average([int(nfo['success']) for nfo in extra['info']])\n    return {'avg_nfev': avg_nfev, 'avg_njev': avg_njev, 'success': success}\n\n\nsolve_and_plot(neqsys)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# print out the first 1000 characters of the raw text to get a sense of what we need to throw out\ntext[:1000]\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax1 = plt.subplots()\n\ncounty = df_county_data["County Name"]\ntick_locations = [value for value in x_axis]\n\nplt.xticks(tick_locations, county, rotation=90)\n\ngrad_rate = df_county_data["Graduation Rate"]\npov_rate = df_county_data["Poverty Rate"]\nt = np.arange(len(county))\nax1.plot(t, pov_rate, \'b-\')\nax1.set_xlabel(\'counties\')\n# Make the y-axis label, ticks and tick labels match the line color.\nax1.set_ylabel(\'Poverty Rate\', color=\'b\')\nax1.tick_params(\'y\', colors=\'b\')\n\nplt.title("High School Graduation Rates and Poverty Rates by County")\n\nax2 = ax1.twinx()\n\nax2.plot(t, grad_rate, \'r*\')\nax2.set_ylabel(\'Graduation Rate\', color=\'r\')\nax2.tick_params(\'y\', colors=\'r\')\nzoom = 5\nw, h = fig.get_size_inches()\nfig.set_size_inches(w * zoom, h * zoom/2)\n# plt.xlim(0,20)\n# fig.tight_layout()\n\nplt.savefig("Images/County_Grad_Poverty_Rates2.png", bbox_inches="tight")\n\nplt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n## fit the linear regression model\nlin_mod = lm(y ~ x, data=train)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport math\nimport pandas as pd\nimport numpy as np\n\n\ndef clean_election_data():\n    \'\'\'\n    Function to clean election data \n    \'\'\'\n        \n    # read in dirty data \n    df = pd.read_csv("2014_election_results.csv")\n    df_clean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\n    for _, row in df_clean.iterrows():\n        row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n        if pd.isnull(row["CANDIDATE NAME"]) or (row["CANDIDATE NAME"] == \'Scattered\'):\n            if pd.isnull(row["CANDIDATE NAME (Last)"]) or (row["CANDIDATE NAME (Last)"] == \'Scattered\'):\n                row["CANDIDATE NAME"] = "UNKNOWN" \n            else:\n                row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"]\n    \n    df_clean = df_clean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\n\n    return df_clean\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\npm.traceplot(trace, lines={"l": l_true, "n": n_true})\n```'}, {'reason': 'stop', 'result': '```python\nImage(filename=pathname + averagePileUp_multCollisionFrac)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nimport sklearn.metrics\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\n\naccuracy_train = []\naccuracy_validation = []\nroc_train = []\nroc_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n\ndef get_positive_class_probabilities(arr):\n    arr_aux = []\n    for entry in arr:\n        arr_aux.append(entry[1])\n    return arr_aux\n\n\ndef get_accuracy(y_pred, y_eval_np):\n    return np.mean(y_pred == y_eval_np)\n\n\ndef show_prediction_accuracy(y_pred, y_eval_np, x_eval_np):\n    print("Predicciones sobre el test set:\\n {}".format(y_pred))\n    print("Score sobre el test set: {:.2f}".format(np.mean(y_pred == y_eval_np)))  # A mano\n    print("Score sobre el test set: {:.2f}".format(arbol.score(x_eval_np, y_eval_np)))  # Usando el método score.\n\n\n# Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train = []\naccuracy_validation = []\nroc_train = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    # print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n\n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n\n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n\n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test))\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train))\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n\n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n\ndf = pd.DataFrame(index=range(1, 6))\ndf.index.name = "Permutación"\n\ndf["Accuracy (training)"] = accuracy_train  # cambiar por accuracies_training\ndf["Accuracy (validación)"] = accuracy_validation  # cambiar por accuracies_validation\ndf["AUC ROC (training)"] = roc_train  # cambiar por aucs_training\ndf["AUC ROC (validación)"] = roc_validation  # cambiar por aucs_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nprint("Coeficiente de Clustering C para la componente gigante: " + str(nx.average_clustering(gig_comp_graph)))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ndef plot_stacked_scores(athlete):\n    plt.figure(figsize=(30, 10))\n\n    x = np.arange(len(athlete))\n\n    athlete[\'additional_index\'] = x\n    scores = [\n        athlete.climbing_scores,\n        athlete.yoga_scores,\n        athlete.tech_scores,\n        athlete.power_scores,\n        athlete.gym_scores,\n        athlete.arc_scores,\n        athlete.hang_scores,\n    ]\n\n    labels = [\'Climbing\', \'Yoga\', \'Technique\', \'Power\', \'Gymnastics\', \'ARC\', \'Hangboarding\']\n    colors = [\'cornflowerblue\', \'darkturquoise\', \'mediumorchid\', \'red\', \'gold\', \'gray\', \'lightgreen\']\n\n    y = np.vstack(scores)\n    plt.stackplot(x, y, labels=labels, colors=colors)\n    plt.plot(x, athlete.scores, marker=\'o\', color=\'lightgray\')\n\n    for index, row in athlete.iterrows():\n        if row.notes:\n            plt.annotate(row.notes, xy=(row.additional_index, row.scores), fontsize=14, fontweight=\'bold\')\n\n    plt.legend(loc=2)\n    plt.show()\n\n\ndef plot_scores(athlete):\n    """\n    Plot the following scores:\n\n    - Hangboard\n    - Climbing\n    - Gymnastics\n    - Technical Scores\n\n    """\n    fig, axes = plt.subplots()\n    ax1 = fig.add_subplot(221)\n    ax2 = fig.add_subplot(222)\n    ax3 = fig.add_subplot(223)\n    ax4 = fig.add_subplot(224)\n\n    ax1.set_title(\'Hangboarding\')\n    ax2.set_title(\'Gymnastics\')\n    ax3.set_title(\'Climbing\')\n    ax4.set_title(\'Technique\')\n\n    athlete_hang_scores = athlete[athlete.hang > 0]\n    athelete_gym_scores = athlete[athlete.gym_scores > 0]\n    athlete_climbing_scores = athlete[athlete.climbing_scores > 0]\n    athlete_tech_scores = athlete[athlete.tech_scores > 0]\n\n    # plt.figure(figsize=(20,10))\n    # plt.plot(athlete_hang_scores.hang_scores, marker=\'o\', color=\'green\')\n\n    athlete_hang_scores.hang_scores.plot(ax=ax1, figsize=(20, 10), marker=\'o\', color=\'green\')\n    athelete_gym_scores.gym_scores.plot(ax=ax2, figsize=(20, 10), marker=\'o\', color=\'gold\')\n    athlete_climbing_scores.climbing_scores.plot(ax=ax3, figsize=(20, 10), marker=\'o\', color=\'cornflowerblue\')\n    athlete_tech_scores.tech_scores.plot(ax=ax4, figsize=(20, 10), marker=\'o\', color=\'mediumorchid\')\n\n    plt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ntry:\n    point[0] = 20\nexcept (TypeError) as er:\n    print("TypeError:", er)\nelse:\n    raise Exception'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfreq_plot <- function(CR, num_tests = 2, Bayes = TRUE) {\n    # if Bayes == TRUE, plot Bayesian estimate and Credible Interval\n    if (Bayes == TRUE) {\n        column = \'Post_mean\'\n        LL = \'Cred_LL\'\n        UL = \'Cred_UL\'\n        title = "Bayesian: Posterior Mean and Credible Interval of Proportion Over Time"\n    }\n    if (Bayes == FALSE) {\n        column = \'CRate\'\n        LL = \'Conf_LL\'\n        UL = \'Conf_UL\'\n        title = "Frequentist: Mean and Confidence Interval of Proportion Over Time"\n    }\n\n    # -------------------------Set plot color ----------------------------\n    cbPalette <- c("#009E73", "#0072B2", "#E69F00", "#D55E00", "#CC79A7", "#F0E442", "#56B4E9", "#999999")\n    fill_colors = makeTransparent(cbPalette)\n    # ------------------------Plot settings:---------------------------------\n    # compute the upper and lower bound of y-axis to be 20% and 80% quantile of the upper and lower bound\n    min_val = min(quantile(CR$Cred_LL, 0.01), quantile(CR$Conf_LL, 0.01))\n    max_val = max(quantile(CR$Cred_UL, 0.99), quantile(CR$Conf_UL, 0.01))\n    max_days = quantile(CR$Day, 0.8)  # x-axis position to put legend\n\n    # -------------------------------------------------------------------------\n    data = CR[CR[, \'Test_group\'] == 0, ]\n    p <- plot(data[, \'Day\'], data[, column], type = "l", lwd = 3, col = "red", lty = 1, ylim = c(min_val, max_val),\n              main = title,\n              xlab = \'Days after tests start\', ylab = \'Proportion\')\n    polygon(c(data[, \'Day\'], rev(data[, \'Day\'])), c(data[, LL], rev(data[, UL])),\n            col = rgb(1, 0, 0, 0.1), border = NA)\n    # -------------------------------------------------------------------------\n    abline(h = 0)\n    # plot the rest test groups\n    for (k in 1:num_tests) {\n        data = CR[CR[, \'Test_group\'] == k, ]\n        lines(data[, \'Day\'], data[, column], type = "l", lwd = 3, col = cbPalette[k], lty = k + 1)\n        polygon(c(data[, \'Day\'], rev(data[, \'Day\'])), c(data[, LL], rev(data[, UL])),\n                col = fill_colors[k], border = NA)\n    }\n    # -------------------------------------------------------------------------\n    # add legend to the plot\n    legend_list = c()\n    for (k in 1:num_tests) {\n        legend_list = c(legend_list, paste0("Test ", k))\n    }\n    legend(max_days, max_val, legend = legend_list,\n           col = c("red", cbPalette[2:k]), lty = 1:(k + 1), cex = 0.8, title = "Test group")\n}\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1, plt_num=1):\n    # Here make sure to preprocess your image_input in a way your network expects\n    # with size, normalization, ect if needed\n    # image_input =\n    # Note: x should be the same name as your network\'s tensorflow data placeholder variable\n    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n    activation = tf_activation.eval(session=sess, feed_dict={x: image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15, 15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6, 8, featuremap+1)  # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap))  # displays the feature map number\n        if activation_min != -1 and activation_max != -1:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", vmin=activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min != -1:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0, :, :, featuremap], interpolation="nearest", cmap="gray")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Treino - DROPNA\ntreino_dropna_slice_nome = treino_dropna['Nome'].str.split(',', expand=True)\ntreino_dropna['Primeiro Nome'] = treino_dropna_slice_nome[0]\ntreino_dropna_nome_slice = treino_dropna_slice_nome[1].str.split('.', expand=True, n=1)\ntreino_dropna_complemento_slice = treino_dropna_nome_slice[1].str.split('(', expand=True)\ntreino_dropna['Saudação'] = treino_dropna_nome_slice[0]\ntreino_dropna['Sobrenome'] = treino_dropna_complemento_slice[0]\ntreino_dropna['Complemento'] = treino_dropna_complemento_slice[1]\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nnull_info_dataframe = reduce(lambda left, right: pandas.merge(left, right, on='index'), \n                             [percent_null,\n                              percent_filled,\n                              filled_count_series,\n                              null_count_series\n                             ]\n                            )\n```"}, {'reason': 'stop', 'result': "```python\nimport numpy as np\n\nmatrix_1 = np.array([[4,2,1,3,5]])\nmatrix_2 = np.array([[4],[2], [1], [3], [5]])\nmatrix_3 = np.dot(matrix_1,matrix_2)\nmatrices_1 = [matrix_1, matrix_2, matrix_3]\nnames_1 = ['matrix_1', 'matrix_2', 'matrix_3']\n\nvisulize_multiplication(matrices_1, names_1)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Here's the number of streets away:\nnum_streets_away = abs(42-34)\n\n# Compute the number of avenues away in a similar way:\n# YOUR CODE HERE\nraise NotImplementedError()\n\nstreet_length_m = 80\navenue_length_m = 274\n\n# Now we compute the total distance Chunhua must walk.\nnum_avenues_away = 0  # Define num_avenues_away before using it\nmanhattan_distance = street_length_m * num_streets_away + avenue_length_m * num_avenues_away\n\n# We've included this line so that you see the distance\n# you've computed when you run this cell.  You don't need\n# to change it, but you can if you want.\nmanhattan_distance\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# If use the ordinary mean() we get "nan" meaning "can\'t calculate on this array".\nprint(np.mean(data2))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# corrs = [full(iteration)[1, 0] for iteration in itervars]\ncorrs[:10]\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# graficamos la longitud de pétalo para el primer tipo de flores\n\nfigure(1)\nscatter(0.1 * randn(50, 1), caracteristicas[1:50, 1])\nscatter(1 + 0.1 * randn(50, 1), caracteristicas[51:100, 1])\nscatter(2 + 0.1 * randn(50, 1), caracteristicas[101:150, 1])\nylabel("Longitud del Pétalo (cm)")\n\n\n# graficamos la anchura del pétalo\n\nfigure(2)\n\n\n# graficamos la longitud del sépalo\n\nfigure(3)\n\n\n# graficamos la anchura del sépalo\n\nfigure(4)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nsparse_data = my_spca.transform(X)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(121)\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nnew_pc_cen = sparse_data - sparse_data.mean(0, keepdims=True)\ncov = new_pc_cen.T @ new_pc_cen / (new_pc_cen.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\n\nplt.subplot(122)\nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red', alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# graficamos los datos nuevamente\nscatter(x[1:50, 1], x[1:50, 2], alpha=0.25, color="b")\nscatter(x[51:100, 1], x[51:100, 2], alpha=0.25, color="r")\nscatter(x[101:150, 1], x[101:150, 2], alpha=0.25, color="g")\nxlabel("Longitud del Pétalo (cm)")\nylabel("Anchura del Pétalo (cm)")\ngrid("on")\n\n# obtenemos los vectores con las características promedio para cada una de las clases de flores\nprom_1 = mean(x[1:50, :], 1)\nprom_2 = mean(x[51:100, :], 1)\nprom_3 = mean(x[101:150, :], 1)\n\n# graficamos los vectores\nquiver(prom_1[1, 1], prom_1[1, 2], angles="xy", scale_units="xy", scale=1, color="b")\nquiver(prom_2[1, 1], prom_2[1, 2], angles="xy", scale_units="xy", scale=1, color="r")\nquiver(prom_3[1, 1], prom_3[1, 2], angles="xy", scale_units="xy", scale=1, color="g")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# perfroming  linear regression with scikit package\nimport sklearn.cross_validation\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\nerrors = []\nk_folds = 10\n\nfor s in getListOfFiles(\'Data/\'):\n    \n    data = getListFromAFile("Data/" + s)\n    kf = sklearn.cross_validation.KFold(n=len(data[0]), n_folds=k_folds, shuffle=False, random_state=None)\n    error = 0\n    regression = linear_model.LinearRegression()\n    \n    for train_index, test_index in kf:\n        # print("TRAIN:", train_index, "TEST:", test_index)\n        X_train, X_test = data[0][train_index], data[0][test_index]\n        y_train, y_test = data[1][train_index], data[1][test_index]\n        regression.fit(X_train.reshape(-1, 1), y_train)\n        error = error + mean_squared_error(y_test, regression.predict(X_test.reshape(-1, 1)))\n       \n    error = error / k_folds\n    createPlots(data[0].reshape(-1, 1), data[1], xlabel="x-axis -->", ylabel="y-axis -->", title="s:" + s + "|degree:linear reegression", plotterRef=regression.predict)\n    errors.append({\'file\': s, \'mse\': error})\n\nprint(errors)\ndf = pd.DataFrame(errors)\nprint(df)\n```'}, {'reason': 'stop', 'result': '```python\nfrom collections import defaultdict\nimport math\nimport logging\n\n"""\nUsage:\npython viterbi.py ner.counts ngram.counts ner_dev.dat > [output_file]\nImplementation of the Viterbi algorithm\nCalculate emission e(x|y) and trigram probability based on data \nin ner_counts,\nRead ner_dev.dat, output prediction to [output_file]\n"""\n\n\nlog_probability = 0\ny_predict = []\ny_actual = []\n\n# First round for q(*, *, y_1)\nfirst_round = True\nfor sent in test_sents:\n    log_probability = 0\n    first_round = True\n    for i in range(len(sent)):\n        word = sent[i][0]\n        # Check if there is an existing label associated to the word\n        if word in counter.count_xy:\n            max_probability = 0\n            for label in list(counter.count_xy[word]):\n                # Calculate e(x|y)\n                emission = float(counter.count_xy[word][label]) / float(counter.count_y[label])\n                # Calculate q(y| y_i-2, y_i-1)\n                # Check for first round\n                if first_round:\n                    y_2 = \'*\'\n                    y_1 = \'*\'\n                    first_round = False\n                bigram = y_2 + \' \' + y_1\n                trigram = y_2 + \' \' + y_1 + \' \' + label\n                parameter = 0.0000000001\n                if trigram in counter.trigram_counts:\n                    parameter = float(counter.trigram_counts[trigram]) / float(counter.bigram_counts[bigram])\n                probability = parameter * emission\n                if probability > max_probability:\n                    max_probability = probability\n                    arg_max = label\n\n            log_probability = log_probability + math.log(max_probability)\n            y_actual.append(sent[i][2])\n            y_predict.append(arg_max)\n            y_2 = y_1\n            y_1 = arg_max\n        else:\n            y_predict.append(\'O\')\n            y_actual.append(sent[i][2])\n\n\n#     # If Count(x~>y) = 0, use _RARE_ \n#     else:\n#         for label in list(count_xy[\'_RARE_\']):\n#             # Calculate e(_RARE_|y)\n#             probability = 0\n#             emission = float(count_xy[\'_RARE_\'][label]) / float(count_y[label])\n#             # Calculate q(y| y_i-2, y_i-1)\n#             # Check for first round\n#             if first_round:\n#                 y_2 = \'*\'\n#                 y_1 = \'*\' \n#                 first_round = False\n#             bigram = y_2 + \' \' + y_1\n#             trigram = y_2 + \' \' + y_1 + \' \' + label\n#             parameter = 0.0000000001\n#             if trigram in trigram_counts:\n#                 parameter = float(trigram_counts[trigram])/float(bigram_counts[bigram])\n#             probability = parameter*emission\n#             if probability > max_probability:\n#                 max_probability = probability\n#                 arg_max = label\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n### Tu código aquí para acomodar los vectores promedio en una matriz llamada W ###\nW = None\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Include the observations, which are Bernoulli\nwith model:\n    obs = pm.Bernoulli("obs", p, observed=occurrences)\n    # To be explained in chapter 3.\n    step = pm.Metropolis()\n    trace = pm.sample(18000, step=step)\n    burned_trace = trace[1000:]\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nimport sms\n\na = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 0].Invalid.mean()\nresult = sms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 0].Invalid).tconfint_mean()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# DATAFRAME STRUCTURE\nBOOKINGS_DF_STRUCT = pd.read_csv(BOOKINGS, error_bad_lines=False, encoding='UTF8', sep='^', nrows=1)\n\n# Dataframe using suggested columns\n# Comment for the second dataframe assignment\nBOOKINGS_DF_EX = pd.read_csv(BOOKINGS, error_bad_lines=False, encoding='UTF8', sep='^', usecols=['arr_port', 'pax'])\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nmc_fraction = 'pileUpFilterEfficiency_MC.png'\ndata_fraction = 'pileUpFilterEfficiency_DATA.png'\n    Image(filename=pathname + mc_fraction)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n### Run the predictions here and use the model to output the prediction for each image.\n\n### Make sure to pre-process the images with the same pre-processing pipeline used earlier.\n\n### Feel free to use as many code cells as needed.\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nsw_inv = np.linalg.inv(Sw)\nI = np.matmul(Sw, sw_inv)\n\nprint('Sw:', Sw)\nprint('Inverted Sw:', sw_inv)\nprint('Identity matrix:', I)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# i = 0\ndraw_rule_and_reg_id(ids_cos_sim_high_3_wk[i])\ni += 1\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Test test test\n# Simulate a single forward pass\nX_prior.reset()\nX.reset()\nZ_prior.reset()\nZ.reset()\nY_cond.reset()\nY.reset()\n\nX_prior.send_sp_msg(X)\nZ_prior.send_sp_msg(Z)\nX.send_sp_msg(Y_cond)\nZ.send_sp_msg(Y_cond)\nY_cond.send_sp_msg(Y)\n\nassert np.allclose(X.marginal(), [0.95, 0.05])\nassert np.allclose(Z.marginal(), [0.8, 0.2])\nassert np.allclose(Y.marginal(), [0.821024, 0.178976])\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# print the zeroth element\nprint(a_list[3])\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n\nX = np.linspace(-1, 1, 256)\nplt.plot(X + 0.5, -Hart(X, 1)[1] + 0.5, "k-", label="H = 1")\n# plt.plot(X + 0.5, -Hart(X, 10)[1] + 0.5, "b-", label="H = 10")\n# plt.plot(X + 0.5, -Hart(X, 0.1)[1] + 0.5, "r-", label="H = 0.1")\nplt.xlabel("z")\nplt.ylabel("$B_{x}$")\nplt.title("Campo magnetico Hartmann")\nplt.grid(True)\nplt.legend()\nplt.savefig("Magnetico_Hartmann")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport itertools\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.externals.six import StringIO\nfrom sklearn.feature_extraction import FeatureHasher, DictVectorizer\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom IPython.core.display import Image, display\nfrom IPython.display import Image\nfrom bokeh.charts import Bar, output_file, show, output_notebook\nfrom tensorflow import keras\n\noutput_notebook()\n\n%matplotlib inline\n```'}, {'reason': 'stop', 'result': '```python\nprint(np.linalg.inv(A))\n```'}, {'reason': 'stop', 'result': "```python\na = RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': "```python\n### TODO: create required RNN model\n# Import keras network libraries\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport keras\n\n# Given - fix random seed - so we can all reproduce the same results on our default time series\nnp.random.seed(0)\n\n\n# TODO: implement build_part1_RNN in my_answers.py\nfrom my_answers import build_part1_RNN\n\nmodel = build_part1_RNN(window_size)\n\n# Build model using keras documentation recommended optimizer initialization\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# Compile the model\nmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n```"}, {'reason': 'stop', 'result': "```python\nimport matplotlib.pyplot as plt\n\n# Dynamic critic of grandiant descent between Distriminator and Generator.\nfilename = 'GANDeconv_t9999_h100_train2_ep36'\nhidden_dim = 100\n\nG, D, train_hist = GAN_CelebA.loadCheckpoint(filename, hidden_dim, use_cuda=use_cuda)\nepoch_num = len(train_hist['D_losses'])\nGAN_CelebA.show_result(G, D, epoch_num, hidden_dim, show=True, save=True, path='figures/' + filename + '.pdf', use_cuda=use_cuda)\n\nplt.plot(range(0, epoch_num), train_hist['D_losses'], label='D_loss')\nplt.plot(range(0, epoch_num), train_hist['G_losses'], label='G_loss')\n# plt.plot(range(0, epoch_num), train_hist['Inc_score'], linestyle='--', label='Inc_score')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('DCGAN dynamic, total time: ' + str(int(train_hist['total_ptime'][-1] / 60)) + ' minutes')\nplt.legend()\nplt.savefig('figures/' + filename + '_Loss.pdf')\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ny_test_predict = clf.predict(X_test)\nnp.size(y_test_predict)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport os\nimport sys\nfrom ROOT import gROOT\nimport numpy as np\n\nsys.path.insert(0, os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Support\'))\nsys.path.insert(0, os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Unfolding\'))\nfrom Utilities import pause\nfrom Root import CalibParams\n\noutPath = "/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/Inputs/"\nrspPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Simulated/PHS/ResponseMatrices/simSideResponse20Mil.root\'\ncalPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/CalibData/\'\n\nos.chdir(outPath)\nprint(\'Currently working in: \\n {}\'.format(os.getcwd()))\n\ndetNames = {0: \'Det0\'}  # , 2: \'Det45\', 4: \'Det90\'}\ncalNames = {0: \'CalibParams_0.txt\'}  # , 2: \'CalibParams_2.txt\', 4: \'CalibParams_4.txt\'}\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# TODO: Choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it.\n# Get an appropriately sized chunk of characters from the text.\nstart_inds = []\n\n# Save output\nf = open(\'text_gen_output/RNN_large_textdata_output.txt\', \'w\')  # Create an output file to write to.\n\n# Load weights\nmodel.load_weights(\'model_weights/best_RNN_large_textdata_weights.hdf5\')\nfor s in start_inds:\n    start_index = s\n    input_chars = text[start_index: start_index + window_size]\n\n    # Use the prediction function\n    predict_input = predict_next_chars(model, input_chars, num_to_predict=100)\n\n    # Print out input characters\n    line = \'-------------------\' + \'\\n\'\n    print(line)\n    f.write(line)\n\n    input_line = \'input chars = \' + \'\\n\' + input_chars + \'"\' + \'\\n\'\n    print(input_line)\n    f.write(input_line)\n\n    # Print out predicted characters\n    predict_line = \'predicted chars = \' + \'\\n\' + predict_input + \'"\' + \'\\n\'\n    print(predict_line)\n    f.write(predict_line)\n\nf.close()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Train your model here.\n# Feel free to use as many code cells as needed.\n```'}, {'reason': 'stop', 'result': '```python\nparams_list = []\nfor key, value in best_model.params.items():\n    params_list.append(str(key) + " = " + str(value[\'actual\']))\n    params_list\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nt_min_max = (vsig.timestamps[0], vsig.timestamps[-1])\nlayer = '1'\nval_arrays = np.load(os.path.join(vsig.out_dir, 'valid_hidden_layer_' + layer + '_output.npy'))\nn_generations, _, n_neurons = val_arrays.shape\nncols = 1\nnrows = n_neurons // ncols\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3))\n\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes#[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(\n            vsig.timestamps[vsig.window_size-1:], \n            vsig.mixed_signal[vsig.window_size-1:], \n            marker='o', \n            c=y_pred_colors, \n            cmap=plt.get_cmap('coolwarm'), \n            vmin=-1, \n            vmax=1\n        )\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n        \n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, 'output', g + 1))\n    plt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n#%%iql2\n# indeedians = from indeedemployeesnapshot yesterday today\n# group by ldap, full_name\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n\n\n# Load dataset and writing the custom parser\ndef parser(x):\n    return datetime.strptime(x, \'%m/%d/%y\')\n\n# Man gotta figure out the strptime function to see how it actually works \n\n# Date_parser always get called to pass a function understand the dates\nseries = read_csv(\n    "/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv",\n    header=0,\n    parse_dates=[0],\n    index_col=0,\n    squeeze=True,\n    date_parser=parser\n)\nseries.head()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\ndef optimize_theta(x, y, kernel, params_0=[0.1, 0.1], sigma_n=0.1):\n    def log_pY(theta):                              # -\n        K = np.matrix(kernel(x, x, theta, sigma_n)) # -\n        f, y_giv_f = find_f(K, y)                   # -\n        W = util.calculate_W(f, y)                  # -\n        inv_k = np.linalg.inv(K)                    # -\n        log_k = np.log(np.linalg.det(K) * np.linalg.det(inv_k+W)) # -\n        Y_giv_f = np.prod(y_giv_f)                                # -\n        output = 0.5 * np.matmul(np.matmul(f.T, inv_k),f)         # -\n        output += 0.5 * log_k                                     # -\n        output -= np.log(Y_giv_f)                                 # -\n        return output                                             # -\n\n    res = minimize(log_pY, params_0, method='nelder-mead', options={'xtol': 1e-8, 'disp': False}) # -\n    return list(res.x) + [sigma_n]    # -\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfirstClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nfirstClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nsecondClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                    dfTitanic.loc[(dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nsecondClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                    dfTitanic.loc[(dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nthirdClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nthirdClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\n\nprint("First Class Male Fraction: {0:.4f} \\t First Class Female Fraction {1:.4f}\\n"\n      "Second Class Male Fraction: {2:.4f} \\t Second Class Female Fraction {3:.4f}\\n"\n      "Third Class Male Fraction: {4:.4f} \\t Third Class Female Fraction {5:.4f}".format(firstClassMRate["Survived"],\n                                                                                           firstClassFRate["Survived"],\n                                                                                           secondClassMRate["Survived"],\n                                                                                           secondClassFRate["Survived"],\n                                                                                           thirdClassMRate["Survived"],\n                                                                                           thirdClassFRate["Survived"]))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Calculate how much information is gained on each column\n# Calculate the entropy of the subset\ndef information_gain(table, dependent, independent):\n    table = table[table[dependent].notnull()]\n    freq = []\n    for dval in table[dependent].unique():\n        freq += [sum(table[dependent] == dval)]\n    freq = [float(f) / sum(freq) for f in freq]\n    e = 0\n    for f in freq:\n        e += -f * np.log(f) / np.log(2)\n    # print 'Subset Entropy:', e\n    vars = []\n\n    # Calculate the entropy of each variable\n    for ind in independent:\n        if ind in categorical:\n            ig = e\n            for ival in table[ind].unique():\n                if np.isnan(ival):\n                    continue\n                sub_table = table[table[ind] == ival]\n                # print sub_table\n                freq = []\n                for dval in table[dependent].unique():\n                    freq += [sum(sub_table[dependent] == dval)]\n                freq = [float(f) / sum(freq) for f in freq]\n                # print freq\n                es = 0\n                for f in freq:\n                    es += -f * np.log(f) / np.log(2) if f != 0 else 0\n                # print es\n                ig -= float(len(sub_table)) / len(table) * es\n            # print 'Information gain from %s: %f'%(ind,ig)\n            vars += [(ig, ind)]\n        else:\n            thresholds = []\n            prev_sub_table_a_len = 0\n            for ival in np.arange(min(table[ind]), max(table[ind]), (max(table[ind]) - min(table[ind])) / 500.0):\n                ig = e\n                sub_table_a = table[table[ind] < ival]\n                sub_table_b = table[table[ind] >= ival]\n                if len(sub_table_a) < 1 or len(sub_table_b) < 1:\n                    continue\n                if len(sub_table_a) == prev_sub_table_a_len:\n                    continue\n                else:\n                    prev_sub_table_a_len = len(sub_table_a)\n                freq = []\n                for dval in table[dependent].unique():\n                    freq += [sum(sub_table_a[dependent] == dval)]\n                freq = [float(f) / sum(freq) for f in freq]\n                # print freq\n                es = 0\n                for f in freq:\n                    es += -f * np.log(f) / np.log(2) if f != 0 else 0\n                # print es\n                ig -= float(len(sub_table_a)) / len(table) * es\n                # print sub_table\n                freq = []\n                for dval in table[dependent].unique():\n                    freq += [sum(sub_table_b[dependent] == dval)]\n                freq = [float(f) / sum(freq) for f in freq]\n                # print freq\n                es = 0\n                for f in freq:\n                    es += -f * np.log(f) / np.log(2) if f != 0 else 0\n                # print es\n                ig -= float(len(sub_table_b)) / len(table) * es\n                thresholds += [(ig, ival)]\n            thresholds.sort(reverse=True)\n            # print thresholds\n            # break\n            # print 'Information gain from %s: %f at threshold:%f'%(ind,thresholds[0][0],thresholds[0][1])\n            if len(thresholds) > 0:\n                vars += [(thresholds[0][0], ind, thresholds[0][1])]\n            else:\n                vars += [(0, ind)]\n    information_gain = pandas.Series([v[0] for v in vars], index=[v[1] for v in vars], name='Information Gain')\n    thresholds = pandas.Series([v[2] for v in vars if len(v) > 2], index=[v[1] for v in vars if len(v) > 2],\n                               name='Threshold')\n    return pandas.concat([information_gain, thresholds], axis=1)\n\n\nif __name__ == '__main__':\n    q_cat = ['resp.simple', 'relapse', 'vital.status', 'overall_survival_binned', 'remission_duration_binned']\n    ts = []\n    for variable in q_cat:\n        print(variable)\n        independent = [v for v in q_training.keys() if v in q_scoring.keys()]\n        ts += [information_gain(q_training, variable, independent)]\n    information_gain = pandas.concat(ts, keys=q_cat, axis=1)\n    information_gain.to_csv('information_gain.csv')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Resposta: Sim\n\nmesmo_numero_passagem = train.groupby(['Sobreviventes', 'PassagemNumero']).size()\n\nmesmo_numero_passagem\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\na = RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid).tconfint_mean()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n\n\ndef formfaktor(winkel, a, amp=1):\n    # winkel = winkel + offset\n    q = 4 * np.pi * n / wavelen * np.sin(winkel * gamma / 2)\n    \n    return 9 * (np.sin(q * a) - (q * a) * np.cos(q * a)) ** 2 / (q * a) ** 6 * amp\n\n\ndef formfaktorQ(q, a, amp=1):\n    return 9 * (np.sin(q * a) - (q * a) * np.cos(q * a)) ** 2 / (q * a) ** 6 * amp\n\n# Funktion funktioniert für yscale="log" bei ca. a=500nm\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Arbol de sklearn #\narbol = DecisionTreeClassifier(max_depth=3)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nstart_day = (pd.to_datetime(\'2016-11-01\').date() - datetime.timedelta(14)).strftime("%Y-%m-%d")\nfor shop_id in ids_zero_morethan1_count_in_last_three_week_but_no_zero_in_last_two_week:\n    for day_number in range(1, 8):  # 1~7\n        predict_day = \'2016-11-\' + \'%02d\' % day_number\n        predict_day = pd.to_datetime(predict_day).date()\n        day_list = [day for day in dateRange(start_day, \'2016-10-31\') if (predict_day - pd.to_datetime(day).date()).days % 7 == 0]\n        shop_info.loc[shop_id, \'wave_model1_\' + str(day_number)] = shop_info.loc[shop_id, day_list].mean()\n\n    if shop_id in ids_stable_period_1_2_3:\n        for i in range(1, 8):\n            shop_info.loc[shop_id, \'wave_model_total_\' + str(i)] = 0.5 * shop_info.loc[shop_id, \'wave_model1_\' + str(i)] + 0.5 * shop_info.loc[shop_id, \'wave_model2_\' + str(i)]\n\n    else:\n        for i in range(1, 8):\n            shop_info.loc[shop_id, \'wave_model_total_\' + str(i)] = shop_info.loc[shop_id, \'wave_model1_\' + str(i)]\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# ignore\nimport pysnooper\n!pip install pysnooper\n\nclear_output()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport sys\n\nprint(sys.executable)\nprint(sys.path)\n\nfrom padma.models.bbox.bbox import find_conteiner\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nrun(track=trk, zs=zs, R=var, Q=.02, P=1., plot_P=True,\n    title='$P=1\\, m^2$')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Valor total - bruto: 891\n# Valor total - Sem Nulos: 712\ntrain_dropna = train.dropna(subset=colunasSemCabine)\n    train_dropna.describe()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\njob_assesement = [\n    'AssessJob1', 'AssessJob2', 'AssessJob3', 'AssessJob4', 'AssessJob5',\n    'AssessJob6', 'AssessJob7', 'AssessJob8', 'AssessJob9', 'AssessJob10',\n]\nff = top10_df.groupby('Country').mean()[job_assesement].reset_index()\nff_p = ff.pivot_table(columns='Country')\nplt.figure(figsize=(14, 8))\nfor country in top_10_list:\n    plt.plot(ff_p[country], label=country)\nplt.legend()\nsns.despine(left=True)\nplt.title('Job assessement comparaison by country', fontsize=21)\n```"}, {'reason': 'stop', 'result': '```python\nfreq_plot(CR=CR, num_tests=3, bayes=True)\n```'}, {'reason': 'stop', 'result': "```python\nparameters_decision_tree = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': randint(1, 200)\n}\n\n(tiempo_random_decision_tree, random_decision) = correr_randomized_y_mostrar(\n    DecisionTreeClassifier(),\n    parameters_decision_tree,\n    5,\n    5,\n    100\n)\n\nverTiempo(tiempo_decision_tree, tiempo_random_decision_tree)\n\nparameters_decision_tree_2 = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': randint(1, 200),\n    'min_samples_split': uniform(0, 1)\n}\n\n(tiempo_random_decision_tree_2, random_decision_tree_2) = correr_randomized_y_mostrar(\n    DecisionTreeClassifier(),\n    parameters_decision_tree_2,\n    5,\n    5,\n    500\n)\n\nverTiempo(tiempo_decision_tree_2, tiempo_random_decision_tree_2)\n```"}, {'reason': 'stop', 'result': "```python\nplt.matshow(doripa[['total', 'total6', 'scores']].corr())\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Produce a scatter matrix without outliers... distribution appears more normal\npd.scatter_matrix(good_data, alpha=0.3, figsize=(14, 8), diagonal='kde')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Join\njoin.isnull().sum()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}]