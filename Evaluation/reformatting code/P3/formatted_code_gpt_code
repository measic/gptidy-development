[{'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# apply the function to transform data:\nCR = transform_data(DF, a=Conf_alpha, a_0=alpha_0, b_0=beta_0)\ndim(CR)\nhead(CR)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfrom kf_book.book_plots import set_figsize\nfrom kf_book.book_plots import figsize\nfrom kf_book.nonlinear_plots import plot_gaussians\nimport numpy as np\n\nP = np.diag([3., 1.])\nnp.random.seed(3)\nMs, Ps = run(count=25, R=10, Q=0.01, P=P, do_plot=False)\n\nwith figsize(x=9, y=5):\n    plot_gaussians(Ms[::7], Ps[::7], (-5, 25), (-5, 5), 75)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n%%time\ntry:\n    subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n    print(f"載入已建立的字典： {en_vocab_file}")\nexcept:\n    print("沒有已建立的字典，從頭建立。")\n    subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n        (en.numpy() for en, _ in train_examples),\n        target_vocab_size=2**13)  # 有需要可以調整字典大小\n\n    # 將字典檔案存下以方便下次 warmstart\n    subword_encoder_en.save_to_file(en_vocab_file)\n\n\nprint(f"字典大小：{subword_encoder_en.vocab_size}")\nprint(f"前 10 個 subwords：{subword_encoder_en.subwords[:10]}")\nprint()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nImage(filename=pathname + DATAppDzNtrk)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Não.\ntest.groupby(['ParentesIrmao']).size()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ndata, labels = vectorize_docs(\n    artist="linkin-park",  # specify artist\n    albums=["hybrid-theory"],  # specify album(s)\n    keep_album=False,  # option to use the album name as a delimiter\n    titlify=True  # converts song title to original format\n)\n\nsong_to_search = "Crawling"\nfor i, song in enumerate(labels):\n    if song == song_to_search:\n        display(Markdown("**Song name: **" + labels[i]))\n        display(Markdown("**Lyrics:**"))\n        print(data[i][:1000] + "...")\n        sample_lyrics = data[i]\n        break\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\ndef character_quotient(M, N, n, r, left_basis=s, right_basis=s):  # a corriger\n    b_tot = M.basis()\n    b_ideal = N.basis()\n    charac = 0\n    q = PolynomialRing(QQ, 'q', r).gens()\n\n    for nu in Partitions(n):\n        basis_nu_tot = {}\n        basis_nu_ideal = {}\n        charac_nu = 0\n        # Get the nu_isotypic part of the bases\n        for key, value in b_tot.iteritems():\n            if Partition(key[1]) == nu:\n                basis_nu_tot[key[0]] = value\n        for key, value in b_ideal.iteritems():\n            if Partition(key[1]) == nu:\n                basis_nu_ideal[key[0]] = value\n\n        # Use the degrees to compute the character\n        for deg, b in basis_nu_tot.iteritems():\n            charac_nu += sum(prod(q[i] ** deg[i] for i in range(0, len(deg))) for p in b)\n        for deg, b in basis_nu_ideal.iteritems():\n            charac_nu -= sum(prod(q[i] ** deg[i] for i in range(0, len(deg))) for p in b)\n        if charac_nu != 0:\n            if left_basis == s:\n                charac_nu = s.from_polynomial(charac_nu).restrict_partition_lengths(r, exact=False)\n            else:\n                charac_nu = left_basis.from_polynomial(charac_nu)\n            # Make the tensor product with s[nu]\n            charac += tensor([charac_nu, right_basis(s(nu))])\n\n    return charac\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nsharedNormSpanningSystem(spanNorm[V3 1 0 0, V3 0 1 0, V3 0 0 1], spanNorm[V3 1 0 0, V3 0 1 0]::Norm ℝ³)\n```'}, {'reason': 'stop', 'result': "```python\nimport os\nimport sys\nfrom ROOT import gROOT\n\npath = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/16MeVTa/Unfold/NSD/'\nos.chdir(path)\nprint('Currently working in: \\n {}'.format(os.getcwd()))\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ncutPt = {0: 0.28, 2: 0.3, 4: 0.29}\nfor detNum, detName in detNames.iteritems():\n    gROOT.ProcessLine(\'AmBe{}->cd()\'.format(detNum))\n    # gROOT.ProcessLine(\'eventTree->Draw("(m_amplitude-m_shape)/m_shape:m_shape>>(1250,0,35000,1024,0,1)","(m_amplitude-m_shape)/m_amplitude<{}","colz")\'.format(cutPt[detNum]))\n    pause()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nwhite_output = \'test_videos_output/solidWhiteRight.mp4\'\nleftline = [(0, 0, 0, 0)]\nrightline = [(0, 0, 0, 0)]\n# To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n# To do so add .subclip(start_second,end_second) to the end of the line below\n# Where start_second and end_second are integer values representing the start and end of the subclip\n# You may also uncomment the following line for a subclip of the first 5 seconds\n# clip1 = VideoFileClip("test_videos/solidWhiteRight.mp4").subclip(0,5)\nclip1 = VideoFileClip("test_videos/solidWhiteRight.mp4")\nwhite_clip = clip1.fl_image(process_image)  # NOTE: this function expects color images!!\n%time white_clip.write_videofile(white_output, audio=False)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# TODO: Execute your algorithm from Step 6 on\n# at least 6 images on your computer.\n# Feel free to use as many code cells as needed.\n\n\ndef detect_image(img_path):\n    disp_image(img_path)\n    whose_a_good_doggy(img_path)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# some simple testing code and such\ndataset = 'mnist'\ntrain_data = data_filepath + dataset + '_data_train.csv'\ntrain_labels = data_filepath + dataset + '_labels_train.csv'\ntrain_ids = data_filepath + dataset + '_ids_train.csv'\ntest_data = data_filepath + dataset + '_data_test.csv'\ntest_ids = data_filepath + dataset + '_ids_test.csv'\ndescription = data_filepath + dataset + '_feature_descriptions.csv'\n\nproc = Preprocessor(train_data_file=train_data,\n                    train_label_file=train_labels,\n                    train_ids_file=train_ids,\n                    test_data_file=test_data,\n                    test_ids_file=test_ids,\n                    instr_file=description)\n\nproc.read_data()\n\nproc.process()\n\n# doesn't do anything yet, hasn't been implemented\nproc.select_features()\n\n# data is written to output directory\n# any existing data is overwritten\nproc.write_data()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfor x in range(4):  # by default range start at 0\n    print(x)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# 函数注解\n# python3.5引入\n# 对函数的参数类型注解\n# 对函数的返回值类型注解\n# 只对函数参数做一个铺助的说明，并不对函数参数进行类型检查\n# 提供给第三方工具，做代码分析，发现隐藏bug\n# 函数注解的信息，保存在__annotations__属性中\n# 变量注解\n# python3.6引入\ni: int = 3\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nfig = plt.figure(figsize=(15, 15))\n\ngr = gridspec.GridSpec(1, 6)\nfor i in range(num_own_examples):\n    probas = proba[i]\n    highest_class = signnames[str(np.argmax(probas, axis=0))]\n    plt.subplot(1, 6, i+1)\n    plt.imshow(X2[i])\n    plt.axis('off')\n    plt.text(0, 1, highest_class, color='k', backgroundcolor='y')\nfig.suptitle('In yellow: prediction made by the model', x=0.5, y=0.6, fontsize=20)\nplt.tight_layout()\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nformula = "accuracy ~ C(subject, Treatment(0)) + C(complexity, Treatment(3)) * C(model, Treatment(1))"\nlm = ols(formula, df)\nfit = lm.fit()\nqqplot(fit.resid)\nprint(fit.summary())\nprint(\'\\nThe accuracy of the classifier depends on the subject, \' +\n      \'model type (deep network versus logistic regression), \' +\n      \'and task complexity (CV versus consonant versus {vowel, location, degree}) \' +\n      \'(ANOVA with subject, model type, task complexity, and model-task complexity interaction, \' +\n      \'f-value: {}, p: {}). \'.format(fit.fvalue, fit.f_pvalue) +\n      \'Within this ANOVA, all treatment coefficients were significant \' +\n      \'at p<.001 with Subject 1, CV task, and logistic regression as the reference treatment.\')\nfor table in fit.summary().tables:\n    print(table.as_latex_tabular())\nplt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nux, uy, uz = Lectura("vx.dat", "vy.dat", "vz.dat")\nx, y, z = Lectura("x.dat", "y.dat", "z.dat")\n\nGrafica("vx.dat", "vy.dat", "x.dat", "y.dat", 32)\nplt.savefig("velocidad")\nGrafica("Bx.dat", "Bz.dat", "x.dat", "y.dat", 32)\nplt.savefig("Magentico")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# fit the linear regression model\nlin_mod = lm(y ~ x, data=train)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\npm.traceplot(trace, lines={"ℓ": ℓ_true, "η": η_true})\n```'}, {'reason': 'stop', 'result': '```python\nImage(filename=pathname + averagePileUp_multCollisionFrac)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nprint("Coeficiente de Clustering C para la componente gigante: " + str(nx.average_clustering(gig_comp_graph)))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ntry:\n    point[0] = 20\nexcept TypeError as er:\n    print("TypeError:", er)\nelse:\n    raise\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nnull_info_dataframe = reduce(lambda left, right: pandas.merge(left, right, on='index'), [percent_null,\n                                                                                        percent_filled,\n                                                                                        filled_count_series,\n                                                                                        null_count_series])\n```"}, {'reason': 'stop', 'result': "```python\nmatrix_1 = np.array([[4, 2, 1, 3, 5]])\nmatrix_2 = np.array([[4], [2], [1], [3], [5]])\nmatrix_3 = np.dot(matrix_1, matrix_2)\nmatrices_1 = [matrix_1, matrix_2, matrix_3]\nnames_1 = ['matrix_1', 'matrix_2', 'matrix_3']\n\n\nvisulize_multiplication(matrices_1, names_1)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# If use the ordinary mean() we get "nan" meaning "can\'t calculate on this array"\nprint(np.mean(data2))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# graficamos la longitud de pétalo para el primer tipo de flores\nfigure(1)\nscatter(0.1 * randn(50, 1), caracteristicas[1:50, 1])\nscatter(1 + 0.1 * randn(50, 1), caracteristicas[51:100, 1])\nscatter(2 + 0.1 * randn(50, 1), caracteristicas[101:150, 1])\nylabel("Longitud del Pétalo (cm)")\n\n# graficamos la anchura del pétalo\nfigure(2)\n\n# graficamos la longitud del sépalo\nfigure(3)\n\n# graficamos la anchura del sépalo\nfigure(4)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nsparse_data = my_spca.transform(X)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(121)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nnew_pc_cen = sparse_data - sparse_data.mean(0, keepdims=True)\ncov = new_pc_cen.T @ new_pc_cen / (new_pc_cen.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\n\nplt.subplot(122)\nplt.scatter(new_pc[y == 0, 0], new_pc[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(new_pc[y == 1, 0], new_pc[y == 1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.005, head_length=0.005, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.005, head_length=0.005, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# train.drop(['sentiment','seven_days'], axis=1, inplace=True)\n# test.drop(['sentiment','seven_days'], axis=1, inplace=True)\nresult_test = []\nresult_train = []\ntot = 0\nfor string in ['share', 'comment', 'zan', 'content_len', '链接', '//@', '@', '#', '【', '《', '\\[']:\n    temp = []\n    for i in test[string + '_histogram']:\n        if isinstance(i, int):\n            temp.append(np.zeros(shape=8))\n            tot += 1\n        else:\n            temp.append(i[0])\n    result_test.append(np.asarray(temp))\n    temp = []\n    for i in train[string + '_histogram']:\n        temp.append(i[0])\n    result_train.append(np.asarray(temp))\n\n    train.drop(string + '_histogram', axis=1, inplace=True)\n    test.drop(string + '_histogram', axis=1, inplace=True)\ntrain.drop(['pid', 'uid'], inplace=True, axis=1)\ntest.drop(['pid', 'uid'], inplace=True, axis=1)\n\ntrain_y = train[['share', 'comment', 'zan']].values\ntrain.drop(['share', 'comme·nt', 'zan'], axis=1, inplace=True)\ntrain_x = train.values\ntest_x = test.values\nfor i in result_train:\n    train_x = np.c_[train_x, i]\nfor i in result_test:\n    test_x = np.c_[test_x, i]\nnp.save('processed_data/train3_np', train_x)\nnp.save('processed_data/test3_np', test_x)\nnp.save('processed_data/target3_np', train_y)\n```"}, {'reason': 'stop', 'result': '```python\n# performing linear regression with scikit package\nerrors = []\nkFolds = 10\nfor s in getListOfFiles(\'Data/\'):\n    data = getListFromAFile("Data/" + s)\n    kf = sklearn.cross_validation.KFold(n=len(data[0]), n_folds=kFolds, shuffle=False, random_state=None)\n    error = 0\n    regression = linear_model.LinearRegression()\n    for train_index, test_index in kf:\n        # print("TRAIN:", train_index, "TEST:", test_index)\n        X_train, X_test = data[0][train_index], data[0][test_index]\n        y_train, y_test = data[1][train_index], data[1][test_index]\n        regression.fit(X_train.reshape(-1, 1), y_train)\n        error = error + mean_squared_error(y_test, regression.predict(X_test.reshape(-1, 1)))\n\n    error = error / kFolds\n    createPlots(data[0].reshape(-1, 1), data[1], xlabel="x-axis -->", ylabel="y-axis -->",\n                title="s:" + s + "|degree:linear regression", plotterRef=regression.predict)\n    errors.append({\'file\': s, \'mse\': error})\n\nprint(errors)\ndf = pd.DataFrame(errors)\nprint(df)\n```'}, {'reason': 'stop', 'result': '```python\nimport sys\nfrom collections import defaultdict\nimport math\nimport logging\n\n# import emission_counts\n# import transition_counts\n\n"""\nUsage:\npython viterbi.py ner.counts ngram.counts ner_dev.dat > [output_file]\nImplementation of the Viterbi algorithm\nCalculate emission e(x|y) and trigram probability based on data \nin ner_counts,\nRead ner_dev.dat, output prediction to [output_file]\n"""\n\n# Go through dev data, predict tag & compute probability based on model above\nlog_probability = 0\ny_predict = []\ny_actual = []\n# First round for q(*, *, y_1)\nfirst_round = True\nfor sent in test_sents:\n    log_probability = 0\n    first_round = True\n    for i in range(len(sent)):\n        word = sent[i][0]\n        # Check if there is an existing label associated to the word\n        if word in counter.count_xy:\n            max_probability = 0\n            for label in list(counter.count_xy[word]):\n                # Calculate e(x|y)\n                emission = float(counter.count_xy[word][label]) / float(counter.count_y[label])\n                # Calculate q(y| y_i-2, y_i-1)\n                # Check for first round\n                if first_round:\n                    y_2 = \'*\'\n                    y_1 = \'*\'\n                    first_round = False\n                bigram = y_2 + \' \' + y_1\n                trigram = y_2 + \' \' + y_1 + \' \' + label\n                parameter = 0.0000000001\n                if trigram in counter.trigram_counts:\n                    parameter = float(counter.trigram_counts[trigram]) / float(counter.bigram_counts[bigram])\n                probability = parameter * emission\n                if probability > max_probability:\n                    max_probability = probability\n                    arg_max = label\n\n            log_probability = log_probability + math.log(max_probability)\n            y_actual.append(sent[i][2])\n            y_predict.append(arg_max)\n            y_2 = y_1\n            y_1 = arg_max\n        else:\n            y_predict.append(\'O\')\n            y_actual.append(sent[i][2])\n\n#     # If Count(x~>y) = 0, use _RARE_ \n#     else:\n#         for label in list(count_xy[\'_RARE_\']):\n#             # Calculate e(_RARE_|y)\n#             probability = 0\n#             emission = float(count_xy[\'_RARE_\'][label]) / float(count_y[label])\n#             # Calculate q(y| y_i-2, y_i-1)\n#             # Check for first round\n#             if first_round:\n#                 y_2 = \'*\'\n#                 y_1 = \'*\'\n#                 first_round = False\n#             bigram = y_2 + \' \' + y_1\n#             trigram = y_2 + \' \' + y_1 + \' \' + label\n#             parameter = 0.0000000001\n#             if trigram in trigram_counts:\n#                 parameter = float(trigram_counts[trigram]) / float(bigram_counts[bigram])\n#             probability = parameter * emission\n#             if probability > max_probability:\n#                 max_probability = probability\n#                 arg_max = label\n```'}, {'reason': 'stop', 'result': '```python\nscale = sd(train$y)\ncenter = mean(train$y)\ntrain$y = (train$y - center) / scale\ntest$y = (test$y - center) / scale\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n### Tu código aquí para acomodar los vectores promedio en una matriz llamada W ###\nW = None\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# include the observations, which are Bernoulli\nwith model:\n    obs = pm.Bernoulli("obs", p, observed=occurrences)\n    # To be explained in chapter 3\n    step = pm.Metropolis()\n    trace = pm.sample(18000, step=step)\n    burned_trace = trace[1000:]\n```'}, {'reason': 'stop', 'result': "```python\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer(use_idf=True,\n                         norm='l2',\n                         smooth_idf=True)\nprint(tfidf.fit_transform(count.fit_transform(docs))\n      .toarray())\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nMC_fraction = 'pileUpFilterEfficiency_MC.png'\nDATA_fraction = 'pileUpFilterEfficiency_DATA.png'\nImage(filename=pathname + MC_fraction)  # Inline comment\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nSw_inv = np.linalg.inv(Sw)\nI = np.matmul(Sw, Sw_inv)\n\nprint('Sw:')\nprint(Sw)\n\nprint('Inverted Sw:')\nprint(Sw_inv)\n\nprint('Identity matrix')\nprint(I)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# print the zeroth element\nprint(a_list[3])\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nX = np.linspace(-1, 1, 256)\nplt.plot(X + 0.5, -Hart(X, 1)[1] + 0.5, "k-", label="H = 1")\n# plt.plot(X + 0.5, -Hart(X, 10)[1] + 0.5, "b-", label="H = 10")\n# plt.plot(X + 0.5, -Hart(X, 0.1)[1] + 0.5, "r-", label="H = 0.1")\nplt.xlabel("z")\nplt.ylabel("$B_{x}$")\nplt.title("Campo magnetico Hartmann")\nplt.grid(True)\nplt.legend()\nplt.savefig("Magnetico_Hartmann")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction import FeatureHasher, DictVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport tensorflow\nimport keras\nimport itertools\nfrom bokeh.charts import Bar, output_file, show, output_notebook\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.externals.six import StringIO\nfrom IPython.core.display import Image, display\nfrom IPython.display import Image\n\noutput_notebook()\n\n\n%matplotlib inline\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\na = RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 3].Invalid).tconfint_mean()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Dynamic critic of grandiant descent between Distriminator and Generator.\nfilename = 'GANDeconv_t9999_h100_train2_ep36'\nhidden_dim = 100\n\nG, D, train_hist = GAN_CelebA.loadCheckpoint(filename, hidden_dim, use_cuda=use_cuda)\nepoch_num = len(train_hist['D_losses'])\nGAN_CelebA.show_result(G, D, epoch_num, hidden_dim, show=True, save=True, path='figures/' + filename + '.pdf', use_cuda=use_cuda)\n\nplt.plot(range(0, epoch_num), train_hist['D_losses'], label='D_loss')\nplt.plot(range(0, epoch_num), train_hist['G_losses'], label='G_loss')\n# plt.plot(range(0, epoch_num), train_hist['Inc_score'], linestyle='--', label='Inc_score')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('DCGAN dynamic, total time: ' + str(int(train_hist['total_ptime'][-1] / 60)) + ' minutes')\nplt.legend()\nplt.savefig('figures/' + filename + '_Loss.pdf')\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\ny_test_predict = clf.predict(X_test)\nnp.size(y_test_predict)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport os\nimport sys\nfrom ROOT import gROOT\nimport numpy as np\n\nsys.path.insert(0, os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Support\'))\nsys.path.insert(0, os.path.abspath(\'/home/pyne-user/Dropbox/UCB/Computational_Tools/Scripts/Python/Unfolding\'))\nfrom Utilities import pause\nfrom Root import CalibParams\n\noutPath = "/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/Inputs/"\nrspPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Simulated/PHS/ResponseMatrices/simSideResponse20Mil.root\'\ncalPath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/CalibData/\'\n\nos.chdir(outPath)\nprint(\'Currently working in: \\n {}\'.format(os.getcwd()))\n\ndetNames = {0: \'Det0\'}  # , 2: \'Det45\', 4: \'Det90\'}\ncalNames = {0: \'CalibParams_0.txt\'}  # , 2: \'CalibParams_2.txt\', 4: \'CalibParams_4.txt\'}\n```\nNote: The W292 warning is related to the absence of a newline at the end of the file, which cannot be fixed programmatically.'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Train your model here.\n# Feel free to use as many code cells as needed.\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# y_pred = np.argmax(states['y_hat'], axis=1)\n# y_pred_colors = np.hstack([vsig.signal_colors[i] for i in y_pred])\nt_min_max = (vsig.timestamps[0], vsig.timestamps[-1])\nlayer = '1'\nval_arrays = np.load(os.path.join(vsig.out_dir, 'valid_hidden_layer_' + layer + '_output.npy'))\nn_generations, _, n_neurons = val_arrays.shape\nncols = 1\nnrows = n_neurons // ncols\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3))\n\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes#[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(\n            vsig.timestamps[vsig.window_size - 1:],\n            # vsig.timestamps,\n            # x_val[:, -1, 0],\n            # x_val[0, :, 0],\n            vsig.mixed_signal[vsig.window_size - 1:],\n            marker='o',\n            c=y_pred_colors,\n            cmap=plt.get_cmap('coolwarm'),\n            vmin=-1,\n            vmax=1\n        )\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n\n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, 'output', g + 1))\n    # plt.savefig(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, 'gen', str(g + 1)]) + '.png'))\n\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Load dataset and writing the custom parser\ndef parser(x):\n    return datetime.strptime(x, \'%m/%d/%y\')\n\n\n# Man gotta figure out the strptime function to see how it actually works\n# Date_parser always get called to pass a function understand the dates\nseries = read_csv("/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv", header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\nseries.head()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\ndef optimize_theta(x, y, kernel, params_0=[0.1, 0.1], sigma_n=0.1):\n    def log_pY(theta):  # -\n        K = np.matrix(kernel(x, x, theta, sigma_n))  # -\n        f, y_giv_f = find_f(K, y)  # -\n        W = util.calculate_W(f, y)  # -\n        inv_k = np.linalg.inv(K)  # -\n        log_k = np.log(np.linalg.det(K) * np.linalg.det(inv_k + W))  # -\n        Y_giv_f = np.prod(y_giv_f)  # -\n        output = 0.5 * np.matmul(np.matmul(f.T, inv_k), f)  # -\n        output += 0.5 * log_k  # -\n        output -= np.log(Y_giv_f)  # -\n        return output  # -\n\n    res = minimize(log_pY, params_0, method='nelder-mead', options={'xtol': 1e-8, 'disp': False})  # -\n    return list(res.x) + [sigma_n]  # -\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nfirstClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nfirstClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 1) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nsecondClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                    dfTitanic.loc[(dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nsecondClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                    dfTitanic.loc[(dfTitanic["Pclass"] == 2) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nthirdClassMRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "male"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "male"), ["Survived"]].count())\nthirdClassFRate = (dfTitanic.loc[(dfTitanic["Survived"] == 1) & (dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "female"), ["Survived"]].count() /\n                   dfTitanic.loc[(dfTitanic["Pclass"] == 3) & (dfTitanic["Sex"] == "female"), ["Survived"]].count())\nprint("First Class Male Fraction: {0:.4f} \\t First Class Female Fraction {1:.4f}\\nSecond Class Male Fraction: {2:.4f} \\t Second Class Female Fraction {3:.4f}\\nThird Class Male Fraction: {4:.4f} \\t Third Class Female Fraction {5:.4f}".format(firstClassMRate["Survived"], firstClassFRate["Survived"], secondClassMRate["Survived"], secondClassFRate["Survived"], thirdClassMRate["Survived"], thirdClassFRate["Survived"]))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nimport sys\nfrom datetime import timedelta\n\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nfrom tqdm import tqdm\nfrom dask.distributed import Client\nfrom dask.distributed import LocalCluster as Cluster\nfrom mcmc import smc, dist\nfrom models import gr4hh\nfrom mcmc_utils import dist_map, get_likelihood_logp, get_prior_logp\nfrom misc import get_precipitation, get_pet, get_label_tree, startswith_label, get_mask, get_masks, str2datetime, get_peq_from_df, gcs_get_dir\nfrom virtual_stations import get_waterlevel\n\nimport random\nimport subprocess\nimport pickle\nimport os\n\nsys.path.append('../python')\n\nimport gcsfs\n\nis_pangeo_data = False  # True if in Pangeo binder, False if in laptop\nif is_pangeo_data:\n    from dask_kubernetes import KubeCluster as Cluster\n    n_workers = 10\nelse:\n    n_workers = 4\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Calculate how much information in gained on each column\n# Calculate the entropy of the subset\ndef information_gain(Table, Dependent, Independent):\n    Table = Table[Table[Dependent].notnull()]\n    freq = []\n    for dval in Table[Dependent].unique():\n        freq += [sum(Table[Dependent] == dval)]\n    Freq = [float(f) / sum(freq) for f in freq]\n    E = 0\n    for f in Freq:\n        E += -f * np.log(f) / np.log(2)\n    # print 'Subset Entropy:', E\n    Vars = []\n\n    # Calculate the entropy of each variable\n    for ind in Independent:\n        if ind in Categorical:\n            IG = E\n            for ival in Table[ind].unique():\n                if np.isnan(ival):\n                    continue\n                SubTable = Table[Table[ind] == ival]\n                # print SubTable\n                freq = []\n                for dval in Table[Dependent].unique():\n                    freq += [sum(SubTable[Dependent] == dval)]\n                Freq = [float(f) / sum(freq) for f in freq]\n                # print Freq\n                ES = 0\n                for f in Freq:\n                    ES += -f * np.log(f) / np.log(2) if f != 0 else 0\n                # print ES\n                IG -= float(len(SubTable)) / len(Table) * ES\n            # print 'Information gain from %s: %f'%(ind,IG)\n            Vars += [(IG, ind)]\n        else:\n            Threeshold = []\n            prev_SubTableA_len = 0\n            for ival in np.arange(min(Table[ind]), max(Table[ind]), (max(Table[ind]) - min(Table[ind])) / 500.0):\n                IG = E\n                SubTableA = Table[Table[ind] < ival]\n                SubTableB = Table[Table[ind] >= ival]\n                if len(SubTableA) < 1 or len(SubTableB) < 1:\n                    continue\n                if len(SubTableA) == prev_SubTableA_len:\n                    continue\n                else:\n                    prev_SubTableA_len = len(SubTableA)\n                freq = []\n                for dval in Table[Dependent].unique():\n                    freq += [sum(SubTableA[Dependent] == dval)]\n                Freq = [float(f) / sum(freq) for f in freq]\n                # print Freq\n                ES = 0\n                for f in Freq:\n                    ES += -f * np.log(f) / np.log(2) if f != 0 else 0\n                # print ES\n                IG -= float(len(SubTableA)) / len(Table) * ES\n                # print SubTable\n                freq = []\n                for dval in Table[Dependent].unique():\n                    freq += [sum(SubTableB[Dependent] == dval)]\n                Freq = [float(f) / sum(freq) for f in freq]\n                # print Freq\n                ES = 0\n                for f in Freq:\n                    ES += -f * np.log(f) / np.log(2) if f != 0 else 0\n                # print ES\n                IG -= float(len(SubTableB)) / len(Table) * ES\n                Threeshold += [(IG, ival)]\n            Threeshold.sort(reverse=True)\n            # print Threeshold\n            # break\n            # print 'Information gain from %s: %f at theeshold:%f'%(ind,Threeshold[0][0],Threeshold[0][1])\n            if len(Threeshold) > 0:\n                Vars += [(Threeshold[0][0], ind, Threeshold[0][1])]\n            else:\n                Vars += [(0, ind)]\n    Information_gain = pandas.Series([v[0] for v in Vars], index=[v[1] for v in Vars], name='Information Gain')\n    Threesholds = pandas.Series([v[2] for v in Vars if len(v) > 2], index=[v[1] for v in Vars if len(v) > 2],\n                                name='Threeshold')\n    return pandas.concat([Information_gain, Threesholds], axis=1)\n\n\nif __name__ == '__main__':\n    Q_Cat = ['resp.simple', 'Relapse', 'vital.status', 'Overall_Survival_binned', 'Remission_Duration_binned']\n    Ts = []\n    for Variable in Q_Cat:\n        print Variable\n        Independent = [v for v in Q_training.keys() if v in Q_scoring.keys()]\n        Ts += [information_gain(Q_training, Variable, Independent)]\n    Information_Gain = pandas.concat(Ts, keys=Q_Cat, axis=1)\n    Information_Gain.to_csv('InformationGain.csv')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Resposta: Sim\nmesmo_numero_passagem = train.groupby(['Sobreviventes', 'PassagemNumero']).size()\nmesmo_numero_passagem\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\na = RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 2].Invalid).tconfint_mean()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nsn.plot_lc(["BessellV"], multiplot=False)\nplt.scatter(p.data["BessellV"]["MJD"], p.data["BessellV"]["flux"], label="Synthetic Bessell V")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# ignore\nimport pysnooper\n!pip install pysnooper\nclear_output()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nimport sys\n\nprint(sys.executable)\nprint(sys.path)\nfrom padma.models.bbox.bbox import find_conteiner\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nrun(track=trk, zs=zs, R=var, Q=.02, P=1., plot_P=True,\n    title='$P=1\\, m^2$')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nFreq_plot(CR=CR, num_tests=3, Bayes=TRUE)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nimport p5_util\n\nis_score_dumped = True\nif is_score_dumped is True:\n    filename = './data/dict_cls_score.dump'\n    dict_cls_score = p5_util.object_load(filename)\nelse:\n    dict_cls_score = dict()\ndict_classifier = dict()\n\ndict_cls_score\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Produce a scatter matrix without outliers... distribution appears more normal\npd.scatter_matrix(good_data, alpha=0.3, figsize=(14, 8), diagonal='kde')  # ;\n```"}, {'reason': 'stop', 'result': "```python\nspent = []\n# 일부\nfor i in range(1, 3):\n    # 전체\n    # for i in range(len(corpora)):\n    start = time.time()\n    model = Word2Vec(corpora[i], **params_tag[i])\n    spent.append('Elapsed time: ' + str(time.time() - start) + ' sec' + ' [' + name_model[i] + ']')\n    model.wv.save(name_model[i])\nprint(spent)\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Join\njoin.isnull().sum()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}]