{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["\"\"\"\"Thing to be done:\n", "1 correlation\n", "2 correlation with other pages\n", "3 predictive model - neural network\n", "\n", "\n", "1 compute another graph\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Near Real-Time Flu Estimation via Wikipedia"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "In the following notebook we are going to reproduce for Italy McIver paper et al. \"Wikipedia usage estimates Prevalence of Influenza-like illness in theUnited States in near real time\". In this paper we show a method of estimating, in near-real time, the level of influenza-like illness (ILI) in Italy by monitoring the rate of particular Wikipedia article views on a daily basis. We calculated on a weekly base the number of times certain influenza- or health-related Wikipedia articles were accessed and compared these data to one of the Italian health protection agency program called \"Influnet\".\n", "\n", "The Notebook in the those three following sections:\n", "\n", "* **Comparing Influnet and Wikipedia's Influenza click throught rate**\n", "    * Retrivial and cleaning of the wikipedia pages:\n", "        * Using 3rd party toolkit wikishark\n", "        * Downloading the raw data from https://dumps.wikimedia.org/ (bonus point)\n", "    * Plot the two scaled curves on the same figure\n", "    * Compute Correlation\n", "* **Comparing Influnet data with other Wikipedia's related pages:**\n", "    * Retrivial and Cleaning\n", "    * Plotting and correlation analysis among each other\n", "* **Estimate Flu outbreaks:**\n", "    * Lasso\n", "    * Jesus\n", "    \n", "In the whole notebook we will make use of the following convetions:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1 - Comparison between Influnet and  Wikipedia Influenza "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this section we are going to define the auxiliary functions that I defined in order to perform in order to perform data analysis in a more efficient way: "]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": true}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from os import listdir\n", "import matplotlib.pyplot as plt\n", "from scipy.stats import pearsonr \n", "import seaborn as sb"]}, {"cell_type": "markdown", "metadata": {}, "source": [""]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false, "scrolled": true}, "outputs": [], "source": ["def importInflunet(path):\n", "    '''\n", "    Reads the Influnet data and creates a unique multiindex dataframe of the format\n", "    \n", "    (year,week) - incidence\n", "    \n", "    :param path: location of the influnet folder\n", "    :return: compacted version of \n", "    '''\n", "    \n", "    df = pd.concat([pd.read_csv(path+t, names=[\"time\", \"incidence\"], sep=\" \", header=1, usecols=[0,4], decimal=\",\") for t in listdir(path)], ignore_index=True)\n", "    df[[\"year\",\"week\"]] = df[\"time\"].str.split(\"-\", expand=True).astype(int)\n", "    df.drop([\"time\"], axis=1, inplace=True)\n", "    df = df.set_index([\"year\",\"week\"])\n", "    df.sortlevel(inplace=True)\n", "    df = df.astype(float)\n", "    df = df.loc[2008:]\n", "    return df\n", "\n", "def padInflunet(aux, year):\n", "    '''\n", "    The influnet dataset lacks information about the weeks that do not belog to the flu season (usally, but not necessarly, from week 17 to 40).\n", "    This functions fills the dataset with empty position in order to match the wikipedia format.\n", "    \n", "    :param aux: Influnet dataframe from a specific year\n", "    :param year: year of the previous Influnet dataframe\n", "    :return: padded version of the original dataframe\n", "    '''\n", "    year_weeks = aux.index.values[-1]\n", "    week_range = range(1,year_weeks+1)\n", "    aux = aux.reindex(week_range, fill_value=0)\n", "    aux[\"year\"] = year\n", "    aux[\"week\"] = week_range\n", "    \n", "    aux.set_index(['year', 'week'], append=False, inplace=True)\n", "    return aux\n", "\n", "\n", "def getInflunet(path = \"/home/aalto/Desktop/DE/hw2/influnet/data/\"):\n", "    '''\n", "    import and reformat the original Influnet dataset\n", "    \n", "    :param path: \n", "    :return: clean and padded version of the Influnet dataset\n", "    '''\n", "    \n", "    df = importInflunet();\n", "    previous = None\n", "    for x,y in df.index.values:\n", "        if previous == None:\n", "            df2 = reindexDF(df.loc[x], x)\n", "        elif x != previous:\n", "            df2 = df2.append(reindexDF(df.loc[x], x))\n", "        previous = x\n", "    return df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At this point we need the wikipedia pages views count in order to compare them with influnet. In order to satisy the requirement for the bonus point I wrote a simple script that downloads the files from https://dumps.wikimedia.org/, scan it to find the words we are intrested in and writes those entries of the files that we are intrested and write them on different files. \n", "\n", "Once those files are collected and stored on our disk we can call this function which loads the element passed them in memoery and group them by week.\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def getWiki(wikiPages, path = \"/home/aalto/PycharmProjects/digitalepidemiology/data/\"):\n", "    '''\n", "    \n", "    :param wikiPages: list of the wikipages that we want to analyze\n", "    :param path: location of the downloaded wikipedia pages\n", "    :return: \n", "    '''\n", "    df = pd.DataFrame()\n", "    for wikiPage in wikiPages:\n", "        wiki2 = pd.read_csv(path+wikiPage+\".csv\", usecols=[0,1], parse_dates=[0], index_col=[0], header=None)\n", "        wiki2 = wiki2.resample(\"W-Sun\").sum()\n", "        df[wikipage] = wiki2\n", "    return df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now to check the previous point we load the influnet dataset in memory and than show its plot"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [], "source": ["influnet = getInflunet()\n", "influnet\n", "\n", "\n", "\n", "x = wiki2.index[wiki2.index.year == 2010]\n", "y = wiki2[wiki2.index.year == 2010]\n", "    y2 = df2.loc[2010:][:len(x)]\n", "\n", "comparePlots([influnet, wiki_influenza], [2010]) #needs to be in the right format"]}, {"cell_type": "markdown", "metadata": {}, "source": [""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wiki_influenza = getWiki(\"Influenza\")"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"collapsed": false, "scrolled": false}, "outputs": [], "source": ["def comparePlots(elems, years):\n", "    '''\n", "    ta\n", "    :param elems: list of dataframes that we  want to compare\n", "    :param years: list of years that we want to anlayze\n", "    :return: None (prompts the plot)\n", "    '''\n", "    for year in years:\n", "        for elem in elems:\n", "            plt.plot(x, y2/y2.max())\n", "            plt.plot(x,y/y.max())"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": false}, "outputs": [{"ename": "SyntaxError", "evalue": "invalid syntax (<ipython-input-5-203aeb467653>, line 1)", "traceback": ["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-203aeb467653>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def getCorrelation(data1, data2)\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"], "output_type": "error"}], "source": ["def getCorrelation(elems, years):\n", "    '''\n", "    \n", "    :param elems: \n", "    :param years: \n", "    :return: \n", "    '''\n", "    heatmap = pd.DataFrame([])\n", "    for y in range(df2.index.min()[0], df2.index.max()[0]):\n", "        try:\n", "            heatmap = heatmap.append(list(pearsonr(df2.loc[year], wiki2[wiki2.index.year == y])))\n", "        except ValueError:\n", "            print y\n", "\n", "    print heatmap\n", "    sb.heatmap(pd.DataFrame(heatmap))\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["# 2 - Other functions\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At this point we are intrested to find if there is any other wikipedia pages that are able to gives us a better insghit about the topics"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": [""]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2.0}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "version": "2.7.12"}}, "nbformat": 4, "nbformat_minor": 0}