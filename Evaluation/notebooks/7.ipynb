{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "4embtkV0pNxM"}, "source": ["Deep Learning\n", "=============\n", "\n", "Assignment 4\n", "------------\n", "\n", "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n", "\n", "The goal of this assignment is make the neural network convolutional."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"cellView": "both", "colab": {"autoexec": {"startup": false, "wait_interval": 0}}, "colab_type": "code", "collapsed": true, "id": "tm2CQN_Cpwj0"}, "outputs": [], "source": ["# These are all the modules we'll be using later. Make sure you can import them\n", "# before proceeding further.\n", "import numpy as np\n", "import tensorflow as tf\n", "from six.moves import cPickle as pickle\n", "from six.moves import range"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"cellView": "both", "colab": {"autoexec": {"startup": false, "wait_interval": 0}, "output_extras": [{"item_id": 1}]}, "colab_type": "code", "collapsed": false, "executionInfo": {"elapsed": 11948, "status": "ok", "timestamp": 1446658914837, "user": {"color": "", "displayName": "", "isAnonymous": false, "isMe": true, "permissionId": "", "photoUrl": "", "sessionId": "0", "userId": ""}, "user_tz": 480}, "id": "y3-cj1bpmuxc", "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Training set (200000, 28, 28) (200000,)\n", "Validation set (10000, 28, 28) (10000,)\n", "Test set (18724, 28, 28) (18724,)\n"]}], "source": ["pickle_file = 'notMNIST.pickle'\n", "\n", "with open(pickle_file, 'rb') as f:\n", "    save = pickle.load(f)\n", "    train_dataset = save['train_dataset']\n", "    train_labels = save['train_labels']\n", "    valid_dataset = save['valid_dataset']\n", "    valid_labels = save['valid_labels']\n", "    test_dataset = save['test_dataset']\n", "    test_labels = save['test_labels']\n", "    del save  # hint to help gc free up memory\n", "    print('Training set', train_dataset.shape, train_labels.shape)\n", "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n", "    print('Test set', test_dataset.shape, test_labels.shape)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "L7aHrm6nGDMB"}, "source": ["Reformat into a TensorFlow-friendly shape:\n", "- convolutions need the image data formatted as a cube (width by height by #channels)\n", "- labels as float 1-hot encodings."]}, {"cell_type": "code", "execution_count": 3, "metadata": {"cellView": "both", "colab": {"autoexec": {"startup": false, "wait_interval": 0}, "output_extras": [{"item_id": 1}]}, "colab_type": "code", "collapsed": false, "executionInfo": {"elapsed": 11952, "status": "ok", "timestamp": 1446658914857, "user": {"color": "", "displayName": "", "isAnonymous": false, "isMe": true, "permissionId": "", "photoUrl": "", "sessionId": "0", "userId": ""}, "user_tz": 480}, "id": "IRSyYiIIGIzS", "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Training set (200000, 28, 28, 1) (200000, 10)\n", "Validation set (10000, 28, 28, 1) (10000, 10)\n", "Test set (18724, 28, 28, 1) (18724, 10)\n"]}], "source": ["image_size = 28\n", "num_labels = 10\n", "num_channels = 1 # grayscale\n", "\n", "def reformat(dataset, labels):\n", "    dataset = dataset.reshape(\n", "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n", "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n", "    return dataset, labels\n", "\n", "train_dataset, train_labels = reformat(train_dataset, train_labels)\n", "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n", "test_dataset, test_labels = reformat(test_dataset, test_labels)\n", "print('Training set', train_dataset.shape, train_labels.shape)\n", "print('Validation set', valid_dataset.shape, valid_labels.shape)\n", "print('Test set', test_dataset.shape, test_labels.shape)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"cellView": "both", "colab": {"autoexec": {"startup": false, "wait_interval": 0}}, "colab_type": "code", "collapsed": true, "id": "AgQDIREv02p1"}, "outputs": [], "source": ["def accuracy(predictions, labels):\n", "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n", "            / predictions.shape[0])"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "5rhgjmROXu2O"}, "source": ["Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."]}, {"cell_type": "code", "execution_count": 8, "metadata": {"cellView": "both", "colab": {"autoexec": {"startup": false, "wait_interval": 0}}, "colab_type": "code", "collapsed": false, "id": "IZYv70SvvOan"}, "outputs": [], "source": ["batch_size = 16\n", "patch_size = 5\n", "depth = 16\n", "num_hidden = 64\n", "\n", "graph = tf.Graph()\n", "\n", "with graph.as_default():\n", "\n", "    # Input data.\n", "    tf_train_dataset = tf.placeholder(\n", "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n", "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n", "    tf_valid_dataset = tf.constant(valid_dataset)\n", "    tf_test_dataset = tf.constant(test_dataset)\n", "  \n", "    # Variables.\n", "    layer1_weights = tf.Variable(tf.truncated_normal(\n", "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n", "    layer1_biases = tf.Variable(tf.zeros([depth]))\n", "    layer2_weights = tf.Variable(tf.truncated_normal(\n", "        [patch_size, patch_size, depth, depth], stddev=0.1))\n", "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n", "    layer3_weights = tf.Variable(tf.truncated_normal(\n", "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n", "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n", "    layer4_weights = tf.Variable(tf.truncated_normal(\n", "        [num_hidden, num_labels], stddev=0.1))\n", "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n", "  \n", "    # Model.\n", "    def model(data):\n", "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n", "        hidden = tf.nn.relu(conv + layer1_biases)\n", "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n", "        hidden = tf.nn.relu(conv + layer2_biases)\n", "        shape = hidden.get_shape().as_list()\n", "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n", "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n", "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n", "  \n", "    # Training computation.\n", "    logits = model(tf_train_dataset)\n", "    loss = tf.reduce_mean(\n", "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n", "    \n", "    # Optimizer.\n", "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n", "  \n", "    # Predictions for the training, validation, and test data.\n", "    train_prediction = tf.nn.softmax(logits)\n", "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n", "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"cellView": "both", "colab": {"autoexec": {"startup": false, "wait_interval": 0}, "output_extras": [{"item_id": 37}]}, "colab_type": "code", "collapsed": false, "executionInfo": {"elapsed": 63292, "status": "ok", "timestamp": 1446658966251, "user": {"color": "", "displayName": "", "isAnonymous": false, "isMe": true, "permissionId": "", "photoUrl": "", "sessionId": "0", "userId": ""}, "user_tz": 480}, "id": "noKFb2UovVFR", "outputId": "28941338-2ef9-4088-8bd1-44295661e628"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Initialized\n", "Step 0 - Loss 2.534234 - Minibatch 18.8% - Validation 11.1%\n", "Step 50 - Loss 1.474175 - Minibatch 50.0% - Validation 50.1%\n", "Step 100 - Loss 0.775141 - Minibatch 81.2% - Validation 73.2%\n", "Step 150 - Loss 0.863082 - Minibatch 68.8% - Validation 74.9%\n", "Step 200 - Loss 1.171659 - Minibatch 75.0% - Validation 78.3%\n", "Step 250 - Loss 0.956151 - Minibatch 75.0% - Validation 76.8%\n", "Step 300 - Loss 0.832626 - Minibatch 68.8% - Validation 78.8%\n", "Step 350 - Loss 1.001116 - Minibatch 75.0% - Validation 79.2%\n", "Step 400 - Loss 0.715934 - Minibatch 87.5% - Validation 81.1%\n", "Step 450 - Loss 0.728667 - Minibatch 75.0% - Validation 76.1%\n", "Step 500 - Loss 0.778035 - Minibatch 75.0% - Validation 81.1%\n", "Step 550 - Loss 0.738171 - Minibatch 75.0% - Validation 81.5%\n", "Step 600 - Loss 1.016576 - Minibatch 62.5% - Validation 80.7%\n", "Step 650 - Loss 0.951612 - Minibatch 68.8% - Validation 82.3%\n", "Step 700 - Loss 0.267146 - Minibatch 87.5% - Validation 81.4%\n", "Step 750 - Loss 0.629344 - Minibatch 81.2% - Validation 82.0%\n", "Step 800 - Loss 0.742801 - Minibatch 87.5% - Validation 82.7%\n", "Step 850 - Loss 0.516399 - Minibatch 75.0% - Validation 82.1%\n", "Step 900 - Loss 0.376723 - Minibatch 87.5% - Validation 82.4%\n", "Step 950 - Loss 0.414260 - Minibatch 87.5% - Validation 82.0%\n", "Step 1000 - Loss 0.734337 - Minibatch 81.2% - Validation 83.2%\n", "Test accuracy: 89.8%\n"]}], "source": ["num_steps = 1001\n", "\n", "with tf.Session(graph=graph) as session:\n", "    tf.initialize_all_variables().run()\n", "    print('Initialized')\n", "    for step in range(num_steps):\n", "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n", "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n", "        batch_labels = train_labels[offset:(offset + batch_size), :]\n", "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n", "        _, l, predictions = session.run(\n", "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n", "        if (step % 50 == 0):\n", "            print('Step %d - Loss %f - Minibatch %.1f%% - Validation %.1f%%' %\n", "                  (step, l, accuracy(predictions, batch_labels),accuracy(\n", "                    valid_prediction.eval(), valid_labels)))\n", "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "KedKkn4EutIK"}, "source": ["---\n", "Problem 1\n", "---------\n", "\n", "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n", "\n", "---"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": true}, "outputs": [], "source": ["from tensorflow.python.framework import ops\n", "ops.reset_default_graph()\n", "\n", "batch_size = 16\n", "patch_size = 5\n", "depth = 16\n", "num_hidden = 64\n", "\n", "graph = tf.Graph()\n", "\n", "with graph.as_default():\n", "\n", "    # Input data.\n", "    tf_train_dataset = tf.placeholder(\n", "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n", "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n", "    tf_valid_dataset = tf.constant(valid_dataset)\n", "    tf_test_dataset = tf.constant(test_dataset)\n", "  \n", "    # Variables.\n", "    layer1_weights = tf.Variable(tf.truncated_normal(\n", "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n", "    layer1_biases = tf.Variable(tf.zeros([depth]))\n", "    layer2_weights = tf.Variable(tf.truncated_normal(\n", "        [patch_size, patch_size, depth, depth], stddev=0.1))\n", "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n", "    layer3_weights = tf.Variable(tf.truncated_normal(\n", "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n", "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n", "    layer4_weights = tf.Variable(tf.truncated_normal(\n", "        [num_hidden, num_labels], stddev=0.1))\n", "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n", "  \n", "    # Model.\n", "    def model(data):\n", "        # layer 1 convo. max_pool 2x2\n", "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n", "        pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "        hidden = tf.nn.relu(pool + layer1_biases)\n", "        # layer 2 convo. max_pool 2x2\n", "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n", "        pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "        hidden = tf.nn.relu(pool + layer2_biases)\n", "        # layer 3 fully connected.\n", "        shape = hidden.get_shape().as_list()\n", "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n", "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n", "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n", "  \n", "    # Training computation.\n", "    logits = model(tf_train_dataset)\n", "    loss = tf.reduce_mean(\n", "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n", "    \n", "    # Optimizer.\n", "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n", "  \n", "    # Predictions for the training, validation, and test data.\n", "    train_prediction = tf.nn.softmax(logits)\n", "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n", "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Initialized\n", "Step 0 - Loss 3.287148 - Minibatch 6.2% - Validation 10.1%\n", "Step 50 - Loss 1.270621 - Minibatch 62.5% - Validation 42.4%\n", "Step 100 - Loss 0.914505 - Minibatch 81.2% - Validation 74.7%\n", "Step 150 - Loss 0.958688 - Minibatch 68.8% - Validation 76.1%\n", "Step 200 - Loss 1.037660 - Minibatch 75.0% - Validation 77.6%\n", "Step 250 - Loss 1.005606 - Minibatch 75.0% - Validation 77.7%\n", "Step 300 - Loss 0.861946 - Minibatch 68.8% - Validation 79.6%\n", "Step 350 - Loss 1.015530 - Minibatch 68.8% - Validation 76.6%\n", "Step 400 - Loss 0.662101 - Minibatch 75.0% - Validation 81.4%\n", "Step 450 - Loss 0.697596 - Minibatch 87.5% - Validation 79.0%\n", "Step 500 - Loss 0.629703 - Minibatch 81.2% - Validation 82.0%\n", "Step 550 - Loss 0.636615 - Minibatch 75.0% - Validation 81.8%\n", "Step 600 - Loss 0.932539 - Minibatch 81.2% - Validation 81.4%\n", "Step 650 - Loss 0.902850 - Minibatch 75.0% - Validation 82.4%\n", "Step 700 - Loss 0.134424 - Minibatch 100.0% - Validation 83.8%\n", "Step 750 - Loss 0.446201 - Minibatch 81.2% - Validation 83.8%\n", "Step 800 - Loss 0.808620 - Minibatch 87.5% - Validation 83.3%\n", "Step 850 - Loss 0.401888 - Minibatch 81.2% - Validation 83.2%\n", "Step 900 - Loss 0.251171 - Minibatch 87.5% - Validation 83.8%\n", "Step 950 - Loss 0.214575 - Minibatch 100.0% - Validation 84.1%\n", "Step 1000 - Loss 0.800670 - Minibatch 81.2% - Validation 84.1%\n", "Test accuracy: 90.5%\n"]}], "source": ["num_steps = 1001\n", "\n", "with tf.Session(graph=graph) as session:\n", "    tf.initialize_all_variables().run()\n", "    print('Initialized')\n", "    for step in range(num_steps):\n", "        offset = (step * batch_size)b % (train_labels.shape[0] - batch_size)\n", "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n", "        batch_labels = train_labels[offset:(offset + batch_size), :]\n", "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n", "        _, l, predictions = session.run(\n", "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n", "        if (step % 50 == 0):\n", "            print('Step %d - Loss %f - Minibatch %.1f%% - Validation %.1f%%' %\n", "                  (step, l, accuracy(predictions, batch_labels),accuracy(\n", "                    valid_prediction.eval(), valid_labels)))\n", "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "klf21gpbAgb-"}, "source": ["---\n", "Problem 2\n", "---------\n", "\n", "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n", "\n", "---"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": true}, "outputs": [], "source": ["import datetime\n", "\n", "def eta(start, n, total):\n", "    now = datetime.datetime.now()\n", "    diff = now - start\n", "    secs = (total-n) * 1.0 * diff.seconds / (n+1) # +1 to avoid zero division.\n", "    ends = now + datetime.timedelta(seconds=secs)\n", "    return ends.strftime(\"%H:%M:%S\")\n"]}, {"cell_type": "code", "execution_count": 33, "metadata": {"collapsed": false}, "outputs": [], "source": ["from tensorflow.python.framework import ops\n", "ops.reset_default_graph()\n", "\n", "batch_size = 16\n", "patch_size = 5\n", "depth = 16\n", "num_hidden_full_1 = 512\n", "num_hidden_full_2 = 64\n", "\n", "def init_weights(shape, method='xavier'):\n", "    if method == 'zeros':\n", "        return tf.Variable(tf.zeros(shape, dtype=tf.float32))\n", "    elif method == 'ones':\n", "        return tf.Variable(tf.ones(shape, dtype=tf.float32))\n", "    elif method == 'uniform':\n", "        return tf.Variable(tf.random_normal(shape, stddev=0.01, dtype=tf.float32))\n", "    elif method == 'altxavier':\n", "        low = -4*np.sqrt(6.0/(shape[0] + shape[1])) # {sigmoid:4, tanh:1} \n", "        high = 4*np.sqrt(6.0/(shape[0] + shape[1]))\n", "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n", "    elif method == 'xavier':\n", "        sd = np.sqrt(3.0/(shape[0] + shape[1]))\n", "        return tf.Variable(tf.truncated_normal(shape, stddev=sd))\n", "    else: # method == 'kaiming':\n", "        sd = np.sqrt(2.0/(shape[0]))\n", "        return tf.Variable(tf.truncated_normal(shape, stddev=sd))\n", "\n", "\n", "graph = tf.Graph()\n", "\n", "with graph.as_default():\n", "\n", "    # Input data.\n", "    tf_train_dataset = tf.placeholder(\n", "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n", "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n", "    tf_valid_dataset = tf.constant(valid_dataset)\n", "    tf_test_dataset = tf.constant(test_dataset)\n", "  \n", "    # Variables.\n", "    layer1_weights = tf.Variable(tf.truncated_normal(\n", "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n", "    layer1_biases = tf.Variable(tf.zeros([depth]))\n", "    layer2_weights = tf.Variable(tf.truncated_normal(\n", "        [patch_size, patch_size, depth, depth * 2], stddev=0.1))\n", "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth * 2]))\n", "    \n", "    layer3_weights = init_weights([image_size // 4 * image_size // 4 * depth * 2, num_hidden_full_1])\n", "    layer3_biases = init_weights([num_hidden_full_1], method='ones')\n", "    keep3 = tf.placeholder(\"float\")\n", "    layer4_weights = init_weights([num_hidden_full_1, num_hidden_full_2])\n", "    layer4_biases = init_weights([num_hidden_full_2], method='ones')\n", "    keep4 = tf.placeholder(\"float\")\n", "    layer5_weights = init_weights([num_hidden_full_2, num_labels])\n", "    layer5_biases = init_weights([num_labels], method='ones')\n", "  \n", "    # Model. using elu not relu.\n", "    def model(data):\n", "        # layer 1 convo. max_pool 2x2.\n", "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv = tf.nn.elu(conv + layer1_biases)\n", "        hidden = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "        # layer 2 convo. max_pool 2x2\n", "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv = tf.nn.elu(conv + layer2_biases)\n", "        hidden = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "        # print('post conv', hidden.get_shape())\n", "        # layer 3 fully connected.\n", "        shape = hidden.get_shape().as_list()\n", "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n", "        hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n", "        hidden = tf.nn.dropout(hidden, keep3)\n", "        # layer 4 fully connected\n", "        hidden = tf.nn.elu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n", "        hidden = tf.nn.dropout(hidden, keep4)\n", "        # layer 5 output\n", "        output = tf.matmul(hidden, layer5_weights) + layer5_biases\n", "        return output\n", "  \n", "    # Training computation.\n", "    logits = model(tf_train_dataset)\n", "    loss = tf.reduce_mean(\n", "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n", "    \n", "    # Optimizer.\n", "    # optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n", "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n", "  \n", "    # Predictions for the training, validation, and test data.\n", "    train_prediction = tf.nn.softmax(logits)\n", "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n", "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"]}, {"cell_type": "code", "execution_count": 34, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Initialized\n", "Step 0 - Loss 4.741639 - Minibatch 12.5% - Validation 6.2% - ETA 13:32:02\n", "Step 500 - Loss 1.037983 - Minibatch 68.8% - Validation 77.2% - ETA 14:38:35\n", "Step 1000 - Loss 1.005712 - Minibatch 68.8% - Validation 78.1% - ETA 14:35:58\n", "Step 1500 - Loss 0.407815 - Minibatch 87.5% - Validation 82.2% - ETA 14:32:53\n", "Step 2000 - Loss 1.493692 - Minibatch 56.2% - Validation 82.1% - ETA 14:31:01\n", "Step 2500 - Loss 0.976753 - Minibatch 75.0% - Validation 81.5% - ETA 14:29:53\n", "Step 3000 - Loss 0.303868 - Minibatch 87.5% - Validation 83.8% - ETA 14:29:21\n", "Step 3500 - Loss 0.618233 - Minibatch 81.2% - Validation 84.2% - ETA 14:30:18\n", "Step 4000 - Loss 0.858066 - Minibatch 81.2% - Validation 84.5% - ETA 14:29:51\n", "Step 4500 - Loss 0.574704 - Minibatch 93.8% - Validation 85.1% - ETA 14:29:39\n", "Step 5000 - Loss 0.336020 - Minibatch 87.5% - Validation 84.6% - ETA 14:29:22\n", "Step 5500 - Loss 0.457557 - Minibatch 81.2% - Validation 86.0% - ETA 14:29:07\n", "Step 6000 - Loss 0.706136 - Minibatch 75.0% - Validation 86.4% - ETA 14:28:48\n", "Step 6500 - Loss 0.333530 - Minibatch 87.5% - Validation 86.5% - ETA 14:28:33\n", "Step 7000 - Loss 0.540335 - Minibatch 81.2% - Validation 86.6% - ETA 14:28:24\n", "Step 7500 - Loss 0.463323 - Minibatch 75.0% - Validation 87.3% - ETA 14:28:18\n", "Step 8000 - Loss 0.140386 - Minibatch 100.0% - Validation 87.3% - ETA 14:28:07\n", "Step 8500 - Loss 0.251511 - Minibatch 87.5% - Validation 87.4% - ETA 14:28:01\n", "Step 9000 - Loss 0.601837 - Minibatch 81.2% - Validation 87.7% - ETA 14:27:58\n", "Step 9500 - Loss 0.066211 - Minibatch 100.0% - Validation 88.2% - ETA 14:27:54\n", "Step 10000 - Loss 0.381814 - Minibatch 81.2% - Validation 88.2% - ETA 14:27:49\n", "Step 10500 - Loss 0.469512 - Minibatch 93.8% - Validation 87.7% - ETA 14:27:43\n", "Step 11000 - Loss 0.195667 - Minibatch 93.8% - Validation 88.3% - ETA 14:27:37\n", "Step 11500 - Loss 0.045437 - Minibatch 100.0% - Validation 88.4% - ETA 14:27:35\n", "Step 12000 - Loss 0.027818 - Minibatch 100.0% - Validation 88.8% - ETA 14:27:29\n", "Step 12500 - Loss 0.423115 - Minibatch 81.2% - Validation 89.4% - ETA 14:27:27\n", "Step 13000 - Loss 0.167997 - Minibatch 93.8% - Validation 88.9% - ETA 14:27:25\n", "Step 13500 - Loss 0.368409 - Minibatch 87.5% - Validation 88.5% - ETA 14:27:24\n", "Step 14000 - Loss 0.492125 - Minibatch 81.2% - Validation 89.1% - ETA 14:27:20\n", "Step 14500 - Loss 0.450166 - Minibatch 87.5% - Validation 89.1% - ETA 14:27:18\n", "Step 15000 - Loss 0.368179 - Minibatch 81.2% - Validation 89.4% - ETA 14:27:16\n", "Step 15500 - Loss 0.100520 - Minibatch 93.8% - Validation 89.4% - ETA 14:27:13\n", "Step 16000 - Loss 0.111864 - Minibatch 93.8% - Validation 89.4% - ETA 14:27:10\n", "Step 16500 - Loss 0.752725 - Minibatch 81.2% - Validation 89.6% - ETA 14:27:08\n", "Step 17000 - Loss 0.092886 - Minibatch 100.0% - Validation 89.4% - ETA 14:27:06\n", "Step 17500 - Loss 0.287848 - Minibatch 93.8% - Validation 89.7% - ETA 14:27:05\n", "Step 18000 - Loss 0.732519 - Minibatch 75.0% - Validation 89.8% - ETA 14:27:04\n", "Step 18500 - Loss 0.186880 - Minibatch 93.8% - Validation 89.9% - ETA 14:27:03\n", "Step 19000 - Loss 0.347449 - Minibatch 87.5% - Validation 89.8% - ETA 14:27:02\n", "Step 19500 - Loss 0.418767 - Minibatch 87.5% - Validation 89.7% - ETA 14:27:01\n", "Step 20000 - Loss 0.393500 - Minibatch 87.5% - Validation 90.3% - ETA 14:26:58\n", "Step 20500 - Loss 0.491707 - Minibatch 87.5% - Validation 90.2% - ETA 14:26:58\n", "Step 21000 - Loss 0.134519 - Minibatch 93.8% - Validation 90.0% - ETA 14:26:56\n", "Step 21500 - Loss 0.175954 - Minibatch 93.8% - Validation 90.2% - ETA 14:26:55\n", "Step 22000 - Loss 0.043911 - Minibatch 100.0% - Validation 90.2% - ETA 14:26:55\n", "Step 22500 - Loss 0.416085 - Minibatch 87.5% - Validation 90.5% - ETA 14:26:54\n", "Step 23000 - Loss 0.591760 - Minibatch 87.5% - Validation 90.2% - ETA 14:26:53\n", "Step 23500 - Loss 0.103497 - Minibatch 100.0% - Validation 90.2% - ETA 14:26:52\n", "Step 24000 - Loss 0.056342 - Minibatch 100.0% - Validation 90.3% - ETA 14:26:52\n", "Step 24500 - Loss 0.475959 - Minibatch 81.2% - Validation 90.4% - ETA 14:26:50\n", "Step 25000 - Loss 0.069777 - Minibatch 100.0% - Validation 90.5% - ETA 14:26:49\n", "Step 25500 - Loss 0.512630 - Minibatch 87.5% - Validation 90.4% - ETA 14:26:48\n", "Step 26000 - Loss 0.008083 - Minibatch 100.0% - Validation 90.0% - ETA 14:26:47\n", "Step 26500 - Loss 0.778804 - Minibatch 75.0% - Validation 90.2% - ETA 14:26:48\n", "Step 27000 - Loss 0.411024 - Minibatch 87.5% - Validation 90.6% - ETA 14:26:49\n", "Step 27500 - Loss 0.330338 - Minibatch 87.5% - Validation 90.5% - ETA 14:26:49\n", "Step 28000 - Loss 1.086951 - Minibatch 87.5% - Validation 90.5% - ETA 14:26:49\n", "Step 28500 - Loss 0.678350 - Minibatch 81.2% - Validation 90.6% - ETA 14:26:49\n", "Step 29000 - Loss 0.379952 - Minibatch 81.2% - Validation 90.4% - ETA 14:26:49\n", "Step 29500 - Loss 0.383826 - Minibatch 81.2% - Validation 90.5% - ETA 14:26:48\n", "Step 30000 - Loss 0.082108 - Minibatch 100.0% - Validation 90.6% - ETA 14:26:50\n", "Step 30500 - Loss 0.323418 - Minibatch 87.5% - Validation 90.8% - ETA 14:26:50\n", "Step 31000 - Loss 0.070790 - Minibatch 93.8% - Validation 90.7% - ETA 14:26:54\n", "Step 31500 - Loss 0.670308 - Minibatch 75.0% - Validation 91.0% - ETA 14:26:54\n", "Step 32000 - Loss 0.378462 - Minibatch 87.5% - Validation 91.0% - ETA 14:26:53\n", "Step 32500 - Loss 0.093044 - Minibatch 100.0% - Validation 91.0% - ETA 14:26:53\n", "Step 33000 - Loss 0.355687 - Minibatch 87.5% - Validation 91.0% - ETA 14:26:51\n", "Step 33500 - Loss 0.339728 - Minibatch 87.5% - Validation 90.9% - ETA 14:26:52\n", "Step 34000 - Loss 0.162638 - Minibatch 93.8% - Validation 90.9% - ETA 14:26:51\n", "Step 34500 - Loss 0.019621 - Minibatch 100.0% - Validation 91.1% - ETA 14:26:49\n", "Step 35000 - Loss 0.308660 - Minibatch 93.8% - Validation 90.7% - ETA 14:26:48\n", "Step 35500 - Loss 0.492420 - Minibatch 87.5% - Validation 91.0% - ETA 14:26:49\n", "Step 36000 - Loss 0.140230 - Minibatch 100.0% - Validation 90.9% - ETA 14:26:48\n", "Step 36500 - Loss 0.003034 - Minibatch 100.0% - Validation 90.9% - ETA 14:26:48\n", "Step 37000 - Loss 0.397529 - Minibatch 87.5% - Validation 91.0% - ETA 14:26:49\n", "Step 37500 - Loss 0.055047 - Minibatch 100.0% - Validation 91.0% - ETA 14:26:52\n", "Step 38000 - Loss 0.450371 - Minibatch 87.5% - Validation 90.7% - ETA 14:26:54\n", "Step 38500 - Loss 0.062178 - Minibatch 100.0% - Validation 90.8% - ETA 14:26:56\n", "Step 39000 - Loss 0.358456 - Minibatch 87.5% - Validation 91.0% - ETA 14:26:57\n", "Step 39500 - Loss 0.223700 - Minibatch 93.8% - Validation 91.3% - ETA 14:26:59\n", "Step 40000 - Loss 0.320921 - Minibatch 87.5% - Validation 91.0% - ETA 14:27:00\n", "Test accuracy: 96.3%\n"]}], "source": ["num_steps = 40001\n", "\n", "with tf.Session(graph=graph) as session:\n", "    tf.initialize_all_variables().run()\n", "    start = datetime.datetime.now()\n", "    print('Initialized')\n", "    for step in range(num_steps):\n", "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n", "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n", "        batch_labels = train_labels[offset:(offset + batch_size), :]\n", "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n", "                    keep3:0.9, keep4:0.9}\n", "        _, l, predictions = session.run(\n", "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n", "        if (step % 500 == 0):\n", "            ends = eta(start, step, num_steps)\n", "            valpred = valid_prediction.eval(feed_dict={keep3:1.0, keep4:1.0})            \n", "            print('Step %d - Loss %f - Minibatch %.1f%% - Validation %.1f%% - ETA %s' %\n", "                  (step, l, accuracy(predictions, batch_labels), accuracy(valpred, valid_labels), ends))\n", "    print('Test accuracy: %.1f%%' %\n", "          accuracy(test_prediction.eval(feed_dict={keep3:1.0, keep4:1.0}), test_labels))"]}, {"cell_type": "code", "execution_count": 36, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1x1 (16, 28, 28, 16)\n", "pool (16, 28, 28, 16)\n", "3x3 (16, 28, 28, 16)\n", "5x5 (16, 28, 28, 16)\n", "(16, 28, 28, 64)\n", "1x1 (10000, 28, 28, 16)\n", "pool (10000, 28, 28, 16)\n", "3x3 (10000, 28, 28, 16)\n", "5x5 (10000, 28, 28, 16)\n", "(10000, 28, 28, 64)\n", "1x1 (18724, 28, 28, 16)\n", "pool (18724, 28, 28, 16)\n", "3x3 (18724, 28, 28, 16)\n", "5x5 (18724, 28, 28, 16)\n", "(18724, 28, 28, 64)\n"]}], "source": ["# DL with inception.\n", "\n", "from tensorflow.python.framework import ops\n", "ops.reset_default_graph()\n", "\n", "batch_size = 16\n", "patch_size = 5\n", "depth = 16\n", "num_hidden_full_1 = 96\n", "num_hidden_full_2 = 96\n", "\n", "graph = tf.Graph()\n", "\n", "with graph.as_default():\n", "\n", "    # Input data.\n", "    tf_train_dataset = tf.placeholder(\n", "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n", "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n", "    tf_valid_dataset = tf.constant(valid_dataset)\n", "    tf_test_dataset = tf.constant(test_dataset)\n", "  \n", "    # Variables.\n", "    #layer1_weights = tf.Variable(tf.truncated_normal(\n", "    #    [patch_size, patch_size, num_channels, depth], stddev=0.1))\n", "    #layer1_biases = tf.Variable(tf.zeros([depth]))\n", "    #layer2_weights = tf.Variable(tf.truncated_normal(\n", "    #    [patch_size, patch_size, depth, depth], stddev=0.1))\n", "    #layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n", "    # standard conv2d:\n", "    # layer3_weights = tf.Variable(tf.truncated_normal(\n", "    #     [image_size // 4 * image_size // 4 * depth, num_hidden_full_1], stddev=0.1))\n", "    # inception is = [16, 28, 28, 64] reshaped (16, 50176)\n", "    layer3_weights = init_weights([image_size * image_size * 64, num_hidden_full_1])\n", "    layer3_biases = init_weights([num_hidden_full_1], method=\"ones\")\n", "    keep3 = tf.placeholder(\"float\")\n", "    layer4_weights = init_weights([num_hidden_full_1, num_hidden_full_2])\n", "    layer4_biases = init_weights([num_hidden_full_2], method=\"ones\")\n", "    keep4 = tf.placeholder(\"float\")\n", "    layer5_weights = init_weights([num_hidden_full_2, num_labels])\n", "    layer5_biases = init_weights([num_labels], method=\"ones\")\n", "    # vars for inception\n", "    inception_1x1_weights = tf.Variable(tf.truncated_normal(\n", "        [1, 1, num_channels, depth], stddev=0.1))\n", "    inception_1x1_biases = tf.Variable(tf.zeros([depth]))\n", "    pre_inception_1x1_weights = tf.Variable(tf.truncated_normal(\n", "        [1, 1, num_channels, depth], stddev=0.1))\n", "    pre_inception_1x1_biases = tf.Variable(tf.zeros([depth]))\n", "    inception_1x1_pool_weights = tf.Variable(tf.truncated_normal(\n", "        [1, 1, num_channels, depth], stddev=0.1))\n", "    inception_1x1_pool_biases = tf.Variable(tf.zeros([depth]))\n", "    inception_3x3_weights = tf.Variable(tf.truncated_normal(\n", "        [3, 3, depth, depth], stddev=0.1))\n", "    inception_3x3_biases = tf.Variable(tf.zeros([depth]))\n", "    inception_5x5_weights = tf.Variable(tf.truncated_normal(\n", "        [5, 5, depth, depth], stddev=0.1))\n", "    inception_5x5_biases = tf.Variable(tf.zeros([depth]))\n", "\n", "    def inception_layer(data):\n", "        # Inception 1x1\n", "        conv_1x1 = tf.nn.conv2d(data, inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv_1x1 = tf.nn.relu(conv_1x1 + inception_1x1_biases)\n", "        print(\"1x1\", conv_1x1.get_shape())\n", "        ## 1x1 - before the bigger patches\n", "        conv_pre = tf.nn.conv2d(data, pre_inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv_pre = tf.nn.relu(conv_pre + pre_inception_1x1_biases)\n", "        # Pooling 3x3\n", "        ## average pool followed by a 1x1\n", "        conv_pool = tf.nn.avg_pool(data, [1, 3, 3, 1], [1, 1, 1, 1], padding='SAME')\n", "        conv_pool = tf.nn.conv2d(conv_pool, inception_1x1_pool_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv_pool = tf.nn.relu(conv_pool + inception_1x1_pool_biases)\n", "        print(\"pool\", conv_pool.get_shape())\n", "        # Inception 3x3\n", "        ## 1x1 followed by a 3x3\n", "        conv_3x3 = tf.nn.conv2d(conv_pre, inception_3x3_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv_3x3 = tf.nn.relu(conv_3x3 + inception_3x3_biases)\n", "        print(\"3x3\", conv_3x3.get_shape())\n", "        # Inception 5x5\n", "        ## 1x1 followed by a 5x5\n", "        conv_5x5 = tf.nn.conv2d(conv_pre, inception_5x5_weights, [1, 1, 1, 1], padding='SAME')\n", "        conv_5x5 = tf.nn.relu(conv_5x5 + inception_5x5_biases)\n", "        print(\"5x5\", conv_5x5.get_shape())\n", "        inception_result = tf.concat(3, [conv_1x1, conv_3x3, conv_5x5, conv_pool])\n", "        print(inception_result.get_shape())\n", "        return inception_result\n", "\n", "    # Model. using elu not relu.\n", "    def model(data):\n", "        # layer 1 convo. max_pool 2x2.\n", "        #conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n", "        #pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "        #hidden = tf.nn.elu(pool + layer1_biases)\n", "        # layer 2 convo. max_pool 2x2\n", "        #conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n", "        #pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "        #hidden = tf.nn.elu(pool + layer2_biases)\n", "        hidden = inception_layer(data)\n", "        # layer 3 fully connected.\n", "        shape = hidden.get_shape().as_list()\n", "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n", "        hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n", "        hidden = tf.nn.dropout(hidden, keep3)\n", "        # layer 4 fully connected\n", "        hidden = tf.nn.elu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n", "        hidden = tf.nn.dropout(hidden, keep4)\n", "        # layer 5 output\n", "        output = tf.matmul(hidden, layer5_weights) + layer5_biases\n", "        return output\n", "  \n", "    # Training computation.\n", "    logits = model(tf_train_dataset)\n", "    loss = tf.reduce_mean(\n", "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n", "    \n", "    # Optimizer.\n", "    # optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n", "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n", "  \n", "    # Predictions for the training, validation, and test data.\n", "    train_prediction = tf.nn.softmax(logits)\n", "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n", "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Initialized\n", "Step 0 - Loss 2.660672 - Minibatch 12.5% - Validation 13.9% - ETA 11:00:12\n", "Step 500 - Loss 0.677274 - Minibatch 68.8% - Validation 80.6% - ETA 12:01:06\n"]}, {"ename": "KeyboardInterrupt", "evalue": "", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-10-3d74169e7a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mvalpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkeep3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             print('Step %d - Loss %f - Minibatch %.1f%% - Validation %.1f%% - ETA %s' %\n\u001b[1;32m     19\u001b[0m                   (step, l, accuracy(predictions, batch_labels), accuracy(valpred, valid_labels), ends))\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3095\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3097\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 511\u001b[0;31m                            feed_dict_string)\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 564\u001b[0;31m                            target_list)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    569\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/Users/hernan/.pyenv/versions/framework_351/envs/tensor/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}], "source": ["num_steps = 10001 # 40001 IMPOSIBLE ON MY NOTEBOOK.\n", "\n", "with tf.Session(graph=graph) as session:\n", "    tf.initialize_all_variables().run()\n", "    start = datetime.datetime.now()\n", "    print('Initialized')\n", "    for step in range(num_steps):\n", "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n", "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n", "        batch_labels = train_labels[offset:(offset + batch_size), :]\n", "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n", "                    keep3:0.9, keep4:0.9}\n", "        _, l, predictions = session.run(\n", "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n", "        if (step % 500 == 0):\n", "            ends = eta(start, step, num_steps)\n", "            valpred = valid_prediction.eval(feed_dict={keep3:1.0, keep4:1.0})            \n", "            print('Step %d - Loss %f - Minibatch %.1f%% - Validation %.1f%% - ETA %s' %\n", "                  (step, l, accuracy(predictions, batch_labels), accuracy(valpred, valid_labels), ends))\n", "    print('Test accuracy: %.1f%%' %\n", "          accuracy(test_prediction.eval(feed_dict={keep3:1.0, keep4:1.0}), test_labels))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"colab": {"default_view": {}, "name": "4_convolutions.ipynb", "provenance": [], "version": "0.3.2", "views": {}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.1"}}, "nbformat": 4, "nbformat_minor": 0}