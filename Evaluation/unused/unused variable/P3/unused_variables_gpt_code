[{'reason': 'stop', 'result': "```python\npd.read_excel(os.path.join(PATH, 'source_data', '2010_Pop_Block_County.xls'), sheetname='San Francisco County', header=4)\n\npd.read_excel(os.path.join(PATH, 'source_data', '2010_Pop_Block_County.xls'), sheetname='San Francisco County', header=4)\npop_df = pop_df.drop('BLOCKS', axis='index').reset_index()\npop_df = pop_df.rename(columns={'index': 'block_str'})\n\nrecords = []\nfor ix, row in pop_df.iterrows():\n    splits = row['block_str'].split(', ')\n    record = row[[x for x in row.index if x != 'block_str']].to_dict()\n    record['block'] = str(splits[0].split('Block ')[-1])\n    record['block_group'] = str(splits[1].split('Block Group ')[-1])\n    census_tract = '0' + str(splits[2].split('Census Tract ')[-1].replace('.', ''))\n    census_tract += '0' * (6-len(census_tract))  # Even if it doesn't have a decimal part, needs to be 6 characters\n    record['census_tract'] = census_tract\n    records.append(record)\npd.DataFrame(records)\n\npd.columns = [to_appropriate_column_name(x) for x in pd.columns]\nstr_columns = ['block', 'block_group', 'census_tract']\nnonstr_columns = [x for x in pd.columns if x not in str_columns]\npd.loc[:, nonstr_columns] = pd[nonstr_columns].astype(int)\npd = pd[str_columns + nonstr_columns]\npd.to_csv(os.path.join(PATH, 'derived_data', 'SF_2010_pop_block.csv'))\npd.head()\n```"}, {'reason': 'stop', 'result': '```python\n# Compute a single transport map from day 7 to 7.5\not_model.compute_transport_map(7,7.5)\n```'}, {'reason': 'stop', 'result': '```python\n# Code specific to window_type == sliding\n\nxmin = 0\nxmax = y_score.shape[0]\nxindex = range(xmin, xmax)\n\n# ax = plt.Axes(fig, [0., 0., 1., 1.])\n# ax.set_axis_off()\n# fig.add_axes(ax)\n\nax[0].scatter(\n    xindex, \n    x_test_clipped, \n    marker=\'.\', \n    c=y_true_colors)\nax[0].set_title(\'epoch = {}\'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].imshow(\n    y_score.T, \n    interpolation=\'nearest\', \n    cmap=plt.get_cmap(\'Spectral\'), \n    origin=\'upper\');\nax[1].spines[\'top\'].set_visible(False)\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].set_ylim((y_score.shape[1], 0))\nax[1].set_yticks([y_score.shape[1]])\n\ndivider = make_axes_locatable(ax[1])\nax1Top = divider.append_axes("top", 0.5, sharex=ax[1])\nax1Top.xaxis.set_tick_params(labelbottom=False)\nax1Top.plot(y_score_mean)\nax1Top.set_title(\'sequence model type = {}\'.format(msig.sequence_type))\nax1Top.set_xlim((xmin, xmax))\nax1Top.set_ylim((-1, 1))\nax1Top.set_yticks((-1, 0, 1))\nax1Top.grid(True)\n\nax[2].imshow(\n    y_score_unshifted_clipped.T, \n    interpolation=\'nearest\', \n    cmap=plt.get_cmap(\'Spectral\'), \n    origin=\'upper\');\nax[2].spines[\'top\'].set_visible(False)\nax[2].set_xlim((xmin, xmax))\nax[2].set_xticks([])\nax[2].set_ylim((y_score_unshifted_clipped.shape[1], 0))\nax[2].set_yticks([y_score_unshifted_clipped.shape[1]])\n\ndivider = make_axes_locatable(ax[2])\nax2Top = divider.append_axes("top", 0.5, sharex=ax[2])\nax2Top.xaxis.set_tick_params(labelbottom=False)\nax2Top.plot(y_score_unshifted_clipped_mean)\nax2Top.set_title(\'{} window size = {}\'.format(window_type, msig.window_size))\nax2Top.set_xlim((xmin, xmax))\nax2Top.set_ylim((-1, 1))\nax2Top.set_yticks((-1, 0, 1))\nax2Top.grid(True)\n\nax[3].scatter(\n    xindex, \n    x_test_clipped,\n    marker=\'.\', \n    c=y_pred_colors)\nax[3].set_title(\'loss = {:<6.4f}, accuracy = {:<.2%}\'.format(*score))\nax[3].set_xlim((xmin, xmax))\nax[3].grid(True)\n\n# plt.draw()\nplt.savefig(os.path.join(msig.out_dir, \'prediction_analysis.png\'), bbox_inches=\'tight\', pad_inches=0.08)\n# plt.show()\n```'}, {'reason': 'stop', 'result': '```python\n# precision-recall curve\ny_scores = -probas_gmm\n_, _, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_gmm = metrics.auc(_, _)\n_, _, _ = metrics.roc_curve(y_true, y_scores)\nauroc_gmm = metrics.roc_auc_score(y_true, y_scores)\nprint("AUROC: %.2f, PR AUC: %.2f" % (auroc_gmm, pr_auc_gmm))\n```'}, {'reason': 'stop', 'result': '```python\nimport sympy as sym\nfrom sympy import *\nA, U, S = symbols("A U S")\nalpha_UA, alpha_AU, alpha_US, alpha_SU  = symbols("alpha_UA alpha_AU alpha_US alpha_SU")\n\n# steady-state solution\nsol_dyn = solve([\n    alpha_UA*U - alpha_AU*A,\n    alpha_AU*A + alpha_SU*S - (alpha_UA + alpha_US)*U,\n    alpha_US*U - alpha_SU*S,\n    A + U + S - 1 # this equation sets the total population size to 1\n], A, U, S)\n\nsol_dyn\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nallData = pd.read_csv('data/allData.csv')"}, {'reason': 'stop', 'result': "```python\ntraining_acc_file = 'accuracies/train_acc_list.p'\nvalid_acc_file = 'accuracies/valid_acc_list.p'\n\nwith open(training_acc_file, mode='rb') as f:\n    combined_train_acc_list = pickle.load(f)\nwith open(valid_acc_file, mode='rb') as f:\n    combined_valid_acc_list = pickle.load(f)\n    \nnp.row_stack([combined_train_acc_list, combined_valid_acc_list[0]])\n```"}, {'reason': 'stop', 'result': '```python\nimport statsmodels.stats.api as sms\n\n#ValidRTCI\nsms.DescrStatsW(RTanalysis.Valid).tconfint_mean()\n```'}, {'reason': 'stop', 'result': "```python\npd.read_csv('https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/ade_sales.csv')\n```"}, {'reason': 'stop', 'result': '```python\nsigmas=[]\nsigma=0\nfor i in range(0,100):\n    I=0\n    sigma=0\n    for _ in range (0,len(l)-1):\n        _,temp2=integrate(f,l[i],l[i+1],1000,10000)\n        sigma+=temp2\n        I+=_\n    sigmas.append(np.sqrt(sigma))\n\nplt.plot(np.arange(0,100,1),sigmas,\'r\')\nplt.ylabel(\'Sigma\')\nplt.xlabel(\'M\')\nplt.show()\nprint("Sigma of sigma=",np.sqrt(np.var(sigmas)))\n```'}, {'reason': 'stop', 'result': '```python\n#inspect 模块\n#signature(callable),获取签名(函数签名包含了一个函数的信息，包含函数名，他的参数类型，它所在的类和名称空间及其信息)\n\nimport inspect\ndef add(x:int,y:int,*args,**kwargs) -> int: #可变参数 不建议加 int 因为可以收集不同类型参数\n    return x+y\ninspect.signature(add)\n```'}, {'reason': 'stop', 'result': '```python\n# %%time\n\n# we need a custom pre-processor to extract correct field,\n# but want to also use default scikit-learn preprocessing (e.g. lowercasing)\ndefault_preprocessor = CountVectorizer().build_preprocessor()\n\n\ndef build_preprocessor(field):\n    field_idx = list(dataset.columns).index(field)\n    # if field == \'playlist_pid\': from IPython.core.debugger import set_trace; set_trace()\n    return lambda x: default_preprocessor(x[field_idx])\n\n\nvectorizer = FeatureUnion([\n    (\n        \'track_artist_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_artist_uri\'))),\n    (\n        \'track_album_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_album_uri\'))),\n    (\n        \'track_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_uri\'))),\n\n    (\n        \'playlist_pid\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'playlist_pid\'))),\n\n    ("playlist_name",\n      CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r"(?u)\\b\\w+\\b",\n            stop_words=None,\n            analyzer = \'word\',\n            # max_features=50000,\n            preprocessor=build_preprocessor("playlist_name"))),\n    \n    ("playlist_description",\n      CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r"(?u)\\b\\w+\\b",\n            stop_words=None,\n            analyzer = \'word\',\n            # max_features=50000,\n            preprocessor=build_preprocessor("playlist_description"))),\n#     (\n#         \'track_pos\',\n#         CountVectorizer(\n#             ngram_range=(1, 1),\n#             token_pattern=r".+",\n#             stop_words=None,\n#             # max_features=50000,\n#             preprocessor=build_preprocessor(\'track_pos\'))),\n\n    (\'track_duration_ms\',\n     ItemSelector(list(dataset.columns).index(\'track_duration_ms\'))),\n])\nvectorizer.fit_transform(data_train.values)\n```'}, {'reason': 'stop', 'result': '```python\nhouse_size = df_county_data["Median Income"]\nx_axis = np.arange(len(house_size))\n# Create a bar chart based upon the above data\n# plt.xticks(tick_locations, county, rotation= 90)\nplt.bar(x_axis, house_size, color="r", align="center")\nplt.title("County Median Income")\nplt.xlabel("Counties")\nplt.ylabel("Median Income Rate")\nplt.text(140, 120000, "Note:\\nMedian Income for all counties in NJ, NY, & PA.")\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Median Income1.png", bbox_inches = "tight")\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\n_, _, _, _ = tf.get_collection("my_important_ops")\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nparametersNaiveBayes = {\n    'priors':priors\n}\n\n(_, _) = correr_y_mostrar(\n    GaussianNB(), \n    parametersNaiveBayes, \n    5, \n    5\n)\n```"}, {'reason': 'stop', 'result': '```python\n_, _, _, channels = dataset.shape\n\nnp.zeros(shape=(7, 7, channels, 2), dtype=np.float32)[:, 3, :, 0] = 1  # vertical line\nnp.zeros(shape=(7, 7, channels, 2), dtype=np.float32)[3, :, :, 1] = 1  # horizontal line\n```'}, {'reason': 'stop', 'result': '```python\nA = A.tocsr()\nprint( type(A) )\n```'}, {'reason': 'stop', 'result': "```python\n_, cumulative_coverage = population_coverage(\n    mutation_fuzzer.population, http_program)\n\nimport matplotlib.pyplot as plt\nplt.plot(cumulative_coverage)\nplt.title('Coverage of urlparse() with random inputs')\nplt.xlabel('# of inputs')\nplt.ylabel('lines covered');\n```"}, {'reason': 'stop', 'result': "```python\nimport pandas as pd\nread_filename = './annotations_201712131226.csv'\n\npd.read_csv(read_filename+'.points', names=('filename', 'x1', 'y1', 'x2', 'y2', 'points'))\n```"}, {'reason': 'stop', 'result': '```python\ndef GPC(x_new, x, y, kernel, theta):\n    K, KS, KSS = get_Ks(x_new, x, kernel, theta)     #+\n    \n    f = find_f(K, y)                        #-\n    KP = util.calculate_KP(K, util.calculate_W(f, y))                     #-\n\n    return(f.squeeze(), np.diagonal(KSS - KS.dot(np.linalg.inv(KP).dot(KS.T))).squeeze())           #-\n```'}, {'reason': 'stop', 'result': "```python\n_, _ = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n                           train_len=95, test_len=5,\n                           save_hyp='plots/hyp_inlinecounter_shake.png',\n                           save_diag='plots/diag_inlinecounter_shake.png',\n                           save_resp='plots/resp_inlinecounter_shake.png')\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nplt.plot(f/3e10, y_nolm_aer, label='AER')\nplt.plot(f/3e10, y_nolm_aer_arts, label='AER_ARTS')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()\n```"}, {'reason': 'stop', 'result': '```python\ndef generate_line(model, words, seed=None, rhymes=None):\n    syllables_left = 10\n    last_stress = 0\n    \n    start_state = np.random.choice(len(model.startprob_), p=model.startprob_)\n    \n    if seed is not None:\n        possible_start_emissions = np.where(rhymes[seed] == 1)\n        probs = np.array(model.emissionprob_[start_state][possible_start_emissions])\n\n        scaled_probs = probs / sum(probs)\n        while True:\n            start_emission = np.random.choice(possible_start_emissions[0], p=scaled_probs)\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1 :\n                break\n    else:\n        while True:\n            start_emission = np.random.choice(len(model.emissionprob_[start_state]), p=model.emissionprob_[start_state])\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1:\n                break\n    \n    line.append(start_emission)\n    start_stress = poetrytools.stress(words[start_emission])\n    syllables_left -= len(start_stress)\n    \n    if len(start_stress) == 1:\n        prev_starting_stress = 1\n    else:\n        prev_starting_stress = int(start_stress[0])\n\n    curr_state = start_state\n    while syllables_left > 0:\n        possible_transitions = model.transmat_[curr_state]\n        curr_state = np.random.choice(len(possible_transitions), p=possible_transitions)\n        possible_emissions = model.emissionprob_[curr_state]\n        while True:\n            curr_emission = np.random.choice(len(possible_emissions), p=possible_emissions)\n            curr_stress = poetrytools.stress(words[curr_emission])\n            if len(curr_stress) == 1:\n                prev_starting_stress = 1 - prev_starting_stress\n                syllables_left -= 1\n                break\n            elif len(curr_stress) > syllables_left or int(curr_stress[-1]) == prev_starting_stress:\n                continue\n            else:\n                prev_starting_stress = int(curr_stress[0])\n                syllables_left -= len(curr_stress)\n                break\n        line.append(curr_emission)\n\n    return line\n\ndef convert_line(sample, words):\n    ret = \'\'\n    i = 0\n    for word in reversed(sample):\n        curr_word = words[word]\n        if i == 0 or (curr_word == \'i\'):\n            ret += curr_word.title() + \' \'\n        else:\n            ret += curr_word + \' \'\n        i += 1\n    return ret\n\ndef generate_pair(model, words, rhymes):\n    while True:\n        a_line = generate_line(model, words)\n        seed = a_line[0]\n        if len(np.where(rhymes[seed] == 1)[0]) > 0:\n            b_line = generate_line(model, words, seed, rhymes)\n            return a_line, b_line\n        \ndef generate_rhyming_and_meter_sonnet():\n    sonnet = \'\'\n    a_lines = []\n    b_lines = []\n    \n    for _ in range(4):\n        a_line, b_line = generate_pair(reversed_quatrain_model, quatrain_words, quatrain_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    for i in range(2):\n        sonnet += convert_line(a_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(a_lines[2 * i + 1], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i + 1], quatrain_words) + \'\\n\'\n    \n    a_lines = []\n    b_lines = []\n    \n    for _ in range(2):\n        a_line, b_line = generate_pair(reversed_volta_model, volta_words, volta_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    sonnet += convert_line(a_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(a_lines[1], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[1], volta_words) + \'\\n\'\n    \n    a_line, b_line = generate_pair(reversed_couplet_model, couplet_words, couplet_rhymes)\n    sonnet += convert_line(a_line, couplet_words) + \'\\n\'\n    sonnet += convert_line(b_line, couplet_words) + \'\\n\'\n    \n    return sonnet\n\ndef generate_10_rhyming_and_meter_sonnets():\n    sonnets = \'\'\n    for i in range(10):\n        print(\'Generating Sonnet \' + str(i + 1))\n        sonnets += str(i) + \'\\n\' + generate_rhyming_and_meter_sonnet() + \'\\n\'\n    \n    f = open("project2data/rhyming_and_meter_shakespeare.txt","w")\n    f.write(sonnets)\n    return sonnets\n```'}, {'reason': 'stop', 'result': '```python\nextra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run([training_op, extra_update_ops],\n                     feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': "```python\nauth = tweepy.OAuthHandler('', '')\nauth.set_access_token('', '')\n\napi = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n```"}, {'reason': 'stop', 'result': '```python\n# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # TODO: Create a decision tree regressor object\n    None\n\n    # TODO: Create a dictionary for the parameter \'max_depth\' with a range from 1 to 10\n    None\n\n    # TODO: Transform \'performance_metric\' into a scoring function using \'make_scorer\' \n    None\n\n    # TODO: Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values \'regressor\', \'params\', \'scoring_fnc\', and \'cv_sets\' respectively.\n    None\n\n    # Fit the grid search object to the data to compute the optimal model\n    None\n\n    # Return the optimal model after fitting the data\n    return None\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nw.get_step_object(step = 3, subset = subset_uuid).indicator_objects[indicator]\n```'}, {'reason': 'stop', 'result': "```python\nwaves_std = np.std(waves, axis=0)\n\nplt.plot(xindex, waves_std)\nplt.xlim((xmin, xmax))\nplt.xticks([])\nplt.grid(True)\n\nplt.scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=waves_std)\nplt.xlim((xmin, xmax))\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'std_dev_analysis.png'), bbox_inches='tight')\n```"}, {'reason': 'stop', 'result': '```python\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    init.run()\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n\nnp.array([[1., 0.]])'}, {'reason': 'stop', 'result': '```python\nos.chdir(os.getcwd())\nos.environ[\'GOOGLE_APPLICATION_CREDENTIALS\'] = "creds" + filesep + "sarasmaster-524142bf5547.json"\n```'}, {'reason': 'stop', 'result': "```python\npd.read_csv('http://www.stat.berkeley.edu/~statlabs/data/babies.data', delim_whitespace=True)\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nk = 3\nheterogeneity = []\ninitial_centroids = get_initial_centroids(tf_idf, k, seed=0)\n_, _ = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n              record_heterogeneity=heterogeneity, verbose=True)\nplot_heterogeneity(heterogeneity, k)\n```'}, {'reason': 'stop', 'result': '```python\nn_classes = 9\n\nnames_keep = np.asarray(names)\nnames_keep = names_keep.tolist()\nprint("classes to keep: " + str(names_keep))\n```'}, {'reason': 'stop', 'result': '```python\n#An example of how to use these functions to provide a directory listing.\ncmdstr = "ls"\nprint("The systemcall method doesn\'t show stdout when used in Jupyter notebook but does from a script.")\nsystemcall(cmdstr)\nprint("")\nprint("With systemcall_pipe you can see the stdout from Jupyter notebook, and can use the results in variables:")\nsystemcall_pipe(cmdstr)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n# 200 new values from x=0 to x=15\nn_new = 200\nX_new = np.linspace(0, 15, n_new)[:,None]\n\n# add the GP conditional to the model, given the new X values\nwith model:\n    gp.conditional("f_pred", X_new)\n\n# Sample from the GP conditional distribution\nwith model:\n    pm.sample_ppc(trace, vars=[f_pred], samples=1000)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nwith tf.Session() as sess:                                              # 책에는 없음\n    init.run()                                                          # 책에는 없음\n    for epoch in range(n_epochs):                                       # 책에는 없음\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):  # 책에는 없음\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n            clip_weights.eval()\n            clip_weights2.eval()                                        # 책에는 없음\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # 책에는 없음\n        print(epoch, "검증 세트 정확도:", accuracy_val)                     # 책에는 없음\n```'}, {'reason': 'stop', 'result': '```python\npm.Exponential("lambda_1", 1.0)\npm.Exponential("lambda_2", 1.0)\n```'}, {'reason': 'stop', 'result': "```python\nflor.Experiment('risecamp_demo').__enter__()\n```"}, {'reason': 'stop', 'result': '```python\n# Encoding Functions\n\ndef MAP(X):\n    """Map all values to integer numbers."""\n    """NaN values are treated as a unique value."""\n    \n    # create an encoding for categorical vars\n    unique_elems = set(X)\n    mapping = {label:idx for idx, label in enumerate(unique_elems)}\n    return X.map(mapping).astype(int)\n\ndef LOO(X):\n    """Perform Leave One Out counting for the features."""\n    \n    # map features to ordinal values first\n    X = MAP(X)\n    \n    # perform counts\n    mapping = {_:(_-1) for _ in enumerate(np.bincount(X))}\n    return X.map(mapping).astype(int)\n    \n\ndef OHE(df_cv, df_all, col_name, feature_names, feature_threshold=0.02):\n    """Map categorical values to a one hot encoding scheme."""\n    \n    X_cv = MAP(df_cv[col_name])\n    X_all = MAP(df_all[col_name])\n    \n    X_cv = X_cv.values.reshape(-1, 1)\n    X_all = X_all.values.reshape(-1, 1)\n    OHE = OneHotEncoder(sparse=False).fit(X_all)\n    X_cv_ohe = OHE.transform(X_cv)\n    X_all_ohe = OHE.transform(X_all)\n    \n    low_freq_features = []\n    for _ in range(X_all_ohe.shape[1]):\n        _ = col_name + str(_)\n        \n        # determine the frequency of the categorical data value\n        _ = np.sum(X_all_ohe[:, _]) / X_all_ohe.shape[0]\n        if _ > feature_threshold:\n            df_cv[_] = X_cv_ohe[:, _]\n            df_all[_] = X_all_ohe[:, _]\n            feature_names.append(_)\n        else:\n            low_freq_features.append(_)\n    \n    # aggregate low frequency features\n    if len(low_freq_features) > 0:\n        _ = col_name + str(X_all_ohe.shape[1])\n        feature_names.append(_)\n        \n        X_all_extra = np.array([0 for x in range(X_all.shape[0])])\n        X_cv_extra = np.array([0 for x in range(X_cv.shape[0])])\n        \n        for _ in low_freq_features:\n            for _ in enumerate(X_all_ohe[:, _]):\n                if _ == 1:\n                    X_all_extra[_] = 1\n            for _ in enumerate(X_cv_ohe[:, _]):\n                if _ == 1:\n                    X_cv_extra[_] = 1\n        \n        df_cv[_] = X_cv_extra\n        df_all[_] = X_all_extra                    \n            \n    feature_names.remove(col_name)\n    df_cv = df_cv.drop(col_name, axis=1)\n    df_all = df_all.drop(col_name, axis=1)\n    \n    return df_cv, df_all, feature_names\n```'}, {'reason': 'stop', 'result': '```python\ndef distance_between(lat1, lon1, lat2, lon2):\n  # haversine formula to compute distance "as the crow flies".  Taxis can\'t fly of course.\n  dist = np.degrees(np.arccos(np.minimum(1,np.sin(np.radians(lat1)) * np.sin(np.radians(lat2)) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.cos(np.radians(lon2 - lon1))))) * 60 * 1.515 * 1.609344\n  return dist\n\ndef estimate_distance(df):\n  return distance_between(df[\'pickuplat\'], df[\'pickuplon\'], df[\'dropofflat\'], df[\'dropofflon\'])\n\ndef compute_rmse(actual, predicted):\n  return np.sqrt(np.mean((actual-predicted)**2))\n\ndef print_rmse(df, rate, name):\n  print ("{1} RMSE = {0}".format(compute_rmse(df[\'fare_amount\'], rate*estimate_distance(df)), name))\n\nFEATURES = [\'pickuplon\',\'pickuplat\',\'dropofflon\',\'dropofflat\',\'passengers\']\nTARGET = \'fare_amount\'\ncolumns = list([TARGET])\ncolumns.extend(FEATURES) # in CSV, target is the first column, after the features\ncolumns.append(\'key\')\ndf_train = pd.read_csv(\'data/taxi-train.csv\', header=None, names=columns)\nrate = df_train[\'fare_amount\'].mean() / estimate_distance(df_train).mean()\nprint ("Rate = ${0}/km".format(rate))\nprint_rmse(df_train, rate, \'Train\')\n#print_rmse(df_valid, rate, \'Valid\')  \n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# Haringey\n# find steady state based on 2012 data\n\ncov_2012 = 0.267007002375\nadpc_2012 = 0.0346976493046\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.190544970144\nadpc_2013 = 0.0184872060681\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nodeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```'}, {'reason': 'stop', 'result': "```python\nsparse_data = my_spca.transform(X)\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = sparse_data - sparse_data.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()\n```"}, {'reason': 'stop', 'result': '```python\nfrustrated_edges = imbalance.keys()\nfor edge in G.edges(data=True):\n    if (edge[0],edge[1]) in frustrated_edges:\n        "black"\n    elif edge[2]["sign"] == -1:\n        "red"\n    else:\n        "green"\n\nfig,ax = plt.subplots(figsize=(15,10))\nnx.draw_networkx(G, pos=nx.drawing.layout.bipartite_layout(G, nodes_one), ax=ax, \n                 with_labels=True, node_size=1000, node_color="white", edge_color=edge_cols)\n```'}, {'reason': 'stop', 'result': '```python\n#\nflux, _ = coco.simulate(b"SN2007uy", \n                    z_obs, 0.0, 0.0, 0.0, 3.1, \n                    mjdmax, mjd_to_sim, \n                    filters_to_sim)\n```'}, {'reason': 'stop', 'result': "```python\n# What's the distribution of prices?\nsales_price = train['SalePrice']\nsns.distplot(sales_price)\n```"}, {'reason': 'stop', 'result': "```python\n# Test to see if DW comp. is working\ncorr_tensor[30, :, :].copy()[(corr_tensor[30, :, :] > -1*0.7) & (corr_tensor[30, :, :] < 0.7)] = 0\nmake_graph(corr_tensor[30, :, :].copy(), nodes, 'signed')\n```"}, {'reason': 'stop', 'result': '```python\n# Re-run the model with the Bib numbers as a feature and for the 5K, 10K and 15K split times to predict 20K time\n\n### set up data for modeling\nX_20K = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\']]\ny_20K = boston_clean[\'20K Duration\'].values.reshape(-1, 1)\nprint(X_20K.shape, y_20K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_20K, X_test_20K, y_train_20K, y_test_20K = train_test_split(X_20K, y_20K, random_state=29)\n# X_train_20K.head()\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_20K = LinearRegression()\nmodel_20K.fit(X_train_20K, y_train_20K)\n\n# Make predictions\n\n_ = model_20K.predict(X_test_20K)\n\n# Plot the residuals\n\nplt.scatter(model_20K.predict(X_train_20K), model_20K.predict(X_train_20K) - y_train_20K, c="blue", label="Training Data")\nplt.scatter(model_20K.predict(X_test_20K), model_20K.predict(X_test_20K) - y_test_20K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_20K.min(), xmax=y_test_20K.max())\nplt.title("Residual Plot 20K")\nplt.savefig(\'model_20k.png\')\nplt.show()\n```'}, {'reason': 'stop', 'result': "```python\nexp_name = 'regression'\nload_model(exps_path, exp_name, 100, camera_parameters_path)\n```"}, {'reason': 'stop', 'result': "```python\nlv_workspace.get_data_filter_object(step=1, subset='A') \n```"}, {'reason': 'stop', 'result': "```python\nsns.boxenplot(data=df['Revenue'], orient='h', color='#2ecc71')\n```"}, {'reason': 'stop', 'result': "```python\ndef plot():\n    plt.figure(figsize=(15,5)); \n\n    plt.subplot(131); \n    plt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\n    plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n    plt.grid(True); \n\n    plt.subplot(132); \n    plt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red',  alpha=0.5)\n    plt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\n    plt.grid(True); \n\n    plt.subplot(133); \n    plt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\n    plt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\n    plt.grid(True); \n\n    plt.show()\nplot()\n```"}, {'reason': 'stop', 'result': '```python\nwith model:\n    # To Be Explained in Chapter 3!\n    step = pm.Metropolis()\n    trace = pm.sample(25000, step=step)\n```'}, {'reason': 'stop', 'result': '```python\nnp.random.seed(1234)\n_ = {user:np.random.rand(31) - 0.5 for user in users}\n_ = {user:np.random.rand(1) - 0.5 for user in users}\n```'}, {'reason': 'stop', 'result': "```python\nCMSpileUpMonitor = 'CMSpileUpMonitor.png'\n\nImage(filename = pathname + CMSpileUpMonitor)\n```"}, {'reason': 'stop', 'result': '```python\nX = np.hstack((X, np.ones((X.shape[0], 1))))\n\n#Initialize Weights to zero:\nnp.zeros(X.shape[1])'}, {'reason': 'stop', 'result': '```python\nclass USZIPCodeRepository:\n\n    def __init__(self, data_url_prefix = \'https://raw.githubusercontent.com/yyu/GeoJSON-US/master\'):\n        self.data_url_prefix = data_url_prefix\n        self.geojson_url_prefix = f\'{data_url_prefix}/perZIPgeojson\'\n\n        self.refresh_zipcode_latlons(f\'{data_url_prefix}/ZIPCodesGazetteer.tsv\')\n        self.refresh_available_zipcodes(f\'{data_url_prefix}/perZIPgeojson/all_zipcodes.txt\')\n\n\n    def refresh_zipcode_latlons(self, url):\n        lines = [ line.decode(\'UTF8\').strip() for line in urllib.request.urlopen(url).readlines() ]\n        tsv = csv.DictReader(lines, delimiter=\'\\t\')\n        self.gazetteer = dict((d[\'GEOID\'], {\'lat\': float(d[\'INTPTLAT\']), \'lon\': float(d[\'INTPTLONG\'])}) for d in tsv)\n\n\n    def refresh_available_zipcodes(self, url):\n        lines = [ zipcode.decode(\'UTF8\').strip() for zipcode in urllib.request.urlopen(url).readlines() ]\n        self.zipcode_list = lines[1:] # ignore the first line\n        self.zipcode_set = set(self.zipcode_list)\n\n\n    def make_url(self, zipcode):\n        return f\'{self.data_url_prefix}/perZIPgeojson/{zipcode[0]}/{zipcode[1]}/{zipcode[2]}/{zipcode}.json\'\n\n\n    def fetch_zipcode(self, zipcode):\n        \'\'\'returns a (dict, err) tuple where err could be a string for error message or None\'\'\'\n\n        url = self.make_url(zipcode)\n\n        if url in USZIPCodeRepository.CACHE:\n            return (USZIPCodeRepository.CACHE[url], None)\n\n        try:\n            s = urllib.request.urlopen(url).read()\n        except urllib.error.URLError as e:\n            return (None, \'failed to get \' + url, \':\', e.reason)\n\n        j = json.loads(s)\n\n        USZIPCodeRepository.CACHE[url] = j\n\n        return (j, None)\n\n\n    def fetch_zipcodes(self, *zipcodes):\n        d = {"type": "FeatureCollection", "features": []}\n\n        available_zipcodes = set(zipcodes) & self.zipcode_set\n\n        for z in available_zipcodes:\n            j, err = self.fetch_zipcode(z)\n\n            if j is not None:\n                d[\'features\'].append(j)\n\n        return d\n```'}, {'reason': 'stop', 'result': '```python\ndef print_policy(policy, terminal_states):\n    \n    idx_to_symbol = {0:\'\\u2190\', 1:\'\\u2191\', 2:\'\\u2192\', 3:\'\\u2193\'}\n    \n    for i in range(policy.shape[0]):\n        \n        for j in range(policy.shape[1]):\n            \n            if (i,j) in terminal_states:\n                print(\'\\u25A0 \', end=\'\')\n            else:\n                print(idx_to_symbol[policy[i, j]]+" ", end=\'\')\n        \n        print()\n    \n    return\n```'}, {'reason': 'stop', 'result': '```python\nconnection_all = []\n\nfor k in range(len(mapIdx)):\n    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n    candA = all_peaks[limbSeq[k][0]-1]\n    candB = all_peaks[limbSeq[k][1]-1]\n\n    nA = len(candA)\n    nB = len(candB)\n    indexA, indexB = limbSeq[k]\n    if(nA != 0 and nB != 0):\n        connection_candidate = []\n        for i in range(nA):\n            for j in range(nB):\n                vec = np.subtract(candB[j][:2], candA[i][:2])\n                # print(\'vec: \',vec)\n                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n                # print(\'norm: \', norm)\n                vec = np.divide(vec, norm)\n                # print(\'normalized vec: \', vec)\n                startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n                               np.linspace(candA[i][1], candB[j][1], num=mid_num))\n                # print(\'startend: \', startend)\n                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n                                  for I in range(len(startend))])\n                # print(\'vec_x: \', vec_x)\n                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n                                  for I in range(len(startend))])\n                # print(\'vec_y: \', vec_y)\n                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                # print(score_midpts)\n                # print(\'score_midpts: \', score_midpts)\n                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n                \n                # print(\'score_with_dist_prior: \', score_with_dist_prior)\n                criterion1 = len(np.nonzero(score_midpts > param[\'thre2\'])[0]) > 0.8 * len(score_midpts)\n                # print(\'score_midpts > param["thre2"]: \', len(np.nonzero(score_midpts > param[\'thre2\'])[0]))\n                criterion2 = score_with_dist_prior > 0\n                \n                if criterion1 and criterion2:\n                    # print(\'match\')\n                    # print(i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2])\n                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n                # print(\'--------end-----------\')\n        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n        # print(\'-------------connection_candidate---------------\')\n        # print(connection_candidate)\n        # print(\'------------------------------------------------\')\n        connection = np.zeros((0,5))\n        for c in range(len(connection_candidate)):\n            i,j,s = connection_candidate[c][0:3]\n            if(i not in connection[:,3] and j not in connection[:,4]):\n                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n                # print(\'----------connection-----------\')\n                # print(connection)\n                # print(\'-------------------------------\')\n                if(len(connection) >= min(nA, nB)):\n                    break\n\n        connection_all.append(connection)\n    else:\n        connection_all.append([])\n```'}, {'reason': 'stop', 'result': '```python\ntrials = 1000\nwith Timer() as t:\n    for i in range(trials):\n        try:\n            url = fuzzer()\n            http_program(url)\n            print("Success!")\n        except ValueError:\n            pass\n\nt.elapsed_time() / trials\n```'}, {'reason': 'stop', 'result': '```python\n# build random forest\n_ = random_forest_build(dataset, 1000, .3, n_jobs=-1)\n```'}, {'reason': 'stop', 'result': '```python\n# Split the training data into separate train and test sets\n(_, _, _, _) = train_test_split(data, labels, test_size=0.25, random_state=0)\n\n# Convert the labels (letters) into one-hot encodings that Keras can work with\nle = LabelEncoder().fit(np.stack(list(_) + list(_)))\n_ = le.transform(_)\n_ = le.transform(_)\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n# import cv2\n\n# for img in X_train:\n#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nX_train_normal = np.array(X_train/255 - 0.5)\nX_valid_normal = np.array(X_valid/255 - 0.5)\nX_test_normal = np.array(X_test/255 - 0.5)\n\nEPOCHS = 15\nBATCH_SIZE = 128\n```'}, {'reason': 'stop', 'result': '```python\nwith tf.Session() as sess:\n    init.run()\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    new_saver.save(sess, "./my_new_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n\n# v1 . v2 = |v1| |v2| cos(a)\n# <=> a = cos-1( (v1.v2) / |v1||v2| )\n\n# 5 degrees tolerance is fine!\n\ndef debug_vectors(v1, v2):\n    print("v1: {0}, v2: {1}".format(v1, v2))\n    print("Angle: {0}".format(v_angle(v1, v2)))\n    print("Perpendicular: {0}". format(v_perpendicular(v1, v2, 4)))\n    print("Parallel: {0}".format(v_parallel(v1, v2, 3)))\n    print("Same Orientation: {0}".format(v_same_orientation(v1, v2)))\n    print("Dot product: {0}\\n".format(np.dot(v1, v2)))\n\ndef debug_all_samples(): \n    for sample in samples[0x10] + samples[0x80]:\n        v1 = (np.array(sample[1]) - np.array(sample[3])) / np.linalg.norm((np.array(sample[1]) - np.array(sample[3])))\n        v2 = (np.array(sample[0]) - np.array(sample[3])) / np.linalg.norm((np.array(sample[0]) - np.array(sample[3])))\n\n        debug_vectors(v1, v2)\n\n# vy (1486,68)\n# vx (1638,213)\n# s  (1581,119)\n# o  (1628,69)\n        \n    \ndebug_all_samples()\n\n\nprint("va: {0}, vb: {1}".format(np.array([1638, 213]), np.array([1486, 68])))\nprint("vat: {0}, vbt: {1}".format((np.array([1638, 213]) + translate), (np.array([1486, 68]) + translate)))\n```'}, {'reason': 'stop', 'result': '```python\nimport pymc3 as pm\n\nwith pm.Model():\n    parameter = pm.Exponential("poisson_param", 1.0)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nfor ax, country in zip(plt.subplots(10, 1, figsize=(14, 34))[1], top_10_list):\n    data = df[df['Country'] == country]\n    sns.countplot(data=data, x='YearsCodingProf', palette='Paired', ax=ax, order=data['YearsCodingProf'].value_counts().index)\n    ax.set_title('Years coding professionally in {}'.format(country), fontsize=16)\n    #plt.xticks(rotation='vertical')\n    sns.despine(left=True)\nplt.subplots_adjust(hspace=.6)\n```"}, {'reason': 'stop', 'result': '```python\nfrom tensorflow.contrib.slim.nets import inception\nimport tensorflow.contrib.slim as slim\n\nreset_graph()\n\nwith slim.arg_scope(inception.inception_v3_arg_scope()):\n    _, _ = inception.inception_v3(tf.placeholder(tf.float32, shape=[None, height, width, channels], name="X"), num_classes=1001, is_training=tf.placeholder_with_default(False, shape=[]))\n```'}, {'reason': 'stop', 'result': "```python\n# assign columns for each wfs so we can use them as features for training\nwfs = trimmed.pop('wfs')\ntrimmed['f9'] = ((wfs == 'newf9') | (wfs =='oldf9')) * 1\ntrimmed['f5'] = (wfs == 'f5') * 1\ntrimmed['mmirs'] = (wfs == 'mmirs') * 1\ntrimmed['bino'] = (wfs == 'binospec') * 1\ntrimmed = trimmed[(trimmed['hour'] > 0) & (trimmed['hour'] <= 13)]\ntrimmed\n```"}, {'reason': 'stop', 'result': '```python\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n```'}, {'reason': 'stop', 'result': '```python\n_, _ = plt.subplots(1,2, figsize=(10,4))\n_[0].imshow(img_advi)\n_[0].set_title("segmented image (ADVI)")\n_[1].hist(y, bins=K);\n_[1].set_title("cluster assignments (ADVI)")\nplt.tight_layout()\n```'}, {'reason': 'stop', 'result': "```python\ntrace_illum_x = []\ntrace_illum_y = []\ntrace_illum_z = []\n\ntrace_viewing_x = []\ntrace_viewing_y = []\ntrace_viewing_z = []\n\nfor i, row in df_viewing.iterrows():\n    illum_x, illum_y, illum_z = polar_to_euclidean(row['illum_theta'],row['illum_phi'])\n    view_x, view_y, view_z = polar_to_euclidean(row['view_theta'],row['view_phi'])\n    \n    _ = [illum_x, 0, view_x]\n    _ = [illum_y, 0, view_y]\n    _ = [illum_z, 0, view_z]\n    \n    trace_illum_x.append(illum_x)\n    trace_illum_y.append(illum_y)\n    trace_illum_z.append(illum_z)\n    \n    trace_viewing_x.append(view_x)\n    trace_viewing_y.append(view_y)\n    trace_viewing_z.append(view_z)\n```"}, {'reason': 'stop', 'result': '```python\n"""\nExperimenting with support vector machines\n"""\n# Importing all the libraries \nimport os\nimport json\nimport numpy                  as np\nimport tensorflow             as tf\nimport matplotlib.pyplot      as plt\nfrom sklearn                  import svm\n\n# These modules are used for the confusion matrix\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.metrics          import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\n\nclass ClassifySVM:\n    """\n    data:   numpy array consisting of data consisting of float values.\n    target: The target values associated with the input data.\n    """\n    \n    # def ConfusionMatrix(self, data, target):   \n    def SupportVectorClassify(self, data, target):\n\n        # Spliting the datasets into training and testing portions\n        train_indices = np.random.choice(len(data), round(len(data) * 0.8), replace=False)\n        test_indices  = np.array(list(set(range(len(data))) - set(train_indices)))\n\n        Beta_train    = data[train_indices]\n        Beta_test     = data[test_indices]\n        target_train  = target[train_indices]\n        target_test   = target[test_indices]\n\n        # Here we want a very large batch size to achieve convergence. \n        # The A variable will take on the 2 x 1 shape. (In the book, this is done because there are 2 predictor variables.)\n        batch_size    = 25000\n        x_data        = tf.placeholder(shape = [None, 2], dtype=tf.float32)\n        y_target      = tf.placeholder(shape = [None, 1], dtype=tf.float32)\n        A = tf.Variable(tf.random_normal(shape=[2,1]))\n        b = tf.Variable(tf.random_normal(shape=[1,1]))\n\n        # For correctly classified points, values of >= 1 if target is 1.\n        model_output = tf.subtract(tf.matmul(x_data, A), b)\n\n        # Calculate the L2 normm of a vector. \n        # Margin parameter: a\n        l2_norm      = tf.reduce_sum(tf.square(A))\n        alpha        = tf.constant([0.1])\n\n        # Declaring classification loss and adding together the two terms.\n        classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., tf.multiply(model_output, y_target))))\n        loss                = tf.add(classification_term, tf.multiply(alpha, l2_norm))\n\n        # Declaring the prediction and accuracy functions to evaluate bot the training and test sets.\n        prediction = tf.sign(model_output)\n        accuracy   = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))\n\n        # Declaring optimization function and initializing model variables.\n        my_opt     = tf.train.GradientDescentOptimizer(0.01)\n        train_step = my_opt.minimize(loss)\n        init       = tf.global_variables_initializer()\n        tf.Session().run(init)\n\n        # Starting the training loop. \n        # Making sure to record loss and training accuracy for both training and test sets. \n        loss_vec = []\n        train_accuracy  = []\n        test_accuracy   = []\n        \n        test_record = {}\n\n        for i in range(500):\n            rand_index = np.random.choice(len(Beta_train), size=batch_size)\n            \n            rand_x     = Beta_train[rand_index]\n            rand_y     = np.transpose([target_train[rand_index]])\n            no_end     = tf.Session().run(train_step, feed_dict = {x_data: rand_x, y_target: rand_y})\n            \n            # Storing the loss from each iteration into loss_vec\n            temp_loss  = tf.Session().run(loss, feed_dict = {x_data:rand_x, y_target:rand_y})\n            loss_vec.append(temp_loss)\n\n            # Storing train accuracies for each iteration of classification\n            train_acc_temp = tf.Session().run(accuracy, feed_dict={x_data:Beta_train, y_target:np.transpose([target_train])})\n            train_accuracy.append(train_acc_temp)\n\n            # Storing test accuracies for each iteration of classification\n            test_acc_temp = tf.Session().run(accuracy, feed_dict={x_data:Beta_test, y_target:np.transpose([target_test])})\n            test_accuracy.append(test_acc_temp)\n            \n            if (i+1)%100 == 0:\n                print(\'Step #\' + str(i + 1) + \' A = \' + str(tf.Session().run(A)) + \' b = \' + str(tf.Session().run(b)))\n                print(\'Loss = \' + str(temp_loss))\n                \n        test_prediction = tf.Session().run(prediction, feed_dict={x_data:Beta_test, y_target:np.transpose([target_test])})\n        print(confusion_matrix(target_test, test_prediction))\n        \n        """\n        with open("/Users/" + os.getlogin() + "/Desktop/SVMData.json", "w") as repository:\n            json.dump(test_record, repository, sort_keys=True, indent=4)\n        repository.close()\n        """\n\n        # Plotting the outputs (fit, loss and accuracy), the coefficients.\n        [[a1], [a2]] = tf.Session().run(A)\n        [[b]]        = tf.Session().run(b)\n        slope        = -a2/a1\n        y_intercept  = b/a1\n        x1_vals      = [d[1] for d in data]\n\n        best_fit     = []\n        for i in x1_vals:\n            best_fit.append(slope*i+y_intercept)\n\n        feature_x     = [d[1] for i,d in enumerate(data) if target[i] == 1]\n        feature_y     = [d[0] for i,d in enumerate(data) if target[i] == 1]\n        not_feature_x = [d[1] for i,d in enumerate(data) if target[i] == -1]\n        not_feature_y = [d[0] for i,d in enumerate(data) if target[i] == -1]\n\n        plt.plot(feature_x, feature_y, \'o\', label=\'I. Distracted\')\n        plt.plot(not_feature_x, not_feature_y, \'x\', label=\'Not Distracted\')\n        plt.plot(x1_vals, best_fit, \'r-\', label=\'Linear Separator\', linewidth=3)\n        plt.ylim([0, 10])\n        plt.legend(loc=\'upper left\')\n        plt.title(\'Boundary Classification\')\n        plt.xlabel(\'Delta & Beta\')\n        plt.ylabel(\'targets\')\n        plt.show()\n\n        plt.plot(train_accuracy, \'k-\', label=\'Training Accuracy\')\n        plt.plot(test_accuracy, \'r--\', label=\'Test Accuracy\')\n        plt.title(\'Train and Test Set Accuracies\')\n        plt.xlabel(\'Generation\')\n        plt.ylabel(\'Accuracy\')\n        plt.legend(loc=\'lower right\')\n        plt.show()\n\n        plt.plot(loss_vec, \'k-\')\n        plt.title(\'Loss per Generation\')\n        plt.xlabel(\'Generation\')\n        plt.ylabel(\'Loss\')\n        plt.show()\n        \n        \n# Reading from the csv file.\npath = r\'/Users/KaushikBhimraj/Desktop/Merged1_Data.csv\'\n\n# List comprehension used to create numpy arrays for inputs and targets.\n# Removing the heading row and converting the string values to int.  \nI           = [[line.split(\',\')[0], line.split(\',\')[1]] for line in open(path) if line[0:1] != \'\\n\'][1:]\nBetaDelta   = np.array([[float(val[0]), float(val[1])] for val in I])\nTarget      = np.array([line.split(\',\')[2] for line in open(path) if line[0:1] != \'\\n\'][1:])\n\n# Converting target to -1 or 1 format.\nT           = np.array([float(1) if y == "Distracted\\n" else float(-1) for y in Target])\n\n# Calling the class object and the function inside it that will execute the suport vector algorithm.\nc = ClassifySVM()\nc.SupportVectorClassify(BetaDelta, T)\n\n```'}, {'reason': 'stop', 'result': '```python\n# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\ntf.random.set_seed(9527)\n\n# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\nv = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\nv\n```'}, {'reason': 'stop', 'result': '```python\n#Select monthly mean PM2.5 data for the LSOA\nmonthly_data.isel(x=1103, y=1045).load()\n```'}, {'reason': 'stop', 'result': '```python\n# building list out of each value of hotel name\nrows = []\ndef createRow(x):    \n    new_list = x.tolist()\n    new_list.insert(0,x.name)\n    rows.append(new_list)\n    \n#converting the list to multi-columns data frame\nnormal_dataFrame.groupby("Hotel Name")["Normal"].transform( createRow )  # group by returns for each hotel a list of his normalized prices\npd.DataFrame.from_records(rows)\n\n# NOTES\n# Vector - each row represents a hotel along with his 160 normalized prices\n```'}, {'reason': 'stop', 'result': '```python\n# window the data using your windowing function\nwindow_size = 7\n_ , _ = window_transform_series(series = dataset,window_size = window_size)\n```'}, {'reason': 'stop', 'result': "```python\n# Generate hypothesis data\ndef gen_hyp_data(model, N, text_len=500):\n    texts, hiddens, hyps = [], [], []\n    for i in range(N):\n        text, hidden = generate(model, '\\n\\n', text_len, 0.8, True)\n        hidden = hidden.reshape(hidden.shape[0], -1)\n        hyp = hypothesis_inlinecounter(text)\n        hiddens.append(hidden)\n        hyps.append(hyp)\n        texts.append(text)\n    return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n```"}, {'reason': 'stop', 'result': '```python\ndef broad_greater_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has two masthead lights and one running light\n    visible for a starboard orientation.\n    """\n    white = (255, 255, 255)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 126)\n        shorter_masthead_light = np.random.randint(130, 186)\n        left_endpoint = np.random.randint(20, 126)\n        right_endpoint = np.random.randint(125, 211)\n        running_light_height_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 16)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = left_endpoint + light_width\n        short_masthead_height = shorter_masthead_light + light_width\n        short_masthead_width = right_endpoint + light_width\n        running_light_start = shorter_masthead_light + running_light_height_diff\n        running_light_width = running_light_start + light_width\n        if right_endpoint - left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(left_endpoint - 20, left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(left_endpoint, right_endpoint)\n        running_light_area = running_light_loc + light_width\n        new_view[taller_masthead_light:tall_masthead_height, left_endpoint:tall_masthead_width] = white\n        new_view[shorter_masthead_light:short_masthead_height, right_endpoint:short_masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```'}, {'reason': 'stop', 'result': "```python\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.callbacks import ReduceLROnPlateau\n\n### TODO: specify the number of epochs that you would like to use to train the model.\n\nBATCH_SIZE = 32\n\n### Do NOT modify the code below this line.\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\n# reduce LR\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=3, min_lr=0.0005, verbose=1)\n\naugment_data = False   # change to True as needed\n\nif not augment_data:\n\n    print('Training... without data augmentation')\n    history = scratch_model.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=, \n          batch_size=BATCH_SIZE, \n          callbacks=[checkpointer], \n          verbose=1)\n\nelse:\n    print('Training... WITH data augmentation')\n    history = scratch_model.fit_generator(datagen_train.flow(train_tensors, train_targets, batch_size=BATCH_SIZE),\n                    steps_per_epoch=train_tensors.shape[0] // BATCH_SIZE,\n                    epochs=, \n                    verbose=2, \n                    callbacks=[checkpointer],\n                    validation_data=datagen_valid.flow(valid_tensors, valid_targets, batch_size=BATCH_SIZE),\n                    validation_steps=valid_tensors.shape[0] // BATCH_SIZE)\n\nprint('Done training')\nshow_history_graph()"}, {'reason': 'stop', 'result': '```python\nP = np.diag([500., 49.])\n_, _ = run(count=50, R=10, Q=0.01, P=P)\n```'}, {'reason': 'stop', 'result': '```python\n_, _, _, _ = metrics.precision_recall_fscore_support(y_test_flattened[filter_items], y_pred_flattened[filter_items])\n```'}, {'reason': 'stop', 'result': '```python\ntf.nn.softmax(logits)\n```'}, {'reason': 'stop', 'result': '```python\nparameter = pm.Exponential("poisson_param", 1.0, testval=0.5)\n\nprint("\\nparameter.tag.test_value =", parameter.tag.test_value)\n```'}, {'reason': 'stop', 'result': '```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, oob_score=True, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\n\nrnd_clf.predict(X_test)\n```'}, {'reason': 'stop', 'result': "```python\n# start off with simplest case for proof of concept\nwave1_coeffs = {\n    'amplitude': {'mean': 0.5, 'delta': 0.05}, \n    'frequency': {'mean': 1.0, 'delta': 0.1},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'A',\n    'color': '#0000ff'\n}\nwave2_coeffs = {\n    'amplitude': {'mean': 0.75, 'delta': 0.075}, \n    'frequency': {'mean': 3.0, 'delta': 0.3},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'B',\n    'color': '#ff0000',\n#     'time': {'t_min': 0, 't_max': 5, 'n_timestamps': 601, 'noise_type': 'pareto', 'pareto_shape': 1.3},\n}\nwave3_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0.1}, \n    'frequency': {'mean': 8.0, 'delta': 0.8},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'C',\n    'color': '#00ff00'\n}\nwave4_coeffs = {\n    'amplitude': {'mean': 1.4, 'delta': 0.1}, \n    'frequency': {'mean': 12.0, 'delta': 1.2},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'D',\n    'color': '#ff00ff'\n}\n\nmwave_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0}, \n    'frequency': {'mean': 1.0, 'delta': 0}, \n    'offset': {'mean': 0, 'delta': 0},\n    'phase': {'mean': 0, 'delta': 1}, \n    'name': 'mixed_wave',\n    'time': {'t_min': 0, 't_max': 2, 'n_timestamps': 4096, 'delta': 0}\n}\n\nsigs_coeffs = [wave1_coeffs, wave2_coeffs, wave3_coeffs, mwave_coeffs, wave4_coeffs]\n\nfeatures=('x', 'dxdt')[0]\nbatch_size = 128\nwindow_size = 4096\nwindow_type = 'sliding'\n\nmsig = MixedSignal(\n    sigs_coeffs, \n    *features,\n    window_size=window_size, \n    window_type=window_type, \n)\n\nmsig.generate()\nn_classes = msig.n_classes\nn_features = msig.n_features\n\nprint(msig.inputs.shape)\nprint(msig.mixed_signal.shape)\nprint(msig.one_hots.shape)\nprint(msig.labels.shape)\nprint(msig.n_timestamps)\nprint(msig.n_samples)\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n# We'd like to look at all 3 pairs of heights, compute the absolute\n# difference between each pair, and then find the smallest of those\n# 3 absolute differences.  This is left to you!  If you're stuck,\n# try computing the value for each step of the process (like the\n# difference between Klay's heigh and Steph's height) on a separate\n# line and giving it a name (like klay_steph_height_diff).\n# YOUR CODE HERE\nraise NotImplementedError()\n```"}, {'reason': 'stop', 'result': '```python\nthreshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n```'}, {'reason': 'stop', 'result': '```python\n@print_log("Awesome function")\ndef _(arg, **other_args):\n    print("Doing something")\n    return\n```'}, {'reason': 'stop', 'result': '```python\n#Setup Axis\nstate=df_county_data[\'State\']\ncounty = df_county_data["County Name"]\ngrad_rate = df_county_data["Graduation Rate"]\nx_axis = np.arange(len(state))\n\n# Create a bar chart based upon the above data\n# plt.xticks(tick_locations, county, rotation= 45)\nplt.bar(x_axis, grad_rate, color="b", align="center")\nplt.title("County Graduation Rates")\nplt.xlabel("Counties")\nplt.ylabel("Graduation Rates")\nplt.text(140, 0.6, "Note:\\nGraduation Rates for all counties in NJ, NY, & PA.")\n\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Graduation_Rates.png", bbox_inches = "tight")\n# plt.zoom=10\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\nlosses = np.array(losses)\nplt.plot(losses.T[0], label=\'Discriminator\')\nplt.plot(losses.T[1], label=\'Generator\')\nplt.title("Training Losses")\nplt.legend()\n```'}, {'reason': 'stop', 'result': '```python\nfrom helpers import *\ndef test_your_least_squares():\n    height, weight, _ = load_data_from_ex02(sub_sample=False, add_outlier=False)\n    x, mean_x, std_x = standardize(height)\n    y, _ = build_model_data(x, weight)\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # least square or grid search: TODO\n    # this code should compare the optimal weights obtained \n    # by least squares vs. grid search\n    # ***************************************************\n    raise NotImplementedError\n```'}, {'reason': 'stop', 'result': "```python\nRTrunanalysis.loc[RTrunanalysis['Run'] == 0].Valid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': "```python\n#train.drop(['sentiment','seven_days'],axis=1,inplace=True)\n#test.drop(['sentiment','seven_days'],axis=1,inplace=True)\n\nfor string in ['share','comment','zan','content_len','链接','//@','@','#','【','《','\\[']:\n    temp = []\n    for i in test[string+'_histogram']:\n        if isinstance(i,int):\n            temp.append(np.zeros(shape=8))\n        else:\n            temp.append(i[0])\n    train.drop(string+'_histogram',axis=1,inplace=True)\n    test.drop(string+'_histogram',axis=1,inplace=True)\n\ntrain.drop(['pid','uid'],inplace=True,axis = 1)\ntest.drop(['pid','uid'],inplace=True,axis = 1)\n\nnp.save('processed_data/train3_np',train.values)\nnp.save('processed_data/test3_np',test.values)\nnp.save('processed_data/target3_np',train[['share','comment','zan']].values)\n```"}, {'reason': 'stop', 'result': '```python\nreset_graph()\n\nX = tf.placeholder(tf.float32, shape=(None, height, width, 1))\nfeature_maps = tf.constant(fmap)\ntf.nn.conv2d(X, feature_maps, strides=[1,1,1,1], padding="SAME")\n```'}, {'reason': 'stop', 'result': "```python\nplt.hist(old_faithful_df.std_waiting, bins=20, color=blue, lw=0, alpha=0.5);\n\nplt.xlabel('Standardized waiting time between eruptions');\nplt.ylabel('Number of eruptions');\n```"}, {'reason': 'stop', 'result': '```python\nreset_graph()\n\nimport tensorflow as tf\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n\ntraining = tf.placeholder_with_default(False, shape=(), name=\'training\')\n\nhidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")\nbn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\nbn1_act = tf.nn.elu(bn1)\n\nhidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")\nbn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\nbn2_act = tf.nn.elu(bn2)\n\nlogits = tf.layers.batch_normalization(bn2_act, training=training,\n                                       momentum=0.9)\n```'}, {'reason': 'stop', 'result': "```python\nxmin = 0\nxmax = x_test.shape[1]\nxindex = np.arange(xmin, xmax)\n\nax[0].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=y_true_colors)\nax[0].set_title('epoch = {}'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=y_pred_colors)\nax[1].set_title('loss = {:<6.4f}, accuracy = {:<.2%}'.format(*score))\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].grid(True)\n\nlegend_labels = []\nfor wave in msig.waves:\n    ax[2].plot(\n        xindex,\n        wave.sample_full, \n        color=wave.color,\n        zorder=1)\n    legend_labels.append(wave.name)\n\nax[2].scatter(\n    xindex[i_fail], \n    x_test[0, i_fail, 0], \n    marker='o', \n    c=y_pred_colors[i_fail],\n    zorder=2)\n\nax[2].set_xlim((xmin, xmax))\nax[2].grid(True)\nax[2].legend(legend_labels)\n\n# plt.draw()\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'prediction_analysis.png'), bbox_inches='tight')\n# plt.show()\n```"}, {'reason': 'stop', 'result': "```python\nint('hello')"}, {'reason': 'stop', 'result': "```python\nfrom math import sqrt\nfrom numpy.random import randn\n\ndef univariate_filter(x0, P, R, Q):\n    f = KalmanFilter(dim_x=1, dim_z=1, dim_u=1)\n    f.x = np.array([[x0]])\n    f.P *= P\n    f.H = np.array([[1.]])\n    f.F = np.array([[1.]])\n    f.B = np.array([[1.]])\n    f.Q *= Q\n    f.R *= R\n    return f\n\ndef plot_1d_2d(xs, xs1d, xs2d):\n    plt.plot(xs1d, label='1D Filter')\n    plt.scatter(range(len(xs2d)), xs2d, c='r', alpha=0.7, label='2D Filter')\n    plt.plot(xs, ls='--', color='k', lw=1, label='track')\n    plt.title('State')\n    plt.legend(loc=4)\n    plt.show()\n    \ndef compare_1D_2D(x0, P, R, Q, vel):\n    # storage for filter output\n    xs, xs1, xs2 = [], [], []\n\n    # 1d KalmanFilter\n    f1D = univariate_filter(x0, P, R, Q)\n\n    #2D Kalman filter\n    f2D = pos_vel_filter(x=(x0, vel), P=P, R=R, Q=0)\n    if np.isscalar(u):\n        u = [u]\n    pos = 0 # true position\n    for i in range(100):\n        pos += vel\n        xs.append(pos)\n\n        # control input u - discussed below\n        f1D.predict(u=u)\n        f2D.predict()\n        \n        z = pos + randn()*sqrt(R) # measurement\n        f1D.update(z)\n        f2D.update(z)\n        \n        xs1.append(f1D.x[0])\n        xs2.append(f2D.x[0])\n    plt.figure()\n    plot_1d_2d(xs, xs1, xs2)\n\ncompare_1D_2D(x0=0, P=50., R=5., Q=.02, vel=1.) \n```"}, {'reason': 'stop', 'result': '```python\nwith tf.Session() as sess:\n    A_np, B_np, Q_np, R_np = sess.run([A, B, Q, R])\n\nx_np = x0\nu_np = (-K@x_np)\nus_np = np.array(u_np)\nfor i in range(T):\n    x_np = A_np@x_np + B_np@u_np\n    u_np = (-K@x_np)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nnp.random.seed(42)\n```'}, {'reason': 'stop', 'result': '```python\ndata = np.array([[1, 3, 5], [2, 2, 3], [5, 3, 1]])\nClusteringNumeric(data)\n```'}, {'reason': 'stop', 'result': "```python\npd.merge(df1, df2, on=['item', 'time'])   #df1とdf2を項目'item'と'time'でマージ\n```"}, {'reason': 'stop', 'result': "```python\n_, _ = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(16, 6))\n\nK = 50\nalpha = 10.\n\nbeta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)\n\n_.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n          label='DP sample CDFs');\n_.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\n_.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\n_.set_title(r'$\\alpha = {}$'.format(alpha));\n_.legend(loc=2);\n\nK = 200\nalpha = 50.\n\nbeta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)\n\n_.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n          label='DP sample CDFs');\n_.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\n_.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\n_.set_title(r'$\\alpha = {}$'.format(alpha));\n_.legend(loc=2);\n```"}, {'reason': 'stop', 'result': "```python\n# Loop over features (polygons) in the shapefile\nfor f in tqdm(feats):\n    # Rasterize the polygon into an array\n    rasterized_image = features.rasterize([(shape(f['geometry']),1)],\n                                          out_shape=out_shape,\n                                          transform=new_aff,\n                                          fill=0,\n                                          all_touched=True)\n\n    # Extract from the xarray where the rasterized polygon is\n    region = data.where(rasterized_image == 1)\n    \n    # Combine x and y into a new dimension called allpoints and calculate the mean over it\n    # and then convert to a dataframe with an appropriate name\n    res = region.stack(allpoints=['x','y']).mean(dim='allpoints').to_dataframe(name=f['properties']['LSOA11CD'])\n    \n    # Append to the list of data frames so we can concatenate them all at the end\n    dfs.append(res)\n    \npd.concat(dfs, axis=1)\n```"}, {'reason': 'stop', 'result': "```python\nfig = plt.figure(figsize=(16,5))\nax = fig.add_subplot(121); ax.axis('off')\nay = fig.add_subplot(122); ay.axis('off')\n\n# -------------\nax.imshow(X[0,0])\nax.scatter(indv_left[1][0][0], indv_left[1][0][1], color='red')\n\n# -------------\nay.imshow(X[1,0])\nay.scatter(indv_right[1][0][0], indv_right[1][0][1], color='red')\n\nplt.show()\n```"}, {'reason': 'stop', 'result': '```python\nn_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': '```python\n# Create DensityForest instance\nDensityForest(max_depth=2, min_subset=.1, n_trees=100,\n              subsample_pct=.1, n_jobs=-1, verbose=10,\n              ig_improvement=.4)\n```'}, {'reason': 'stop', 'result': "```python\n_, _ = loadGapData('data/gapdet/large/',\n                  slider=slider_size,\n                  seq=True)\n```"}, {'reason': 'stop', 'result': '```python\n# Train the model\nAdaModel = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                             n_estimators=100, learning_rate=0.05)\nAdaModel = AdaModel.fit(X_train, y_train)\n```'}, {'reason': 'stop', 'result': "```python\n# Read in csv file as a Pandas DataFrame\npd.read_csv('twitter-archive-enhanced.csv')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\narbol = MiClasificadorArbol(max_depth=3)\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train           = []\nroc_validation      = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test) )\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train\ndf["Accuracy (validación)"] = accuracy_validation\ndf["AUC ROC (training)"]    = roc_train\ndf["AUC ROC (validación)"]  = roc_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()\n```'}, {'reason': 'stop', 'result': '```python\ny = pd.DataFrame(y_train)\ny.columns = [\'class\']\ny[\'index\'] = y.index\n\nnum_image = 5\nfor c in range(n_classes):\n    # filter \n    y[y[\'class\']== c].sample(num_image)\n    plt.imshow(X_train[y[\'index\']][0])\n    plt.text(0,1,signnames[str(c)],color=\'w\',backgroundcolor=\'r\', fontsize=5, weight="bold") \n    plt.axis(\'off\')\nplt.tight_layout()\nplt.show()\n```'}, {'reason': 'stop', 'result': "```python\nimport gensim\n\nmodel_path = 'GoogleNews-vectors-negative300.bin'\ngensim.models.Word2Vec.load_word2vec_format(model_path, binary=True)\n```"}, {'reason': 'stop', 'result': "```python\nX = Variable(name='X', num_states=2)\n\nZ = Variable(name='Z', num_states=2)\n\nY = Variable(name='Y', num_states=2)\n```"}, {'reason': 'stop', 'result': "```python\nfor _ in range(len(multiplier)):\n    imageToTest = cv.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv.INTER_CUBIC)\n    imageToTest_padded, pad = padRightDownCorner(imageToTest, 8, 128)\n\n    transposeImage = np.transpose(np.float32(imageToTest_padded[:,:,:]), (2,0,1))/256 - 0.5\n    testimage = transposeImage\n    cmodel = mx.mod.Module(symbol=sym, label_names=[])\n    cmodel.bind(data_shapes=[('data', (1,3,\n                                   testimage.shape[1],testimage.shape[2]))])\n    cmodel.init_params(arg_params=arg_params, aux_params=aux_params)\n    onedata = DataBatch(mx.nd.array([testimage[:,:,:]]), 0)\n    \n    cmodel.forward(onedata)\n    result=cmodel.get_outputs()\n    heatmap = np.moveaxis(result[1].asnumpy()[0], 0, -1)\n\n\n    heatmap = cv.resize(heatmap, (0,0), fx=model['stride'], fy=model['stride'], interpolation=cv.INTER_CUBIC)\n    heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n    heatmap = cv.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv.INTER_CUBIC)\n        \n    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n    \n   \n    f = plt.figure(_)\n    plt.imshow(oriImg[:,:,[2,1,0]])\n    ax2 = plt.imshow(heatmap[:,:,18], alpha=.5)\n    f.show()\n```"}, {'reason': 'stop', 'result': "```python\nX_train_feature = X_train_feature.drop('AnimalType_Dog', 1)\nX_dev_feature = X_dev_feature.drop('AnimalType_Dog', 1)\n\nmodel3 = Sequential([\n    Dense(32, input_shape=(12,)),\n    Dropout(0.1),  \n    Activation('sigmoid'),\n    Dense(5),\n    Activation('softmax'),\n])\n\nmodel3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel3.fit(np.array(X_train_feature), y_train_hot, epochs=10, batch_size=32)\n```"}, {'reason': 'stop', 'result': '```python\nlogits = LeNetTrafficSign(x, n_classes)\nprint(logits)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n```'}, {'reason': 'stop', 'result': "```python\npd.merge(\n    pd.merge(\n        tracks_df, playlist_map_df_negative, left_index=True, right_on='track_uri'),\n    playlist_df,\n    on='playlist_pid')\n```"}, {'reason': 'stop', 'result': '```python\nwith pm.Model() as:\n    theta = pm.Exponential("theta", 2.0)\n```'}, {'reason': 'stop', 'result': '```python\n# construct pipeline\npipe_dt = make_pipeline(\n    MinMaxScaler(), # used to normalize data onto a similar scale\n    SelectPercentile(), # used to filter out features that add noise\n    DecisionTreeRegressor()\n)\n\n# create the parameter grid for hyperparameter tuning\nparam_grid_dt = {\n    \'selectpercentile__percentile\':range(5, 30, 5), # what upper percentile of features to take\n    \'decisiontreeregressor__max_features\':["auto", "sqrt", "log2", None], # the number of features to conside when splitting\n    \'decisiontreeregressor__max_depth\':range(1, 10), # maximum depth of the decision tree\n    \'decisiontreeregressor__min_samples_leaf\':range(1, 4) # minimum number of samples required to be at a leaf node\n}\n\n# perform grid search of pipeline\ndt_grid = GridSearchCV(pipe_dt, param_grid_dt)\n\n# use results to create model on training data\ndt_grid.fit(train_features, train_outcome)'}, {'reason': 'stop', 'result': '```python\n# Separa o dataset em conjunto treino e teste\n_ = keystrokes[:split_data_idx]\n_ = keystrokes[split_data_idx:]\n```'}, {'reason': 'stop', 'result': '```python\ncreate_decision_tree(dataset)\n```'}, {'reason': 'stop', 'result': '```python\nn_outputs = len(flower_classes)\n\nwith tf.name_scope("new_output_layer"):\n    flower_logits = tf.layers.dense(prelogits, n_outputs, name="flower_logits")\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\ndf_hues = ['Todos','Sin distinguidos Betweenness', 'Sin distinguidos Grado']\ngig_comp_degree = gig_comp_graph.degree()\ngig_bet_degree = gig_bet_graph.degree()\ngig_degree_degree = gig_degree_graph.degree()\ndegree_frequencies1 = Counter([degree for (author, degree) in gig_comp_degree])\ndegree_frequencies2 = Counter([degree for (author, degree) in gig_bet_degree])\ndegree_frequencies3 = Counter([degree for (author, degree) in gig_degree_degree])\ndegree_info = []\nfor k,v in degree_frequencies1.iteritems(): degree_info.append((k,v,df_hues[0]))\nfor k,v in degree_frequencies2.iteritems(): degree_info.append((k,v,df_hues[1]))\nfor k,v in degree_frequencies3.iteritems(): degree_info.append((k,v,df_hues[2]))\n```"}, {'reason': 'stop', 'result': "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = None\ny = None\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(141)\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.subplot(142)\nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.subplot(143)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.subplot(144)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.show()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n# To be explained in Chapter 3!\nwith model:\n    step = pm.Metropolis(vars=[p])\n    trace = pm.sample(40000, step=step)\n```'}, {'reason': 'stop', 'result': '```python\n# Create datapoints between X_min and X_max to visualize the line of best fit\nnp.arange(X.numpy().min(), X.numpy().max(), 0.001)[:,None]\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# perform D reduction\nX = X.T @ X /(X.shape[0] - 1)\n_, _ = np.linalg.eigh(X)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:,idx]\n\nvec_reduced = np.zeros_like(vec)\nvec_reduced[:,:1] = vec[:,:1]\nval_reduced = val.copy()\nval_reduced[-1:] = 0\n\nproject_X = X @ vec_reduced\nproject_V = vec_reduced.T @ vec_reduced\nrevert_X = project_X @ np.linalg.inv(vec_reduced+0.0001)\nrevertedV = project_V.T @ np.linalg.inv(vec_reduced+0.0001)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(131)\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.subplot(132)\nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.subplot(133)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.grid(True)\n\nplt.show()\n```"}, {'reason': 'stop', 'result': '```python\nfrom pylab import polyfit, poly1d\nplt.scatter(RTValidityEffect, posttest.ExplicitMem, s=50,c=sns.xkcd_rgb[\'green\'],alpha=0.6,edgecolors=\'k\');\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None);\npolyfit(RTValidityEffect, posttest.ExplicitMem, 1);\npoly1d(fit);\nplt.plot(RTValidityEffect, fit_fn(RTValidityEffect), \'k\');\nplt.plot([-50,0,50,100,150],[2,2,2,2,2], \'r--\');\nplt.ylim(-0.1,4.1);\nplt.xlim(-50,150);\nplt.title(\'RT Validity Effect Predicts S-S Memory\', fontsize=26,fontweight="bold");\nplt.ylabel(\'S-S Memory Performance\\n(# Pairs Correct)\', fontsize=24,fontweight="bold");\nplt.xlabel(\'S-S RT Validity Effect (ms)\', fontsize=24,fontweight="bold");\nsns.despine();\n\nplt.show()\nstats.linregress(RTValidityEffect,posttest.ExplicitMem) #see stats below graph\n```'}, {'reason': 'stop', 'result': '```python\nprint(\'Loading case data ...\')\n\n#cases_800                    = pd.read_csv("pancancer_case_features_800.csv")\n#cases_1000                   = pd.read_csv("pancancer_case_features_1000.csv")\n#cases_1500                   = pd.read_csv("pancancer_case_features_1500.csv")\n#cases_bestfit_8000           = pd.read_csv("pancancer_case_features_bestfit_8000_topgenes_1000.csv")\ncases_bestfit_10000          = pd.read_csv("pancancer_case_features_bestfit_10000_topgenes_2000.csv")\ncases_bestfit_15000          = pd.read_csv("pancancer_case_features_bestfit_15000_topgenes_3000.csv")\ncases_bestfit_15000_allgenes = pd.read_csv("pancancer_case_features_bestfit_15000_topgenes_None.csv")\ncases_allgenes               = pd.read_csv("pancancer_case_features_all.csv")\n# all_data = {\n#     #\'800\':                     getDataAndLabels(cases_800),\n#     #\'1000\':                    getDataAndLabels(cases_1000),\n#     #\'1500\':                    getDataAndLabels(cases_1500),\n#     \'best_fit_10000\':          getDataAndLabels(cases_bestfit_10000),\n#     \'best_fit_15000\':          getDataAndLabels(cases_bestfit_15000),\n#     \'best_fit_15000_allgenes\': getDataAndLabels(cases_bestfit_15000_allgenes),\n#     \'genes_all\':               getDataAndLabels(cases_allgenes)\n# }\nprint("done.")\n```'}, {'reason': 'stop', 'result': "```python\n### Coletando as fontes de dados\npd.read_csv('train.csv', sep = ',', encoding = 'UTF-8')\npd.read_csv('test.csv', sep = ',', encoding = 'UTF-8')\n```"}, {'reason': 'stop', 'result': "```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nCountVectorizer(stop_words='english',\n                max_df=.1,\n                max_features=5000).fit_transform(df['review'].values)\n```"}, {'reason': 'stop', 'result': "```python\nfrom docplex.mp.model import Model\n\nModel(name='portfolio_miqp')\n```"}, {'reason': 'stop', 'result': "```python\n# Code specific to window_type == sliding\n\n# j = 10\n# s0 = slice(0, j)\n# s1 = slice(-j, -1)\n# s0 = s1\n# print(y_hat[s0, s1])\n# print(y_true[s0, s1])\n# print(y_true_value[s0, s1])\n# print(y_pred[s0, s1])\n# print(y_pred_value[s0, s1])\n# print(y_penalty[s0, s1])\n# print(y_score[s0, s1])\n# print(np.min(y_score), np.max(y_score))\n\nif msig.sequence_type == 'many2many':\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\nelse:\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\n```"}, {'reason': 'stop', 'result': "```python\n####### Defining network #######\n# input: state x\n# output: control u\n\n# input_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\n# fc1 = tf.layers.dense(inputs=input_layer, units=1, activation=tf.nn.tanh, name='fc1', reuse=tf.AUTO_REUSE)\nu = tf.layers.dense(inputs=input_layer, units=1, activation=tf.nn.tanh, name='fc_out', reuse=tf.AUTO_REUSE)\n# u = tf.layers.dense(inputs=input_layer, units=1, name='u_out_layer', reuse=tf.AUTO_REUSE)\n\n### LOSS FUNCTION ### \nloss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n\n# xs = tf.identity(x, name='xs')\n# us = tf.constant(0, name='us')\n# xs = x\n# us = u\n\n# cond = lambda i, x, l, xs, us: i < T\n\n# def body(i, x, l, xs, us):\n#     next_i = i+1\n#     next_x = tf.add(tf.matmul(A, x), tf.multiply(u,B))\n#     next_l = tf.add(l,\n#                     tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x),\n#                            tf.matmul(tf.transpose(u), tf.multiply(R, u))))\n#     next_xs = tf.concat(xs, next_x)\n#     next_us = tf.concat(us, u)\n#     return (next_i, next_x, next_l, next_xs, next_us)\n\n# i, xloss_f, traj_f = tf.while_loop(cond, body, \n#                                    loop_vars=[tf.constant(0), x, loss, xs, us],\n#                                    shape_invariants=[tf.TensorShape([1,]), tf.TensorShape([2, 1]), \n#                                                      tf.TensorShape([1,]) , tf.TensorShape([2, None]), \n#                                                      tf.TensorShape([1, None])])\n# train = tf.train.GradientDescentOptimizer(0.01).minimize(xloss_f.loss)\n\nfor i in range(T):\n    # LQR loss \n#     x_term = tf.matmul(tf.matmul(tf.transpose(x), Q), x, name='x_term')\n#     u_term = tf.matmul(tf.transpose(u), tf.multiply(R, u), name='u_term')\n#     loss = tf.add(loss, tf.add(x_term, u_term), name='loss')  # accumulate loss\n    \n    # Dynamics: advancing the system dynamics\n    Ax = tf.matmul(A, x, name='Ax'+str(i))\n    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n\n    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n    \n#     u = tf.layers.dense(inputs=tf.transpose(x), units=1, name='u_out_layer', reuse=True)\n    \n    fc1 = tf.layers.dense(inputs=tf.transpose(x), units=1, name='fc1', reuse=True)\n    u = tf.layers.dense(inputs=fc1, units=1, name='fc_out', reuse=True)\n    \n#     xs = tf.concat([xs, x], 1)\n#     us = tf.concat([us, u], 1)\n    \nopt = tf.train.GradientDescentOptimizer(0.0001)\ntrain = opt.minimize(loss)\n# grads_and_vars = opt.compute_gradients(loss)\n```"}, {'reason': 'stop', 'result': '```python\nnp.arange(250)'}, {'reason': 'stop', 'result': '```python\ngROOT.ProcessLine(\'HistogramOperations ops\')\ngROOT.ProcessLine(\'lightTables.setBirksParams(1.0,6.90)\')\n\nfor _, _ in detNames.iteritems():\n    params = CalibParams(calPath+calNames[_])\n    gROOT.ProcessLine(\'vector<TH1*> phs{1} = ops.loadHistograms("33MeVTa_{0}_ls_{1}_fittedPSDCut.root")\'.format(runNum,_))\n    gROOT.ProcessLine(\'ops.applyCalibration(phs{0}[1],{1},{2})\'.format(_, params.a, params.b))\n    gROOT.ProcessLine(\'TFile *tgt{0} = new TFile("33MeVTa_{0}_ls_{1}_calibFittedPSDCut.root","recreate")\'.format(runNum,_))\n    gROOT.ProcessLine(\'phs{0}[1]->Rebin(3)\'.format(_))\n    gROOT.ProcessLine(\'phs{0}[1]->Draw()\'.format(_))    \n    gROOT.ProcessLine(\'phs{0}[1]->Write()\'.format(str(_)))\n    pause()\n```'}, {'reason': 'stop', 'result': "```python\n# First, we've written down the values of the 3 universal\n# constants that show up in Newton's formula.\n\n# G, the universal constant measuring the strength of gravity.\ngravity_constant = 6.674 * 10**-11\n\n# M, the moon's mass, in kilograms.\nmoon_mass_kg = 7.34767309 * 10**22\n\n# R, the radius of the moon, in meters.\nmoon_radius_m = 1.737 * 10**6\n\n# The distance the hammer should have fallen over the\n# duration of the fall, in meters, according to Newton's\n# law of gravity.  The text above describes the formula\n# for this distance given by Newton's law.\n# **YOU FILL THIS PART IN.**\npredicted_distance_m = ...\n\n# Here we've computed the difference between the predicted\n# fall distance and the distance we actually measured.\n# If you've filled in the above code, this should just work.\npredicted_distance_m - estimated_distance_m\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# Create the mfusg headfile object\nheadfile = os.path.join(modelpth, '{0}.hds'.format(modelname))\nheadobj = flopy.utils.HeadFile(headfile)\ntimes = headobj.get_times()\n_ = headobj.get_data(totim=times[-1])\n```"}, {'reason': 'stop', 'result': '```python\n# Re-run the model with ONLY female runners, and  features of the 5K, 10K 15K 20K 25K, half and 35K split times to predict 40K time\n\n### set up data for modeling\nX_F40K = boston_females[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\', \'20K Duration\',\'Half Duration\', \'25K Duration\', \'30K Duration\', \'35K Duration\']]\ny_F40K = boston_females[\'40K Duration\'].values.reshape(-1, 1)\nprint(X_F40K.shape, y_F40K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_F40K, X_test_F40K, y_train_F40K, y_test_F40K = train_test_split(X_F40K, y_F40K, random_state=29)\n# X_train_F40K.head()\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_F40K = LinearRegression()\nmodel_F40K.fit(X_train_F40K, y_train_F40K)\n\n# Make predictions\n\n\n# Plot the residuals\n\nplt.scatter(model_F40K.predict(X_train_F40K), model_F40K.predict(X_train_F40K) - y_train_F40K, c="blue", label="Training Data")\nplt.scatter(model_F40K.predict(X_test_F40K), model_F40K.predict(X_test_F40K) - y_test_F40K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_F40K.min(), xmax=y_test_F40K.max())\nplt.title("Residual Plot Female Runners 40K")\nplt.savefig(\'model_F40K.png\')\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2014, test_diag_fun(x)[1] - adpc_2014], \n    [0.09, 0.25] \n    )\n```'}, {'reason': 'stop', 'result': "```python\n# tranform each class\nY1 = np.matmul(X1,v2)\nY2 = np.matmul(X2,v2)\n\n## show me dont tell me\nax = plt.gca()\nax.hist(Y1,color='blue', alpha=0.5, label='1')\nax.hist(Y2,color='yellow', alpha=0.5, label='2')\nplt.legend(loc='upper right')\nplt.xlabel('y')\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nclass MyScaler():\n\n    def __init__(self, cols):\n        self.cols = cols\n    \n    def fit(self, X):\n        self.ss = StandardScaler()\n        self.ss.fit(X[self.cols])\n        return self\n    \n    def transform(self, X):\n        return self.ss.transform(X[self.cols])\n```'}, {'reason': 'stop', 'result': "```python\ntrain_corpus ,test_corpus = preprocessing.clean_corpus()\ntest_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\ntrain_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n\ntrain_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\ntest_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n\n_,_ = preprocessing.encode_label()\n\npreprocessing.bag_of_word(train_corpus['corpus'].values,test_corpus['corpus'].values,min_df=10)\n```"}, {'reason': 'stop', 'result': '```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = df_county_data["Speak a language other than English"]\ny = df_county_data["Graduation Rate"] \nplt.scatter(x, y, color="g", marker="o", alpha=0.9) \n\n#Calculate and add R2 value\n~np.isnan(x) & ~np.isnan(y)\n\n\n#Add regression line\nsns.regplot(df_county_data["Speak a language other than English"], \n              df_county_data["Graduation Rate"], color=\'r\',label = "Speak a language other than English" )\n\n# Incorporate the other graph properties\nplt.title("High School Graduation Rates and ESL by County")\nplt.ylabel("Graduation Rate")\nplt.xlabel("Speak a language other than English") \n\nplt.legend(loc=\'best\')\nplt.grid(True)\nsns.set_style(\'whitegrid\')\nplt.text(65, 0.925, "Note:\\nAreas with one or more Foreign languages beside English language \\ntend to have a Lower graduation rate.")\n\nplt.savefig("Images/County_Grad_Speak a language other than English3.png", bbox_inches = "tight")\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\nif ld is not None:\n    dictenc(ae.D)\n```'}, {'reason': 'stop', 'result': '```python\nwith tf.Session() as sess:\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_new_model_final.ckpt")    \n```'}, {'reason': 'stop', 'result': '```python\nnx.algorithms.centrality.betweenness_centrality(gig_comp_graph)\n```'}, {'reason': 'stop', 'result': '```python\nmodel_5K.predict(X_test_5K)\n```'}, {'reason': 'stop', 'result': '```python\nplt.figure(figsize=(11, 4))\nfor subplot, learning_rate in ((121, 1), (122, 0.5)):\n    sample_weights = np.ones(len(X_train))\n    for i in range(5):\n        plt.subplot(subplot)\n        svm_clf = SVC(kernel="rbf", C=0.05, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n        y_pred = svm_clf.predict(X_train)\n        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n        plt.title("learning_rate = {}".format(learning_rate), fontsize=16)\n\nplt.subplot(121)\nplt.text(-0.7, -0.65, "1", fontsize=14)\nplt.text(-0.6, -0.10, "2", fontsize=14)\nplt.text(-0.5,  0.10, "3", fontsize=14)\nplt.text(-0.4,  0.55, "4", fontsize=14)\nplt.text(-0.3,  0.90, "5", fontsize=14)\nsave_fig("boosting_plot")\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\n# Devemos especificar o dispositivo que vai rodar as operações da rede.\nwith tf.device("/device:GPU:0"):\n    # As variáveis x e y são placeholders para os vetores de entrada e\n    # labels (classes) respectivamente.\n    x = tf.placeholder(tf.float32, shape=[None, 31])\n    y = tf.placeholder(tf.float32, shape=[None, 51])\n\n    # W1 é a matriz que vai levar o input para um estado intermedirário\n    # de processamento da rede.\n    # b1 é o vetor responsável pela translação nesse novo espaço.\n    W1 = tf.get_variable("W1", shape=[31, 64], initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.zeros([64]), name="b1")\n    \n    # A variável layer1 armazena o valor da primeira camada da rede. Isso\n    # significa que layer1 utiliza W1 e b1, bem como uma função de ativação\n    # (não-linearidade) para computar o valor da primeira camada da rede.\n    layer1 = tf.nn.relu(tf.matmul(x,W1) + b1)\n    \n    # Analogamente, W2 e b2 serão utilizados para gerar a segunda camada da\n    # rede (camada final).\n    W2 = tf.get_variable("W2", shape=[64, 51], initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.zeros([51]), name="b2")\n\n    \n    # A variável y_ armazena a última camada da rede utilizando W2 e b2 para\n    # calculá-la.\n    y_ = tf.matmul(layer1,W2) + b2\n\n    # Aplicamos o softmax em y_ e definimos como função de custo a cross-entropy.\n    # O tensorflow tem uma versão otimizada para realizar essas operações em uma\n    # única função (o que ajuda a reduzir o tempo de treino).\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_))\n```'}, {'reason': 'stop', 'result': '```python\nplt.subplots(figsize=(7, 3));\nQ1 = [18, 58];\nlabels = [\'Yes\',\'No\']\ncolors = [\'lightcoral\', \'lightskyblue\'];\npatches, texts = plt.pie(Q1, colors=colors, startangle=90)\nplt.legend(patches, labels, loc=\'lower left\')\nplt.title(\'Q1: Noticed Any Regularities, Sequences, or Pairs?\', fontsize=17,fontweight="bold");\nplt.axis(\'equal\');\nplt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nRTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': '```python\nreset_graph()\n\nX = tf.placeholder(tf.float32, shape=(None, 28 * 28), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, 300, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, 50, activation=tf.nn.relu, name="hidden2")\n    logits = tf.layers.dense(hidden2, 10, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.MomentumOptimizer(0.01, 0.9)\n    training_op = optimizer.minimize(loss)    \n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n```'}, {'reason': 'stop', 'result': '```python\nuseravg_std = moviesstd.mean().round(decimals=5)\n\nuseravg_std\n```'}, {'reason': 'stop', 'result': "```python\nget_dataset_path('gamma_test_large.simtel.gz')\n\nCameraCalibrator()\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nds.GetRasterBand(1)#elev\nds.GetRasterBand(2)\nds.GetRasterBand(3)\n```'}, {'reason': 'stop', 'result': '```python\nhe_init = tf.variance_scaling_initializer()\n_ = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                          kernel_initializer=he_init, name="hidden1")\n```'}, {'reason': 'stop', 'result': '```python\n# Dudley\n# find steady state based on 2012 data\n\ncov_2012 = 0.0750667240187\nadpc_2012 = 0.0057129570304\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.238873910562\nadpc_2013 = 0.0199612670162\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nodeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```'}, {'reason': 'stop', 'result': "```python\nfrom scipy.stats import uniform\n\nparametersKNN = {\n    'n_neighbors' : randint(1, 360),\n    'weights'     : ['uniform', 'distance']\n}\n\n(tiempo_random_KNN, _) = correr_randomized_y_mostrar(\n    KNeighborsClassifier(), \n    parametersKNN, \n    5, \n    10,\n    200\n)\n\nverTiempo(tiempo_KNN, tiempo_random_KNN)\n```"}, {'reason': 'stop', 'result': '```python\nplt.subplots(figsize=(8,4))\ndfTitanic.hist(column="Age");\nprint(dfTitanic["Age"].describe())\n```'}, {'reason': 'stop', 'result': "```python\nscores = []\nfor rep in range(number_of_replicates):\n    env.reset()\n    done = False\n    episode_score = 0.0\n    while not done:\n        action, _, _ = \\\n            model_smoothed_fitted_q(env, gamma, RandomForestRegressor, number_of_value_iterations, transition_model_fitter,\n                                    pairwise_kernels_=None, kernel_sums=None, smoothing_method='mse')\n        _, r, done = env.step(action)\n        episode_score += r\n    scores.append(episode_score)\n    print('score: {}'.format(score))\nprint('mean score: {} se: {}'.format(np.mean(scores), np.std(scores) / np.sqrt(number_of_replicates)))\n```"}, {'reason': 'stop', 'result': '```python\n# Load pickled data\nimport pickle\n\n# TODO: Fill this in based on where you saved the training and testing data\n\ntraining_file = "../traffic-signs-data/train.p"\nvalidation_file="../traffic-signs-data/valid.p"\ntesting_file = "../traffic-signs-data/test.p"\n\nwith open(training_file, mode=\'rb\') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode=\'rb\') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode=\'rb\') as f:\n    test = pickle.load(f)\n```'}, {'reason': 'stop', 'result': "```python\n# y_pred = np.argmax(states['y_hat'], axis=1)\n# y_pred_colors = np.hstack([vsig.signal_colors[i] for i in y_pred])\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3))\n\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes#[i // ncols, i % ncols]\n        ax.cla()\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(\n            vsig.timestamps[vsig.window_size-1:], \n#             vsig.timestamps, \n#             x_val[:, -1, 0], \n#             x_val[0, :, 0], \n            vsig.mixed_signal[vsig.window_size-1:], \n            marker='o', \n            cmap=plt.get_cmap('coolwarm'), \n            vmin=-1, \n            vmax=1\n        )\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n        \n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, 'output', g + 1))\n#     plt.savefig(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, 'gen', str(g + 1)]) + '.png'))\n\nplt.show()\n```"}, {'reason': 'stop', 'result': "```python\nimport numpy as \n\ndef word2embedding(sent, i):\n    word = sent[i][0]\n            \n    if word in model.vocab:\n        worde = word\n    else:\n        worde = 'null'\n    \n    res = model[worde]\n     \n    if i > 0:\n        word1 = sent[i-1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n    \n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n  \n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n    \n    res.shape = (1,900)\n    return res\n        \ndef sent2embedding(sent):\n        rese = []\n        for  i in range(len(sent)):\n            line = word2embedding(sent,i)\n            rese.append(line)\n        \n        resee = np.vstack(rese)\n                #rese.extend(word2embedding(sent, i))\n        #print(resee.shape)\n        return resee\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nBASE = path.join(path.dirname(attitude.__file__),'..','js-frontend')\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n#dataframe om alle gediplomeerden in Categorie "Meerdere" te selecteren\ndf_me = output.loc[output[\'Categorie\'] == \'Meerdere\']\ndf_me\n```'}, {'reason': 'stop', 'result': '```python\n# TODO: create states with emission probability distributions P(word | tag) and add to the model\n# (Hint: you may need to loop & create/add new states)\n#basic_model.add_states()\n\nfor tag, word_dict in emission_counts.items():\n    emission_dict = defaultdict(float)\n    for word in word_dict.keys():\n        emission_dict[word] = emission_counts[tag][word] / tag_unigrams[tag] \n    state_emission = DiscreteDistribution(dict(emission_dict))\n    states[tag] = State(state_emission, name=tag)\n    \nbasic_model.add_states(list(states.values()))\n\n# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n# (Hint: you may need to loop & add transitions\n#basic_model.add_transition()\nfor tag in data.training_set.tagset:\n    state = states[tag]\n    basic_model.add_transition(basic_model.start, state, tag_starts[tag]/len(data.training_set))\n    basic_model.add_transition(state, basic_model.end, tag_ends[tag]/tag_unigrams[tag])\n    for next_tag in data.training_set.tagset:\n        next_state = states[next_tag]\n        basic_model.add_transition(state, next_state, tag_bigrams[(tag, next_tag)]/tag_unigrams[tag])\n\n# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n# finalize the model\nbasic_model.bake()\n\nassert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n       "Every state in your network should use the name of the associated tag, which must be one of the training set tags."\nassert basic_model.edge_count() == 168, \\\n       ("Your network should have an edge from the start node to each state, one edge between every " +\n        "pair of tags (states), and an edge from each state to the end node.")\nHTML(\'<div class="alert alert-block alert-success">Your HMM network topology looks good!</div>\')\n```'}, {'reason': 'stop', 'result': "```python\n# Get the components.\n\n# get the circuit by Name\nqp.get_circuit('Circuit')\n\n# get the Quantum Register by Name\nqp.get_quantum_register('qr')\n\n# get the Classical Register by Name\nqp.get_classical_register('cr')\n```"}, {'reason': 'stop', 'result': '```python\nfrom yellowbrick.text import FreqDistVisualizer\n\nplt.figure(figsize=(15,20))\n_ = FreqDistVisualizer(n=100, features = cv_description.get_feature_names())\n#visualizer.fit(dt_mat_description)\n#visualizer.poof()\n```'}, {'reason': 'stop', 'result': '```python\n# use your function\nwindow_size = 100\nstep_size = 5\n_, _ = encode_io_pairs(text,window_size,step_size)\n```'}, {'reason': 'stop', 'result': '```python\nDotPlot(gepard_command, out_dir)\n```'}, {'reason': 'stop', 'result': "```python\nfor ID in range(10,86):\n    sub = cdat[cdat.subject == ID]\n    lists[0].append(ID)\n    validRT_trials = sub[sub.TrialType == 'Valid'].RT.mean()\n    invalidRT_trials = sub[sub.TrialType == 'Invalid'].RT.mean()\n    lists[1].append(validRT_trials)\n    lists[2].append(invalidRT_trials)\n    \nRTanalysis['SubjectID'] = lists[0]\nRTanalysis['Valid'] = lists[1]\nRTanalysis['Invalid'] = lists[2]\n```"}, {'reason': 'stop', 'result': "```python\npd.options.display.max_colwidth = 400\npd.read_csv(join(path, 'survey_results_schema.csv'), low_memory=False)\n```"}, {'reason': 'stop', 'result': '```python\nwith pm.Model():\n    \n    obs_A = pm.Bernoulli("obs_A", p_A, observed=observations_A)\n    obs_B = pm.Bernoulli("obs_B", p_B, observed=observations_B)\n\n    step = pm.Metropolis()\n    trace = pm.sample(20000, step=step)\n```'}, {'reason': 'stop', 'result': '```python\n##  get river name\n_ = data.iloc[0]["River"]\n```'}, {'reason': 'stop', 'result': "```python\npd.DataFrame({'labels':labels, 'count':count})\n```"}, {'reason': 'stop', 'result': '```python\nwith model:\n    pm.Binomial("number_cheaters", 100, p_skewed, observed=35)\n```'}, {'reason': 'stop', 'result': '```python\n_, _ = make_regression(n_samples=100, n_features=1, n_informative=1, noise=30, \n                       random_state=2018)\n```'}, {'reason': 'stop', 'result': '```python\nvectorizer.transform(data_test.values)\n```'}, {'reason': 'stop', 'result': '```python\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                   scope="hidden[34]|outputs")\n```'}, {'reason': 'stop', 'result': '```python\nalgorithms.UncontrolledCharging()'}, {'reason': 'stop', 'result': '```python\nnp.average([nx.average_shortest_path_length(C) for C in nx.connected_component_subgraphs(random_degree_graph)])\n```'}, {'reason': 'stop', 'result': "```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer()\ndocs = np.array([\n        'The sun is shining',\n        'The weather is sweet',\n        'The sun is shining, the weather is sweet, and one and one is two'])\ncount.fit_transform(docs)\n```"}, {'reason': 'stop', 'result': "```python\nscores = []\nfor _ in range(number_of_replicates):\n    env.reset()\n    done = False\n    episode_score = 0.0\n    while not done:\n        _, _, done = env.step(np.random.binomial(1, 0.3))\n        episode_score += r\n    scores.append(episode_score)\nprint('mean score: {} se: {}'.format(np.mean(scores), np.std(scores) / np.sqrt(number_of_replicates)))\n```"}, {'reason': 'stop', 'result': '```python\nn_epochs = 10\nbatch_size = 100\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        print(epoch, "Train accuracy:", accuracy.eval(feed_dict={X: X_batch, y: y_batch}), "Test accuracy:", accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels}))\n\n        saver.save(sess, "./my_mnist_model")\n```'}, {'reason': 'stop', 'result': '```python\nBaggingClassifier(\n    DecisionTreeClassifier(splitter="random", max_leaf_nodes=16, random_state=42),\n    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)\n```'}, {'reason': 'stop', 'result': '```python\n# North Lincolnshire\n# find steady state based on 2012 data\n\ncov_2012 = 0.100807801953\nadpc_2012 = 0.0111652211547\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.173269822929\nadpc_2013 = 0.0216211803756\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nodeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```'}, {'reason': 'stop', 'result': "```python\ndef fit_lstm(train,batch_size,nb_epoch,neurons):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    #define the loss function and optimization algorithm here\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\nfrom keras.applications.resnet50 import ResNet50\n\n# define ResNet50 model\n_ = ResNet50(weights='imagenet')\n```"}, {'reason': 'stop', 'result': '```python\ndef broad_less_than_50_meters_port():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible.\n    """\n    white = (255, 255, 255)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 201)\n        distance_bw_left_endpoint = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = distance_bw_left_endpoint + light_width\n        running_light_start = tall_masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if distance_bw_left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(distance_bw_left_endpoint - 20, distance_bw_left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(25, distance_bw_left_endpoint + 20)\n        running_light_area = running_light_loc + light_width\n        new_array[taller_masthead_light:tall_masthead_height, distance_bw_left_endpoint:tall_masthead_width] = white\n        new_array[running_light_start:running_light_width, running_light_loc: running_light_area] = red\n        new_array = new_array.flatten()\n        all_broad_images[i] = new_array\n\n    return all_broad_images\n```'}, {'reason': 'stop', 'result': '```python\n##  Ask for input file, full path\n##  i.e. Y:\\LRMF\\R_tables\\columbia_river_orig.csv\ninputFile = input("Please provide a full-path input file:")\n##inputFile = "Y:\\\\LRMF\\\\R_tables\\\\colorado_river_orig.csv"\n\npd.read_csv(inputFile,header=0)\n```'}, {'reason': 'stop', 'result': '```python\nt = np.linspace(temperature.min() - 5, temperature.max()+5, 50)[:, None]\np_t = logistic(t.T, beta_samples, alpha_samples)\n\n_ = p_t.mean(axis=0)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nplt.plot(t[first:last], traj_data_us[first:last],  label='Angles',  marker='o',linewidth=0.3, markersize=1.5)\nplt.legend();\nplt.ylabel('Angles [deg]')\nplt.xlabel('Time')\nplt.title('Angles');\n\nplt.plot(t[first:last], vel_us[first:last],  label='Angular velocity',  marker='o',linewidth=0.3, markersize=1.5)\nplt.legend();\nplt.ylabel('Angular velocity [deg/s]')\nplt.xlabel('Time')\nplt.title('Velocity');\n\nplt.plot(t[first:last], acc_us[first:last],  label='Angular acceleration',  marker='o',linewidth=0.3, markersize=1.5)\nplt.legend();\nplt.ylabel('Angular acceleration [deg/s^2]')\nplt.xlabel('Time')\nplt.title('Acceleration');\n\nplt.plot(t[first:last], np.sum(np.abs(acc_us[first:last]), axis=1),  label='Summed acc',  marker='o',linewidth=0.3, markersize=1.5)\nplt.legend();\nplt.ylabel('Summed accelerations [deg/s^2]')\nplt.xlabel('Time')\nplt.title('Summed accelerations');\n\nplt.plot(t[first:last], psi_us[first:last],  label='Pseudo power',  marker='o',linewidth=0.3, markersize=1.5)\nplt.legend();\nplt.ylabel('Pseudo power [deg/s^3]')\nplt.xlabel('Time')\nplt.title('Pseudo power');\n\nplt.plot(power_time_ds[first:last], power_data_250[first:last],  label='Power',  marker='o',linewidth=0.3, markersize=1.5)\nplt.legend();\nplt.ylabel('Power [W]')\nplt.xlabel('Time')\nplt.title('Power');\n```"}, {'reason': 'stop', 'result': '```python\n# Shuffle data\nimages_seq, labels_seq = correspondingShuffle([images_seq, labels_seq])\n\n    \n# Split data on train and test dataset\ndiv = int(train_set * len(images_seq))\n\nprint("Training images:", div)\nprint("Testing images:", len(images_seq) - div)\n```'}, {'reason': 'stop', 'result': '```python\n# 2 1d ODEs\ndef ode(state):\n    # x and y are first two components of state vector\n    x = state[0]\n\n    # Compute state derivatives.  Mess around here! \n    dx = .4* np.square(x) - 2\n\n    # Return the state derivatives\n    return [dx]\n```'}, {'reason': 'stop', 'result': '```python\nreset_graph()\n\ntf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n```'}, {'reason': 'stop', 'result': '```python\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    gray = grayscale(image)\n    blur_gray = gaussian_blur(gray, kernel_size)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n    # This time we are defining a four sided polygon to mask\n    vertices = np.array([[(0, image.shape[0]), (image.shape[1]*0.48, image.shape[0]*0.6), (image.shape[1]*0.52, image.shape[0]*0.6), (image.shape[1], image.shape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges, vertices)\n \n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    rho = 1 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 90     # minimum number of votes (intersections in Hough grid cell)\n    min_line_length = 30 #minimum number of pixels making up a line\n    max_line_gap = 30    # maximum gap in pixels between connectable line segments\n    line_image = np.copy(image)*0 # creating a blank to draw lines on\n\n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap, vertices)\n\n    # Draw the lines on the edge image\n    result = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n    \n    return result\n```'}, {'reason': 'stop', 'result': '```python\nnp.min(errors)'}, {'reason': 'stop', 'result': "```python\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_topics=10,\n                                random_state=123,\n                                learning_method='batch')\nlda.fit_transform(X)\n```"}, {'reason': 'stop', 'result': '```python\nConsedRunner()```'}, {'reason': 'stop', 'result': "```python\nfrom numpy.random import seed\nseed(8923)\n\nf = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=np.diag([500., 49.]))\n_, zs = compute_dog_data(3., .02, count)\n\n_, _, _, _ = f.batch_filter(zs)\n_, _, _, _ = f.rts_smoother(Xs, Covs)\n\nbook_plots.plot_measurements(zs)\nplt.plot(Xs[:, 0], ls='--', label='Kalman Position')\nplt.plot(Ms[:, 0], label='RTS Position')\nplt.legend(loc=4);\n```"}, {'reason': 'stop', 'result': '```python\ndef bow_lights_gt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    \n    Ships greater than 50m in length.\n    """\n    white = (255, 255, 255)\n    red = (255, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 126)\n        tall_mh_height = taller_masthead_light + light_width\n        shorter_masthead_light = np.random.randint(tall_mh_height + 2, tall_mh_height + 16)\n        short_mast_head_height = shorter_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(15, 56)\n        running_light_dist_vert = np.random.randint(short_mast_head_height, short_mast_head_height + 51)\n        new_view[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        new_view[shorter_masthead_light:short_mast_head_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_view = new_view.flatten()\n        all_bow_images[i] = new_view\n    \n    return all_bow_images\n```'}, {'reason': 'stop', 'result': "```python\nACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': '```python\nwith tf.name_scope("posterior"):\n    qpi = Empirical(tf.get_variable("qpi/params", [K],initializer=tf.constant_initializer(1.0/K)))\n    qmu = Empirical(tf.get_variable("qmu/params", [K, D],initializer=tf.zeros_initializer()))\n    qsigma = Empirical(tf.get_variable("qsigma/params", [K, D],initializer=tf.ones_initializer()))\n    qz = Empirical(tf.get_variable("qz/params", [N],initializer=tf.zeros_initializer(),dtype=tf.int32))\n```'}, {'reason': 'stop', 'result': "```python\n# reload(accuracy)\nreload(slope)\nplt.close()\n\nle = .12\nre = .02\nte = .1\nbe = .11\nh_gap = .13\n\nw = .5\nh = 1. - te - be\n\nax_lines = plt.figure(figsize=(5, 4)).add_axes([le, be, w, h])\nax_slopes = plt.figure(figsize=(5, 4)).add_axes([le + w + h_gap, be, 1. - w - h_gap - le - re, h])\n\nkey = fracs[-1]\n\nslope.plot_cv_slope(subjects, deep_all, linear_all, chance[0], training_size, fracs, (ax_lines, ax_slopes),\n                    legend=True, normalize_chance=False)\n\nf.text(.05, 1. - te + .02, 'A', **letter_fontstyle)\nf.text(le + w + h_gap - .075, 1. - te + .02, 'B', **letter_fontstyle)\n\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.eps'), dpi=300)\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.png'), dpi=300)\n```"}, {'reason': 'stop', 'result': "```python\n# This looks much better, so let's replace the SalePrice with the log-transformed version (will need to exponentiate predictions)\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n# Re-define the full dataset - and work on this until we are ready to split out test and train sets again\npd.concat([train, test], keys = ['train', 'test'])\n```"}, {'reason': 'stop', 'result': '```python\nX, _ = train[0:-1], train[-1]\nX\n```'}, {'reason': 'stop', 'result': "```python\nplt.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n        label='DP sample CDFs');\nplt.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\nplt.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\nplt.title(r'$\\alpha = {}$'.format(alpha));\nplt.legend(loc=2);\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nfor _, slices in storage.get_slices().items():\n    output = slices.to_output()\n    print(output, '\\n')\n```"}, {'reason': 'stop', 'result': "```python\nwith tf.Session() as session:\n    weights_layer_1 = session.run(weights['layer_1'])\n\n    fig, axes = plt.subplots(7,7)\n\n    for i, ax in enumerate(axes.flat):\n        if i < 48:\n            ax.imshow(weights_layer_1[:,:,0,i].reshape([3,3]), vmin=np.min(weights_layer_1), vmax=np.max(weights_layer_1), cmap='seismic')\n            ax.axis('off')\n\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n```"}, {'reason': 'stop', 'result': '```python\nparametrosRF = {\n    "max_features": ["sqrt", "log2", None] + [1, 20, 50, 100, 150, 200],\n    "max_depth": [3, 6, 12],\n    "min_samples_split": [2, 6, 12],\n    "n_estimators": [10, 50, 120, 200]\n}\n\ncorrer_y_mostrar(\n    RandomForestClassifier(),\n    parametrosRF,\n    5,\n    5\n)\n\nplot_learning_curve(grid_random_forest.best_estimator_, "Learning Curve Random Forest Mejor segun GridSearch", X_dev_np, y_dev_np, cv=5)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\ntf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ntf.placeholder(tf.int32, shape=(None), name="y")\n```'}, {'reason': 'stop', 'result': '```python\n# TODO: Apply your clustering algorithm of choice to the reduced data \n\n# TODO: Predict the cluster for each data point\n\n# TODO: Find the cluster centers\n\n# TODO: Predict the cluster for each transformed sample data point\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nnew_pc = eigvecs[:,-2:]\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = new_pc - new_pc.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()\n```"}, {'reason': 'stop', 'result': '```python\ndata = ["acacag", "acacgg", "aaaaaacgg"]\nClusteringSeq(data)\n```'}, {'reason': 'stop', 'result': '```python\noutput_dir = "nmt"\n\nif not os.path.exists(output_dir):\n  os.makedirs(output_dir)\n```'}, {'reason': 'stop', 'result': '```python\n# Edward Model\nwith tf.name_scope("model"):\n    Dirichlet(concentration=tf.constant([1.0] * K, name="pi/weights"), name= "pi")\n    Normal(loc= tf.ones(D, name="centroids/loc") * 127, \n                scale= tf.ones(D, name="centroids/scale") * 80, sample_shape=K, name= "centroids")\n    InverseGamma(concentration=tf.ones(D, name="variability/concentration"), \n                         rate=tf.ones(D, name="variability/rate"), sample_shape=K, name= "variability")\n    ParamMixture(pi_ed, {\'loc\': mu_ed, \'scale_diag\': tf.sqrt(sigmasq_ed)},\n                     MultivariateNormalDiag, sample_shape=N, name= "mixture")\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\npd.read_csv('posttest.csv')\n```"}, {'reason': 'stop', 'result': '```python\n# The duration of the fall in the experiment, in seconds.\n\n# The estimated distance the hammer actually fell, in meters.\n```'}, {'reason': 'stop', 'result': '```python\n#InvalidACCCI\nACCanalysis.Invalid.mean()\nsms.DescrStatsW(ACCanalysis.Invalid).tconfint_mean()\n```'}, {'reason': 'stop', 'result': '```python\ntrainer.predict(Xt)\n```'}, {'reason': 'stop', 'result': '```python\nimport googlemaps\n\ngooglemaps.Client(key="PASTE YOUR API KEY HERE")\n```'}, {'reason': 'stop', 'result': "```python\nn_samples_train = 2048\nn_samples_valid = 512\nepochs = 200\n\nx_valid, y_valid, *_ = msig.generate_samples(n_samples_valid, 'tf_tc')\n\nfor i in range(epochs):\n    x_train, y_train, *_ = msig.generate_samples(n_samples_train, 'tf_tc')\n    model.fit(\n        x_train, y_train,\n        epochs=1, \n        validation_data=(x_valid, y_valid),\n        batch_size=batch_size,\n        verbose=1, \n        callbacks=[\n            csvlogger,\n            checkpointer\n        ],\n    )\n    if stateful:\n        model.reset_states()\n\n# Code specific to window_type == sliding\n\n#     if (i + 1) % dump_every == 0:     \n#         test_dict['epoch'].append(i + 1)\n#         score = model.evaluate(test_dict['X'], test_dict['y'], batch_size=batch_size)\n#         if stateful:\n#             model.reset_states()\n#         test_dict['score'].append(score)\n\n#         y_hat = model.predict(test_dict['X'], batch_size=batch_size)\n#         if stateful:\n#             model.reset_states()\n#         test_dict['y_hat'].append(y_hat)\n\n    if (i + 1) % == 0:\n        print('#' * 50)\n        print(f'Epoch: {(i + 1)}/{epochs}')\n        print('#' * 50)\n#         model.save(msig.model_filename)\n```"}, {'reason': 'stop', 'result': "```python\n#Set the pipelines for categorical variables\ndiscrete_pipe_dog = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['dog'], hashing=None))])\ndiscrete_pipe_cat = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['cat'], hashing=None))])\n\n#Set the pipelines for continuous variables\ncontinuous_pipe_cat = Pipeline(steps=[('Scale', MyScaler(continuous['cat']))])\ncontinuous_pipe_dog = Pipeline(steps=[('Scale', MyScaler(continuous['dog']))])\n```"}, {'reason': 'stop', 'result': '```python\nband1.ReadAsArray().flatten()\nband2.ReadAsArray().flatten()\nband3.ReadAsArray().flatten()\n```'}, {'reason': 'stop', 'result': '```python\nprovidence_data_file = os.path.join("data/words_sentences/providence_avg_prosody_pos.csv")\nbrent_data_file      = os.path.join("data/words_sentences/brent_avg_prosody_pos.csv")\nboth_data_file       = os.path.join("data/words_sentences/brentprovidence_avg_prosody_pos.csv")\n\n# define x-fields (column IDs) to keep at 1    = word itself\n#                                         2    = pos\n#                                         3    = length (letters)\n#                                         4    = frequency\n#                                         5-93 = egemaps prosody features\nfeatures  = list(range(1,93))\n\npos_filter = None  #[[\'pos\', \'nouns\',\'function_words\']]\n\n# define name (col header) of y-variable in data file\ny         = \'y\'\n\n# load data, x-fields, y-field, train/dev/test split from input file\nprint("extracting providence...")\nprovidence_x_train, providence_y_train, _, _, _, _, _ = get_data_from_tsv(providence_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\nprint("extracting brent...")\nbrent_x_train, brent_y_train, _, _, _, _, _ = get_data_from_tsv(brent_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\n\nprint("extracting brentprovidence...")\nboth_x_train, both_y_train, _, _, _, _, _ = get_data_from_tsv(both_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\n\nif corpus == "Providence":\n    x_train = providence_x_train\n    y_train = providence_y_train\nelif corpus == "Brent":\n    x_train = brent_x_train\n    y_train = brent_y_train\nelif corpus == "BrentProvidence":\n    x_train = both_x_train\n    y_train = both_y_train\n\nfirst_numeric_feature = x_train.columns.tolist().index(\'log_length\')\nfirst_egemaps_feature = x_train.columns.tolist().index(\'F0semitoneFrom27.5Hz_sma3nz_amean\')\n\nprint(first_numeric_feature,first_egemaps_feature)\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                              name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n                              name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")\n```'}, {'reason': 'stop', 'result': '```python\ndef full_test(model, hypothesis, name, train_len=95, test_len=10):\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(), hypothesis,\n                                         train_len=train_len, test_len=train_len)\n    metric_pearsonr = lambda a, b: stats.pearsonr(a, b)[0]\n    \n    print("Hypothesis: {} (normal)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(class_weight=\'balanced\'),\n                                         hypothesis, train_len=train_len, test_len=test_len)\n    print("Hypothesis: {} (balanced)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))\n```'}, {'reason': 'stop', 'result': '```python\nnp.expm1(model_ridge.predict(X_test))\nnp.expm1(model_lasso.predict(X_test))\nnp.expm1(model_elastic.predict(X_test))\n```'}, {'reason': 'stop', 'result': "```python\n#make the flopy model\nflopy.modflow.Modflow(modelname=modelname, exe_name=mfexe, model_ws=modelpth)\nflopy.modflow.ModflowDis(nlay, nrow, ncol,\n                         delr=delr, delc=delc, \n                         top=botm[0, :, :], botm=botm[1:, :, :], \n                         perlen=1, nstp=1, steady=True)\nflopy.modflow.ModflowBas(ibound=ibound, strt=strt)\nflopy.modflow.ModflowLpf(hk=0.0001, laytyp=4)\nflopy.modflow.ModflowOc(stress_period_data={(0,0): ['print budget', 'print head', \n                                                     'save head', 'save budget']})\nflopy.modflow.ModflowSms(nonlinmeth=1, linmeth=1,\n                         numtrack=50, btol=1.1, breduc=0.70, reslim = 0.0,\n                         theta=0.85, akappa=0.0001, gamma=0., amomentum=0.1,\n                         iacl=2, norder=0, level=5, north=7, iredsys=0, rrctol=0.,\n                         idroptol=1, epsrn=1.e-5,\n                         mxiter=500, hclose=1.e-3, hiclose=1.e-3, iter1=50)\nwrite_input()\n\n# remove any existing head files\ntry:\n    os.remove(os.path.join(model_ws, '{0}.hds'.format(modelname)))\nexcept:\n    pass\n\n# run the model\nrun_model()\n```"}, {'reason': 'stop', 'result': '```python\nerror_line_traces(x_values, y_values, 1.7, 10)'}, {'reason': 'stop', 'result': "```python\nim = Model(name='integer_programming')\nim.print_information()\n```"}, {'reason': 'stop', 'result': '```python\n_, _ = prepare_batch(flower_paths_and_classes_train, batch_size=4)\n```'}, {'reason': 'stop', 'result': '```python\nsy.lambdify(x, M_Nm)```'}, {'reason': 'stop', 'result': "```python\n#Set name of MODFLOW exe\n#  assumes executable is in users path statement\nexe_name = 'mfusg'\nif platform.system() == 'Windows':\n    exe_name += '.exe'\nmfexe = exe_name\n\nmodelpth = os.path.join('data')\n\n#make sure modelpth directory exists\nif not os.path.exists(modelpth):\n    os.makedirs(modelpth)\n```"}, {'reason': 'stop', 'result': '```python\n# bluid poly\ntx_tr = build_poly(Strain1_z, 1)\ntx_te = build_poly(Stest1_z, 1)\n\nfor _ in range(len(lambdas)):\n    # ridge regression\n    _ = ridge_regression(Strain1_y, tx_tr, _)\nprint(_, _)\n#print("last weight:",weight)\n```'}, {'reason': 'stop', 'result': '```python\nlearning_rate = 0.01\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    _ = optimizer.minimize(loss)\n```'}, {'reason': 'stop', 'result': '```python\n# TODO: Build your pipeline that will draw lane lines on the test_images\n# then save them to the test_images directory.\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport cv2\n\n\n# Read in and grayscale the image\nimage = mpimg.imread(\'test_images/whiteCarLaneSwitch.jpg\')\n\n# Define a kernel size and apply Gaussian smoothing\nkernel_size = 5\n\n# Define our parameters for Canny and apply\nlow_threshold = 60\nhigh_threshold = 100\n\n# Next we\'ll create a masked edges image using cv2.fillPoly()\nmask = np.zeros_like(edges)   \nignore_mask_color = 255   \n\n# This time we are defining a four sided polygon to mask\nimshape = image.shape\nvertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\n \n# Define the Hough transform parameters\n# Make a blank the same size as our image to draw on\nrho = 1 # distance resolution in pixels of the Hough grid\ntheta = np.pi/180 # angular resolution in radians of the Hough grid\nthreshold = 41     # minimum number of votes (intersections in Hough grid cell)\nmin_line_length = 30 #minimum number of pixels making up a line\nmax_line_gap = 15    # maximum gap in pixels between connectable line segments\n\n# Run Hough on edge detected image\n# Output "lines" is an array containing endpoints of detected line segments\n\n# Draw the lines on the edge image\n \nplt.imshow(lines)\nplt.figure()\nplt.imshow(result)\n\n\n```'}, {'reason': 'stop', 'result': "```python\nleast_squares(Strain1_y, Strain1_z)      #beta from y = X^T*beta --> missing the column of ones'\n#print(beta1)\ndefinitive_res(Stest1_z.dot(_))\n#print(res1)\nlen(Stest1_y[Stest1_y==_])/len(Stest1_y)\n```"}, {'reason': 'stop', 'result': '```python\nfor _ in range(10):\n    print(repr(mutate("A quick brown fox")))\n```'}, {'reason': 'stop', 'result': '```python\ninput_vocab_size = subword_encoder_en.vocab_size + 2\ntarget_vocab_size = subword_encoder_zh.vocab_size + 2\n\nprint("input_vocab_size:", input_vocab_size)\nprint("target_vocab_size:", target_vocab_size)\n```'}, {'reason': 'stop', 'result': '```python\n# Plot Frequency again\nsign_frequencies = get_frequencies(y_train_augmented, sign_dict)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nclasses = list(sign_dict.values())\nind = np.arange(len(classes))\nwidth = 0.8\n\nax.bar(ind, sign_frequencies.values(), width, align="edge", alpha=0.5)\nax.set_ylabel(\'Frequency\')\nax.set_title(\'Traffic Sign Classes\')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(sign_frequencies.keys(), rotation=90)\nplt.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\nsorted(filter(lambda r: len(r[0]) == 1, storage.relevancies.relevancy.iteritems()), \n               key=lambda r: r[1], reverse=True)\n```'}, {'reason': 'stop', 'result': '```python\nnum_nodes = 64\n\n# Parameters:\n# Input gate: input, previous output, and bias.\nix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\nim = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\nib = tf.Variable(tf.zeros([1, num_nodes]))\n# Forget gate: input, previous output, and bias.\nfx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\nfm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\nfb = tf.Variable(tf.zeros([1, num_nodes]))\n# Memory cell: input, state and bias.                             \ncx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\ncm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\ncb = tf.Variable(tf.zeros([1, num_nodes]))\n# Output gate: input, previous output, and bias.\nox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\nom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\nob = tf.Variable(tf.zeros([1, num_nodes]))\n# Variables saving state across unrollings.\nsaved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\nsaved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n# Classifier weights and biases.\nw = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\nb = tf.Variable(tf.zeros([vocabulary_size]))\n\n# Definition of the cell computation.\ndef lstm_cell(i, o, state):\n  """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n  Note that in this formulation, we omit the various connections between the\n  previous state and the gates."""\n  input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n  forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n  update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n  state = forget_gate * state + input_gate * tf.tanh(update)\n  output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n  return output_gate * tf.tanh(state), state\n\n# Input data.\ntrain_data = list()\nfor _ in range(num_unrollings + 1):\n  train_data.append(\n    tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\ntrain_inputs = train_data[:num_unrollings]\ntrain_labels = train_data[1:]  # labels are inputs shifted by one time step.\n\n# Unrolled LSTM loop.\noutputs = list()\noutput = saved_output\nstate = saved_state\nfor i in train_inputs:\n  output, state = lstm_cell(i, output, state)\n  outputs.append(output)\n\n# State saving across unrollings.\nwith tf.control_dependencies([saved_output.assign(output),\n                              saved_state.assign(state)]):\n  # Classifier.\n  logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n  loss = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(\n      labels=tf.concat(train_labels, 0), logits=logits))\n\n# Optimizer.\nglobal_step = tf.Variable(0)\nlearning_rate = tf.train.exponential_decay(\n  10.0, global_step, 5000, 0.1, staircase=True)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ngradients, v = zip(*optimizer.compute_gradients(loss))\ngradients, _ = tf.clip_by_global_norm(gradients, 1.25)\noptimizer = optimizer.apply_gradients(\n  zip(gradients, v), global_step=global_step)\n\n# Predictions.\ntrain_prediction = tf.nn.softmax(logits)\n\n# Sampling and validation eval: batch 1, no unrolling.\nsample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\nsaved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\nsaved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\nreset_sample_state = tf.group(\n  saved_sample_output.assign(tf.zeros([1, num_nodes])),\n  saved_sample_state.assign(tf.zeros([1, num_nodes])))\nsample_output, sample_state = lstm_cell(\n  sample_input, saved_sample_output, saved_sample_state)\nwith tf.control_dependencies([saved_sample_output.assign(sample_output),\n                              saved_sample_state.assign(sample_state)]):\n  sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n```'}, {'reason': 'stop', 'result': "```python\nmsig.timestamps[0], msig.timestamps[-1]\nnp.hstack([msig.waves[i].color for i in y_pred])\nprint(y_pred_colors[:3])\nprint(y_pred.shape, y_pred_colors.shape)\nplt.subplots(nrows=1, ncols=1, figsize=(15, 3))\nplt.plot(msig.timestamps, msig.mixed_signal, color='grey', alpha=0.3)\n# plt.scatter(msig.timestamps[msig.window_size-1:], x_test[:, -1, 0], marker='.', c=y_pred_colors)\n# plt.scatter(msig.timestamps, x_test[0, :, 0], marker='.', c=y_pred_colors)\nplt.scatter(msig.timestamps[msig.window_size-1:], msig.mixed_signal[msig.window_size-1:], marker='.', c=y_pred_colors)\nplt.xlabel('time')\nplt.xlim(t_min_max)\nplt.grid(True)\nplt.show()\n```"}, {'reason': 'stop', 'result': '```python\nwith fiona.open(path.join(DATA,\'field-orientations.geojson\')) as:\n    for _,item in ds.items():\n        if item[\'properties\'][\'planeType\'].strip() != \'Bedding\':\n            continue\n\n        alt = item[\'properties\'].get("aster_smoothed")\n        alt -= 40 # Global datum is higher than local\n\n        err = 0.1*N.pi/180\n        a = ReconstructedPlane(item[\'properties\'][\'strike\'], item[\'properties\'][\'dip\'],0,err,err)\n        collection.append(a.to_mapping(\n            center=(*item[\'geometry\'][\'coordinates\'],alt),\n            color=\'#444\', type=\'in-situ\'))\n\ncollection = [c for c in collection if 1600 < c[\'center\'][2] < 1680]\n```'}, {'reason': 'stop', 'result': '```python\nx = linspace(0, 5, 10)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\n# variables for total production\ndesk = tm2.integer_var(name='desk', lb=100)\ncell = tm2.continuous_var(name='cell', lb=100)\n```"}, {'reason': 'stop', 'result': '```python\nfor _ in range(10):\n    print(repr(flip_random_character(seed_input)))\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n_, _ = get_probabilities()\n```'}, {'reason': 'stop', 'result': '```python\n# Find clusters in projected data\nfor i, x in enumerate(X_proj):\n    kmeans = KMeans(n_clusters=5)#, init=np.array([(i*200/6.0, 25) for i in range(1,6)]))\n    kmeans.fit(x)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nweights2 = tf.get_default_graph().get_tensor_by_name("hidden2/kernel:0")\nclipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n```'}, {'reason': 'stop', 'result': "```python\nACCrunanalysis.loc[ACCrunanalysis['Run'] == 1].Valid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 1].Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': '```python\nf = plt.figure()\nplt.imshow(oriImg[:,:,[2,1,0]])\nplt.imshow(heatmap_avg[:,:,1], alpha=.5)\nf.show()\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': "```python\ntry:\n    json.loads(open('university_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for universities has not been saved yet. Let''s create a new dictionary.')\n    \ntry:\n    json.loads(open('institution_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for institutions has not been saved yet. Let''s create a new dictionary.')\n```"}, {'reason': 'stop', 'result': '```python\nz_obs = info.get_sn_info("SN2007uy")["z_obs"]\n\nprint(z_obs)\n```'}, {'reason': 'stop', 'result': '```python\nload_factors_of_interest = np.linspace(0.1, 0.8, 8)\ncapacity = 3e4\nnum_items_to_insert = 500\nnum_runs = 30\n\nfor load in tqdm(load_factors_of_interest, "current run"):\n    for run in tqdm(range(num_runs)):\n        c_filt, _ = return_cuckoo_filter_with_specified_load_factor(int(capacity),\\\n                                                                                 finger_print_size=2, load_factor=load)\n        b_filt, _ = return_bloom_filter_with_specified_load_factor(int(capacity), percent_to_fill=load)\n        \n        start = time.time()\n        for item_to_insert in range(num_items_to_insert):\n            item = "".join(random.sample(string.ascii_lowercase, 12))\n            c_filt.add(item)\n        dt_cuckoo = time.time() - start\n        \n        start = time.time()\n        for item_to_insert in range(num_items_to_insert):\n            item = "".join(random.sample(string.ascii_lowercase, 12))\n            b_filt.add(item)\n        dt_bloom = time.time() - start\n```'}, {'reason': 'stop', 'result': '```python\ndef expectedFit(x, a, b):\n    """\n    Returns the expected fit for the histogram\n    \n    Arguments: x - the x value in the equation\n               a - the first fit parameter\n               b - the second fit paramter\n               \n    Returned: The expected fit function\n    """\n    return a * np.exp(-b * x)\n\noccurenceRegion = bigOccurences[7:14] # Only fits region of interest\nmagnitudeRegion = bigMagnitudes[7:14]\n\nparameters, covariance = curve_fit(expectedFit, magnitudeRegion, occurenceRegion)\n\naFit = parameters[0]\nbFit = parameters[1]\n\nprint("A = ", aFit, "b = ", bFit)\n```'}]