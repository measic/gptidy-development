[{'reason': 'stop', 'result': "Unused variables:\n['pop_df']\n\nUpdated code:\n```python\npop_df = pd.read_excel(os.path.join(PATH, 'source_data', '2010_Pop_Block_County.xls'), sheetname='San Francisco County', header=4)\npop_df = pop_df.drop('BLOCKS', axis='index').reset_index()\npop_df = pop_df.rename(columns={'index': 'block_str'})\n\nrecords = []\nfor ix, row in pop_df.iterrows():\n    splits = row['block_str'].split(', ')\n    record = row[[x for x in row.index if x != 'block_str']].to_dict()\n    record['block'] = str(splits[0].split('Block ')[-1])\n    record['block_group'] = str(splits[1].split('Block Group ')[-1])\n    census_tract = '0' + str(splits[2].split('Census Tract ')[-1].replace('.', ''))\n    census_tract += '0' * (6-len(census_tract))  # Even if it doesn't have a decimal part, needs to be 6 characters\n    record['census_tract'] = census_tract\n    records.append(record)\npop_df = pd.DataFrame(records)\n\npop_df.columns = [to_appropriate_column_name(x) for x in pop_df.columns]\nstr_columns = ['block', 'block_group', 'census_tract']\nnonstr_columns = [x for x in pop_df.columns if x not in str_columns]\npop_df.loc[:, nonstr_columns] = pop_df[nonstr_columns].astype(int)\npop_df = pop_df[str_columns + nonstr_columns]\npop_df.to_csv(os.path.join(PATH, 'derived_data', 'SF_2010_pop_block.csv'))\npop_df.head()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['tmap_annotated']\n\nUpdated code:\n```python\n# Compute a single transport map from day 7 to 7.5\n_ = ot_model.compute_transport_map(7,7.5)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['fig', 'ax1Top', 'ax2Top']\n\nUpdated code:\n```python\nxmin = 0\nxmax = y_score.shape[0]\nxindex = range(xmin, xmax)\n\nfig, ax = plt.subplots(nrows=4, ncols=1, figsize=(10, 16))\n\nax[0].scatter(\n    xindex, \n    x_test_clipped, \n    marker='.', \n    c=y_true_colors)\nax[0].set_title('epoch = {}'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].imshow(\n    y_score.T, \n    interpolation='nearest', \n    cmap=plt.get_cmap('Spectral'), \n    origin='upper');\nax[1].spines['top'].set_visible(False)\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].set_ylim((y_score.shape[1], 0))\nax[1].set_yticks([y_score.shape[1]])\n\ndivider = make_axes_locatable(ax[1])\nax[1].plot(y_score_mean)\nax[1].set_title('sequence model type = {}'.format(msig.sequence_type))\nax[1].set_xlim((xmin, xmax))\nax[1].set_ylim((-1, 1))\nax[1].set_yticks((-1, 0, 1))\nax[1].grid(True)\n\nax[2].imshow(\n    y_score_unshifted_clipped.T, \n    interpolation='nearest', \n    cmap=plt.get_cmap('Spectral'), \n    origin='upper');\nax[2].spines['top'].set_visible(False)\nax[2].set_xlim((xmin, xmax))\nax[2].set_xticks([])\nax[2].set_ylim((y_score_unshifted_clipped.shape[1], 0))\nax[2].set_yticks([y_score_unshifted_clipped.shape[1]])\n\ndivider = make_axes_locatable(ax[2])\nax[2].plot(y_score_unshifted_clipped_mean)\nax[2].set_title('{} window size = {}'.format(window_type, msig.window_size))\nax[2].set_xlim((xmin, xmax))\nax[2].set_ylim((-1, 1))\nax[2].set_yticks((-1, 0, 1))\nax[2].grid(True)\n\nax[3].scatter(\n    xindex, \n    x_test_clipped,\n    marker='.', \n    c=y_pred_colors)\nax[3].set_title('loss = {:<6.4f}, accuracy = {:<.2%}'.format(*score))\nax[3].set_xlim((xmin, xmax))\nax[3].grid(True)\n\nplt.savefig(os.path.join(msig.out_dir, 'prediction_analysis.png'), bbox_inches='tight', pad_inches=0.08)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fpr_gmm\', \'tpr_gmm\']\n\nUpdated code:\n```python\n# precision-recall curve\ny_scores = -probas_gmm\nprecision_gmm, recall_gmm, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_gmm = metrics.auc(recall_gmm, precision_gmm)\nauroc_gmm = metrics.roc_auc_score(y_true, y_scores)\nprint("AUROC: %.2f, PR AUC: %.2f" % (auroc_gmm, pr_auc_gmm))\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'U_fun\', \'A_fun\', \'S_fun\']\n\nUpdated code:\n```python\nimport sympy as sym\nfrom sympy import *\nA, U, S = symbols("A U S")\nalpha_UA, alpha_AU, alpha_US, alpha_SU  = symbols("alpha_UA alpha_AU alpha_US alpha_SU")\n\nmodel_dyn = [\n    alpha_UA*U - alpha_AU*A,\n    alpha_AU*A + alpha_SU*S - (alpha_UA + alpha_US)*U,\n    alpha_US*U - alpha_SU*S,\n    A + U + S - 1 # this equation sets the total population size to 1\n    ]\n\n# steady-state solution\nsol_dyn = solve(model_dyn, A, U, S)\n\n# functions for calculating the proportion of the population in each compartment at \n# steady state, given transition rates between compartments\ndyn_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A] + sol_dyn[S])\n\nsol_dyn\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'loss\']\n\nUpdated code:\n```python\nreset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300\nn_hidden2 = 50\nn_hidden3 = 50\nn_hidden4 = 50\nn_hidden5 = 50\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3")\n    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4")\n    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name="hidden5")\n    logits = tf.layers.dense(hidden5, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    tf.reduce_mean(xentropy, name="loss")\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'n_outputs\']\n\nUpdated code:\n```python\nreset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300 # 재사용\nn_hidden2 = 50  # 재사용\nn_hidden3 = 50  # 재사용\nn_hidden4 = 20  # 새로 만듦!\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['r1', 'r2']\n\nUpdated code:\n```python\nallData = pd.read_csv('data/allData.csv')\n_ = allData[allData['Round']==1]\n_ = allData[allData['Round']==2]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['all_acc']\n\nUpdated code:\n```python\ntraining_acc_file = 'accuracies/train_acc_list.p'\nvalid_acc_file = 'accuracies/valid_acc_list.p'\n\nwith open(training_acc_file, mode='rb') as f:\n    combined_train_acc_list = pickle.load(f)\nwith open(valid_acc_file, mode='rb') as f:\n    combined_valid_acc_list = pickle.load(f)\n    \nnp.row_stack([combined_train_acc_list, combined_valid_acc_list[0]])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\nimport statsmodels.stats.api as sms\n\n#ValidRTCI\nsms.DescrStatsW(RTanalysis.Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\npd.read_csv('https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/ade_sales.csv')\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'I\']\n\nUpdated code:\n```python\nsigmas=[]\nsigma=0\nfor i in range(0,100):\n    sigma=0\n    for i in range (0,len(l)-1):\n        temp,temp2=integrate(f,l[i],l[i+1],1000,10000)\n        sigma+=temp2\n    sigmas.append(np.sqrt(sigma))\n\nplt.plot(np.arange(0,100,1),sigmas,\'r\')\nplt.ylabel(\'Sigma\')\nplt.xlabel(\'M\')\nplt.show()\nprint("Sigma of sigma=",np.sqrt(np.var(sigmas)))\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['sig']\n\nUpdated code:\n```python\nimport inspect\n\ndef add(x:int, y:int, *args, **kwargs) -> int:\n    return x + y\n\ninspect.signature(add)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'X_train\']\n\nUpdated code:\n```python\n# %%time\n\n# we need a custom pre-processor to extract correct field,\n# but want to also use default scikit-learn preprocessing (e.g. lowercasing)\ndefault_preprocessor = CountVectorizer().build_preprocessor()\n\n\ndef build_preprocessor(field):\n    field_idx = list(dataset.columns).index(field)\n    # if field == \'playlist_pid\': from IPython.core.debugger import set_trace; set_trace()\n    return lambda x: default_preprocessor(x[field_idx])\n\n\nvectorizer = FeatureUnion([\n    (\n        \'track_artist_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_artist_uri\'))),\n    (\n        \'track_album_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_album_uri\'))),\n    (\n        \'track_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_uri\'))),\n\n    (\n        \'playlist_pid\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'playlist_pid\'))),\n\n    ("playlist_name",\n      CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r"(?u)\\b\\w+\\b",\n            stop_words=None,\n            analyzer = \'word\',\n            # max_features=50000,\n            preprocessor=build_preprocessor("playlist_name"))),\n    \n    ("playlist_description",\n      CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r"(?u)\\b\\w+\\b",\n            stop_words=None,\n            analyzer = \'word\',\n            # max_features=50000,\n            preprocessor=build_preprocessor("playlist_description"))),\n#     (\n#         \'track_pos\',\n#         CountVectorizer(\n#             ngram_range=(1, 1),\n#             token_pattern=r".+",\n#             stop_words=None,\n#             # max_features=50000,\n#             preprocessor=build_preprocessor(\'track_pos\'))),\n\n    (\'track_duration_ms\',\n     ItemSelector(list(dataset.columns).index(\'track_duration_ms\'))),\n])\nvectorizer.fit_transform(data_train.values)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'county\', \'tick_locations\']\n\nUpdated code:\n```python\nhouse_size = df_county_data["Median Income"]\nx_axis = np.arange(len(house_size))\n# Create a bar chart based upon the above data\n# plt.xticks(tick_locations, county, rotation= 90)\nplt.bar(x_axis, house_size, color="r", align="center")\nplt.title("County Median Income")\nplt.xlabel("Counties")\nplt.ylabel("Median Income Rate")\nplt.text(140, 120000, "Note:\\nMedian Income for all counties in NJ, NY, & PA.")\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Median Income1.png", bbox_inches = "tight")\nplt.show()\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'accuracy\', \'training_op\']\n\nUpdated code:\n```python\nX, y, _, _ = tf.get_collection("my_important_ops")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['init', 'saver']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['tiempo_bayes']\n\nUpdated code:\n```python\nparametersNaiveBayes = {\n    'priors':priors\n}\n\n_, grid_bayes = correr_y_mostrar(\n    GaussianNB(), \n    parametersNaiveBayes, \n    5, \n    5\n)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['batch_size', 'height', 'width']\n\nUpdated code:\n```python\n_, _, _, channels = dataset.shape\n\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\nfilters[:, 3, :, 0] = 1  # vertical line\nfilters[3, :, :, 1] = 1  # horizontal line\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['b']\n\nUpdated code:\n```python\nA = A.tocsr()\nprint(type(A))\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['all_coverage']\n\nUpdated code:\n```python\n_, cumulative_coverage = population_coverage(\n    mutation_fuzzer.population, http_program)\n\nimport matplotlib.pyplot as plt\nplt.plot(cumulative_coverage)\nplt.title('Coverage of urlparse() with random inputs')\nplt.xlabel('# of inputs')\nplt.ylabel('lines covered');\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['df']\n\nUpdated code:\n```python\nimport pandas as pd\nread_filename = './annotations_201712131226.csv'\n\npd.read_csv(read_filename+'.points', names=('filename', 'x1', 'y1', 'x2', 'y2', 'points'))\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_giv_f']\n\nUpdated code:\n```python\ndef GPC(x_new, x, y, kernel, theta):\n    K, KS, KSS = get_Ks(x_new, x, kernel, theta)\n    \n    f = find_f(K, y)\n    W = util.calculate_W(f, y)\n    KP = util.calculate_KP(K, W)\n\n    f_bar = np.matmul(np.matmul(KS, np.linalg.inv(K)), f)\n    var = KSS - KS.dot(np.linalg.inv(KP).dot(KS.T))\n    var = np.diagonal(var)\n    return(f_bar.squeeze(), var.squeeze())\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_true']\n\nUpdated code:\n```python\n_, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n                                train_len=95, test_len=5,\n                                save_hyp='plots/hyp_inlinecounter_shake.png',\n                                save_diag='plots/diag_inlinecounter_shake.png',\n                                save_resp='plots/resp_inlinecounter_shake.png')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['theta', 'omega']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['l']\n\nUpdated code:\n```python\nplt.plot(f/3e10, y_nolm_aer, label='AER')\nplt.plot(f/3e10, y_nolm_aer_arts, label='AER_ARTS')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'line\', \'ret\']\n\nUpdated code:\n```python\ndef generate_line(model, words, seed=None, rhymes=None):\n    syllables_left = 10\n    last_stress = 0\n    \n    line = []\n    start_probs = model.startprob_\n    emission_probs = model.emissionprob_\n    transition_probs = model.transmat_\n    \n    start_state = np.random.choice(len(start_probs), p=start_probs)\n    \n    if seed is not None:\n        possible_start_emissions = np.where(rhymes[seed] == 1)\n        probs = np.array(emission_probs[start_state][possible_start_emissions])\n\n        scaled_probs = probs / sum(probs)\n        while True:\n            start_emission = np.random.choice(possible_start_emissions[0], p=scaled_probs)\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1:\n                break\n    else:\n        while True:\n            start_emission = np.random.choice(len(emission_probs[start_state]), p=emission_probs[start_state])\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1:\n                break\n    \n    line.append(start_emission)\n    start_stress = poetrytools.stress(words[start_emission])\n    syllables_left -= len(start_stress)\n    \n    if len(start_stress) == 1:\n        prev_starting_stress = 1\n    else:\n        prev_starting_stress = int(start_stress[0])\n\n    curr_state = start_state\n    while syllables_left > 0:\n        possible_transitions = transition_probs[curr_state]\n        curr_state = np.random.choice(len(possible_transitions), p=possible_transitions)\n        possible_emissions = emission_probs[curr_state]\n        while True:\n            curr_emission = np.random.choice(len(possible_emissions), p=possible_emissions)\n            curr_stress = poetrytools.stress(words[curr_emission])\n            if len(curr_stress) == 1:\n                prev_starting_stress = 1 - prev_starting_stress\n                syllables_left -= 1\n                break\n            elif len(curr_stress) > syllables_left or int(curr_stress[-1]) == prev_starting_stress:\n                continue\n            else:\n                prev_starting_stress = int(curr_stress[0])\n                syllables_left -= len(curr_stress)\n                break\n        line.append(curr_emission)\n\n    return line\n\ndef convert_line(sample, words):\n    ret = \'\'\n    i = 0\n    for word in reversed(sample):\n        curr_word = words[word]\n        if i == 0 or (curr_word == \'i\'):\n            ret += curr_word.title() + \' \'\n        else:\n            ret += curr_word + \' \'\n        i += 1\n    return ret\n\ndef generate_pair(model, words, rhymes):\n    while True:\n        a_line = generate_line(model, words)\n        seed = a_line[0]\n        if len(np.where(rhymes[seed] == 1)[0]) > 0:\n            b_line = generate_line(model, words, seed, rhymes)\n            return a_line, b_line\n        \ndef generate_rhyming_and_meter_sonnet():\n    sonnet = \'\'\n    a_lines = []\n    b_lines = []\n    \n    for _ in range(4):\n        a_line, b_line = generate_pair(reversed_quatrain_model, quatrain_words, quatrain_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    for i in range(2):\n        sonnet += convert_line(a_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(a_lines[2 * i + 1], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i + 1], quatrain_words) + \'\\n\'\n    \n    a_lines = []\n    b_lines = []\n    \n    for _ in range(2):\n        a_line, b_line = generate_pair(reversed_volta_model, volta_words, volta_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    sonnet += convert_line(a_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(a_lines[1], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[1], volta_words) + \'\\n\'\n    \n    a_line, b_line = generate_pair(reversed_couplet_model, couplet_words, couplet_rhymes)\n    sonnet += convert_line(a_line, couplet_words) + \'\\n\'\n    sonnet += convert_line(b_line, couplet_words) + \'\\n\'\n    \n    return sonnet\n\ndef generate_10_rhyming_and_meter_sonnets():\n    sonnets = \'\'\n    for i in range(10):\n        print(\'Generating Sonnet \' + str(i + 1))\n        sonnets += str(i) + \'\\n\' + generate_rhyming_and_meter_sonnet() + \'\\n\'\n    \n    f = open("project2data/rhyming_and_meter_shakespeare.txt","w")\n    f.write(sonnets)\n    return sonnets\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nextra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run([training_op, extra_update_ops],\n                     feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['consumer_key', 'consumer_secret', 'access_token', 'access_secret', 'api']\n\nUpdated code:\n```python\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'regressor\', \'params\', \'scoring_fnc\']\n\nUpdated code:\n```python\n# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\nfrom sklearn.metrics import make_scorer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter=10, test_size=0.20, random_state=0)\n\n    # TODO: Create a decision tree regressor object\n    regressor = DecisionTreeRegressor()\n\n    # TODO: Create a dictionary for the parameter \'max_depth\' with a range from 1 to 10\n    params = {\'max_depth\': range(1, 11)}\n\n    # TODO: Transform \'performance_metric\' into a scoring function using \'make_scorer\' \n    scoring_fnc = make_scorer(performance_metric)\n\n    # TODO: Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values \'regressor\', \'params\', \'scoring_fnc\', and \'cv_sets\' respectively.\n    grid = GridSearchCV(estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['full_links_ids_path', 'regionspath']\n\nUpdated code:\n```python\ndatapath = '.'\n#file contains data about links in nyc\nlinkspath = datapath + '/links.csv'\n#file contains data about intersections in nyc\nnodespath = datapath + '/nodes.csv'\n\n#actual pandas dataframes\nlinks=pd.read_csv(linkspath)\nnodes=pd.read_csv(nodespath)\nnodes = nodes[['node_id','xcoord','ycoord']]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['dw_obj']\n\nUpdated code:\n```python\nw.get_step_object(step=3, subset=subset_uuid).indicator_objects[indicator]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['i_std']\n\nUpdated code:\n```python\nwaves_std = np.std(waves, axis=0)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 6))\n\nax[0].plot(xindex, waves_std)\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=waves_std)\nax[1].set_xlim((xmin, xmax))\nax[1].grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'std_dev_analysis.png'), bbox_inches='tight')\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nreuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                               scope="hidden[123]") # 정규 표현식\nrestore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    init.run()\n    restore_saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_new_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['H']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'app_path\']\n\nUpdated code:\n```python\nos.chdir(os.getcwd())\nfilesep = \'\\\\\' if platform.system() == \'Windows\' else \'/\'\nos.environ[\'GOOGLE_APPLICATION_CREDENTIALS\'] = "creds" + filesep + "sarasmaster-524142bf5547.json"\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['dfBabyDirt']\n\nUpdated code:\n```python\npd.read_csv('http://www.stat.berkeley.edu/~statlabs/data/babies.data', delim_whitespace=True)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['subjects']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['cluster_assignment']\n\nUpdated code:\n```python\nk = 3\nheterogeneity = []\ninitial_centroids = get_initial_centroids(tf_idf, k, seed=0)\ncentroids, _ = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n                      record_heterogeneity=heterogeneity, verbose=True)\nplot_heterogeneity(heterogeneity, k)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'classes_to_keep\']\n\nUpdated code:\n```python\nn_classes = 9\n\nnames_keep = np.asarray(names)\nnames_keep = names_keep.tolist()\nprint("classes to keep: " + str(names_keep))\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'retcode\', \'stdout\', \'stderr\']\n\nUpdated code:\n```python\n#An example of how to use these functions to provide a directory listing.\ncmdstr = "ls"\nprint("The systemcall method doesn\'t show stdout when used in Jupyter notebook but does from a script.")\nsystemcall(cmdstr)\nprint("")\nprint("With systemcall_pipe you can see the stdout from Jupyter notebook, and can use the results in variables:")\nsystemcall_pipe(cmdstr)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['n', 'wavelen', 'gamma']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'pred_samples\']\n\nUpdated code:\n```python\n# 200 new values from x=0 to x=15\nn_new = 200\nX_new = np.linspace(0, 15, n_new)[:,None]\n\n# add the GP conditional to the model, given the new X values\nwith model:\n    f_pred = gp.conditional("f_pred", X_new)\n\n# Sample from the GP conditional distribution\nwith model:\n    pm.sample_ppc(trace, vars=[f_pred], samples=1000)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['x_values', 'y_values']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'err_names\', \'extvec\']\n\nUpdated code:\n```python\n# ----------------------------------------------------------------------\n# Define feature names\nmag_names = ["Jmag", "Hmag", "Kmag"]\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n            clip_weights.eval()\n            clip_weights2.eval()\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n    saver.save(sess, "./my_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'tau\']\n\nUpdated code:\n```python\nwith pm.Model() as model:\n    lambda_1 = pm.Exponential("lambda_1", 1.0)\n    lambda_2 = pm.Exponential("lambda_2", 1.0)\n\nnew_deterministic_variable = lambda_1 + lambda_2\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['ex']\n\nUpdated code:\n```python\nflor.Experiment('risecamp_demo').__enter__()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'mapping\']\n\nUpdated code:\n```python\n# Encoding Functions\n\ndef MAP(X):\n    """Map all values to integer numbers."""\n    """NaN values are treated as a unique value."""\n    \n    # create an encoding for categorical vars\n    unique_elems = set(X)\n    mapping = {label:idx for idx, label in enumerate(unique_elems)}\n    return X.map(mapping).astype(int)\n\ndef LOO(X):\n    """Perform Leave One Out counting for the features."""\n    \n    # map features to ordinal values first\n    X = MAP(X)\n    \n    # perform counts\n    mapping = {idx:(count-1) for idx, count in enumerate(np.bincount(X))}\n    return X.map(mapping).astype(int)\n    \n\ndef OHE(df_cv, df_all, col_name, feature_names, feature_threshold=0.02):\n    """Map categorical values to a one hot encoding scheme."""\n    \n    X_cv = MAP(df_cv[col_name])\n    X_all = MAP(df_all[col_name])\n    \n    X_cv = X_cv.values.reshape(-1, 1)\n    X_all = X_all.values.reshape(-1, 1)\n    OHE = OneHotEncoder(sparse=False).fit(X_all)\n    X_cv_ohe = OHE.transform(X_cv)\n    X_all_ohe = OHE.transform(X_all)\n    \n    low_freq_features = []\n    for i in range(X_all_ohe.shape[1]):\n        new_feature = col_name + str(i)\n        \n        # determine the frequency of the categorical data value\n        freq = np.sum(X_all_ohe[:, i]) / X_all_ohe.shape[0]\n        if freq > feature_threshold:\n            df_cv[new_feature] = X_cv_ohe[:, i]\n            df_all[new_feature] = X_all_ohe[:, i]\n            feature_names.append(new_feature)\n        else:\n            low_freq_features.append(i)\n    \n    # aggregate low frequency features\n    if len(low_freq_features) > 0:\n        extra_label = col_name + str(X_all_ohe.shape[1])\n        feature_names.append(extra_label)\n        \n        X_all_extra = np.array([0 for x in range(X_all.shape[0])])\n        X_cv_extra = np.array([0 for x in range(X_cv.shape[0])])\n        \n        for i in low_freq_features:\n            for idx, val in enumerate(X_all_ohe[:, i]):\n                if val == 1:\n                    X_all_extra[idx] = 1\n            for idx, val in enumerate(X_cv_ohe[:, i]):\n                if val == 1:\n                    X_cv_extra[idx] = 1\n        \n        df_cv[extra_label] = X_cv_extra\n        df_all[extra_label] = X_all_extra                    \n            \n    feature_names.remove(col_name)\n    df_cv = df_cv.drop(col_name, axis=1)\n    df_all = df_all.drop(col_name, axis=1)\n    \n    return df_cv, df_all, feature_names\n\n\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'df_valid\']\n\nUpdated code:\n```python\ndef distance_between(lat1, lon1, lat2, lon2):\n  # haversine formula to compute distance "as the crow flies".  Taxis can\'t fly of course.\n  dist = np.degrees(np.arccos(np.minimum(1,np.sin(np.radians(lat1)) * np.sin(np.radians(lat2)) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.cos(np.radians(lon2 - lon1))))) * 60 * 1.515 * 1.609344\n  return dist\n\ndef estimate_distance(df):\n  return distance_between(df[\'pickuplat\'], df[\'pickuplon\'], df[\'dropofflat\'], df[\'dropofflon\'])\n\ndef compute_rmse(actual, predicted):\n  return np.sqrt(np.mean((actual-predicted)**2))\n\ndef print_rmse(df, rate, name):\n  print ("{1} RMSE = {0}".format(compute_rmse(df[\'fare_amount\'], rate*estimate_distance(df)), name))\n\nFEATURES = [\'pickuplon\',\'pickuplat\',\'dropofflon\',\'dropofflat\',\'passengers\']\nTARGET = \'fare_amount\'\ncolumns = list([TARGET])\ncolumns.extend(FEATURES) # in CSV, target is the first column, after the features\ncolumns.append(\'key\')\ndf_train = pd.read_csv(\'data/taxi-train.csv\', header=None, names=columns)\nrate = df_train[\'fare_amount\'].mean() / estimate_distance(df_train).mean()\nprint ("Rate = ${0}/km".format(rate))\nprint_rmse(df_train, rate, \'Train\')\n#print_rmse(df_valid, rate, \'Valid\')  \n```'}, {'reason': 'stop', 'result': "Unused variables:\n['elderSlope', 'elderArea', 'foxSlope', 'foxArea', 'hankSlope', 'hankArea', 'drySlope', 'dryArea']\n\nUpdated code:\n```python\n#Band 1 = area\n#Band 2 = slope\n# Band 3 = watershed\n\n#watershed key:\n#Dry = 4\n#Hank = 3\n#Elder = 1\n#Fox = 2\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['sol_haringey']\n\nUpdated code:\n```python\n# Haringey\n# find steady state based on 2012 data\n\ncov_2012 = 0.267007002375\nadpc_2012 = 0.0346976493046\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.190544970144\nadpc_2013 = 0.0184872060681\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nodeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['pca11', 'pca12', 'pca21', 'pca22']\n\nUpdated code:\n```python\nsparse_data = my_spca.transform(X)\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = sparse_data - sparse_data.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fig\', \'ax\']\n\nUpdated code:\n```python\nedge_cols = []\nfrustrated_edges = imbalance.keys()\nfor edge in G.edges(data=True):\n    if (edge[0],edge[1]) in frustrated_edges:\n        edge_cols.append("black")\n    elif edge[2]["sign"] == -1:\n        edge_cols.append("red")\n    else:\n        edge_cols.append("green")\n\nplt.figure(figsize=(15,10))\nnx.draw_networkx(G, pos=nx.drawing.layout.bipartite_layout(G, nodes_one), \n                 with_labels=True, node_size=1000, node_color="white", edge_color=edge_cols)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'flux_err\']\n\nUpdated code:\n```python\n#\nflux, _ = coco.simulate(b"SN2007uy", \n                    z_obs, 0.0, 0.0, 0.0, 3.1, \n                    mjdmax, mjd_to_sim, \n                    filters_to_sim)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['graph']\n\nUpdated code:\n```python\n# What's the distribution of prices?\nsales_price = train['SalePrice']\nsns.distplot(sales_price)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['G', 'density']\n\nUpdated code:\n```python\n# Test to see if DW comp. is working\ncorr_mat = corr_tensor[30, :, :].copy()\n        \ncorr_mat[(corr_mat > -1*0.7) & (corr_mat < 0.7)] = 0\nmake_graph(corr_mat, nodes, 'signed')\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'predictions_20K\']\n\nUpdated code:\n```python\n# Re-run the model with the Bib numbers as a feature and for the 5K, 10K and 15K split times to predict 20K time\n\n### set up data for modeling\nX_20K = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\']]\ny_20K = boston_clean[\'20K Duration\'].values.reshape(-1, 1)\nprint(X_20K.shape, y_20K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_20K, X_test_20K, y_train_20K, y_test_20K = train_test_split(X_20K, y_20K, random_state=29)\n# X_train_20K.head()\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_20K = LinearRegression()\nmodel_20K.fit(X_train_20K, y_train_20K)\n\n# Plot the residuals\n\nplt.scatter(model_20K.predict(X_train_20K), model_20K.predict(X_train_20K) - y_train_20K, c="blue", label="Training Data")\nplt.scatter(model_20K.predict(X_test_20K), model_20K.predict(X_test_20K) - y_test_20K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_20K.min(), xmax=y_test_20K.max())\nplt.title("Residual Plot 20K")\nplt.savefig(\'model_20k.png\')\nplt.show()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['regression_network']\n\nUpdated code:\n```python\nexp_name = 'regression'\nload_model(exps_path, exp_name, 100, camera_parameters_path)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['f']\n\nUpdated code:\n```python\nlv_workspace.get_data_filter_object(step=1, subset='A')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['ax']\n\nUpdated code:\n```python\n#use boxenplot to see the data\nsns.boxenplot(data=df['Revenue'], orient='h', color='#2ecc71')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['idx', 'pca11', 'pca12', 'pca21', 'pca22']\n\nUpdated code:\n```python\n# do the reduction but in a different way\ncov = X.T @ X /(X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nval = val[np.argsort(val)[::-1]]\nvec = vec[:, np.argsort(val)[::-1]]\nproject_X = X @ vec\nproject_V = vec.T @ vec\n\ninversed_vec = np.linalg.inv(vec)\ninversed_vec = inversed_vec - inversed_vec.mean(1)\nrevert_X = project_X @ inversed_vec\nrevertedV = project_V @ inversed_vec.T\n\n\ndef plot():\n    plt.figure(figsize=(15,5)); \n\n    plt.subplot(131); \n    plt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\n    plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n    plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green',  label='First PC')\n    plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True); \n\n    plt.subplot(132); \n    plt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red',  alpha=0.5)\n    plt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\n    plt.arrow(0, 0, *project_V[:,0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green',  label='First PC')\n    plt.arrow(0, 0, *project_V[:,1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True); \n\n    plt.subplot(133); \n    plt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\n    plt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\n    plt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green',  label='First PC')\n    plt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True); \n\n    plt.show()\nplot()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['burned_trace']\n\nUpdated code:\n```python\nwith model:\n    # To Be Explained in Chapter 3!\n    step = pm.Metropolis()\n    trace = pm.sample(25000, step=step)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['W', 'b']\n\nUpdated code:\n```python\nnp.random.seed(1234)\n_ = {user:np.random.rand(31) - 0.5 for user in users}\n_ = {user:np.random.rand(1) - 0.5 for user in users}\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['averagePileUp_multCollisionFrac', 'numberOfEventsRun262274_eventAccepted', 'removedFractionRun262274_ilterEfficiency']\n\nUpdated code:\n```python\nCMSpileUpMonitor = 'CMSpileUpMonitor.png'\n\nImage(filename = pathname + CMSpileUpMonitor)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['w']\n\nUpdated code:\n```python\nX = np.hstack((X, np.ones((X.shape[0], 1))))\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'d\']\n\nUpdated code:\n```python\nclass USZIPCodeRepository:\n    CACHE = {}\n\n    def __init__(self, data_url_prefix = \'https://raw.githubusercontent.com/yyu/GeoJSON-US/master\'):\n        self.data_url_prefix = data_url_prefix\n        self.geojson_url_prefix = f\'{data_url_prefix}/perZIPgeojson\'\n\n        self.refresh_zipcode_latlons(f\'{data_url_prefix}/ZIPCodesGazetteer.tsv\')\n        self.refresh_available_zipcodes(f\'{data_url_prefix}/perZIPgeojson/all_zipcodes.txt\')\n\n\n    def refresh_zipcode_latlons(self, url):\n        lines = [ line.decode(\'UTF8\').strip() for line in urllib.request.urlopen(url).readlines() ]\n        tsv = csv.DictReader(lines, delimiter=\'\\t\')\n        self.gazetteer = dict((d[\'GEOID\'], {\'lat\': float(d[\'INTPTLAT\']), \'lon\': float(d[\'INTPTLONG\'])}) for d in tsv)\n\n\n    def refresh_available_zipcodes(self, url):\n        lines = [ zipcode.decode(\'UTF8\').strip() for zipcode in urllib.request.urlopen(url).readlines() ]\n        self.zipcode_list = lines[1:] # ignore the first line\n        self.zipcode_set = set(self.zipcode_list)\n\n\n    def make_url(self, zipcode):\n        return f\'{self.data_url_prefix}/perZIPgeojson/{zipcode[0]}/{zipcode[1]}/{zipcode[2]}/{zipcode}.json\'\n\n\n    def fetch_zipcode(self, zipcode):\n        \'\'\'returns a (dict, err) tuple where err could be a string for error message or None\'\'\'\n\n        url = self.make_url(zipcode)\n\n        if url in USZIPCodeRepository.CACHE:\n            return (USZIPCodeRepository.CACHE[url], None)\n\n        try:\n            s = urllib.request.urlopen(url).read()\n        except urllib.error.URLError as e:\n            return (None, \'failed to get \' + url, \':\', e.reason)\n\n        j = json.loads(s)\n\n        USZIPCodeRepository.CACHE[url] = j\n\n        return (j, None)\n\n\n    def fetch_zipcodes(self, *zipcodes):\n        result = {"type": "FeatureCollection", "features": []}\n\n        available_zipcodes = set(zipcodes) & self.zipcode_set\n\n        for z in available_zipcodes:\n            j, err = self.fetch_zipcode(z)\n\n            if j is not None:\n                result[\'features\'].append(j)\n\n        return result\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'border_str\']\n\nUpdated code:\n```python\ndef print_policy(policy, terminal_states):\n    \n    idx_to_symbol = {0:\'\\u2190\', 1:\'\\u2191\', 2:\'\\u2192\', 3:\'\\u2193\'}\n    \n    for i in range(policy.shape[0]):\n        \n        string = ""\n        \n        for j in range(policy.shape[1]):\n            \n            if (i,j) in terminal_states:\n                string += \'\\u25A0 \'\n            else:\n                string += idx_to_symbol[policy[i, j]]+" "\n        \n        print(string)\n    \n    return\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'special_k\']\n\nUpdated code:\n```python\nconnection_all = []\nmid_num = 10\n\nfor k in range(len(mapIdx)):\n    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n    candA = all_peaks[limbSeq[k][0]-1]\n    candB = all_peaks[limbSeq[k][1]-1]\n\n    nA = len(candA)\n    nB = len(candB)\n    indexA, indexB = limbSeq[k]\n    if(nA != 0 and nB != 0):\n        connection_candidate = []\n        for i in range(nA):\n            for j in range(nB):\n                vec = np.subtract(candB[j][:2], candA[i][:2])\n                # print(\'vec: \',vec)\n                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n                # print(\'norm: \', norm)\n                vec = np.divide(vec, norm)\n                # print(\'normalized vec: \', vec)\n                startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n                               np.linspace(candA[i][1], candB[j][1], num=mid_num))\n                # print(\'startend: \', startend)\n                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n                                  for I in range(len(startend))])\n                # print(\'vec_x: \', vec_x)\n                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n                                  for I in range(len(startend))])\n                # print(\'vec_y: \', vec_y)\n                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                # print(score_midpts)\n                # print(\'score_midpts: \', score_midpts)\n                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n                \n                # print(\'score_with_dist_prior: \', score_with_dist_prior)\n                criterion1 = len(np.nonzero(score_midpts > param[\'thre2\'])[0]) > 0.8 * len(score_midpts)\n                # print(\'score_midpts > param["thre2"]: \', len(np.nonzero(score_midpts > param[\'thre2\'])[0]))\n                criterion2 = score_with_dist_prior > 0\n                \n                if criterion1 and criterion2:\n                    # print(\'match\')\n                    # print(i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2])\n                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n                # print(\'--------end-----------\')\n        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n        # print(\'-------------connection_candidate---------------\')\n        # print(connection_candidate)\n        # print(\'------------------------------------------------\')\n        connection = np.zeros((0,5))\n        for c in range(len(connection_candidate)):\n            i,j,s = connection_candidate[c][0:3]\n            if(i not in connection[:,3] and j not in connection[:,4]):\n                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n                # print(\'----------connection-----------\')\n                # print(connection)\n                # print(\'-------------------------------\')\n                if(len(connection) >= min(nA, nB)):\n                    break\n\n        connection_all.append(connection)\n    else:\n        connection_all.append([])\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'duration_per_run_in_seconds\']\n\nUpdated code:\n```python\ntrials = 1000\nwith Timer() as t:\n    for i in range(trials):\n        try:\n            url = fuzzer()\n            result = http_program(url)\n            print("Success!")\n        except ValueError:\n            pass\n\nduration_per_run_in_seconds = t.elapsed_time() / trials\nprint(duration_per_run_in_seconds)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['root_nodes']\n\nUpdated code:\n```python\n# build random forest\nrandom_forest_build(dataset, 1000, .3, n_jobs=-1)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_train', 'X_test']\n\nUpdated code:\n```python\n# Split the training data into separate train and test sets\n(_, _, y_train, y_test) = train_test_split(data, labels, test_size=0.25, random_state=0)\n\n# Convert the labels (letters) into one-hot encodings that Keras can work with\nle = LabelEncoder().fit(np.stack(list(y_train) + list(y_test)))\ny_train = le.transform(y_train)\ny_test = le.transform(y_test)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['tf', 'shuffle']\n\nUpdated code:\n```python\nimport numpy as np\n# import cv2\n\n# for img in X_train:\n#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nX_train_normal = np.array(X_train/255 - 0.5)\nX_valid_normal = np.array(X_valid/255 - 0.5)\nX_test_normal = np.array(X_test/255 - 0.5)\n\nEPOCHS = 15\nBATCH_SIZE = 128\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nwith tf.Session() as sess:\n    init.run()\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    new_saver.save(sess, "./my_new_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'s\', \'ot\']\n\nUpdated code:\n```python\nimport numpy as np\n\n# v1 . v2 = |v1| |v2| cos(a)\n# <=> a = cos-1( (v1.v2) / |v1||v2| )\n\n# 5 degrees tolerance is fine!\n\ndef debug_vectors(v1, v2):\n    print("v1: {0}, v2: {1}".format(v1, v2))\n    print("Angle: {0}".format(v_angle(v1, v2)))\n    print("Perpendicular: {0}". format(v_perpendicular(v1, v2, 4)))\n    print("Parallel: {0}".format(v_parallel(v1, v2, 3)))\n    print("Same Orientation: {0}".format(v_same_orientation(v1, v2)))\n    print("Dot product: {0}\\n".format(np.dot(v1, v2)))\n\ndef debug_all_samples(): \n    for sample in samples[0x10] + samples[0x80]:\n        va = np.array(sample[1])\n        vb = np.array(sample[0])\n        o = np.array(sample[3])\n\n        v1 = (va - o) / np.linalg.norm((va - o))\n        v2 = (vb - o) / np.linalg.norm((vb - o))\n\n        debug_vectors(v1, v2)\n\n# vy (1486,68)\n# vx (1638,213)\n# s  (1581,119)\n# o  (1628,69)\n        \n    \ndebug_all_samples()\n\n\nva = np.array([1638, 213]) \nvb = np.array([1486, 68]) \no = np.array([1628, 69])\n\nreal_origin = np.array([0, 0])\ntranslate = real_origin - o\n\nvat = (va + translate)\nvbt = (vb + translate)\n\ndebug_vectors(va - o, vb - o)\ndebug_vectors(vat, vbt)\n\nprint("va: {0}, vb: {1}".format(va,vb))\nprint("vat: {0}, vbt: {1}".format(vat,vbt))\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'model\', \'data_generator\']\n\nUpdated code:\n```python\nimport pymc3 as pm\n\nwith pm.Model():\n    parameter = pm.Exponential("poisson_param", 1.0)\n    pm.Poisson("data_generator", parameter)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['a1_x', 'a1_y', 'a1_z', 'a2_x', 'a2_y', 'a2_z']\n\nUpdated code:\n```python\n_ = np.array(df['a1_x']/10000)[1500:]\n_ = np.array(df['a1_y']/10000)[1500:]\n_ = np.array(df['a1_z']/10000)[1500:]\n_ = np.array(df['a2_x']/10000)[1500:]\n_ = np.array(df['a2_y']/10000)[1500:]\n_ = np.array(df['a2_z']/10000)[1500:]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['K']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['fig']\n\nUpdated code:\n```python\n_, axes = plt.subplots(10, 1, figsize=(14, 34))\n\nfor ax, country in zip(axes, top_10_list):\n    data = df[df['Country'] == country]\n    sns.countplot(data=data, x='YearsCodingProf', palette='Paired', ax=ax, order=data['YearsCodingProf'].value_counts().index)\n    ax.set_title('Years coding professionally in {}'.format(country), fontsize=16)\n    #plt.xticks(rotation='vertical')\n    sns.despine(left=True)\nplt.subplots_adjust(hspace=.6)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'logits\', \'end_points\', \'inception_saver\']\n\nUpdated code:\n```python\nfrom tensorflow.contrib.slim.nets import inception\nimport tensorflow.contrib.slim as slim\n\nreset_graph()\n\nX = tf.placeholder(tf.float32, shape=[None, height, width, channels], name="X")\ntraining = tf.placeholder_with_default(False, shape=[])\nwith slim.arg_scope(inception.inception_v3_arg_scope()):\n    inception.inception_v3(X, num_classes=1001, is_training=training)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['labels']\n\nUpdated code:\n```python\n# assign columns for each wfs so we can use them as features for training\nwfs = trimmed.pop('wfs')\ntrimmed['f9'] = ((wfs == 'newf9') | (wfs =='oldf9')) * 1\ntrimmed['f5'] = (wfs == 'f5') * 1\ntrimmed['mmirs'] = (wfs == 'mmirs') * 1\ntrimmed['bino'] = (wfs == 'binospec') * 1\ntrimmed = trimmed[(trimmed['hour'] > 0) & (trimmed['hour'] <= 13)]\ntrimmed\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'accuracy\']\n\nUpdated code:\n```python\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    tf.reduce_mean(tf.cast(correct, tf.float32))\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fig\']\n\nUpdated code:\n```python\n_, axs = plt.subplots(1,2, figsize=(10,4))\naxs[0].imshow(img_advi)\naxs[0].set_title("segmented image (ADVI)")\naxs[1].hist(y, bins=K);\naxs[1].set_title("cluster assignments (ADVI)")\nplt.tight_layout()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['line_x', 'line_y', 'line_z']\n\nUpdated code:\n```python\ntrace_illum_x = []\ntrace_illum_y = []\ntrace_illum_z = []\n\ntrace_viewing_x = []\ntrace_viewing_y = []\ntrace_viewing_z = []\n\nfor i, row in df_viewing.iterrows():\n    illum_x, illum_y, illum_z = polar_to_euclidean(row['illum_theta'],row['illum_phi'])\n    view_x, view_y, view_z = polar_to_euclidean(row['view_theta'],row['view_phi'])\n    \n    trace_illum_x.append(illum_x)\n    trace_illum_y.append(illum_y)\n    trace_illum_z.append(illum_z)\n    \n    trace_viewing_x.append(view_x)\n    trace_viewing_y.append(view_y)\n    trace_viewing_z.append(view_z)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'val1\', \'val2\']\n\nUpdated code:\n```python\n"""\nExperimenting with support vector machines\n"""\n# Importing all the libraries \nimport os\nimport json\nimport numpy                  as np\nimport tensorflow             as tf\nimport matplotlib.pyplot      as plt\nfrom sklearn                  import svm\n\n# These modules are used for the confusion matrix\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.metrics          import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\n\nclass ClassifySVM:\n    """\n    data:   numpy array consisting of data consisting of float values.\n    target: The target values associated with the input data.\n    """\n    \n    # def ConfusionMatrix(self, data, target):   \n    def SupportVectorClassify(self, data, target):\n\n        # Spliting the datasets into training and testing portions\n        sess          = tf.Session()\n        train_indices = np.random.choice(len(data), round(len(data) * 0.8), replace=False)\n        test_indices  = np.array(list(set(range(len(data))) - set(train_indices)))\n\n        Beta_train    = data[train_indices]\n        Beta_test     = data[test_indices]\n        target_train  = target[train_indices]\n        target_test   = target[test_indices]\n\n        # Here we want a very large batch size to achieve convergence. \n        # The A variable will take on the 2 x 1 shape. (In the book, this is done because there are 2 predictor variables.)\n        batch_size    = 25000\n        x_data        = tf.placeholder(shape = [None, 2], dtype=tf.float32)\n        y_target      = tf.placeholder(shape = [None, 1], dtype=tf.float32)\n        A = tf.Variable(tf.random_normal(shape=[2,1]))\n        b = tf.Variable(tf.random_normal(shape=[1,1]))\n\n        # For correctly classified points, values of >= 1 if target is 1.\n        model_output = tf.subtract(tf.matmul(x_data, A), b)\n\n        # Calculate the L2 normm of a vector. \n        # Margin parameter: a\n        l2_norm      = tf.reduce_sum(tf.square(A))\n        alpha        = tf.constant([0.1])\n\n        # Declaring classification loss and adding together the two terms.\n        classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., tf.multiply(model_output, y_target))))\n        loss                = tf.add(classification_term, tf.multiply(alpha, l2_norm))\n\n        # Declaring the prediction and accuracy functions to evaluate bot the training and test sets.\n        prediction = tf.sign(model_output)\n        accuracy   = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))\n\n        # Declaring optimization function and initializing model variables.\n        my_opt     = tf.train.GradientDescentOptimizer(0.01)\n        train_step = my_opt.minimize(loss)\n        init       = tf.global_variables_initializer()\n        sess.run(init)\n\n        # Starting the training loop. \n        # Making sure to record loss and training accuracy for both training and test sets. \n        loss_vec = []\n        train_accuracy  = []\n        test_accuracy   = []\n        \n        test_record = {}\n\n        for i in range(500):\n            rand_index = np.random.choice(len(Beta_train), size=batch_size)\n            \n            rand_x     = Beta_train[rand_index]\n            rand_y     = np.transpose([target_train[rand_index]])\n            no_end     = sess.run(train_step, feed_dict = {x_data: rand_x, y_target: rand_y})\n            \n            # Storing the loss from each iteration into loss_vec\n            temp_loss  = sess.run(loss, feed_dict = {x_data:rand_x, y_target:rand_y})\n            loss_vec.append(temp_loss)\n\n            # Storing train accuracies for each iteration of classification\n            train_acc_temp = sess.run(accuracy, feed_dict={x_data:Beta_train, y_target:np.transpose([target_train])})\n            train_accuracy.append(train_acc_temp)\n\n            # Storing test accuracies for each iteration of classification\n            test_acc_temp = sess.run(accuracy, feed_dict={x_data:Beta_test, y_target:np.transpose([target_test])})\n            test_accuracy.append(test_acc_temp)\n            \n            if (i+1)%100 == 0:\n                print(\'Step #\' + str(i + 1) + \' A = \' + str(sess.run(A)) + \' b = \' + str(sess.run(b)))\n                print(\'Loss = \' + str(temp_loss))\n                \n        test_prediction = sess.run(prediction, feed_dict={x_data:Beta_test, y_target:np.transpose([target_test])})\n        print(confusion_matrix(target_test, test_prediction))\n        \n        """\n        with open("/Users/" + os.getlogin() + "/Desktop/SVMData.json", "w") as repository:\n            json.dump(test_record, repository, sort_keys=True, indent=4)\n        repository.close()\n        """\n\n        # Plotting the outputs (fit, loss and accuracy), the coefficients.\n        [[a1], [a2]] = sess.run(A)\n        [[b]]        = sess.run(b)\n        slope        = -a2/a1\n        y_intercept  = b/a1\n        x1_vals      = [d[1] for d in data]\n\n        best_fit     = []\n        for i in x1_vals:\n            best_fit.append(slope*i+y_intercept)\n\n        feature_x     = [d[1] for i,d in enumerate(data) if target[i] == 1]\n        feature_y     = [d[0] for i,d in enumerate(data) if target[i] == 1]\n        not_feature_x = [d[1] for i,d in enumerate(data) if target[i] == -1]\n        not_feature_y = [d[0] for i,d in enumerate(data) if target[i] == -1]\n\n        plt.plot(feature_x, feature_y, \'o\', label=\'I. Distracted\')\n        plt.plot(not_feature_x, not_feature_y, \'x\', label=\'Not Distracted\')\n        plt.plot(x1_vals, best_fit, \'r-\', label=\'Linear Separator\', linewidth=3)\n        plt.ylim([0, 10])\n        plt.legend(loc=\'upper left\')\n        plt.title(\'Boundary Classification\')\n        plt.xlabel(\'Delta & Beta\')\n        plt.ylabel(\'targets\')\n        plt.show()\n\n        plt.plot(train_accuracy, \'k-\', label=\'Training Accuracy\')\n        plt.plot(test_accuracy, \'r--\', label=\'Test Accuracy\')\n        plt.title(\'Train and Test Set Accuracies\')\n        plt.xlabel(\'Generation\')\n        plt.ylabel(\'Accuracy\')\n        plt.legend(loc=\'lower right\')\n        plt.show()\n\n        plt.plot(loss_vec, \'k-\')\n        plt.title(\'Loss per Generation\')\n        plt.xlabel(\'Generation\')\n        plt.ylabel(\'Loss\')\n        plt.show()\n        \n        \n# Reading from the csv file.\npath = r\'/Users/KaushikBhimraj/Desktop/Merged1_Data.csv\'\n\n# List comprehension used to create numpy arrays for inputs and targets.\n# Removing the heading row and converting the string values to int.  \nI           = [[line.split(\',\')[0], line.split(\',\')[1]] for line in open(path) if line[0:1] != \'\\n\'][1:]\nBetaDelta   = np.array([[float(val[0]), float(val[1])] for val in I])\nTarget      = np.array([line.split(\',\')[2] for line in open(path) if line[0:1] != \'\\n\'][1:])\n\n# Converting target to -1 or 1 format.\nT           = np.array([float(1) if y == "Distracted\\n" else float(-1) for y in Target])\n\n# Calling the class object and the function inside it that will execute the suport vector algorithm.\nc = ClassifySVM()\nc.SupportVectorClassify(BetaDelta, T)\n\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['q', 'k']\n\nUpdated code:\n```python\n# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\ntf.random.set_seed(9527)\n\n# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\nv = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\nv\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['ts']\n\nUpdated code:\n```python\n# Select monthly mean PM2.5 data for the LSOA\nmonthly_data.isel(x=1103, y=1045).load()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'vector\']\n\nUpdated code:\n```python\n# building list out of each value of hotel name\nrows = []\ndef createRow(x):    \n    new_list = x.tolist()\n    new_list.insert(0,x.name)\n    rows.append(new_list)\n    \n#converting the list to multi-columns data frame\nnormal_dataFrame.groupby("Hotel Name")["Normal"].transform( createRow )  # group by returns for each hotel a list of his normalized prices\npd.DataFrame.from_records(rows)\n\n# NOTES\n# Vector - each row represents a hotel along with his 160 normalized prices\n\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['X', 'y']\n\nUpdated code:\n```python\n# window the data using your windowing function\nwindow_size = 7\n_ , _ = window_transform_series(series = dataset, window_size = window_size)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['train_texts', 'train_hyps', 'train_hiddens', 'test_texts', 'test_hyps', 'test_hiddens']\n\nUpdated code:\n```python\n# Generate hypothesis data\ndef gen_hyp_data(model, N, text_len=500):\n    texts, hiddens, hyps = [], [], []\n    for i in range(N):\n        text, hidden = generate(model, '\\n\\n', text_len, 0.8, True)\n        hidden = hidden.reshape(hidden.shape[0], -1)\n        hyp = hypothesis_inlinecounter(text)\n        hiddens.append(hidden)\n        hyps.append(hyp)\n        texts.append(text)\n    return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n\ngen_hyp_data(model_gru, 500)\ngen_hyp_data(model_gru, 2)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'black\']\n\nUpdated code:\n```python\ndef broad_greater_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has two masthead lights and one running light\n    visible for a starboard orientation.\n    """\n    white = (255, 255, 255)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 126)\n        shorter_masthead_light = np.random.randint(130, 186)\n        left_endpoint = np.random.randint(20, 126)\n        right_endpoint = np.random.randint(125, 211)\n        running_light_height_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 16)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = left_endpoint + light_width\n        short_masthead_height = shorter_masthead_light + light_width\n        short_masthead_width = right_endpoint + light_width\n        running_light_start = shorter_masthead_light + running_light_height_diff\n        running_light_width = running_light_start + light_width\n        if right_endpoint - left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(left_endpoint - 20, left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(left_endpoint, right_endpoint)\n        running_light_area = running_light_loc + light_width\n        new_view[taller_masthead_light:tall_masthead_height, left_endpoint:tall_masthead_width] = white\n        new_view[shorter_masthead_light:short_masthead_height, right_endpoint:short_masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['reduce_lr']\n\nUpdated code:\n```python\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.callbacks import ReduceLROnPlateau\n\n### TODO: specify the number of epochs that you would like to use to train the model.\n\nBATCH_SIZE = 32\nepochs = 8\n\n### Do NOT modify the code below this line.\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\naugment_data = False   # change to True as needed\n\nif not augment_data:\n\n    print('Training... without data augmentation')\n    history = scratch_model.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, \n          batch_size=BATCH_SIZE, \n          callbacks=[checkpointer], \n          verbose=1)\n\nelse:\n    print('Training... WITH data augmentation')\n    history = scratch_model.fit_generator(datagen_train.flow(train_tensors, train_targets, batch_size=BATCH_SIZE),\n                    steps_per_epoch=train_tensors.shape[0] // BATCH_SIZE,\n                    epochs=epochs, \n                    verbose=2, \n                    callbacks=[checkpointer],\n                    validation_data=datagen_valid.flow(valid_tensors, valid_targets, batch_size=BATCH_SIZE),\n                    validation_steps=valid_tensors.shape[0] // BATCH_SIZE)\n\nprint('Done training')\nshow_history_graph(history)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['Ms']\n\nUpdated code:\n```python\nP = np.diag([500., 49.])\n_, Ps = run(count=50, R=10, Q=0.01, P=P)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['p', 'r', 'f', 's']\n\nUpdated code:\n```python\n_, _, _, _ = metrics.precision_recall_fscore_support(y_test_flattened[filter_items], y_pred_flattened[filter_items])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_pred']\n\nUpdated code:\n```python\ntf.nn.softmax(logits)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'model\']\n\nUpdated code:\n```python\nparameter = pm.Exponential("poisson_param", 1.0, testval=0.5)\nprint("\\nparameter.tag.test_value =", parameter.tag.test_value)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['y_pred_rf']\n\nUpdated code:\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, oob_score=True, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\n\nrnd_clf.predict(X_test)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['sigs_coeffs', 'batch_size', 'network_type', 'sequence_type', 'n_classes', 'n_features', 'input_shape']\n\nUpdated code:\n```python\nwave1_coeffs = {\n    'amplitude': {'mean': 0.5, 'delta': 0.05}, \n    'frequency': {'mean': 1.0, 'delta': 0.1},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'A',\n    'color': '#0000ff'\n}\nwave2_coeffs = {\n    'amplitude': {'mean': 0.75, 'delta': 0.075}, \n    'frequency': {'mean': 3.0, 'delta': 0.3},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'B',\n    'color': '#ff0000',\n#     'time': {'t_min': 0, 't_max': 5, 'n_timestamps': 601, 'noise_type': 'pareto', 'pareto_shape': 1.3},\n}\nwave3_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0.1}, \n    'frequency': {'mean': 8.0, 'delta': 0.8},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'C',\n    'color': '#00ff00'\n}\nwave4_coeffs = {\n    'amplitude': {'mean': 1.4, 'delta': 0.1}, \n    'frequency': {'mean': 12.0, 'delta': 1.2},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'D',\n    'color': '#ff00ff'\n}\n\nmwave_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0}, \n    'frequency': {'mean': 1.0, 'delta': 0}, \n    'offset': {'mean': 0, 'delta': 0},\n    'phase': {'mean': 0, 'delta': 1}, \n    'name': 'mixed_wave',\n    'time': {'t_min': 0, 't_max': 2, 'n_timestamps': 4096, 'delta': 0}\n}\n\nsigs_coeffs = [wave1_coeffs, wave2_coeffs, wave3_coeffs, mwave_coeffs, wave4_coeffs]\n\nfeatures=('x', 'dxdt')[0]\nwindow_size = 4096\nwindow_type = 'sliding'\n\nmsig = MixedSignal(\n    sigs_coeffs, \n    *features,\n    window_size=window_size, \n    window_type=window_type,\n)\n\nmsig.generate()\n\nprint(msig.inputs.shape)\nprint(msig.mixed_signal.shape)\nprint(msig.one_hots.shape)\nprint(msig.labels.shape)\nprint(msig.n_timestamps)\nprint(msig.n_samples)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['N', 'K']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['list_of_terms', 'x_value', 'delta_x']\n\nUpdated code:\n```python\ndef delta_f(_, _, _):\n    pass\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['generator', 'list_op']\n\nUpdated code:\n```python\nmerge(merge(partial_derivatives(v.parent()), steenrod_operators(1, 1)), steenrod_operators(1, 2))\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['all_waypoints']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'klay\', \'steph\', \'kevin\']\n\nUpdated code:\n```python\n# The three players\' heights, in meters:\nklay =  2.01 # Klay Thompson is 6\'7"\nsteph = 1.91 # Steph Curry is 6\'3"\nkevin = 2.06 # Kevin Durant is officially 6\'9", but many suspect that he is taller.\n             # (Further complicating matters, membership of the "Splash Triplets" \n             #  is disputed, since it was originally used in reference to \n             #  Klay Thompson, Steph Curry, and Draymond Green.)\n\n# We\'d like to look at all 3 pairs of heights, compute the absolute\n# difference between each pair, and then find the smallest of those\n# 3 absolute differences.  This is left to you!  If you\'re stuck,\n# try computing the value for each step of the process (like the\n# difference between Klay\'s heigh and Steph\'s height) on a separate\n# line and giving it a name (like klay_steph_height_diff).\n# YOUR CODE HERE\nraise NotImplementedError()\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'clip_weights\']\n\nUpdated code:\n```python\nthreshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\ntf.assign(weights, clipped_weights)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'arg\', \'**other_args\']\n\nUpdated code:\n```python\n@print_log("Awesome function")\ndef do_something():\n    print("Doing something")\n    return\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'tick_locations\']\n\nUpdated code:\n```python\n#Setup Axis\nstate=df_county_data[\'State\']\ncounty = df_county_data["County Name"]\ngrad_rate = df_county_data["Graduation Rate"]\nx_axis = np.arange(len(state))\n\n# Create a bar chart based upon the above data\n# tick_locations = [value for value in x_axis]\n# plt.xticks(tick_locations, county, rotation= 45)\nplt.bar(x_axis, grad_rate, color="b", align="center")\nplt.title("County Graduation Rates")\nplt.xlabel("Counties")\nplt.ylabel("Graduation Rates")\nplt.text(140, 0.6, "Note:\\nGraduation Rates for all counties in NJ, NY, & PA.")\n\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Graduation_Rates.png", bbox_inches = "tight")\n# plt.zoom=10\nplt.show()\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fig\', \'ax\']\n\nUpdated code:\n```python\n_, _ = plt.subplots()\nlosses = np.array(losses)\nplt.plot(losses.T[0], label=\'Discriminator\')\nplt.plot(losses.T[1], label=\'Generator\')\nplt.title("Training Losses")\nplt.legend()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['gender', 'tx']\n\nUpdated code:\n```python\nfrom helpers import *\ndef test_your_least_squares():\n    height, weight, _ = load_data_from_ex02(sub_sample=False, add_outlier=False)\n    x, mean_x, std_x = standardize(height)\n    y, _ = build_model_data(x, weight)\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # least square or grid search: TODO\n    # this code should compare the optimal weights obtained \n    # by least squares vs. grid search\n    # ***************************************************\n    raise NotImplementedError\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\nRTrunanalysis.loc[RTrunanalysis['Run'] == 0].Valid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['result_train', 'tot']\n\nUpdated code:\n```python\nresult_test = []\nfor string in ['share','comment','zan','content_len','链接','//@','@','#','【','《','\\[']:\n    temp = []\n    for i in test[string+'_histogram']:\n        if isinstance(i,int):\n            temp.append(np.zeros(shape=8))\n        else:\n            temp.append(i[0])\n    result_test.append(np.asarray(temp))\n    temp = []\n    for i in train[string+'_histogram']:\n        temp.append(i[0])\n    result_train.append(np.asarray(temp))\n    \n    train.drop(string+'_histogram',axis=1,inplace=True)\n    test.drop(string+'_histogram',axis=1,inplace=True)\ntrain.drop(['pid','uid'],inplace=True,axis=1)\ntest.drop(['pid','uid'],inplace=True,axis=1)\n\ntrain_y = train[['share','comment','zan']].values\ntrain.drop(['share','comment','zan'],axis=1,inplace=True)\ntrain_x = train.values\ntest_x  = test.values\nfor i in result_train:\n    train_x = np.c_[train_x,i]\nfor i in result_test:\n    test_x = np.c_[test_x,i]\nnp.save('processed_data/train3_np',train_x)\nnp.save('processed_data/test3_np',test_x)\nnp.save('processed_data/target3_np',train_y)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'convolution\']\n\nUpdated code:\n```python\nreset_graph()\n\nX = tf.placeholder(tf.float32, shape=(None, height, width, 1))\nfeature_maps = tf.constant(fmap)\ntf.nn.conv2d(X, feature_maps, strides=[1,1,1,1], padding="SAME")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['fig']\n\nUpdated code:\n```python\n_, ax = plt.subplots(figsize=(8, 6))\n\nn_bins = 20\nax.hist(old_faithful_df.std_waiting, bins=n_bins, color=blue, lw=0, alpha=0.5);\n\nax.set_xlabel('Standardized waiting time between eruptions');\nax.set_ylabel('Number of eruptions');\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'logits_before_bn\']\n\nUpdated code:\n```python\nreset_graph()\n\nimport tensorflow as tf\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n\ntraining = tf.placeholder_with_default(False, shape=(), name=\'training\')\n\nhidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")\nbn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\nbn1_act = tf.nn.elu(bn1)\n\nhidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")\nbn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\nbn2_act = tf.nn.elu(bn2)\n\nlogits = tf.layers.batch_normalization(bn2_act, training=training, momentum=0.9)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['nrows']\n\nUpdated code:\n```python\nxmin = 0\nxmax = x_test.shape[1]\nxindex = np.arange(xmin, xmax)\nfig, ax = plt.subplots(nrows=3, ncols=1, figsize=(15, 3*3))\n\nax[0].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=y_true_colors)\nax[0].set_title('epoch = {}'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=y_pred_colors)\nax[1].set_title('loss = {:<6.4f}, accuracy = {:<.2%}'.format(*score))\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].grid(True)\n\nlegend_labels = []\nfor wave in msig.waves:\n    ax[2].plot(\n        xindex,\n        wave.sample_full, \n        color=wave.color,\n        zorder=1)\n    legend_labels.append(wave.name)\n\nax[2].scatter(\n    xindex[i_fail], \n    x_test[0, i_fail, 0], \n    marker='o', \n    c=y_pred_colors[i_fail],\n    zorder=2)\n\nax[2].set_xlim((xmin, xmax))\nax[2].grid(True)\nax[2].legend(legend_labels)\n\n# plt.draw()\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'prediction_analysis.png'), bbox_inches='tight')\n# plt.show()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['an_integer']\n\nUpdated code:\n```python\nint('hello')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['f']\n\nUpdated code:\n```python\nfrom math import sqrt\nfrom numpy.random import randn\n\ndef univariate_filter(x0, P, R, Q):\n    f = KalmanFilter(dim_x=1, dim_z=1, dim_u=1)\n    f.x = np.array([[x0]])\n    f.P *= P\n    f.H = np.array([[1.]])\n    f.F = np.array([[1.]])\n    f.B = np.array([[1.]])\n    f.Q *= Q\n    f.R *= R\n    return f\n\ndef plot_1d_2d(xs, xs1d, xs2d):\n    plt.plot(xs1d, label='1D Filter')\n    plt.scatter(range(len(xs2d)), xs2d, c='r', alpha=0.7, label='2D Filter')\n    plt.plot(xs, ls='--', color='k', lw=1, label='track')\n    plt.title('State')\n    plt.legend(loc=4)\n    plt.show()\n    \ndef compare_1D_2D(x0, P, R, Q, vel, u=None):\n    # storage for filter output\n    xs, xs1, xs2 = [], [], []\n\n    # 1d KalmanFilter\n    f1D = univariate_filter(x0, P, R, Q)\n\n    #2D Kalman filter\n    f2D = pos_vel_filter(x=(x0, vel), P=P, R=R, Q=0)\n    if np.isscalar(u):\n        u = [u]\n    pos = 0 # true position\n    for i in range(100):\n        pos += vel\n        xs.append(pos)\n\n        # control input u - discussed below\n        f1D.predict(u=u)\n        f2D.predict()\n        \n        z = pos + randn()*sqrt(R) # measurement\n        f1D.update(z)\n        f2D.update(z)\n        \n        xs1.append(f1D.x[0])\n        xs2.append(f2D.x[0])\n    plt.figure()\n    plot_1d_2d(xs, xs1, xs2)\n\ncompare_1D_2D(x0=0, P=50., R=5., Q=.02, vel=1.) \n```"}, {'reason': 'stop', 'result': "Unused variables:\n['eig', 'loss_np']\n\nUpdated code:\n```python\nwith tf.Session() as sess:\n    A_np, B_np, Q_np, R_np = sess.run([A, B, Q, R])\n\nK, P, _ = controlpy.synthesis.controller_lqr_discrete_time(A_np, B_np, Q_np, R_np)\nx_np = x0\nu_np = (-K@x_np)\nxs_np, us_np = np.array(x_np), np.array(u_np)\nfor i in range(T):\n    x_np = A_np@x_np + B_np@u_np\n    u_np = (-K@x_np)\n    xs_np = np.hstack([xs_np, x_np])\n    us_np = np.hstack([us_np, u_np])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['train_tensors', 'valid_tensors', 'test_tensors']\n\nUpdated code:\n```python\nfrom PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\npaths_to_tensor(train_files).astype('float32')/255\npaths_to_tensor(valid_files).astype('float32')/255\npaths_to_tensor(test_files).astype('float32')/255\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y']\n\nUpdated code:\n```python\nnp.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['c']\n\nUpdated code:\n```python\ndata = np.array([[1, 3, 5], [2, 2, 3], [5, 3, 1]])\nClusteringNumeric(data)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['df']\n\nUpdated code:\n```python\npd.merge(df1, df2, on=['item', 'time'])   #df1とdf2を項目'item'と'time'でマージ\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['fig', 'l_ax', 'r_ax']\n\nUpdated code:\n```python\n_, _ = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(16, 6))\n\nK = 50\nalpha = 10.\n\nbeta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)\n\nplt.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n          label='DP sample CDFs');\nplt.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\nplt.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\nplt.title(r'$\\alpha = {}$'.format(alpha));\nplt.legend(loc=2);\n\nK = 200\nalpha = 50.\n\nbeta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)\n\nplt.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n          label='DP sample CDFs');\nplt.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\nplt.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\nplt.title(r'$\\alpha = {}$'.format(alpha));\nplt.legend(loc=2);\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['stats']\n\nUpdated code:\n```python\n# Loop over features (polygons) in the shapefile\ndfs = []\nfor f in tqdm(feats):\n    # Rasterize the polygon into an array\n    rasterized_image = features.rasterize([(shape(f['geometry']),1)],\n                                          out_shape=out_shape,\n                                          transform=new_aff,\n                                          fill=0,\n                                          all_touched=True)\n\n    # Extract from the xarray where the rasterized polygon is\n    region = data.where(rasterized_image == 1)\n    \n    # Combine x and y into a new dimension called allpoints and calculate the mean over it\n    # and then convert to a dataframe with an appropriate name\n    res = region.stack(allpoints=['x','y']).mean(dim='allpoints').to_dataframe(name=f['properties']['LSOA11CD'])\n    \n    # Append to the list of data frames so we can concatenate them all at the end\n    dfs.append(res)\n    \nstats = pd.concat(dfs, axis=1)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['P1', 'P2', 'K1', 'K2', 'Rt1', 'Rt2', 'annot1', 'annot2']\n\nUpdated code:\n```python\nfig = plt.figure(figsize=(16,5))\nax = fig.add_subplot(121); ax.axis('off')\nay = fig.add_subplot(122); ay.axis('off')\n\nindv_left = Annotations[0][0][0]\nindv_right = Annotations[1][0][1]\n\nlefthand_left = indv_left[1][0]\nlefthand_right = indv_right[1][0]\n\n# -------------\nax.imshow(X[0,0])\nax.scatter(lefthand_left[0], lefthand_left[1], color='red')\n\n# -------------\nay.imshow(X[1,0])\nay.scatter(lefthand_right[0], lefthand_right[1], color='red')\n\nplt.show()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nn_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_model_final.ckpt")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['clf_df']\n\nUpdated code:\n```python\n# Create DensityForest instance\nDensityForest(max_depth=2, min_subset=.1, n_trees=100,\n              subsample_pct=.1, n_jobs=-1, verbose=10,\n              ig_improvement=.4)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['images_seq', 'labels_seq']\n\nUpdated code:\n```python\n_, _ = loadGapData('data/gapdet/large/',\n                   slider=slider_size,\n                   seq=True)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_pred']\n\nUpdated code:\n```python\n# Train the model\nAdaModel = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                             n_estimators=100, learning_rate=0.05)\nAdaModel = AdaModel.fit(X_train, y_train)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['twitter_archive']\n\nUpdated code:\n```python\n# Read in csv file as a Pandas DataFrame\npd.read_csv('twitter-archive-enhanced.csv')\n```"}, {'reason': 'stop', 'result': "Unused variables:\nNone\n\nUpdated code:\n```python\n#Plot visualized correlation matrix\ndef plot_corr(data,size=20):\n    corr = data.corr()\n    fig, ax = plt.subplots(figsize=(25, 25))\n    cax = ax.matshow(corr)\n    fig.colorbar(cax)\n    plt.xticks(range(len(corr.columns)), corr.columns,rotation='vertical');\n    plt.yticks(range(len(corr.columns)), corr.columns);\n    plt.show()\nplot_corr(data2)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'accuracies_training\', \'accuracies_validation\', \'aucs_training\', \'aucs_validation\']\n\nUpdated code:\n```python\narbol = MiClasificadorArbol(max_depth=3)\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train           = []\nroc_validation      = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test) )\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train\ndf["Accuracy (validación)"] = accuracy_validation\ndf["AUC ROC (training)"]    = roc_train\ndf["AUC ROC (validación)"]  = roc_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'gb\']\n\nUpdated code:\n```python\ny = pd.DataFrame(y_train)\ny.columns = [\'class\']\ny[\'index\'] = y.index\n\nnum_image = 5\nfor c in range(n_classes):\n    # filter \n    yf = y[y[\'class\']== c].sample(num_image)\n    idx = (yf[\'index\'])\n    fig = plt.figure()\n    for i in range(num_image):\n        fig.add_subplot(1,num_image,i+1)\n        plt.imshow(X_train[idx][i])\n        plt.text(0,1,signnames[str(c)],color=\'w\',backgroundcolor=\'r\', fontsize=5, weight="bold") \n        plt.axis(\'off\')\n    plt.tight_layout()\n    plt.show()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['logging']\n\nUpdated code:\n```python\nimport gensim\n\nmodel_path = 'GoogleNews-vectors-negative300.bin'\nmodel = gensim.models.Word2Vec.load_word2vec_format(model_path, binary=True)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_prior', 'Z_prior', 'Y_cond']\n\nUpdated code:\n```python\nX = Variable(name='X', num_states=2)\n\nZ = Variable(name='Z', num_states=2)\n\nY = Variable(name='Y', num_states=2)\nf_Y_cond = [\n    [ #Y = 0\n        [ # X = 0\n          0.9999, # Z = 0\n          0.3     # Z = 1\n        ],\n        [ # X = 1\n          0.1,  # Z = 0\n          0.01  # Z = 1\n        ]\n    ],\n    [  #Y = 1\n        [ # X = 0\n          0.0001, # Z = 0\n          0.7     # Z = 1\n        ],\n        [ # X = 1\n          0.9,  # Z = 0\n          0.99  # Z = 1\n        ]\n    ]\n]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['scale']\n\nUpdated code:\n```python\nfor i in range(len(multiplier)):\n    imageToTest = cv.resize(oriImg, (0,0), fx=multiplier[i], fy=multiplier[i], interpolation=cv.INTER_CUBIC)\n    imageToTest_padded, pad = padRightDownCorner(imageToTest, 8, 128)\n\n    transposeImage = np.transpose(np.float32(imageToTest_padded[:,:,:]), (2,0,1))/256 - 0.5\n    testimage = transposeImage\n    cmodel = mx.mod.Module(symbol=sym, label_names=[])\n    cmodel.bind(data_shapes=[('data', (1,3,\n                                   testimage.shape[1],testimage.shape[2]))])\n    cmodel.init_params(arg_params=arg_params, aux_params=aux_params)\n    onedata = DataBatch(mx.nd.array([testimage[:,:,:]]), 0)\n    \n    cmodel.forward(onedata)\n    result=cmodel.get_outputs()\n    heatmap = np.moveaxis(result[1].asnumpy()[0], 0, -1)\n\n\n    heatmap = cv.resize(heatmap, (0,0), fx=model['stride'], fy=model['stride'], interpolation=cv.INTER_CUBIC)\n    heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n    heatmap = cv.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv.INTER_CUBIC)\n        \n    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n    \n   \n    f = plt.figure(i)\n    plt.imshow(oriImg[:,:,[2,1,0]])\n    ax2 = plt.imshow(heatmap[:,:,18], alpha=.5)\n    f.show()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_train_dog', 'X_dev_dog']\n\nUpdated code:\n```python\nX_train_feature = X_train_feature.drop('AnimalType_Dog', 1)\nX_dev_feature = X_dev_feature.drop('AnimalType_Dog', 1)\n\nmodel3 = Sequential([\n    Dense(32, input_shape=(12,)),\n    Dropout(0.1),  \n    Activation('sigmoid'),\n    Dense(5),\n    Activation('softmax'),\n])\n\nmodel3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel3.fit(np.array(X_train_feature), y_train_hot, epochs=10, batch_size=32)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['training_operation']\n\nUpdated code:\n```python\nlogits = LeNetTrafficSign(x, n_classes)\nprint(logits)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['negative_samples']\n\nUpdated code:\n```python\n# Create Negative Samples\npd.merge(\n    pd.merge(\n        tracks_df, playlist_map_df_negative, left_index=True, right_on='track_uri'),\n    playlist_df,\n    on='playlist_pid')\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'data_generator\']\n\nUpdated code:\n```python\nwith pm.Model() as model:\n    theta = pm.Exponential("theta", 2.0)\n    pm.Poisson("data_generator", theta)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'dt_best_params\', \'dt_grid_score\', \'dt_mae\', \'dt_evs\']\n\nUpdated code:\n```python\n# construct pipeline\npipe_dt = make_pipeline(\n    MinMaxScaler(), # used to normalize data onto a similar scale\n    SelectPercentile(), # used to filter out features that add noise\n    DecisionTreeRegressor()\n)\n\n# create the parameter grid for hyperparameter tuning\nparam_grid_dt = {\n    \'selectpercentile__percentile\':range(5, 30, 5), # what upper percentile of features to take\n    \'decisiontreeregressor__max_features\':["auto", "sqrt", "log2", None], # the number of features to conside when splitting\n    \'decisiontreeregressor__max_depth\':range(1, 10), # maximum depth of the decision tree\n    \'decisiontreeregressor__min_samples_leaf\':range(1, 4) # minimum number of samples required to be at a leaf node\n}\n\n# perform grid search of pipeline\ndt_grid = GridSearchCV(pipe_dt, param_grid_dt)\n\n# use results to create model on training data\ndt_grid.fit(train_features, train_outcome)'}, {'reason': 'stop', 'result': "Unused variables:\n['train_data', 'test_data']\n\nUpdated code:\n```python\n# Separa o dataset em conjunto treino e teste\n_ = keystrokes[:split_data_idx]\n_ = keystrokes[split_data_idx:]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['root']\n\nUpdated code:\n```python\ncreate_decision_tree(dataset)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'Y_proba\']\n\nUpdated code:\n```python\nn_outputs = len(flower_classes)\n\nwith tf.name_scope("new_output_layer"):\n    flower_logits = tf.layers.dense(prelogits, n_outputs, name="flower_logits")\n    _ = tf.nn.softmax(flower_logits, name="Y_proba")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['colunasTeste']\n\nUpdated code:\n```python\n[\n    'PassageiroId',\n    'Idade',\n    'FaixaEtaria',\n    'Classe',\n    'PortoEmbarqueNum',\n    'ParentesIrmao',\n    'ParentesFilhos',\n    'SexoNum',\n    'SaudacaoNum',\n    'PassagemPreco',\n    'FamiliaQtde',\n    'EstaSozinho',\n    'TemCabine',\n    'FamiliaQtde'\n]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['features_polar']\n\nUpdated code:\n```python\n# TODO add features for polar coordinate values where the nose is the origin\n# Name these 'polar-rr', 'polar-rtheta', 'polar-lr', and 'polar-ltheta'\n# Note that 'polar-rr' and 'polar-rtheta' refer to the radius and angle\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['df_hues']\n\nUpdated code:\n```python\ngig_comp_degree = gig_comp_graph.degree()\ngig_bet_degree = gig_bet_graph.degree()\ngig_degree_degree = gig_degree_graph.degree()\ndegree_frequencies1 = Counter([degree for (author, degree) in gig_comp_degree])\ndegree_frequencies2 = Counter([degree for (author, degree) in gig_bet_degree])\ndegree_frequencies3 = Counter([degree for (author, degree) in gig_degree_degree])\ndegree_info = []\nfor k,v in degree_frequencies1.items(): degree_info.append((k,v,'Todos'))\nfor k,v in degree_frequencies2.items(): degree_info.append((k,v,'Sin distinguidos Betweenness'))\nfor k,v in degree_frequencies3.items(): degree_info.append((k,v,'Sin distinguidos Grado'))\ndegree_df = pd.DataFrame(degree_info)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['pca11', 'pca12', 'pca21', 'pca22']\n\nUpdated code:\n```python\n# Do EVD to see the projection matrix\ncov = X.T @ X /(X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:,idx]\nproject_X = X @ vec\nproject_V = vec.T @ vec\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(141)\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.subplot(142)\nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *project_V[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *project_V[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\ninverse = np.linalg.inv(vec)\ninverse = inverse - inverse.mean(0, keepdims=True)\nrevert_X = project_X @ inverse\nrevertedV = project_V @ inverse.T\n\nplt.subplot(143)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\ninverse = np.linalg.inv(vec)\ninverse = inverse - inverse*inverse.std(0, keepdims=True)\nrevert_X = project_X @ inverse\nrevertedV = project_V @ inverse.T\n\nplt.subplot(144)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.show()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['hiddens']\n\nUpdated code:\n```python\nprint('\\n============== LSTM MODEL ==============\\n')\ntext, _ = generate(model_lstm, '\\n\\n', 500, 0.8, True)\nprint(text)\n\nprint('\\n============== GRU MODEL ===============\\n')\ntext, _ = generate(model_gru, '\\n\\n', 500, 0.8, True)\nprint(text)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['burned_trace']\n\nUpdated code:\n```python\n# To be explained in Chapter 3!\nwith model:\n    step = pm.Metropolis(vars=[p])\n    trace = pm.sample(40000, step=step)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_best_fit']\n\nUpdated code:\n```python\n# Create datapoints between X_min and X_max to visualize the line of best fit\n_ = np.arange(X.numpy().min(), X.numpy().max(), 0.001)[:,None]\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'dfBabies\']\n\nUpdated code:\n```python\ndfBabyDirt = dfBabyDirt[(dfBabyDirt["bwt"] != 999) & (dfBabyDirt["gestation"] != 999) & (dfBabyDirt["parity"] != 9) & (dfBabyDirt["height"] != 99) & (dfBabyDirt["weight"] != 999) & (dfBabyDirt["smoke"] != 9)]\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['idx', 'pca11', 'pca12', 'pca21', 'pca22']\n\nUpdated code:\n```python\n# perform D reduction\ncov = X.T @ X /(X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\n\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:,idx]\n\nvec_reduced = np.zeros_like(vec)\nvec_reduced[:,:1] = vec[:,:1]\nval_reduced = val.copy()\nval_reduced[-1:] = 0\n\nproject_X = X @ vec_reduced\nproject_V = vec_reduced.T @ vec_reduced\nrevert_X = project_X @ np.linalg.inv(vec_reduced+0.0001)\nrevertedV = project_V.T @ np.linalg.inv(vec_reduced+0.0001)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(131)\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.subplot(132)\nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *project_V[:,0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *project_V[:,1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.subplot(133)\nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *revertedV[:,0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *revertedV[:,1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\n\nplt.show()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fig\', \'ax\', \'fit\', \'fit_fn\']\n\nUpdated code:\n```python\nfrom pylab import polyfit, poly1d\nplt.scatter(RTValidityEffect, posttest.ExplicitMem, s=50,c=sns.xkcd_rgb[\'green\'],alpha=0.6,edgecolors=\'k\');\nsns.set(context=\'notebook\', style=\'white\', font=\'Myriad Pro\', font_scale=2, color_codes=False, rc=None);\nplt.plot(RTValidityEffect, poly1d(polyfit(RTValidityEffect, posttest.ExplicitMem, 1))(RTValidityEffect), \'k\');\nplt.plot([-50,0,50,100,150],[2,2,2,2,2], \'r--\');\nplt.ylim(-0.1,4.1);\nplt.xlim(-50,150);\nplt.title(\'RT Validity Effect Predicts S-S Memory\', fontsize=26,fontweight="bold");\nplt.ylabel(\'S-S Memory Performance\\n(# Pairs Correct)\', fontsize=24,fontweight="bold");\nplt.xlabel(\'S-S RT Validity Effect (ms)\', fontsize=24,fontweight="bold");\nsns.despine();\n\nplt.show()\nstats.linregress(RTValidityEffect,posttest.ExplicitMem) #see stats below graph\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'cases_bestfit_10000\', \'cases_bestfit_15000\', \'cases_bestfit_15000_allgenes\', \'cases_allgenes\']\n\nUpdated code:\n```python\nprint(\'Loading case data ...\')\n\n#cases_800                    = pd.read_csv("pancancer_case_features_800.csv")\n#cases_1000                   = pd.read_csv("pancancer_case_features_1000.csv")\n#cases_1500                   = pd.read_csv("pancancer_case_features_1500.csv")\n#cases_bestfit_8000           = pd.read_csv("pancancer_case_features_bestfit_8000_topgenes_1000.csv")\npd.read_csv("pancancer_case_features_bestfit_10000_topgenes_2000.csv")\npd.read_csv("pancancer_case_features_bestfit_15000_topgenes_3000.csv")\npd.read_csv("pancancer_case_features_bestfit_15000_topgenes_None.csv")\npd.read_csv("pancancer_case_features_all.csv")\nall_data = {\n    #\'800\':                     getDataAndLabels(cases_800),\n    #\'1000\':                    getDataAndLabels(cases_1000),\n    #\'1500\':                    getDataAndLabels(cases_1500),\n    \'best_fit_10000\':          getDataAndLabels(),\n    \'best_fit_15000\':          getDataAndLabels(),\n    \'best_fit_15000_allgenes\': getDataAndLabels(),\n    \'genes_all\':               getDataAndLabels()\n}\nprint("done.")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['train_csv', 'test_csv']\n\nUpdated code:\n```python\n### Coletando as fontes de dados\npd.read_csv('train.csv', sep = ',', encoding = 'UTF-8')\npd.read_csv('test.csv', sep = ',', encoding = 'UTF-8')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X']\n\nUpdated code:\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english',\n                        max_df=.1,\n                        max_features=5000)\ncount.fit_transform(df['review'].values)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['mdl']\n\nUpdated code:\n```python\nfrom docplex.mp.model import Model\n\nModel(name='portfolio_miqp')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_score_mean', 'y_score_unshifted_clipped_mean']\n\nUpdated code:\n```python\n# Code specific to window_type == sliding\n\n# j = 10\n# s0 = slice(0, j)\n# s1 = slice(-j, -1)\n# s0 = s1\n# print(y_hat[s0, s1])\n# print(y_true[s0, s1])\n# print(y_true_value[s0, s1])\n# print(y_pred[s0, s1])\n# print(y_pred_value[s0, s1])\n# print(y_penalty[s0, s1])\n# print(y_score[s0, s1])\n# print(np.min(y_score), np.max(y_score))\n\nif msig.sequence_type == 'many2many':\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\nelse:\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['input_layer', 'fc1', 'xs', 'us', 'grads_and_vars']\n\nUpdated code:\n```python\n# Defining network\n# input: state x\n# output: control u\n\ntf.placeholder(tf.float32, (None,2), name='in_layer')\nfc1 = tf.layers.dense(inputs=input_layer, units=1, activation=tf.nn.tanh, name='fc1', reuse=tf.AUTO_REUSE)\nu = tf.layers.dense(inputs=fc1, units=1, activation=tf.nn.tanh, name='fc_out', reuse=tf.AUTO_REUSE)\n\n# LOSS FUNCTION\nloss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n\nx\nu\n\nfor i in range(T):\n    # LQR loss \n    Ax = tf.matmul(A, x, name='Ax'+str(i))\n    Bu = tf.multiply(u, B, name='Bu'+str(i))\n    x = tf.add(Ax, Bu, name='state'+str(i))\n\n    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))\n    \n    fc1 = tf.layers.dense(inputs=tf.transpose(x), units=1, name='fc1', reuse=True)\n    u = tf.layers.dense(inputs=fc1, units=1, name='fc_out', reuse=True)\n    \nopt = tf.train.GradientDescentOptimizer(0.0001)\ntrain = opt.minimize(loss)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['x_plot']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'detNames\']\n\nUpdated code:\n```python\ngROOT.ProcessLine(\'HistogramOperations ops\')\ngROOT.ProcessLine(\'lightTables.setBirksParams(1.0,6.90)\')\n\nfor detNum, detName in detNames.iteritems():\n    params = CalibParams(calPath+calNames[detNum])\n    gROOT.ProcessLine(\'vector<TH1*> phs{1} = ops.loadHistograms("33MeVTa_{0}_ls_{1}_fittedPSDCut.root")\'.format(runNum,detNum))\n    gROOT.ProcessLine(\'ops.applyCalibration(phs{0}[1],{1},{2})\'.format(detNum, params.a, params.b))\n    gROOT.ProcessLine(\'TFile *tgt{0} = new TFile("33MeVTa_{0}_ls_{1}_calibFittedPSDCut.root","recreate")\'.format(runNum,detNum))\n    gROOT.ProcessLine(\'phs{0}[1]->Rebin(3)\'.format(detNum))\n    gROOT.ProcessLine(\'phs{0}[1]->Draw()\'.format(detNum))    \n    gROOT.ProcessLine(\'phs{0}[1]->Write()\'.format(str(detNum)))\n    pause()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['difference']\n\nUpdated code:\n```python\n# First, we've written down the values of the 3 universal\n# constants that show up in Newton's formula.\n\n# G, the universal constant measuring the strength of gravity.\ngravity_constant = 6.674 * 10**-11\n\n# M, the moon's mass, in kilograms.\nmoon_mass_kg = 7.34767309 * 10**22\n\n# R, the radius of the moon, in meters.\nmoon_radius_m = 1.737 * 10**6\n\n# The distance the hammer should have fallen over the\n# duration of the fall, in meters, according to Newton's\n# law of gravity.  The text above describes the formula\n# for this distance given by Newton's law.\n# **YOU FILL THIS PART IN.**\npredicted_distance_m = ...\n\npredicted_distance_m - estimated_distance_m\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['x']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['n_epochs', 'batch_size']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['mfusghead']\n\nUpdated code:\n```python\n# Create the mfusg headfile object\nheadfile = os.path.join(modelpth, '{0}.hds'.format(modelname))\nheadobj = flopy.utils.HeadFile(headfile)\ntimes = headobj.get_times()\n_ = headobj.get_data(totim=times[-1])\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'predictions_F40K\']\n\nUpdated code:\n```python\n# Re-run the model with ONLY female runners, and  features of the 5K, 10K 15K 20K 25K, half and 35K split times to predict 40K time\n\n### set up data for modeling\nX_F40K = boston_females[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\', \'20K Duration\',\'Half Duration\', \'25K Duration\', \'30K Duration\', \'35K Duration\']]\ny_F40K = boston_females[\'40K Duration\'].values.reshape(-1, 1)\nprint(X_F40K.shape, y_F40K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_F40K, X_test_F40K, y_train_F40K, y_test_F40K = train_test_split(X_F40K, y_F40K, random_state=29)\n# X_train_F40K.head()\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_F40K = LinearRegression()\nmodel_F40K.fit(X_train_F40K, y_train_F40K)\n\n# Make predictions\n\nmodel_F40K.predict(X_test_F40K)\n\n# Plot the residuals\n\nplt.scatter(model_F40K.predict(X_train_F40K), model_F40K.predict(X_train_F40K) - y_train_F40K, c="blue", label="Training Data")\nplt.scatter(model_F40K.predict(X_test_F40K), model_F40K.predict(X_test_F40K) - y_test_F40K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_F40K.min(), xmax=y_test_F40K.max())\nplt.title("Residual Plot Female Runners 40K")\nplt.savefig(\'model_F40K.png\')\nplt.show()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['sol_13_14']\n\nUpdated code:\n```python\n# incidence and screening based on 2014 data\ncov_2014 = 493327. / 3500026.\nadpc_2014 = 47437. / 3500026.\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2014, test_diag_fun(x)[1] - adpc_2014], \n    [0.09, 0.25] \n    )\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\n# solve, 2013-2014\nodeint(dydt, \n       sol_12_13[999,:], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['Y']\n\nUpdated code:\n```python\n# tranform each class\nY1 = np.matmul(X1,v2)\nY2 = np.matmul(X2,v2)\n\n## show me dont tell me\nax = plt.gca()\nax.hist(Y1,color='blue', alpha=0.5, label='1')\nax.hist(Y2,color='yellow', alpha=0.5, label='2')\nplt.legend(loc='upper right')\nplt.xlabel('y')\n\nnp.matmul(X,v2)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['scale']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y']\n\nUpdated code:\n```python\nclass MyScaler():\n\n    def __init__(self, cols):\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        self.ss = StandardScaler()\n        self.ss.fit(X[self.cols])\n        return self\n    \n    def transform(self, X):\n        return self.ss.transform(X[self.cols])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['train_uid', 'test_uid']\n\nUpdated code:\n```python\ntrain_corpus, test_corpus = preprocessing.clean_corpus()\ntest_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\ntrain_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n\ntrain_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\ntest_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n\n_, _ = preprocessing.encode_label()\n\npreprocessing.bag_of_word(train_corpus['corpus'].values, test_corpus['corpus'].values, min_df=10)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'mask\']\n\nUpdated code:\n```python\nfig = plt.figure(figsize=(8,6)) \nx = df_county_data["Speak a language other than English"]\ny = df_county_data["Graduation Rate"] \nplt.scatter(x, y, color="g", marker="o", alpha=0.9) \n\n#Calculate and add R2 value\n_ = ~np.isnan(x) & ~np.isnan(y)\n\n\n#Add regression line\nsns.regplot(df_county_data["Speak a language other than English"], \n              df_county_data["Graduation Rate"], color=\'r\',label = "Speak a language other than English" )\n\n# Incorporate the other graph properties\nplt.title("High School Graduation Rates and ESL by County")\nplt.ylabel("Graduation Rate")\nplt.xlabel("Speak a language other than English") \n\nplt.legend(loc=\'best\')\nplt.grid(True)\nsns.set_style(\'whitegrid\')\nplt.text(65, 0.925, "Note:\\nAreas with one or more Foreign languages beside English language \\ntend to have a Lower graduation rate.")\n\nplt.savefig("Images/County_Grad_Speak a language other than English3.png", bbox_inches = "tight")\nplt.show()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['atoms_D']\n\nUpdated code:\n```python\nif ld is not None:\n    dictenc(ae.D)\n    atoms(ae.D)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nwith tf.Session() as sess:\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_new_model_final.ckpt")    \n```'}, {'reason': 'stop', 'result': "Unused variables:\n['betweenness_dict']\n\nUpdated code:\n```python\nnx.algorithms.centrality.betweenness_centrality(gig_comp_graph)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['predictions_5K']\n\nUpdated code:\n```python\nmodel_5K.predict(X_test_5K)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'m\']\n\nUpdated code:\n```python\nplt.figure(figsize=(11, 4))\nfor subplot, learning_rate in ((121, 1), (122, 0.5)):\n    sample_weights = np.ones(len(X_train))\n    for i in range(5):\n        plt.subplot(subplot)\n        svm_clf = SVC(kernel="rbf", C=0.05, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n        y_pred = svm_clf.predict(X_train)\n        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n        plt.title("learning_rate = {}".format(learning_rate), fontsize=16)\n\nplt.subplot(121)\nplt.text(-0.7, -0.65, "1", fontsize=14)\nplt.text(-0.6, -0.10, "2", fontsize=14)\nplt.text(-0.5,  0.10, "3", fontsize=14)\nplt.text(-0.4,  0.55, "4", fontsize=14)\nplt.text(-0.3,  0.90, "5", fontsize=14)\nsave_fig("boosting_plot")\nplt.show()\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'train_step\']\n\nUpdated code:\n```python\n# Devemos especificar o dispositivo que vai rodar as operações da rede.\nwith tf.device("/device:GPU:0"):\n    # As variáveis x e y são placeholders para os vetores de entrada e\n    # labels (classes) respectivamente.\n    x = tf.placeholder(tf.float32, shape=[None, 31])\n    y = tf.placeholder(tf.float32, shape=[None, 51])\n\n    # W1 é a matriz que vai levar o input para um estado intermedirário\n    # de processamento da rede.\n    # b1 é o vetor responsável pela translação nesse novo espaço.\n    W1 = tf.get_variable("W1", shape=[31, 64], initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.zeros([64]), name="b1")\n    \n    # A variável layer1 armazena o valor da primeira camada da rede. Isso\n    # significa que layer1 utiliza W1 e b1, bem como uma função de ativação\n    # (não-linearidade) para computar o valor da primeira camada da rede.\n    layer1 = tf.nn.relu(tf.matmul(x,W1) + b1)\n    \n    # Analogamente, W2 e b2 serão utilizados para gerar a segunda camada da\n    # rede (camada final).\n    W2 = tf.get_variable("W2", shape=[64, 51], initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.zeros([51]), name="b2")\n\n    \n    # A variável y_ armazena a última camada da rede utilizando W2 e b2 para\n    # calculá-la.\n    y_ = tf.matmul(layer1,W2) + b2\n\n    # Aplicamos o softmax em y_ e definimos como função de custo a cross-entropy.\n    # O tensorflow tem uma versão otimizada para realizar essas operações em uma\n    # única função (o que ajuda a reduzir o tempo de treino).\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_))\n\n    # Por fim, definimos o otimizador como o Adam.\n    _ = tf.train.AdamOptimizer().minimize(cross_entropy)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fig\', \'ax\']\n\nUpdated code:\n```python\nimport matplotlib.pyplot as plt\n\nQ1 = [18, 58]\nlabels = [\'Yes\', \'No\']\ncolors = [\'lightcoral\', \'lightskyblue\']\npatches, texts = plt.pie(Q1, colors=colors, startangle=90)\nplt.legend(patches, labels, loc=\'lower left\')\nplt.title(\'Q1: Noticed Any Regularities, Sequences, or Pairs?\', fontsize=17, fontweight="bold")\nplt.axis(\'equal\')\nplt.show()\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'df\', \'fig\']\n\nUpdated code:\n```python\ndef plot_categorical_count(column, title=\'\', limit=2, xtick_rotation=\'horizontal\'):\n    column_count = df[column].value_counts()[:limit]\n    \n    fig = plt.figure(figsize=(14, 8))\n    sns.barplot(x=column_count.index, y=column_count.values, palette=\'Paired\')\n    sns.despine(left=True)\n    plt.title(title, fontsize=16)\n    plt.xticks(rotation=xtick_rotation)\n\ndef plot_heatmap(variable):\n    l = []\n    for name, group in top10_df.groupby(\'Country\'):\n        dff = pd.DataFrame(group[variable].value_counts() / group[variable].count()) \n        dff[\'Country\'] = name\n        dff[\'rate\'] = dff.index\n        l.append(dff)\n    fig = plt.figure(figsize=(14, 8))\n    df_2 = pd.concat(l)\n    dff = df_2.pivot_table(index=\'Country\', columns=\'rate\')\n    sns.heatmap(dff, cmap="YlGnBu", linewidths=.3)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\nRTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid.mean()\nsms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'X\', \'y\']\n\nUpdated code:\n```python\nreset_graph()\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 50\nn_outputs = 10\n\nlearning_rate = 0.01\nmomentum = 0.9\n\nwith tf.name_scope("dnn"):\n    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n    y = tf.placeholder(tf.int32, shape=(None), name="y")\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n    training_op = optimizer.minimize(loss)    \n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['moviesavg_std']\n\nUpdated code:\n```python\nuseravg_std = moviesstd.mean().round(decimals=5)\n\nuseravg_std\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['infile', 'dl1_parameters_filename', 'allowed_tels', 'max_events', 'cal']\n\nUpdated code:\n```python\ncal = CameraCalibrator()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'ax\']\n\nUpdated code:\n```python\n# Plot the revenues for different months and years\nsns.lineplot(y=\'Revenue\', x=df.index.year, hue=df.index.month, data=df, palette="ch:r=-.5,l=.75", legend=False)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['flower_paths_and_classes_train', 'flower_paths_and_classes_test']\n\nUpdated code:\n```python\ntest_ratio = 0.2\ntrain_size = int(len(flower_paths_and_classes) * (1 - test_ratio))\n\nnp.random.shuffle(flower_paths_and_classes)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['n_epochs', 'batch_size']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['band1', 'band2', 'band3']\n\nUpdated code:\n```python\nds.GetRasterBand(1)#elev\nds.GetRasterBand(2)\nds.GetRasterBand(3)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'hidden1\']\n\nUpdated code:\n```python\n_ = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                          kernel_initializer=he_init, name="hidden1")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['sol_dudley']\n\nUpdated code:\n```python\n# Dudley\n# find steady state based on 2012 data\n\ncov_2012 = 0.0750667240187\nadpc_2012 = 0.0057129570304\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.238873910562\nadpc_2013 = 0.0199612670162\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nodeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['random_knn']\n\nUpdated code:\n```python\nfrom scipy.stats import uniform\n\nparametersKNN = {\n    'n_neighbors' : randint(1, 360),\n    'weights'     : ['uniform', 'distance']\n}\n\n(tiempo_random_KNN, _) = correr_randomized_y_mostrar(\n    KNeighborsClassifier(), \n    parametersKNN, \n    5, \n    10,\n    200\n)\n\nverTiempo(tiempo_KNN, tiempo_random_KNN)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fig\']\n\nUpdated code:\n```python\n_, ax = plt.subplots(figsize=(8,4))\ndfTitanic.hist(column="Age", ax=ax)\nprint(dfTitanic["Age"].describe())\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['pairwise_kernel_', 'kernel_sums']\n\nUpdated code:\n```python\nscores = []\nfor rep in range(number_of_replicates):\n    env.reset()\n    done = False\n    episode_score = 0.0\n    while not done:\n        action, _, _ = \\\n            model_smoothed_fitted_q(env, gamma, RandomForestRegressor, number_of_value_iterations, transition_model_fitter,\n                                    pairwise_kernels_=None, kernel_sums=None, smoothing_method='mse')\n        _, r, done = env.step(action)\n        episode_score += r\n    scores.append(episode_score)\n    print('score: {}'.format(score))\nprint('mean score: {} se: {}'.format(np.mean(scores), np.std(scores) / np.sqrt(number_of_replicates)))\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'X_train\', \'y_train\', \'X_valid\', \'y_valid\', \'X_test\', \'y_test\']\n\nUpdated code:\n```python\n# Load pickled data\nimport pickle\n\n# TODO: Fill this in based on where you saved the training and testing data\n\ntraining_file = "../traffic-signs-data/train.p"\nvalidation_file="../traffic-signs-data/valid.p"\ntesting_file = "../traffic-signs-data/test.p"\n\nwith open(training_file, mode=\'rb\') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode=\'rb\') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode=\'rb\') as f:\n    test = pickle.load(f)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['t_min_max']\n\nUpdated code:\n```python\n# y_pred = np.argmax(states['y_hat'], axis=1)\n# y_pred_colors = np.hstack([vsig.signal_colors[i] for i in y_pred])\nlayer = '1'\nval_arrays = np.load(os.path.join(vsig.out_dir, 'valid_hidden_layer_' + layer + '_output.npy'))\nn_generations, _, n_neurons = val_arrays.shape\nncols = 1\nnrows = n_neurons // ncols\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3))\n\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes#[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(\n            vsig.timestamps[vsig.window_size-1:], \n#             vsig.timestamps, \n#             x_val[:, -1, 0], \n#             x_val[0, :, 0], \n            vsig.mixed_signal[vsig.window_size-1:], \n            marker='o', \n            c=y_pred_colors, \n            cmap=plt.get_cmap('coolwarm'), \n            vmin=-1, \n            vmax=1\n        )\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(vsig.timestamps[0], vsig.timestamps[-1])\n        ax.grid(True)\n        \n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, 'output', g + 1))\n#     plt.savefig(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, 'gen', str(g + 1)]) + '.png'))\n\nplt.show()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['postag', 'word1']\n\nUpdated code:\n```python\nimport numpy as np\n\ndef word2embedding(sent, i):\n    word = sent[i][0]\n            \n    if word in model.vocab:\n        worde = word\n    else:\n        worde = 'null'\n    \n    res = model[worde]\n     \n    if i > 0:\n        word1 = sent[i-1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n    \n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n  \n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n    \n    res.shape = (1,900)\n    return res\n        \ndef sent2embedding(sent):\n        rese = []\n        for  i in range(len(sent)):\n            line = word2embedding(sent,i)\n            rese.append(line)\n        \n        resee = np.vstack(rese)\n        return resee\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['DATA']\n\nUpdated code:\n```python\nBASE = path.join(path.dirname(attitude.__file__),'..','js-frontend')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['num_examples', 'im_shape']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'df_gme\']\n\nUpdated code:\n```python\n#dataframe om alle gediplomeerden in Categorie "Meerdere" te selecteren\ndf_me = output.loc[output[\'Categorie\'] == \'Meerdere\']\ndf_me.groupby([\'Gemeente\']).sum()[[\'2013   AANT\',\'2014   AANT\',\'2015   AANT\',\'2016   AANT\',\'2017   AANT\']]\ndf_me\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'basic_model\']\n\nUpdated code:\n```python\ntags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\n\n# TODO: create states with emission probability distributions P(word | tag) and add to the model\n# (Hint: you may need to loop & create/add new states)\n#basic_model.add_states()\n\nemission_counts = pair_counts(tags, words)\nstates = {}\nfor tag, word_dict in emission_counts.items():\n    emission_dict = defaultdict(float)\n    for word in word_dict.keys():\n        emission_dict[word] = emission_counts[tag][word] / tag_unigrams[tag] \n    state_emission = DiscreteDistribution(dict(emission_dict))\n    states[tag] = State(state_emission, name=tag)\n    \nbasic_model.add_states(list(states.values()))\n\n# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n# (Hint: you may need to loop & add transitions\n#basic_model.add_transition()\nfor tag in data.training_set.tagset:\n    state = states[tag]\n    basic_model.add_transition(basic_model.start, state, tag_starts[tag]/len(data.training_set))\n    basic_model.add_transition(state, basic_model.end, tag_ends[tag]/tag_unigrams[tag])\n    for next_tag in data.training_set.tagset:\n        next_state = states[next_tag]\n        basic_model.add_transition(state, next_state, tag_bigrams[(tag, next_tag)]/tag_unigrams[tag])\n\n# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n# finalize the model\nbasic_model.bake()\n\nassert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n       "Every state in your network should use the name of the associated tag, which must be one of the training set tags."\nassert basic_model.edge_count() == 168, \\\n       ("Your network should have an edge from the start node to each state, one edge between every " +\n        "pair of tags (states), and an edge from each state to the end node.")\nHTML(\'<div class="alert alert-block alert-success">Your HMM network topology looks good!</div>\')\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['circuit', 'quantum_r', 'classical_r']\n\nUpdated code:\n```python\n# Get the components.\n\n# get the circuit by Name\nqp.get_circuit('Circuit')\n\n# get the Quantum Register by Name\nqp.get_quantum_register('qr')\n\n# get the Classical Register by Name\nqp.get_classical_register('cr')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['visualizer']\n\nUpdated code:\n```python\nfrom yellowbrick.text import FreqDistVisualizer\n\nplt.figure(figsize=(15,20))\n_ = FreqDistVisualizer(n=100, features=cv_description.get_feature_names())\n#visualizer.fit(dt_mat_description)\n#visualizer.poof()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X', 'y']\n\nUpdated code:\n```python\n# use your function\nwindow_size = 100\nstep_size = 5\n_, _ = encode_io_pairs(text, window_size, step_size)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['d']\n\nUpdated code:\n```python\nDotPlot(gepard_command, out_dir)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['RTanalysis']\n\nUpdated code:\n```python\nlists = [[] for list in range(0,5)]\n\nfor ID in range(10,86):\n    sub = cdat[cdat.subject == ID]\n    lists[0].append(ID)\n    validRT_trials = sub[sub.TrialType == 'Valid'].RT.mean()\n    invalidRT_trials = sub[sub.TrialType == 'Invalid'].RT.mean()\n    lists[1].append(validRT_trials)\n    lists[2].append(invalidRT_trials)\n    \nRTanalysis = pd.DataFrame()\nRTanalysis['SubjectID'] = lists[0]\nRTanalysis['Valid'] = lists[1]\nRTanalysis['Invalid'] = lists[2]\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['schema']\n\nUpdated code:\n```python\npd.options.display.max_colwidth = 400\npd.read_csv(join(path, 'survey_results_schema.csv'), low_memory=False)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'burned_trace\']\n\nUpdated code:\n```python\n# Set up the pymc3 model. Again assume Uniform priors for p_A and p_B.\nwith pm.Model() as model:\n    p_A = pm.Uniform("p_A", 0, 1)\n    p_B = pm.Uniform("p_B", 0, 1)\n    \n    # Define the deterministic delta function. This is our unknown of interest.\n    delta = pm.Deterministic("delta", p_A - p_B)\n\n    \n    # Set of observations, in this case we have two observation datasets.\n    obs_A = pm.Bernoulli("obs_A", p_A, observed=observations_A)\n    obs_B = pm.Bernoulli("obs_B", p_B, observed=observations_B)\n\n    # To be explained in chapter 3.\n    step = pm.Metropolis()\n    trace = pm.sample(20000, step=step)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'river\']\n\nUpdated code:\n```python\n##  get river name\n_ = data.iloc[0]["River"]\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['data']\n\nUpdated code:\n```python\npd.DataFrame({'labels':labels, 'count':count})\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'yes_responses\']\n\nUpdated code:\n```python\nwith model:\n    pm.Binomial("number_cheaters", 100, p_skewed, observed=35)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['X', 'y']\n\nUpdated code:\n```python\n_, _ = make_regression(n_samples=100, n_features=1, n_informative=1, noise=30, \n                       random_state=2018)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_test']\n\nUpdated code:\n```python\n_ = vectorizer.transform(data_test.values)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'training_op\']\n\nUpdated code:\n```python\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                   scope="hidden[34]|outputs")\n    optimizer.minimize(loss, var_list=train_vars)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['sch']\n\nUpdated code:\n```python\nalgorithms.UncontrolledCharging()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['random_average_min_path_length']\n\nUpdated code:\n```python\nnp.average([nx.average_shortest_path_length(C) for C in nx.connected_component_subgraphs(random_degree_graph)])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['bag']\n\nUpdated code:\n```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer()\ndocs = np.array([\n        'The sun is shining',\n        'The weather is sweet',\n        'The sun is shining, the weather is sweet, and one and one is two'])\ncount.fit_transform(docs)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['rep']\n\nUpdated code:\n```python\nscores = []\nfor _ in range(number_of_replicates):\n    env.reset()\n    done = False\n    episode_score = 0.0\n    while not done:\n        action = np.random.binomial(1, 0.3)\n        _, r, done = env.step(action)\n        episode_score += r\n    scores.append(episode_score)\nprint('mean score: {} se: {}'.format(np.mean(scores), np.std(scores) / np.sqrt(number_of_replicates)))\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'save_path\']\n\nUpdated code:\n```python\nn_epochs = 10\nbatch_size = 100\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)\n\n        _ = saver.save(sess, "./my_mnist_model")\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'bag_clf\']\n\nUpdated code:\n```python\nBaggingClassifier(\n    DecisionTreeClassifier(splitter="random", max_leaf_nodes=16, random_state=42),\n    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['sol_n_lincs']\n\nUpdated code:\n```python\n# North Lincolnshire\n# find steady state based on 2012 data\n\ncov_2012 = 0.100807801953\nadpc_2012 = 0.0111652211547\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.173269822929\nadpc_2013 = 0.0216211803756\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nodeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['model']\n\nUpdated code:\n```python\ndef fit_lstm(train,batch_size,nb_epoch,neurons):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    #define the loss function and optimization algorithm here\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['x_values', 'y_values', 'm', 'b', 'x']\n\nUpdated code:\n```python\ndef error_line_trace(_, _, _, _, _):\n    pass\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['ResNet50_model']\n\nUpdated code:\n```python\nfrom keras.applications.resnet50 import ResNet50\n\n# define ResNet50 model\n_ = ResNet50(weights='imagenet')\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'black\']\n\nUpdated code:\n```python\ndef broad_less_than_50_meters_port():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible.\n    """\n    white = (255, 255, 255)\n    red = (255, 0, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 201)\n        distance_bw_left_endpoint = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = distance_bw_left_endpoint + light_width\n        running_light_start = tall_masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if distance_bw_left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(distance_bw_left_endpoint - 20, distance_bw_left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(25, distance_bw_left_endpoint + 20)\n        running_light_area = running_light_loc + light_width\n        new_array[taller_masthead_light:tall_masthead_height, distance_bw_left_endpoint:tall_masthead_width] = white\n        new_array[running_light_start:running_light_width, running_light_loc: running_light_area] = red\n        new_array = new_array.flatten()\n        all_broad_images[i] = new_array\n\n    return all_broad_images\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'data\']\n\nUpdated code:\n```python\n##  Ask for input file, full path\n##  i.e. Y:\\LRMF\\R_tables\\columbia_river_orig.csv\ninputFile = input("Please provide a full-path input file:")\n##inputFile = "Y:\\\\LRMF\\\\R_tables\\\\colorado_river_orig.csv"\n\npd.read_csv(inputFile, header=0)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['mean_prob_t']\n\nUpdated code:\n```python\nt = np.linspace(temperature.min() - 5, temperature.max()+5, 50)[:, None]\np_t = logistic(t.T, beta_samples, alpha_samples)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['data', 'centroids']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['fig1', 'fig2', 'fig3', 'fig4', 'fig5', 'fig6']\n\nUpdated code:\n```python\n_, ax1 = plt.subplots()\nax1.plot(t[first:last], traj_data_us[first:last],  label='Angles',  marker='o',linewidth=0.3, markersize=1.5)\nax1.legend();\nax1.set_ylabel('Angles [deg]')\nax1.set_xlabel('Time')\nax1.set_title('Angles');\n\n_, ax2 = plt.subplots()\nax2.plot(t[first:last], vel_us[first:last],  label='Angular velocity',  marker='o',linewidth=0.3, markersize=1.5)\nax2.legend();\nax2.set_ylabel('Angular velocity [deg/s]')\nax2.set_xlabel('Time')\nax2.set_title('Velocity');\n\n_, ax3 = plt.subplots()\nax3.plot(t[first:last], acc_us[first:last],  label='Angular acceleration',  marker='o',linewidth=0.3, markersize=1.5)\nax3.legend();\nax3.set_ylabel('Angular acceleration [deg/s^2]')\nax3.set_xlabel('Time')\nax3.set_title('Acceleration');\n\n_, ax4 = plt.subplots()\nax4.plot(t[first:last], np.sum(np.abs(acc_us[first:last]), axis=1),  label='Summed acc',  marker='o',linewidth=0.3, markersize=1.5)\nax4.legend();\nax4.set_ylabel('Summed accelerations [deg/s^2]')\nax4.set_xlabel('Time')\nax4.set_title('Summed accelerations');\n\n_, ax5 = plt.subplots()\nax5.plot(t[first:last], psi_us[first:last],  label='Pseudo power',  marker='o',linewidth=0.3, markersize=1.5)\nax5.legend();\nax5.set_ylabel('Pseudo power [deg/s^3]')\nax5.set_xlabel('Time')\nax5.set_title('Pseudo power');\n\n_, ax6 = plt.subplots()\nax6.plot(power_time_ds[first:last], power_data_250[first:last],  label='Power',  marker='o',linewidth=0.3, markersize=1.5)\nax6.legend();\nax6.set_ylabel('Power [W]')\nax6.set_xlabel('Time')\nax6.set_title('Power');\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'trainImages\', \'testImages\', \'trainLabels\', \'testLabels\']\n\nUpdated code:\n```python\n# Shuffle data\nimages_seq, labels_seq = correspondingShuffle([images_seq, labels_seq])\n\n    \n# Split data on train and test dataset\ndiv = int(train_set * len(images_seq))\n\n_ = images_seq[0:div]\n_ = images_seq[div:]\n\n_ = labels_seq[0:div]\n_ = labels_seq[div:]\n\nprint("Training images:", div)\nprint("Testing images:", len(images_seq) - div)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['t']\n\nUpdated code:\n```python\n# 2 1d ODEs\ndef ode(state, _):\n    # x and y are first two components of state vector\n    x = state[0]\n\n    # Compute state derivatives.  Mess around here! \n    dx = .4 * np.square(x) - 2\n\n    # Return the state derivatives\n    return [dx]\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'X\']\n\nUpdated code:\n```python\nreset_graph()\n\n_ = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'line_image\']\n\nUpdated code:\n```python\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n\n    gray = grayscale(image)\n\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray,kernel_size)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges,vertices)\n \n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    rho = 1 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 90     # minimum number of votes (intersections in Hough grid cell)\n    min_line_length = 30 #minimum number of pixels making up a line\n    max_line_gap = 30    # maximum gap in pixels between connectable line segments\n\n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap,vertices)\n\n    # Draw the lines on the edge image\n    result = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n    \n    return result\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['min_error']\n\nUpdated code:\n```python\nnp.min(errors)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_topics']\n\nUpdated code:\n```python\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_topics=10,\n                                random_state=123,\n                                learning_method='batch')\nlda.fit_transform(X)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['r']\n\nUpdated code:\n```python\nConsedRunner()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['count']\n\nUpdated code:\n```python\nfrom numpy.random import seed\nseed(8923)\n\nP = np.diag([500., 49.])\nf = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=P)\ntrack, zs = compute_dog_data(3., .02, count)\nXs, Covs, _, _ = f.batch_filter(zs)\nMs, Ps, _, _ = f.rts_smoother(Xs, Covs)\n\nbook_plots.plot_measurements(zs)\nplt.plot(Xs[:, 0], ls='--', label='Kalman Position')\nplt.plot(Ms[:, 0], label='RTS Position')\nplt.legend(loc=4);\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'black\']\n\nUpdated code:\n```python\ndef bow_lights_gt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    \n    Ships greater than 50m in length.\n    """\n    white = (255, 255, 255)\n    red = (255, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 126)\n        tall_mh_height = taller_masthead_light + light_width\n        shorter_masthead_light = np.random.randint(tall_mh_height + 2, tall_mh_height + 16)\n        short_mast_head_height = shorter_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(15, 56)\n        running_light_dist_vert = np.random.randint(short_mast_head_height, short_mast_head_height + 51)\n        new_view[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        new_view[shorter_masthead_light:short_mast_head_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_view = new_view.flatten()\n        all_bow_images[i] = new_view\n    \n    return all_bow_images\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\nACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'qpi\', \'qmu\', \'qsigma\', \'qz\']\n\nUpdated code:\n```python\nT = 1000  # number of samples\nwith tf.name_scope("posterior"):\n    Empirical(tf.get_variable("qpi/params", [T, K],initializer=tf.constant_initializer(1.0/K)))\n    Empirical(tf.get_variable("qmu/params", [T, K, D],initializer=tf.zeros_initializer()))\n    Empirical(tf.get_variable("qsigma/params", [T, K, D],initializer=tf.ones_initializer()))\n    Empirical(tf.get_variable("qz/params", [T, N],initializer=tf.zeros_initializer(),dtype=tf.int32))\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['f', 'x0', 'y0', 'x1']\n\nUpdated code:\n```python\n# reload(accuracy)\nreload(slope)\nplt.close()\n\nle = .12\nre = .02\nte = .1\nbe = .11\nh_gap = .13\n\nw = .5\nh = 1. - te - be\n\nax_lines = plt.figure(figsize=(5, 4)).add_axes([le, be, w, h])\nax_slopes = plt.figure(figsize=(5, 4)).add_axes([le + w + h_gap, be, 1. - w - h_gap - le - re, h])\n\nkey = fracs[-1]\n\nslope.plot_cv_slope(subjects, deep_all, linear_all, chance[0], training_size, fracs, (ax_lines, ax_slopes),\n                    legend=True, normalize_chance=False)\n\nf.text(le, 1. - te + .02, 'A', **letter_fontstyle)\nf.text(le + w + h_gap - .075, 1. - te + .02, 'B', **letter_fontstyle)\n\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.eps'), dpi=300)\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.png'), dpi=300)\n```"}, {'reason': 'stop', 'result': "Unused variables:\nNone\n\nUpdated code:\n```python\n# This looks much better, so let's replace the SalePrice with the log-transformed version (will need to exponentiate predictions)\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n# Re-define the full dataset - and work on this until we are ready to split out test and train sets again\ndata_full = pd.concat([train, test], keys = ['train', 'test'])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y']\n\nUpdated code:\n```python\nX = train[0:-1]\nX\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['fig']\n\nUpdated code:\n```python\n_, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n        label='DP sample CDFs');\nax.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75);\nax.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF');\n\nax.set_title(r'$\\alpha = {}$'.format(alpha));\nax.legend(loc=2);\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['BUCKET', 'PROJECT', 'REGION']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['feature_set']\n\nUpdated code:\n```python\nfor _, slices in storage.get_slices().items():\n    output = slices.to_output()\n    print(output, '\\n')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['save_file']\n\nUpdated code:\n```python\nsaver = tf.train.Saver()\n\nwith tf.Session() as session:\n    saver.restore(session, './train_model_best.ckpt')\n    weights_layer_1 = session.run(weights['layer_1'])\n\n    # min/max values of weights\n    wmin = np.min(weights_layer_1)\n    wmax = np.max(weights_layer_1)\n\n    fig, axes = plt.subplots(7,7)\n\n    for i, ax in enumerate(axes.flat):\n        if i < 48:\n            image = weights_layer_1[:,:,0,i].reshape([3,3]) \n            ax.imshow(image, vmin=wmin, vmax=wmax, cmap='seismic')\n            ax.axis('off')\n\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'tiempo_random_forest\']\n\nUpdated code:\n```python\nparametrosRF = {\n    "max_features": ["sqrt", "log2", None] + [1, 20, 50, 100, 150, 200],\n    "max_depth": [3, 6, 12],\n    "min_samples_split": [2, 6, 12],\n    "n_estimators": [10, 50, 120, 200]\n}\n\n(_, grid_random_forest) = correr_y_mostrar(\n    RandomForestClassifier(),\n    parametrosRF,\n    5,\n    5\n)\n\nrandomForest = grid_random_forest.best_estimator_\nplot_learning_curve(randomForest, "Learning Curve Random Forest Mejor segun GridSearch", X_dev_np, y_dev_np, cv=5)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['A', 'b']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'X\', \'y\']\n\nUpdated code:\n```python\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n_ = tf.placeholder(tf.int32, shape=(None), name="y")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['clusterer', 'preds', 'centers', 'sample_preds', 'score']\n\nUpdated code:\n```python\n# TODO: Apply your clustering algorithm of choice to the reduced data\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['EPOCHS', 'BATCH_SIZE', 'rate']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['pca11', 'pca12', 'pca21', 'pca22']\n\nUpdated code:\n```python\nnew_pc = eigvecs[:,-2:]\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = new_pc - new_pc.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'c\']\n\nUpdated code:\n```python\ndata = ["acacag", "acacgg", "aaaaaacgg"]\nClusteringSeq(data)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'en_vocab_file\', \'zh_vocab_file\', \'checkpoint_path\', \'log_dir\', \'download_dir\']\n\nUpdated code:\n```python\noutput_dir = "nmt"\n\nif not os.path.exists(output_dir):\n  os.makedirs(output_dir)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'z_ed\']\n\nUpdated code:\n```python\n# Edward Model\nwith tf.name_scope("model"):\n    pi_ed = Dirichlet(concentration=tf.constant([1.0] * K, name="pi/weights"), name= "pi")\n    mu_ed = Normal(loc= tf.ones(D, name="centroids/loc") * 127, \n                scale= tf.ones(D, name="centroids/scale") * 80, sample_shape=K, name= "centroids")\n    sigmasq_ed = InverseGamma(concentration=tf.ones(D, name="variability/concentration"), \n                         rate=tf.ones(D, name="variability/rate"), sample_shape=K, name= "variability")\n    x_ed = ParamMixture(pi_ed, {\'loc\': mu_ed, \'scale_diag\': tf.sqrt(sigmasq_ed)},\n                     MultivariateNormalDiag, sample_shape=N, name= "mixture")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['YOUR_BUCKET_NAME']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['posttest']\n\nUpdated code:\n```python\npd.read_csv('posttest.csv')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['time', 'estimated_distance_m']\n\nUpdated code:\n```python\n# Fill this in.\n...\n\n# Fill this in.\n...\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\n#InvalidACCCI\nsms.DescrStatsW(ACCanalysis.Invalid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_predict']\n\nUpdated code:\n```python\ntrainer.predict(Xt)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'gmaps\']\n\nUpdated code:\n```python\nimport googlemaps\n\ngooglemaps.Client(key="PASTE YOUR API KEY HERE")\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['dump_every']\n\nUpdated code:\n```python\nn_samples_train = 2048\nn_samples_valid = 512\nepochs = 200\nstatus_update_every = 50\n\nx_valid, y_valid, *_ = msig.generate_samples(n_samples_valid, 'tf_tc')\n\nfor i in range(epochs):\n    x_train, y_train, *_ = msig.generate_samples(n_samples_train, 'tf_tc')\n    model.fit(\n        x_train, y_train,\n        epochs=1, \n        validation_data=(x_valid, y_valid),\n        batch_size=batch_size,\n        verbose=1, \n        callbacks=[\n            csvlogger,\n            checkpointer\n        ],\n    )\n    if stateful:\n        model.reset_states()\n\n# Code specific to window_type == sliding\n\n#     if (i + 1) % dump_every == 0:     \n#         test_dict['epoch'].append(i + 1)\n#         score = model.evaluate(test_dict['X'], test_dict['y'], batch_size=batch_size)\n#         if stateful:\n#             model.reset_states()\n#         test_dict['score'].append(score)\n\n#         y_hat = model.predict(test_dict['X'], batch_size=batch_size)\n#         if stateful:\n#             model.reset_states()\n#         test_dict['y_hat'].append(y_hat)\n\n    if (i + 1) % status_update_every == 0:\n        print('#' * 50)\n        print(f'Epoch: {(i + 1)}/{epochs}')\n        print('#' * 50)\n#         model.save(msig.model_filename)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['union_dog', 'union_cat']\n\nUpdated code:\n```python\n#Set the pipelines for categorical variables\ndiscrete_pipe_dog = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['dog'], hashing=None))])\ndiscrete_pipe_cat = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['cat'], hashing=None))])\n\n#Set the pipelines for continuous variables\ncontinuous_pipe_cat = Pipeline(steps=[('Scale', MyScaler(continuous['cat']))])\ncontinuous_pipe_dog = Pipeline(steps=[('Scale', MyScaler(continuous['dog']))])\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['array_1', 'array_2', 'array_3']\n\nUpdated code:\n```python\nband1.ReadAsArray().flatten()\nband2.ReadAsArray().flatten()\nband3.ReadAsArray().flatten()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'corpus\']\n\nUpdated code:\n```python\nprovidence_data_file = os.path.join("data/words_sentences/providence_avg_prosody_pos.csv")\nbrent_data_file      = os.path.join("data/words_sentences/brent_avg_prosody_pos.csv")\nboth_data_file       = os.path.join("data/words_sentences/brentprovidence_avg_prosody_pos.csv")\n\n# define x-fields (column IDs) to keep at 1    = word itself\n#                                         2    = pos\n#                                         3    = length (letters)\n#                                         4    = frequency\n#                                         5-93 = egemaps prosody features\nfeatures  = list(range(1,93))\n\npos_filter = None  #[[\'pos\', \'nouns\',\'function_words\']]\n\n# define name (col header) of y-variable in data file\ny         = \'y\'\n\n# load data, x-fields, y-field, train/dev/test split from input file\nprint("extracting providence...")\nprovidence_x_train, providence_y_train, _, _, _, _, labels = get_data_from_tsv(providence_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\nprint("extracting brent...")\nbrent_x_train, brent_y_train, _, _, _, _, labels = get_data_from_tsv(brent_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\n\nprint("extracting brentprovidence...")\nboth_x_train, both_y_train, _, _, _, _, labels = get_data_from_tsv(both_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\n\nif corpus == "Providence":\n    x_train = providence_x_train\n    y_train = providence_y_train\nelif corpus == "Brent":\n    x_train = brent_x_train\n    y_train = brent_y_train\nelif corpus == "BrentProvidence":\n    x_train = both_x_train\n    y_train = both_y_train\n\nfirst_numeric_feature = x_train.columns.tolist().index(\'log_length\')\nfirst_egemaps_feature = x_train.columns.tolist().index(\'F0semitoneFrom27.5Hz_sma3nz_amean\')\n\nprint(first_numeric_feature,first_egemaps_feature)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'matrix_id\', \'mat_as_group\']\n\nUpdated code:\n```python\n#ignore\n# @pysnooper.snoop()\ndef plot_2d(value, ax=None, group=None, mask=None, group_id=None, linewidths=0, is_string=False, fmt="d", square=False):\n  \n  if hasattr(value, "numpy"):\n    value = value.numpy()\n  if group is not None and hasattr(group, "numpy"):\n    group = group.numpy()\n  if mask is not None and hasattr(mask, "numpy"):\n    mask = tf.squeeze(mask)\n    mask = tf.ones_like(value) * mask\n    mask = mask.numpy()\n    \n\n  cmaps = [\'PuOr\', \'tab20b\', \'RdBu\']\n  group_id = int(group[0][0])\n  cmap = cmaps[group_id]\n  \n  if is_string:\n    fmt = \'\'\n  \n  sns.heatmap(group, \n              fmt=fmt,\n#               cmap=cmap,\n              cmap=cmap,\n              annot=value, \n              cbar=False, \n              xticklabels=False, \n              yticklabels=False, \n              square=square,\n              mask=mask,\n              linewidths=linewidths,\n              ax=ax)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'max_norm_reg\']\n\nUpdated code:\n```python\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_regularizer(threshold=1.0), name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_regularizer(threshold=1.0), name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'plot\', \'ex_name\']\n\nUpdated code:\n```python\ndef full_test(model, hypothesis, name, train_len=95, test_len=10):\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(), hypothesis,\n                                         train_len=train_len, test_len=train_len,\n                                         save_hyp=\'plots/hyp_{}.png\'.format(name),\n                                         save_diag=\'plots/diag_{}.png\'.format(name),\n                                         save_resp=\'plots/resp_{}.png\'.format(name))\n    metric_pearsonr = lambda a, b: stats.pearsonr(a, b)[0]\n    \n    print("Hypothesis: {} (normal)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(class_weight=\'balanced\'),\n                                         hypothesis, train_len=train_len, test_len=test_len,\n                                         save_hyp=\'plots/hyp_{}_balanced.png\'.format(name),\n                                         save_diag=\'plots/diag_{}_balanced.png\'.format(name),\n                                         save_resp=\'plots/resp_{}_balanced.png\'.format(name))\n    print("Hypothesis: {} (balanced)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['ridge_preds', 'lasso_preds', 'elastic_preds']\n\nUpdated code:\n```python\nnp.expm1(model_ridge.predict(X_test))\nnp.expm1(model_lasso.predict(X_test))\nnp.expm1(model_elastic.predict(X_test))\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['oc', 'sms']\n\nUpdated code:\n```python\n#make the flopy model\nmf = flopy.modflow.Modflow(modelname=modelname, exe_name=mfexe, model_ws=modelpth)\ndis = flopy.modflow.ModflowDis(mf, nlay, nrow, ncol,\n                               delr=delr, delc=delc, \n                               top=botm[0, :, :], botm=botm[1:, :, :], \n                               perlen=1, nstp=1, steady=True)\nbas = flopy.modflow.ModflowBas(mf, ibound=ibound, strt=strt)\nlpf = flopy.modflow.ModflowLpf(mf, hk=0.0001, laytyp=4)\nmf.write_input()\n\n# remove any existing head files\ntry:\n    os.remove(os.path.join(model_ws, '{0}.hds'.format(modelname)))\nexcept:\n    pass\n\n# run the model\nmf.run_model()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['errors_for_regression']\n\nUpdated code:\n```python\n_ = error_line_traces(x_values, y_values, 1.7, 10)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['b', 'ijk']\n\nUpdated code:\n```python\nim = Model(name='integer_programming')\nim.binary_var(name='boolean_var')\nim.integer_var(name='int_var')\nim.print_information()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['X_batch', 'y_batch']\n\nUpdated code:\n```python\n_, _ = prepare_batch(flower_paths_and_classes_train, batch_size=4)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['f_M_Nm']\n\nUpdated code:\n```python\nsy.lambdify(x, M_Nm)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['modelname']\n\nUpdated code:\n```python\n# Set name of MODFLOW exe\n# Assumes executable is in user's path statement\nexe_name = 'mfusg'\nif platform.system() == 'Windows':\n    exe_name += '.exe'\nmfexe = exe_name\n\nmodelpth = os.path.join('data')\n\n# Make sure modelpth directory exists\nif not os.path.exists(modelpth):\n    os.makedirs(modelpth)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'rmse_tr\', \'rmse_te\']\n\nUpdated code:\n```python\nlambdas = np.logspace(-5, 0, 15)\n\n# bluid poly\ntx_tr = build_poly(Strain1_z, 1)\ntx_te = build_poly(Stest1_z, 1)\n\n# ridge regression with different lambda\nfor ind, lambda_ in enumerate(lambdas):\n    # ridge regression\n    weight = ridge_regression(Strain1_y, tx_tr, lambda_)\n    print(np.sqrt(2 * compute_mse(Strain1_y, tx_tr, weight)), np.sqrt(2 * compute_mse(Stest1_y, tx_te, weight)))\n#print("last weight:",weight)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'training_op\']\n\nUpdated code:\n```python\nlearning_rate = 0.01\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    _ = optimizer.minimize(loss)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'mask\', \'ignore_mask_color\']\n\nUpdated code:\n```python\n# TODO: Build your pipeline that will draw lane lines on the test_images\n# then save them to the test_images directory.\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport cv2\n\n\n# Read in and grayscale the image\nimage = mpimg.imread(\'test_images/whiteCarLaneSwitch.jpg\')\ngray = grayscale(image)\n\n# Define a kernel size and apply Gaussian smoothing\nkernel_size = 5\nblur_gray = gaussian_blur(gray,kernel_size)\n\n# Define our parameters for Canny and apply\nlow_threshold = 60\nhigh_threshold = 100\nedges = canny(blur_gray, low_threshold, high_threshold)\n\n# Next we\'ll create a masked edges image using cv2.fillPoly()\nimshape = image.shape\nvertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\nmasked_edges = region_of_interest(edges,vertices)\n \n# Define the Hough transform parameters\n# Make a blank the same size as our image to draw on\nrho = 1 # distance resolution in pixels of the Hough grid\ntheta = np.pi/180 # angular resolution in radians of the Hough grid\nthreshold = 41     # minimum number of votes (intersections in Hough grid cell)\nmin_line_length = 30 #minimum number of pixels making up a line\nmax_line_gap = 15    # maximum gap in pixels between connectable line segments\nline_image = np.copy(image)*0 # creating a blank to draw lines on\n\n# Run Hough on edge detected image\n# Output "lines" is an array containing endpoints of detected line segments\nlines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap,vertices)\n\n# Draw the lines on the edge image\nresult = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n \nplt.imshow(lines)\nplt.figure()\nplt.imshow(result)\n\n\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['l']\n\nUpdated code:\n```python\nbeta1, _ = least_squares(Strain1_y, Strain1_z)      #beta from y = X^T*beta --> missing the column of ones'\n#print(beta1)\nres1 = definitive_res(Stest1_z.dot(beta1))\n#print(res1)\nlen(Stest1_y[Stest1_y==res1])/len(Stest1_y)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'i\']\n\nUpdated code:\n```python\nfor _ in range(10):\n    print(repr(mutate("A quick brown fox")))\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'num_layers\', \'d_model\', \'dff\', \'num_heads\', \'dropout_rate\']\n\nUpdated code:\n```python\ninput_vocab_size = subword_encoder_en.vocab_size + 2\ntarget_vocab_size = subword_encoder_zh.vocab_size + 2\n\nprint("input_vocab_size:", input_vocab_size)\nprint("target_vocab_size:", target_vocab_size)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'rects\']\n\nUpdated code:\n```python\n# Plot Frequency again\nsign_frequencies = get_frequencies(y_train_augmented, sign_dict)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nclasses = list(sign_dict.values())\nind = np.arange(len(classes))\nwidth = 0.8\n\nax.bar(ind, sign_frequencies.values(), width, align="edge", alpha=0.5)\nax.set_ylabel(\'Frequency\')\nax.set_title(\'Traffic Sign Classes\')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(sign_frequencies.keys(), rotation=90)\nplt.show()\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['features_norm']\n\nUpdated code:\n```python\n# TODO add features for normalized by speaker values of left, right, x, y\n# Name these 'norm-rx', 'norm-ry', 'norm-lx', and 'norm-ly'\n# using Z-score scaling (X-Xmean)/Xstd\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['bivar']\n\nUpdated code:\n```python\n_ = sorted(filter(lambda r: len(r[0]) == 1, storage.relevancies.relevancy.iteritems()), \n               key=lambda r: r[1], reverse=True)\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'num_nodes\']\n\nUpdated code:\n```python\ngraph = tf.Graph()\nwith graph.as_default():\n  \n  # Parameters:\n  # Input gate: input, previous output, and bias.\n  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n  ib = tf.Variable(tf.zeros([1, num_nodes]))\n  # Forget gate: input, previous output, and bias.\n  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n  fb = tf.Variable(tf.zeros([1, num_nodes]))\n  # Memory cell: input, state and bias.                             \n  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n  cb = tf.Variable(tf.zeros([1, num_nodes]))\n  # Output gate: input, previous output, and bias.\n  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n  ob = tf.Variable(tf.zeros([1, num_nodes]))\n  # Variables saving state across unrollings.\n  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n  # Classifier weights and biases.\n  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n  b = tf.Variable(tf.zeros([vocabulary_size]))\n  \n  # Definition of the cell computation.\n  def lstm_cell(i, o, state):\n    """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n    state = forget_gate * state + input_gate * tf.tanh(update)\n    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n    return output_gate * tf.tanh(state), state\n\n  # Input data.\n  train_data = list()\n  for _ in range(num_unrollings + 1):\n    train_data.append(\n      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n  train_inputs = train_data[:num_unrollings]\n  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n\n  # Unrolled LSTM loop.\n  outputs = list()\n  output = saved_output\n  state = saved_state\n  for i in train_inputs:\n    output, state = lstm_cell(i, output, state)\n    outputs.append(output)\n\n  # State saving across unrollings.\n  with tf.control_dependencies([saved_output.assign(output),\n                                saved_state.assign(state)]):\n    # Classifier.\n    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n    loss = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(\n        labels=tf.concat(train_labels, 0), logits=logits))\n\n  # Optimizer.\n  global_step = tf.Variable(0)\n  learning_rate = tf.train.exponential_decay(\n    10.0, global_step, 5000, 0.1, staircase=True)\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  gradients, v = zip(*optimizer.compute_gradients(loss))\n  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n  optimizer = optimizer.apply_gradients(\n    zip(gradients, v), global_step=global_step)\n\n  # Predictions.\n  train_prediction = tf.nn.softmax(logits)\n  \n  # Sampling and validation eval: batch 1, no unrolling.\n  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n  reset_sample_state = tf.group(\n    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n  sample_output, sample_state = lstm_cell(\n    sample_input, saved_sample_output, saved_sample_state)\n  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n                                saved_sample_state.assign(sample_state)]):\n    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['t_min_max']\n\nUpdated code:\n```python\n_, y_pred_colors = np.hstack([msig.waves[i].color for i in y_pred])\nprint(y_pred_colors[:3])\nprint(y_pred.shape, y_pred_colors.shape)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 3))\nax.plot(msig.timestamps, msig.mixed_signal, color='grey', alpha=0.3)\n# ax.scatter(msig.timestamps[msig.window_size-1:], x_test[:, -1, 0], marker='.', c=y_pred_colors)\n# ax.scatter(msig.timestamps, x_test[0, :, 0], marker='.', c=y_pred_colors)\nax.scatter(msig.timestamps[msig.window_size-1:], msig.mixed_signal[msig.window_size-1:], marker='.', c=y_pred_colors)\nax.set_xlabel('time')\nplt.grid(True)\nplt.show()\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fn\']\n\nUpdated code:\n```python\nwith fiona.open(path.join(DATA,\'field-orientations.geojson\')) as ds:\n    for i,item in ds.items():\n        p = item[\'properties\']\n        if p[\'planeType\'].strip() != \'Bedding\':\n            continue\n\n        asm = p.get("aster_smoothed")\n        alt = asm\n\n        alt -= 40 # Global datum is higher than local\n        center = (*item[\'geometry\'][\'coordinates\'],alt)\n\n        err = 0.1*N.pi/180\n        a = ReconstructedPlane(p[\'strike\'], p[\'dip\'],0,err,err)\n        orientation = a.to_mapping(\n            center=center,\n            color=\'#444\', type=\'in-situ\')\n        collection.append(orientation)\n\nremovedUIDs = ["89636280","6031fd6f"]\ncollection = [c for c in collection if 1600 < c[\'center\'][2] < 1680]\ncollection = [c for c in collection if c[\'uid\'] not in removedUIDs]\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['y']\n\nUpdated code:\n```python\nx = linspace(0, 5, 10)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['redundancy', 'subset', 'feature']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['GCS_PATTERN', 'CLASSES']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['desk', 'cell', 'desk1', 'cell1', 'desk2', 'cell2']\n\nUpdated code:\n```python\n# two variables per machine type:\n_ = tm2.integer_var(name='desk1')\n_ = tm2.integer_var(name='cell1')\n\n_ = tm2.integer_var(name='desk2')\n_ = tm2.integer_var(name='cell2')\n\n# yes no variable\n_ = tm2.binary_var(name='z')\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['i']\n\nUpdated code:\n```python\nfor _ in range(10):\n    print(repr(flip_random_character(seed_input)))\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'p_A_samples\', \'p_B_samples\', \'delta_samples\']\n\nUpdated code:\n```python\n_ = burned_trace["p_A"]\n_ = burned_trace["p_B"]\n_ = burned_trace["delta"]\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['letter', 'bigram']\n\nUpdated code:\n```python\n_, _ = get_probabilities()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['y_kmeans_proj']\n\nUpdated code:\n```python\n# Find clusters in projected data\ncenters_kmeans_proj = []\nfor i, x in enumerate(X_proj):\n    kmeans = KMeans(n_clusters=5)#, init=np.array([(i*200/6.0, 25) for i in range(1,6)]))\n    kmeans.fit(x)\n    centers_kmeans_proj.append(kmeans.cluster_centers_)\n    _ = kmeans.predict(x)\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['yhat']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'clip_weights2\']\n\nUpdated code:\n```python\nweights2 = tf.get_default_graph().get_tensor_by_name("hidden2/kernel:0")\nclipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\ntf.assign(weights2, clipped_weights2)\n```'}, {'reason': 'stop', 'result': "Unused variables:\n['a']\n\nUpdated code:\n```python\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 1].Valid).tconfint_mean()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['ax2']\n\nUpdated code:\n```python\nf = plt.figure()\nplt.imshow(oriImg[:,:,[2,1,0]])\nplt.imshow(heatmap_avg[:,:,1], alpha=.5)\nf.show()\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['width', 'height', 'channels']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused variables:\n['university_canton_dict', 'institution_canton_dict']\n\nUpdated code:\n```python\ntry:\n    json.loads(open('university_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for universities has not been saved yet. Let''s create a new dictionary.')\n\ntry:\n    json.loads(open('institution_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for institutions has not been saved yet. Let''s create a new dictionary.')\n```"}, {'reason': 'stop', 'result': 'Unused variables:\n[\'m\']\n\nUpdated code:\n```python\nz_obs = info.get_sn_info("SN2007uy")["z_obs"]\n\nprint(z_obs)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'c_filt\', \'b_filt\', \'dt_cuckoo\', \'dt_bloom\']\n\nUpdated code:\n```python\nload_factors_of_interest = np.linspace(0.1, 0.8, 8)\ncapacity = 3e4\nnum_items_to_insert = 500\nnum_runs = 30\ntimings_cuckoo = defaultdict(list)\ntimings_bloom = defaultdict(list)\n\nfor load in tqdm(load_factors_of_interest, "current run"):\n    for run in tqdm(range(num_runs)):\n        _, _ = return_cuckoo_filter_with_specified_load_factor(int(capacity),\\\n                                                                                 finger_print_size=2, load_factor=load)\n        _, _ = return_bloom_filter_with_specified_load_factor(int(capacity), percent_to_fill=load)\n        \n        start = time.time()\n        for item_to_insert in range(num_items_to_insert):\n            item = "".join(random.sample(string.ascii_lowercase, 12))\n            _ = add(item)\n        _ = time.time() - start\n        timings_cuckoo[load].append(_)\n        \n        start = time.time()\n        for item_to_insert in range(num_items_to_insert):\n            item = "".join(random.sample(string.ascii_lowercase, 12))\n            _ = add(item)\n        _ = time.time() - start\n        timings_bloom[load].append(_)\n```'}, {'reason': 'stop', 'result': 'Unused variables:\n[\'fitX\', \'fitY\']\n\nUpdated code:\n```python\ndef expectedFit(x, a, b):\n    """\n    Returns the expected fit for the histogram\n    \n    Arguments: x - the x value in the equation\n               a - the first fit parameter\n               b - the second fit paramter\n               \n    Returned: The expected fit function\n    """\n    return a * np.exp(-b * x)\n\noccurenceRegion = bigOccurences[7:14] # Only fits region of interest\nmagnitudeRegion = bigMagnitudes[7:14]\n\nparameters, covariance = curve_fit(expectedFit, magnitudeRegion, occurenceRegion)\n\naFit = parameters[0]\nbFit = parameters[1]\n\nfitY = aFit * np.exp(-bFit * magnitudeRegion)\n\nprint("A = ", aFit, "b = ", bFit)\n```'}]