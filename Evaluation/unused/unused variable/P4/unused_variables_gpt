[{'identified': 'None', 'updated_code': "pop_df = pd.read_excel(os.path.join(PATH, 'source_data', '2010_Pop_Block_County.xls'), sheetname='San Francisco County', header=4)\npop_df = pop_df.drop('BLOCKS', axis='index').reset_index()\npop_df = pop_df.rename(columns={'index': 'block_str'})\n\nrecords = []\nfor ix, row in pop_df.iterrows():\n    splits = row['block_str'].split(', ')\n    record = row[[x for x in row.index if x != 'block_str']].to_dict()\n    record['block'] = str(splits[0].split('Block ')[-1])\n    record['block_group'] = str(splits[1].split('Block Group ')[-1])\n    census_tract = '0' + str(splits[2].split('Census Tract ')[-1].replace('.', ''))\n    census_tract += '0' * (6-len(census_tract))  # Even if it doesn't have a decimal part, needs to be 6 characters\n    record['census_tract'] = census_tract\n    records.append(record)\npop_df = pd.DataFrame(records)\n\npop_df.columns = [to_appropriate_column_name(x) for x in pop_df.columns]\nstr_columns = ['block', 'block_group', 'census_tract']\nnonstr_columns = [x for x in pop_df.columns if x not in str_columns]\npop_df.loc[:, nonstr_columns] = pop_df[nonstr_columns].astype(int)\npop_df = pop_df[str_columns + nonstr_columns]\npop_df.to_csv(os.path.join(PATH, 'derived_data', 'SF_2010_pop_block.csv'))\npop_df.head()"}, {'identified': 'None', 'updated_code': '# Compute a single transport map from day 7 to 7.5\ntmap_annotated = ot_model.compute_transport_map(7,7.5)'}, {'identified': "['ax1Top', 'ax2Top']", 'updated_code': 'xmin = 0\nxmax = y_score.shape[0]\nxindex = range(xmin, xmax)\n\nfig, ax = plt.subplots(nrows=4, ncols=1, figsize=(10, 16))\n# ax = plt.Axes(fig, [0., 0., 1., 1.])\n# ax.set_axis_off()\n# fig.add_axes(ax)\n\nax[0].scatter(\n    xindex, \n    x_test_clipped, \n    marker=\'.\', \n    c=y_true_colors)\nax[0].set_title(\'epoch = {}\'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].imshow(\n    y_score.T, \n    interpolation=\'nearest\', \n    cmap=plt.get_cmap(\'Spectral\'), \n    origin=\'upper\');\nax[1].spines[\'top\'].set_visible(False)\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].set_ylim((y_score.shape[1], 0))\nax[1].set_yticks([y_score.shape[1]])\n\ndivider = make_axes_locatable(ax[1])\nax1Top = divider.append_axes("top", 0.5, sharex=ax[1])\nax1Top.xaxis.set_tick_params(labelbottom=False)\nax1Top.plot(y_score_mean)\nax1Top.set_title(\'sequence model type = {}\'.format(msig.sequence_type))\nax1Top.set_xlim((xmin, xmax))\nax1Top.set_ylim((-1, 1))\nax1Top.set_yticks((-1, 0, 1))\nax1Top.grid(True)\n\nax[2].imshow(\n    y_score_unshifted_clipped.T, \n    interpolation=\'nearest\', \n    cmap=plt.get_cmap(\'Spectral\'), \n    origin=\'upper\');\nax[2].spines[\'top\'].set_visible(False)\nax[2].set_xlim((xmin, xmax))\nax[2].set_xticks([])\nax[2].set_ylim((y_score_unshifted_clipped.shape[1], 0))\nax[2].set_yticks([y_score_unshifted_clipped.shape[1]])\n\ndivider = make_axes_locatable(ax[2])\nax2Top = divider.append_axes("top", 0.5, sharex=ax[2])\nax2Top.xaxis.set_tick_params(labelbottom=False)\nax2Top.plot(y_score_unshifted_clipped_mean)\nax2Top.set_title(\'{} window size = {}\'.format(window_type, msig.window_size))\nax2Top.set_xlim((xmin, xmax))\nax2Top.set_ylim((-1, 1))\nax2Top.set_yticks((-1, 0, 1))\nax2Top.grid(True)\n\nax[3].scatter(\n    xindex, \n    x_test_clipped,\n    marker=\'.\', \n    c=y_pred_colors)\nax[3].set_title(\'loss = {:<6.4f}, accuracy = {:<.2%}\'.format(*score))\nax[3].set_xlim((xmin, xmax))\nax[3].grid(True)\n\n# plt.draw()\nplt.savefig(os.path.join(msig.out_dir, \'prediction_analysis.png\'), bbox_inches=\'tight\', pad_inches=0.08)\n# plt.show()'}, {'identified': 'None', 'updated_code': '# precision-recall curve\ny_scores = -probas_gmm\nprecision_gmm, recall_gmm, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_gmm = metrics.auc(recall_gmm, precision_gmm)\nfpr_gmm, tpr_gmm, _ = metrics.roc_curve(y_true, y_scores)\nauroc_gmm = metrics.roc_auc_score(y_true, y_scores)\nprint("AUROC: %.2f, PR AUC: %.2f" % (auroc_gmm, pr_auc_gmm))'}, {'identified': 'None', 'updated_code': 'import sympy as sym\nfrom sympy import *\nA, U, S = symbols("A U S")\nalpha_UA, alpha_AU, alpha_US, alpha_SU  = symbols("alpha_UA alpha_AU alpha_US alpha_SU")\n\nmodel_dyn = [\n    alpha_UA*U - alpha_AU*A,\n    alpha_AU*A + alpha_SU*S - (alpha_UA + alpha_US)*U,\n    alpha_US*U - alpha_SU*S,\n    A + U + S - 1 # this equation sets the total population size to 1\n    ]\n\n# steady-state solution\nsol_dyn = solve(model_dyn, A, U, S)\n\n# functions for calculating the proportion of the population in each compartment at \n# steady state, given transition rates between compartments\ndyn_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A] + sol_dyn[S])\nU_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[U])\nA_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A])\nS_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[S])\n\nsol_dyn'}, {'identified': 'None', 'updated_code': 'reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300\nn_hidden2 = 50\nn_hidden3 = 50\nn_hidden4 = 50\nn_hidden5 = 50\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3")\n    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4")\n    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name="hidden5")\n    logits = tf.layers.dense(hidden5, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")'}, {'identified': 'None', 'updated_code': 'reset_graph()\n\nn_inputs = 28 * 28  # MNIST\nn_hidden1 = 300  # 재사용\nn_hidden2 = 50  # 재사용\nn_hidden3 = 50  # 재사용\nn_hidden4 = 20  # 새로 만듦!\nn_outputs = 10  # 새로 만듦!\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")'}, {'identified': 'None', 'updated_code': "allData = pd.read_csv('data/allData.csv')\nr1 = allData[allData['Round']==1]\nr2 = allData[allData['Round']==2]"}, {'identified': "['all_acc']", 'updated_code': "training_acc_file = 'accuracies/train_acc_list.p'\nvalid_acc_file = 'accuracies/valid_acc_list.p'\n\nwith open(training_acc_file, mode='rb') as f:\n    combined_train_acc_list = pickle.load(f)\nwith open(valid_acc_file, mode='rb') as f:\n    combined_valid_acc_list = pickle.load(f)"}, {'identified': "['a']", 'updated_code': 'import statsmodels.stats.api as sms\n\n# ValidRTCI\nsms.DescrStatsW(RTanalysis.Valid).tconfint_mean()'}, {'identified': "['a']", 'updated_code': "pd.read_csv('https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/ade_sales.csv')"}, {'identified': "['I']", 'updated_code': 'sigmas=[]\nsigma=0\nfor i in range(0,100):\n    sigma=0\n    for i in range (0,len(l)-1):\n        temp,temp2=integrate(f,l[i],l[i+1],1000,10000)\n        sigma+=temp2\n    sigmas.append(np.sqrt(sigma))\n\nplt.plot(np.arange(0,100,1),sigmas,\'r\')\nplt.ylabel(\'Sigma\')\nplt.xlabel(\'M\')\nplt.show()\nprint("Sigma of sigma=",np.sqrt(np.var(sigmas)))'}, {'identified': "['sig']", 'updated_code': 'import inspect\n\ndef add(x: int, y: int, *args, **kwargs) -> int:\n    return x + y\n\ninspect.signature(add)'}, {'identified': "['X_train', 'default_preprocessor']", 'updated_code': '# %%time\n\n# we need a custom pre-processor to extract correct field,\n# but want to also use default scikit-learn preprocessing (e.g. lowercasing)\n\ndef build_preprocessor(field):\n    field_idx = list(dataset.columns).index(field)\n    # if field == \'playlist_pid\': from IPython.core.debugger import set_trace; set_trace()\n    return lambda x: default_preprocessor(x[field_idx])\n\n\nvectorizer = FeatureUnion([\n    (\n        \'track_artist_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_artist_uri\'))),\n    (\n        \'track_album_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_album_uri\'))),\n    (\n        \'track_uri\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'track_uri\'))),\n\n    (\n        \'playlist_pid\',\n        CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r".+",\n            stop_words=None,\n            # max_features=50000,\n            preprocessor=build_preprocessor(\'playlist_pid\'))),\n\n    ("playlist_name",\n      CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r"(?u)\\b\\w+\\b",\n            stop_words=None,\n            analyzer = \'word\',\n            # max_features=50000,\n            preprocessor=build_preprocessor("playlist_name"))),\n    \n    ("playlist_description",\n      CountVectorizer(\n            ngram_range=(1, 1),\n            token_pattern=r"(?u)\\b\\w+\\b",\n            stop_words=None,\n            analyzer = \'word\',\n            # max_features=50000,\n            preprocessor=build_preprocessor("playlist_description"))),\n#     (\n#         \'track_pos\',\n#         CountVectorizer(\n#             ngram_range=(1, 1),\n#             token_pattern=r".+",\n#             stop_words=None,\n#             # max_features=50000,\n#             preprocessor=build_preprocessor(\'track_pos\'))),\n\n    (\'track_duration_ms\',\n     ItemSelector(list(dataset.columns).index(\'track_duration_ms\'))),\n])'}, {'identified': "['county']", 'updated_code': 'house_size = df_county_data["Median Income"]\nx_axis = np.arange(len(house_size))\n# Create a bar chart based upon the above data\ntick_locations = [value for value in x_axis]\n# plt.xticks(tick_locations, county, rotation= 90)\nplt.bar(x_axis, house_size, color="r", align="center")\nplt.title("County Median Income")\nplt.xlabel("Counties")\nplt.ylabel("Median Income Rate")\nplt.text(140, 120000, "Note:\\nMedian Income for all counties in NJ, NY, & PA.")\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Median Income1.png", bbox_inches = "tight")\nplt.show()'}, {'identified': "['X', 'accuracy', 'training_op', 'y']", 'updated_code': '_, _, _, _ = tf.get_collection("my_important_ops")'}, {'identified': "['init', 'saver']", 'updated_code': ''}, {'identified': "['grid_bayes', 'tiempo_bayes']", 'updated_code': "parametersNaiveBayes = {\n    'priors':priors\n}\n\n(_, _) = correr_y_mostrar(\n    GaussianNB(), \n    parametersNaiveBayes, \n    5, \n    5\n)"}, {'identified': 'None', 'updated_code': 'batch_size, height, width, channels = dataset.shape\n\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\nfilters[:, 3, :, 0] = 1  # vertical line\nfilters[3, :, :, 1] = 1  # horizontal line'}, {'identified': 'None', 'updated_code': 'A = A.tocsr()\nprint(type(A))\n\nb = np.random.rand(1000)'}, {'identified': 'None', 'updated_code': "all_coverage, cumulative_coverage = population_coverage(\n    mutation_fuzzer.population, http_program)\n\nimport matplotlib.pyplot as plt\nplt.plot(cumulative_coverage)\nplt.title('Coverage of urlparse() with random inputs')\nplt.xlabel('# of inputs')\nplt.ylabel('lines covered');"}, {'identified': 'None', 'updated_code': "import pandas as pd\nread_filename = './annotations_201712131226.csv'\n\ndf = pd.read_csv(read_filename+'.points', names=('filename', 'x1', 'y1', 'x2', 'y2', 'points'))"}, {'identified': "['KP', 'W', 'f_bar', 'var', 'y_giv_f']", 'updated_code': 'def GPC(x_new, x, y, kernel, theta):\n    K, KS, KSS = get_Ks(x_new, x, kernel, theta)\n\n    f = find_f(K, y)\n\n    f_bar = np.matmul(np.matmul(KS, np.linalg.inv(K)), f)\n    return f_bar.squeeze()'}, {'identified': 'None', 'updated_code': "y_true, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n                                     train_len=95, test_len=5,\n                                     save_hyp='plots/hyp_inlinecounter_shake.png',\n                                     save_diag='plots/diag_inlinecounter_shake.png',\n                                     save_resp='plots/resp_inlinecounter_shake.png')"}, {'identified': "['omega', 'theta']", 'updated_code': ''}, {'identified': "['l']", 'updated_code': "plt.plot(f/3e10, y_nolm_aer, label='AER')\nplt.plot(f/3e10, y_nolm_aer_arts, label='AER_ARTS')\nplt.plot(f/3e10, ty.physics.planck(f, 300), label='Planck')\nplt.ylabel('Radiance')\nplt.xlabel('Wavenumber')\nplt.legend()"}, {'identified': 'None', 'updated_code': None}, {'identified': "['save_path']", 'updated_code': 'extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run([training_op, extra_update_ops],\n                     feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)'}, {'identified': "['access_secret', 'access_token', 'consumer_key', 'consumer_secret']", 'updated_code': 'auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)'}, {'identified': "['grid', 'params', 'regressor', 'scoring_fnc']", 'updated_code': '# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Create a decision tree regressor object\n    # TODO: Create a dictionary for the parameter \'max_depth\' with a range from 1 to 10\n\n    # Transform \'performance_metric\' into a scoring function using \'make_scorer\' \n\n    # Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values \'regressor\', \'params\', \'scoring_fnc\', and \'cv_sets\' respectively.\n\n    # Fit the grid search object to the data to compute the optimal model\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_'}, {'identified': "['datapath']", 'updated_code': "#file contains data about links in nyc\nlinkspath = '/links.csv'\n#file contains data about intersections in nyc\nnodespath = '/nodes.csv'\n#link in nyc that contains enough info to analysis\nfull_links_ids_path = '/full_link_ids.txt'\n#kml file that contains boundaries of regions we are trying to parse\n#this file needs to be imported from the google maps\nregionspath = '/Manhattan_13.kml'\n\n#actual pandas dataframes\nlinks=pd.read_csv(linkspath)\nnodes=pd.read_csv(nodespath)\nnodes = nodes[['node_id','xcoord','ycoord']]"}, {'identified': 'None', 'updated_code': 'dw_obj = w.get_step_object(step=3, subset=subset_uuid).indicator_objects[indicator]'}, {'identified': 'None', 'updated_code': "waves_std = np.std(waves, axis=0)\ni_std = np.where(waves_std < 0.32245)[0]\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 6))\n\nax[0].plot(xindex, waves_std)\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=waves_std)\nax[1].set_xlim((xmin, xmax))\nax[1].grid(True)\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'std_dev_analysis.png'), bbox_inches='tight')"}, {'identified': "['reuse_vars', 'save_path']", 'updated_code': 'init = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    init.run()\n    restore_saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    saver.save(sess, "./my_new_model_final.ckpt")'}, {'identified': "['H']", 'updated_code': 'import numpy as np\n\nnp.array([[1., 0.]])'}, {'identified': "['app_path', 'os.environ']", 'updated_code': "os.chdir(os.getcwd())\nfilesep = '\\\\' if platform.system() == 'Windows' else '/'"}, {'identified': "['dfBabyDirt']", 'updated_code': "pd.read_csv('http://www.stat.berkeley.edu/~statlabs/data/babies.data', delim_whitespace=True)"}, {'identified': 'None', 'updated_code': "subjects = ['ec2', 'ec9', 'gp31', 'gp33']"}, {'identified': "['heterogeneity']", 'updated_code': 'k = 3\ninitial_centroids = get_initial_centroids(tf_idf, k, seed=0)\ncentroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n                                       record_heterogeneity=_, verbose=True)\nplot_heterogeneity(_, k)'}, {'identified': 'None', 'updated_code': '# remove class\nn_classes = 9\nclasses_to_keep = np.asarray([x for x in range(1, n_classes)])\n\nnames_keep = np.asarray(names)\nnames_keep = names_keep.tolist()\nprint("classes to keep: " + str(names_keep))'}, {'identified': 'None', 'updated_code': '#An example of how to use these functions to provide a directory listing.\ncmdstr = "ls"\nprint("The systemcall method doesn\'t show stdout when used in Jupyter notebook but does from a script.")\nretcode = systemcall(cmdstr)\nprint("")\nprint("With systemcall_pipe you can see the stdout from Jupyter notebook, and can use the results in variables:")\nstdout, stderr = systemcall_pipe(cmdstr)'}, {'identified': "['gamma', 'n', 'wavelen']", 'updated_code': ''}, {'identified': 'None', 'updated_code': '# 200 new values from x=0 to x=15\nn_new = 200\nX_new = np.linspace(0, 15, n_new)[:,None]\n\n# add the GP conditional to the model, given the new X values\nwith model:\n    f_pred = gp.conditional("f_pred", X_new)\n\n# Sample from the GP conditional distribution\nwith model:\n    pred_samples = pm.sample_ppc(trace, vars=[f_pred], samples=1000)'}, {'identified': "['x_values', 'y_values']", 'updated_code': ''}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'with tf.Session() as sess:                                              \n    init.run()                                                          \n    for epoch in range(n_epochs):                                       \n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):  \n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n            clip_weights.eval()\n            clip_weights2.eval()                                        \n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n        print(epoch, "검증 세트 정확도:", accuracy_val)                     \n\n    save_path = saver.save(sess, "./my_model_final.ckpt")              '}, {'identified': 'None', 'updated_code': 'with pm.Model() as model:\n    lambda_1 = pm.Exponential("lambda_1", 1.0)\n    lambda_2 = pm.Exponential("lambda_2", 1.0)\n    tau = pm.DiscreteUniform("tau", lower=0, upper=10)\n\nnew_deterministic_variable = lambda_1 + lambda_2'}, {'identified': "['ex']", 'updated_code': "flor.Experiment('risecamp_demo').__enter__()"}, {'identified': "['mapping']", 'updated_code': '# Encoding Functions\n\ndef MAP(X):\n    """Map all values to integer numbers."""\n    """NaN values are treated as a unique value."""\n    \n    # create an encoding for categorical vars\n    unique_elems = set(X)\n    return X.map({label:idx for idx, label in enumerate(unique_elems)}).astype(int)\n\ndef LOO(X):\n    """Perform Leave One Out counting for the features."""\n    \n    # map features to ordinal values first\n    X = MAP(X)\n    \n    # perform counts\n    return X.map({idx:(count-1) for idx, count in enumerate(np.bincount(X))}).astype(int)\n    \n\ndef OHE(df_cv, df_all, col_name, feature_names, feature_threshold=0.02):\n    """Map categorical values to a one hot encoding scheme."""\n    \n    X_cv = MAP(df_cv[col_name])\n    X_all = MAP(df_all[col_name])\n    \n    X_cv = X_cv.values.reshape(-1, 1)\n    X_all = X_all.values.reshape(-1, 1)\n    OHE = OneHotEncoder(sparse=False).fit(X_all)\n    X_cv_ohe = OHE.transform(X_cv)\n    X_all_ohe = OHE.transform(X_all)\n    \n    low_freq_features = []\n    for i in range(X_all_ohe.shape[1]):\n        new_feature = col_name + str(i)\n        \n        # determine the frequency of the categorical data value\n        freq = np.sum(X_all_ohe[:, i]) / X_all_ohe.shape[0]\n        if freq > feature_threshold:\n            df_cv[new_feature] = X_cv_ohe[:, i]\n            df_all[new_feature] = X_all_ohe[:, i]\n            feature_names.append(new_feature)\n        else:\n            low_freq_features.append(i)\n    \n    # aggregate low frequency features\n    if len(low_freq_features) > 0:\n        extra_label = col_name + str(X_all_ohe.shape[1])\n        feature_names.append(extra_label)\n        \n        X_all_extra = np.array([0 for x in range(X_all.shape[0])])\n        X_cv_extra = np.array([0 for x in range(X_cv.shape[0])])\n        \n        for i in low_freq_features:\n            for idx, val in enumerate(X_all_ohe[:, i]):\n                if val == 1:\n                    X_all_extra[idx] = 1\n            for idx, val in enumerate(X_cv_ohe[:, i]):\n                if val == 1:\n                    X_cv_extra[idx] = 1\n        \n        df_cv[extra_label] = X_cv_extra\n        df_all[extra_label] = X_all_extra                    \n            \n    feature_names.remove(col_name)\n    df_cv = df_cv.drop(col_name, axis=1)\n    df_all = df_all.drop(col_name, axis=1)\n    \n    return df_cv, df_all, feature_names'}, {'identified': "['df_valid']", 'updated_code': 'def distance_between(lat1, lon1, lat2, lon2):\n  # haversine formula to compute distance "as the crow flies".  Taxis can\'t fly of course.\n  dist = np.degrees(np.arccos(np.minimum(1,np.sin(np.radians(lat1)) * np.sin(np.radians(lat2)) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.cos(np.radians(lon2 - lon1))))) * 60 * 1.515 * 1.609344\n  return dist\n\ndef estimate_distance(df):\n  return distance_between(df[\'pickuplat\'], df[\'pickuplon\'], df[\'dropofflat\'], df[\'dropofflon\'])\n\ndef compute_rmse(actual, predicted):\n  return np.sqrt(np.mean((actual-predicted)**2))\n\ndef print_rmse(df, rate, name):\n  print ("{1} RMSE = {0}".format(compute_rmse(df[\'fare_amount\'], rate*estimate_distance(df)), name))\n\nFEATURES = [\'pickuplon\',\'pickuplat\',\'dropofflon\',\'dropofflat\',\'passengers\']\nTARGET = \'fare_amount\'\ncolumns = list([TARGET])\ncolumns.extend(FEATURES) # in CSV, target is the first column, after the features\ncolumns.append(\'key\')\ndf_train = pd.read_csv(\'data/taxi-train.csv\', header=None, names=columns)\nrate = df_train[\'fare_amount\'].mean() / estimate_distance(df_train).mean()\nprint ("Rate = ${0}/km".format(rate))\nprint_rmse(df_train, rate, \'Train\')\n#print_rmse(df_valid, rate, \'Valid\')  '}, {'identified': 'None', 'updated_code': 'elderSlope = array2[array3 == 1]\nelderArea = array1[array3 == 1]\nfoxSlope = array2[array3 == 2]\nfoxArea = array1[array3 == 2]\nhankSlope = array2[array3 == 3]\nhankArea = array1[array3 == 3]\ndrySlope = array2[array3 == 4]\ndryArea = array1[array3 == 4]\n\n#Band 1 = area\n#Band 2 = slope\n# Band 3 = watershed\n\n#watershed key:\n#Dry = 4\n#Hank = 3\n#Elder = 1\n#Fox = 2'}, {'identified': "['adpc_2012', 'adpc_2013', 'cov_2012', 'cov_2013', 'incsol', 'scrsol']", 'updated_code': '# Haringey\n# find steady state based on 2012 data\n\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - 0.267007002375, test_diag_fun(x)[1] - 0.0346976493046], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - 0.190544970144, test_diag_fun(x)[1] - 0.0184872060681], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nsol_haringey = odeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )'}, {'identified': "['pca11', 'pca12', 'pca21', 'pca22']", 'updated_code': "sparse_data = my_spca.transform(X)\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = sparse_data - sparse_data.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()"}, {'identified': 'None', 'updated_code': 'edge_cols = []\nfrustrated_edges = imbalance.keys()\nfor edge in G.edges(data=True):\n    if (edge[0],edge[1]) in frustrated_edges:\n        edge_cols.append("black")\n    elif edge[2]["sign"] == -1:\n        edge_cols.append("red")\n    else:\n        edge_cols.append("green")\n\nfig,ax = plt.subplots(figsize=(15,10))\nnx.draw_networkx(G, pos=nx.drawing.layout.bipartite_layout(G, nodes_one), ax=ax, \n                 with_labels=True, node_size=1000, node_color="white", edge_color=edge_cols)'}, {'identified': "['coco.simulate(...)']", 'updated_code': '#\nflux, flux_err = coco.simulate(b"SN2007uy", \n                    z_obs, 0.0, 0.0, 0.0, 3.1, \n                    mjdmax, mjd_to_sim, \n                    filters_to_sim)'}, {'identified': 'None', 'updated_code': "sales_price = train['SalePrice']\ngraph = sns.distplot(sales_price)"}, {'identified': "['corr_mat']", 'updated_code': "# Test to see if DW comp. is working\ncorr_mat = corr_tensor[30, :, :].copy()\n        \ncorr_mat[(corr_mat > -1*0.7) & (corr_mat < 0.7)] = 0\nG, density = make_graph(corr_mat, nodes, 'signed')"}, {'identified': "['plt']", 'updated_code': '# Re-run the model with the Bib numbers as a feature and for the 5K, 10K and 15K split times to predict 20K time\n\n### set up data for modeling\nX_20K = boston_clean[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\']]\ny_20K = boston_clean[\'20K Duration\'].values.reshape(-1, 1)\nprint(X_20K.shape, y_20K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_20K, X_test_20K, y_train_20K, y_test_20K = train_test_split(X_20K, y_20K, random_state=29)\n# X_train_20K.head()\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_20K = LinearRegression()\nmodel_20K.fit(X_train_20K, y_train_20K)\n\n# Make predictions\n\npredictions_20K = model_20K.predict(X_test_20K)\n\n# Plot the residuals\n\nplt.scatter(model_20K.predict(X_train_20K), model_20K.predict(X_train_20K) - y_train_20K, c="blue", label="Training Data")\nplt.scatter(model_20K.predict(X_test_20K), model_20K.predict(X_test_20K) - y_test_20K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_20K.min(), xmax=y_test_20K.max())\nplt.title("Residual Plot 20K")\nplt.savefig(\'model_20k.png\')\nplt.show()'}, {'identified': "['exp_name']", 'updated_code': 'regression_network = load_model(exps_path, exp_name, 100, camera_parameters_path)'}, {'identified': "['f']", 'updated_code': "lv_workspace.get_data_filter_object(step=1, subset='A')"}, {'identified': 'None', 'updated_code': "#use boxenplot to see the data\nax = sns.boxenplot(data=df['Revenue'], orient='h', color='#2ecc71')"}, {'identified': "['idx', 'pca11', 'pca12', 'pca21', 'pca22', 'revertedV']", 'updated_code': "# do the reduction but in a different way\ncov = X.T @ X /(X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\n\nproject_X = X @ vec\nproject_V = vec.T @ vec\n\ninversed_vec = np.linalg.inv(vec)\ninversed_vec = inversed_vec - inversed_vec.mean(1)\n\nrevert_X = project_X @ inversed_vec\n\ndef plot():\n    plt.figure(figsize=(15,5))\n\n    plt.subplot(131)\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\n    plt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    plt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n\n    plt.subplot(132)\n    plt.scatter(project_X[y == 0, 0], project_X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(project_X[y == 1, 0], project_X[y == 1, 1], color='blue', alpha=0.5)\n    plt.arrow(0, 0, *project_V[:,0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    plt.arrow(0, 0, *project_V[:,1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n\n    plt.subplot(133)\n    plt.scatter(revert_X[y == 0, 0], revert_X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(revert_X[y == 1, 0], revert_X[y == 1, 1], color='blue', alpha=0.5)\n    plt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    plt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n\n    plt.show()\n\nplot()"}, {'identified': "['burned_trace']", 'updated_code': 'with model:\n    # To Be Explained in Chapter 3!\n    step = pm.Metropolis()\n    trace = pm.sample(25000, step=step)'}, {'identified': 'None', 'updated_code': 'np.random.seed(1234)\nW = {user:np.random.rand(31) - 0.5 for user in users}\nb = {user:np.random.rand(1) - 0.5 for user in users}'}, {'identified': "['CMSpileUpMonitor', 'averagePileUp_multCollisionFrac', 'numberOfEventsRun262274_eventAccepted', 'removedFractionRun262274_ilterEfficiency']", 'updated_code': 'Image(filename = pathname + CMSpileUpMonitor)'}, {'identified': 'None', 'updated_code': 'X = np.hstack((X, np.ones((X.shape[0], 1))))\n\n#Initialize Weights to zero:\nw = np.zeros(X.shape[1])'}, {'identified': 'None', 'updated_code': 'class USZIPCodeRepository:\n    CACHE = {}\n\n    def __init__(self, data_url_prefix=\'https://raw.githubusercontent.com/yyu/GeoJSON-US/master\'):\n        self.data_url_prefix = data_url_prefix\n        self.geojson_url_prefix = f\'{data_url_prefix}/perZIPgeojson\'\n\n        self.refresh_zipcode_latlons(f\'{data_url_prefix}/ZIPCodesGazetteer.tsv\')\n        self.refresh_available_zipcodes(f\'{data_url_prefix}/perZIPgeojson/all_zipcodes.txt\')\n\n    def refresh_zipcode_latlons(self, url):\n        lines = [line.decode(\'UTF8\').strip() for line in urllib.request.urlopen(url).readlines()]\n        tsv = csv.DictReader(lines, delimiter=\'\\t\')\n        self.gazetteer = dict((d[\'GEOID\'], {\'lat\': float(d[\'INTPTLAT\']), \'lon\': float(d[\'INTPTLONG\'])}) for d in tsv)\n\n    def refresh_available_zipcodes(self, url):\n        lines = [zipcode.decode(\'UTF8\').strip() for zipcode in urllib.request.urlopen(url).readlines()]\n        self.zipcode_list = lines[1:]  # ignore the first line\n        self.zipcode_set = set(self.zipcode_list)\n\n    def make_url(self, zipcode):\n        return f\'{self.data_url_prefix}/perZIPgeojson/{zipcode[0]}/{zipcode[1]}/{zipcode[2]}/{zipcode}.json\'\n\n    def fetch_zipcode(self, zipcode):\n        \'\'\'returns a (dict, err) tuple where err could be a string for error message or None\'\'\'\n\n        url = self.make_url(zipcode)\n\n        if url in USZIPCodeRepository.CACHE:\n            return (USZIPCodeRepository.CACHE[url], None)\n\n        try:\n            s = urllib.request.urlopen(url).read()\n        except urllib.error.URLError as e:\n            return (None, \'failed to get \' + url, \':\', e.reason)\n\n        j = json.loads(s)\n        USZIPCodeRepository.CACHE[url] = j\n\n        return (j, None)\n\n    def fetch_zipcodes(self, *zipcodes):\n        d = {"type": "FeatureCollection", "features": []}\n\n        available_zipcodes = set(zipcodes) & self.zipcode_set\n\n        for z in available_zipcodes:\n            j, err = self.fetch_zipcode(z)\n\n            if j is not None:\n                d[\'features\'].append(j)\n\n        return d'}, {'identified': "['idx_to_symbol']", 'updated_code': 'def print_policy(policy, terminal_states):\n    \n    border_str = "\\u00B7 "\n    for i in range(policy.shape[0]):\n        border_str += "\\u2015 "\n    border_str += "\\u00B7 "\n    #print(border_str)\n    \n    for i in range(policy.shape[0]):\n        \n        string = ""\n        #string = "| "\n        for j in range(policy.shape[1]):\n            \n            if (i,j) in terminal_states:\n                string += \'\\u25A0 \'\n            else:\n                string += \'\\u2190\'+" "\n        \n        #string += "|"\n        print(string)\n    \n    #print(border_str)\n    \n    return'}, {'identified': "['mid_num']", 'updated_code': 'connection_all = []\nspecial_k = []\n\nfor k in range(len(mapIdx)):\n    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n    candA = all_peaks[limbSeq[k][0]-1]\n    candB = all_peaks[limbSeq[k][1]-1]\n\n    nA = len(candA)\n    nB = len(candB)\n    indexA, indexB = limbSeq[k]\n    if(nA != 0 and nB != 0):\n        connection_candidate = []\n        for i in range(nA):\n            for j in range(nB):\n                vec = np.subtract(candB[j][:2], candA[i][:2])\n                # print(\'vec: \',vec)\n                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n                # print(\'norm: \', norm)\n                vec = np.divide(vec, norm)\n                # print(\'normalized vec: \', vec)\n                startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n                               np.linspace(candA[i][1], candB[j][1], num=mid_num))\n                # print(\'startend: \', startend)\n                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n                                  for I in range(len(startend))])\n                # print(\'vec_x: \', vec_x)\n                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n                                  for I in range(len(startend))])\n                # print(\'vec_y: \', vec_y)\n                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                # print(score_midpts)\n                # print(\'score_midpts: \', score_midpts)\n                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n                \n                # print(\'score_with_dist_prior: \', score_with_dist_prior)\n                criterion1 = len(np.nonzero(score_midpts > param[\'thre2\'])[0]) > 0.8 * len(score_midpts)\n                # print(\'score_midpts > param["thre2"]: \', len(np.nonzero(score_midpts > param[\'thre2\'])[0]))\n                criterion2 = score_with_dist_prior > 0\n                \n                if criterion1 and criterion2:\n                    # print(\'match\')\n                    # print(i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2])\n                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n                # print(\'--------end-----------\')\n        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n        # print(\'-------------connection_candidate---------------\')\n        # print(connection_candidate)\n        # print(\'------------------------------------------------\')\n        connection = np.zeros((0,5))\n        for c in range(len(connection_candidate)):\n            i,j,s = connection_candidate[c][0:3]\n            if(i not in connection[:,3] and j not in connection[:,4]):\n                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n                # print(\'----------connection-----------\')\n                # print(connection)\n                # print(\'-------------------------------\')\n                if(len(connection) >= min(nA, nB)):\n                    break\n\n        connection_all.append(connection)\n    else:\n        special_k.append(k)\n        connection_all.append([])'}, {'identified': 'None', 'updated_code': 'trials = 1000\nwith Timer() as t:\n    for i in range(trials):\n        try:\n            url = fuzzer()\n            result = http_program(url)\n            print("Success!")\n        except ValueError:\n            pass\n\nduration_per_run_in_seconds = t.elapsed_time() / trials\nduration_per_run_in_seconds'}, {'identified': 'None', 'updated_code': '# build random forest\nroot_nodes = random_forest_build(dataset, 1000, .3, n_jobs=-1)'}, {'identified': 'None', 'updated_code': '# Split the training data into separate train and test sets\n(X_train, X_test, y_train, y_test) = train_test_split(data, labels, test_size=0.25, random_state=0)\n\n# Convert the labels (letters) into one-hot encodings that Keras can work with\nle = LabelEncoder().fit(np.stack(list(y_train) + list(y_test)))\ny_train = le.transform(y_train)\ny_test = le.transform(y_test)'}, {'identified': "['X_test_normal', 'X_train_normal', 'X_valid_normal']", 'updated_code': '### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n### converting to grayscale, etc.\n### Feel free to use as many code cells as needed.\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.utils import shuffle\n# import cv2\n\n# for img in X_train:\n#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nX_train_normal = np.array(X_train/255 - 0.5)\nX_valid_normal = np.array(X_valid/255 - 0.5)\nX_test_normal = np.array(X_test/255 - 0.5)\n\nEPOCHS = 15\nBATCH_SIZE = 128'}, {'identified': 'None', 'updated_code': 'with tf.Session() as sess:\n    init.run()\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = new_saver.save(sess, "./my_new_model_final.ckpt")'}, {'identified': 'None', 'updated_code': 'import numpy as np\n\ndef debug_vectors(v1, v2):\n    print("v1: {0}, v2: {1}".format(v1, v2))\n    print("Angle: {0}".format(v_angle(v1, v2)))\n    print("Perpendicular: {0}". format(v_perpendicular(v1, v2, 4)))\n    print("Parallel: {0}".format(v_parallel(v1, v2, 3)))\n    print("Same Orientation: {0}".format(v_same_orientation(v1, v2)))\n    print("Dot product: {0}\\n".format(np.dot(v1, v2)))\n\ndef debug_all_samples(): \n    for sample in samples[0x10] + samples[0x80]:\n        va = np.array(sample[1])\n        vb = np.array(sample[0])\n        o = np.array(sample[3])\n        s = np.array(sample[2])\n\n        v1 = (va - o) / np.linalg.norm((va - o))\n        v2 = (vb - o) / np.linalg.norm((vb - o))\n\n        debug_vectors(v1, v2)\n\ndebug_all_samples()\n\nva = np.array([1638, 213]) \nvb = np.array([1486, 68]) \no = np.array([1628, 69])\n\nreal_origin = np.array([0, 0])\ntranslate = real_origin - o\n\not = o + translate\n\nvat = (va + translate)\nvbt = (vb + translate)\n\ndebug_vectors(va - o, vb - o)\ndebug_vectors(vat, vbt)\n\nprint("va: {0}, vb: {1}".format(va,vb))\nprint("vat: {0}, vbt: {1}".format(vat,vbt))'}, {'identified': 'None', 'updated_code': 'import pymc3 as pm\n\nwith pm.Model() as model:\n    parameter = pm.Exponential("poisson_param", 1.0)\n    data_generator = pm.Poisson("data_generator", parameter)'}, {'identified': 'None', 'updated_code': "a1_x = np.array(df['a1_x']/10000)[1500:]\na1_y = np.array(df['a1_y']/10000)[1500:]\na1_z = np.array(df['a1_z']/10000)[1500:]\na2_x = np.array(df['a2_x']/10000)[1500:]\na2_y = np.array(df['a2_y']/10000)[1500:]\na2_z = np.array(df['a2_z']/10000)[1500:]"}, {'identified': "['K']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "axes = plt.subplots(10, 1, figsize=(14, 34))[1]\n\nfor ax, country in zip(axes, top_10_list):\n    data = df[df['Country'] == country]\n    sns.countplot(data=data, x='YearsCodingProf', palette='Paired', ax=ax, order=data['YearsCodingProf'].value_counts().index)\n    ax.set_title('Years coding professionally in {}'.format(country), fontsize=16)\n    #plt.xticks(rotation='vertical')\n    sns.despine(left=True)\nplt.subplots_adjust(hspace=.6)"}, {'identified': "['end_points', 'inception_saver', 'logits']", 'updated_code': 'from tensorflow.contrib.slim.nets import inception\nimport tensorflow.contrib.slim as slim\n\nreset_graph()\n\nX = tf.placeholder(tf.float32, shape=[None, height, width, channels], name="X")\ntraining = tf.placeholder_with_default(False, shape=[])\nwith slim.arg_scope(inception.inception_v3_arg_scope()):\n    inception.inception_v3(X, num_classes=1001, is_training=training)'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'with tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))'}, {'identified': 'None', 'updated_code': 'fig, axs = plt.subplots(1,2, figsize=(10,4))\naxs[0].imshow(img_advi)\naxs[0].set_title("segmented image (ADVI)")\naxs[1].hist(y, bins=K);\naxs[1].set_title("cluster assignments (ADVI)")\nplt.tight_layout()'}, {'identified': 'None', 'updated_code': "trace_illum_x = []\ntrace_illum_y = []\ntrace_illum_z = []\n\ntrace_viewing_x = []\ntrace_viewing_y = []\ntrace_viewing_z = []\n\nfor i, row in df_viewing.iterrows():\n    illum_x, illum_y, illum_z = polar_to_euclidean(row['illum_theta'],row['illum_phi'])\n    view_x, view_y, view_z = polar_to_euclidean(row['view_theta'],row['view_phi'])\n    \n    line_x = [illum_x, 0, view_x]\n    line_y = [illum_y, 0, view_y]\n    line_z = [illum_z, 0, view_z]\n    \n    trace_illum_x.append(illum_x)\n    trace_illum_y.append(illum_y)\n    trace_illum_z.append(illum_z)\n    \n    trace_viewing_x.append(view_x)\n    trace_viewing_y.append(view_y)\n    trace_viewing_z.append(view_z)"}, {'identified': "['BetaDelta', 'path']", 'updated_code': None}, {'identified': "['k', 'q']", 'updated_code': '# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\ntf.random.set_seed(9527)\n\n# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\nv = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\nv'}, {'identified': 'None', 'updated_code': None}, {'identified': "['rows', 'vector']", 'updated_code': 'def createRow(x):    \n    new_list = x.tolist()\n    new_list.insert(0,x.name)\n    rows.append(new_list)\n\nnormal_dataFrame.groupby("Hotel Name")["Normal"].transform(createRow)  # group by returns for each hotel a list of his normalized prices\npd.DataFrame.from_records(rows)'}, {'identified': 'None', 'updated_code': '# window the data using your windowing function\nwindow_size = 7\nX, y = window_transform_series(series=dataset, window_size=window_size)'}, {'identified': 'None', 'updated_code': "# Generate hypothesis data\ndef gen_hyp_data(model, N, text_len=500):\n    texts, hiddens, hyps = [], [], []\n    for i in range(N):\n        text, hidden = generate(model, '\\n\\n', text_len, 0.8, True)\n        hidden = hidden.reshape(hidden.shape[0], -1)\n        hyp = hypothesis_inlinecounter(text)\n        hiddens.append(hidden)\n        hyps.append(hyp)\n        texts.append(text)\n    return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n\n# Generate train and test data\ntrain_texts, train_hyps, train_hiddens = gen_hyp_data(model_gru, 500)\ntest_texts, test_hyps, test_hiddens = gen_hyp_data(model_gru, 2)"}, {'identified': 'None', 'updated_code': 'def broad_greater_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has two masthead lights and one running light\n    visible for a starboard orientation.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 126)\n        shorter_masthead_light = np.random.randint(130, 186)\n        left_endpoint = np.random.randint(20, 126)\n        right_endpoint = np.random.randint(125, 211)\n        running_light_height_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 16)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = left_endpoint + light_width\n        short_masthead_height = shorter_masthead_light + light_width\n        short_masthead_width = right_endpoint + light_width\n        running_light_start = shorter_masthead_light + running_light_height_diff\n        running_light_width = running_light_start + light_width\n        if right_endpoint - left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(left_endpoint - 20, left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(left_endpoint, right_endpoint)\n        running_light_area = running_light_loc + light_width\n        new_view[taller_masthead_light:tall_masthead_height, left_endpoint:tall_masthead_width] = white\n        new_view[shorter_masthead_light:short_masthead_height, right_endpoint:short_masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images'}, {'identified': "['augment_data']", 'updated_code': "from keras.callbacks import ModelCheckpoint \nfrom keras.callbacks import ReduceLROnPlateau\n\n### TODO: specify the number of epochs that you would like to use to train the model.\n\nBATCH_SIZE = 32\nepochs = 8\n\n### Do NOT modify the code below this line.\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\n# reduce LR\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=3, min_lr=0.0005, verbose=1)\n\nprint('Training... without data augmentation')\nhistory = scratch_model.fit(train_tensors, train_targets, \n      validation_data=(valid_tensors, valid_targets),\n      epochs=epochs, \n      batch_size=BATCH_SIZE, \n      callbacks=[checkpointer], \n      verbose=1)\n\nprint('Done training')\nshow_history_graph(history)"}, {'identified': "['Ms', 'Ps']", 'updated_code': 'P = np.diag([500., 49.])\n_, _ = run(count=50, R=10, Q=0.01, P=P)'}, {'identified': 'None', 'updated_code': 'p, r, f, s = metrics.precision_recall_fscore_support(y_test_flattened[filter_items], y_pred_flattened[filter_items])'}, {'identified': 'None', 'updated_code': 'y_pred = tf.nn.softmax(logits)'}, {'identified': 'None', 'updated_code': 'with pm.Model() as model:\n    parameter = pm.Exponential("poisson_param", 1.0, testval=0.5)\n\nprint("\\nparameter.tag.test_value =", parameter.tag.test_value)'}, {'identified': 'None', 'updated_code': 'from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, oob_score=True, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\n\ny_pred_rf = rnd_clf.predict(X_test)'}, {'identified': "['batch_size', 'input_shape', 'network_type', 'sequence_type']", 'updated_code': "# start off with simplest case for proof of concept\nwave1_coeffs = {\n    'amplitude': {'mean': 0.5, 'delta': 0.05}, \n    'frequency': {'mean': 1.0, 'delta': 0.1},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'A',\n    'color': '#0000ff'\n}\nwave2_coeffs = {\n    'amplitude': {'mean': 0.75, 'delta': 0.075}, \n    'frequency': {'mean': 3.0, 'delta': 0.3},\n    'offset': {'mean': 0.0, 'delta': 0.1}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'B',\n    'color': '#ff0000',\n#     'time': {'t_min': 0, 't_max': 5, 'n_timestamps': 601, 'noise_type': 'pareto', 'pareto_shape': 1.3},\n}\nwave3_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0.1}, \n    'frequency': {'mean': 8.0, 'delta': 0.8},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'C',\n    'color': '#00ff00'\n}\nwave4_coeffs = {\n    'amplitude': {'mean': 1.4, 'delta': 0.1}, \n    'frequency': {'mean': 12.0, 'delta': 1.2},\n    'offset': {'mean': 0.0, 'delta': 0.2}, \n    'phase': {'mean': 0.0, 'delta': 1.0},\n    'name': 'D',\n    'color': '#ff00ff'\n}\n\nmwave_coeffs = {\n    'amplitude': {'mean': 1.0, 'delta': 0}, \n    'frequency': {'mean': 1.0, 'delta': 0}, \n    'offset': {'mean': 0, 'delta': 0},\n    'phase': {'mean': 0, 'delta': 1}, \n    'name': 'mixed_wave',\n    'time': {'t_min': 0, 't_max': 2, 'n_timestamps': 4096, 'delta': 0}\n}\n\nsigs_coeffs = [wave1_coeffs, wave2_coeffs, wave3_coeffs, mwave_coeffs, wave4_coeffs]\n\nfeatures=('x', 'dxdt')[0]\nwindow_size = 4096\nwindow_type = 'sliding'\n\nmsig = MixedSignal(\n    sigs_coeffs, \n    *features,\n    window_size=window_size, \n    window_type=window_type\n)\n\nmsig.generate()\nn_classes = msig.n_classes\nn_features = msig.n_features\n\nprint(msig.inputs.shape)\nprint(msig.mixed_signal.shape)\nprint(msig.one_hots.shape)\nprint(msig.labels.shape)\nprint(msig.n_timestamps)\nprint(msig.n_samples)"}, {'identified': "['N']", 'updated_code': 'K = 30'}, {'identified': 'None', 'updated_code': 'def delta_f(list_of_terms, x_value, delta_x):\n    pass'}, {'identified': "['list_op']", 'updated_code': 'generator = {v.multidegree() : [v]}'}, {'identified': 'None', 'updated_code': 'all_waypoints = ["USS Alabama, Battleship Parkway, Mobile, AL",\n                 "Grand Canyon National Park, Arizona",\n                 "Toltec Mounds, Scott, AR",\n                 "San Andreas Fault, San Benito County, CA",\n                 "Cable Car Museum, 94108, 1201 Mason St, San Francisco, CA 94108",\n                 "Pikes Peak, Colorado",\n                 "The Mark Twain House & Museum, Farmington Avenue, Hartford, CT",\n                 "New Castle Historic District, Delaware",\n                 "White House, Pennsylvania Avenue Northwest, Washington, DC",\n                 "Cape Canaveral, FL",\n                 "Okefenokee Swamp Park, Okefenokee Swamp Park Road, Waycross, GA",\n                 "Craters of the Moon National Monument & Preserve, Arco, ID",\n                 "Lincoln Home National Historic Site Visitor Center, 426 South 7th Street, Springfield, IL",\n                 "West Baden Springs Hotel, West Baden Avenue, West Baden Springs, IN",\n                 "Terrace Hill, Grand Avenue, Des Moines, IA",\n                 "C. W. Parker Carousel Museum, South Esplanade Street, Leavenworth, KS",\n                 "Mammoth Cave National Park, Mammoth Cave Pkwy, Mammoth Cave, KY",\n                 "French Quarter, New Orleans, LA",\n                 "Acadia National Park, Maine",\n                 "Maryland State House, 100 State Cir, Annapolis, MD 21401",\n                 "USS Constitution, Boston, MA",\n                 "Olympia Entertainment, Woodward Avenue, Detroit, MI",\n                 "Fort Snelling, Tower Avenue, Saint Paul, MN",\n                 "Vicksburg National Military Park, Clay Street, Vicksburg, MS",\n                 "Gateway Arch, Washington Avenue, St Louis, MO",\n                 "Glacier National Park, West Glacier, MT",\n                 "Ashfall Fossil Bed, Royal, NE",\n                 "Hoover Dam, NV",\n                 "Omni Mount Washington Resort, Mount Washington Hotel Road, Bretton Woods, NH",\n                 "Congress Hall, Congress Place, Cape May, NJ 08204",\n                 "Carlsbad Caverns National Park, Carlsbad, NM",\n                 "Statue of Liberty, Liberty Island, NYC, NY",\n                 "Wright Brothers National Memorial Visitor Center, Manteo, NC",\n                 "Fort Union Trading Post National Historic Site, Williston, North Dakota 1804, ND",\n                 "Spring Grove Cemetery, Spring Grove Avenue, Cincinnati, OH",\n                 "Chickasaw National Recreation Area, 1008 W 2nd St, Sulphur, OK 73086",\n                 "Columbia River Gorge National Scenic Area, Oregon",\n                 "Liberty Bell, 6th Street, Philadelphia, PA",\n                 "The Breakers, Ochre Point Avenue, Newport, RI",\n                 "Fort Sumter National Monument, Sullivan\'s Island, SC",\n                 "Mount Rushmore National Memorial, South Dakota 244, Keystone, SD",\n                 "Graceland, Elvis Presley Boulevard, Memphis, TN",\n                 "The Alamo, Alamo Plaza, San Antonio, TX",\n                 "Bryce Canyon National Park, Hwy 63, Bryce, UT",\n                 "Shelburne Farms, Harbor Road, Shelburne, VT",\n                 "Mount Vernon, Fairfax County, Virginia",\n                 "Hanford Site, Benton County, WA",\n                 "Lost World Caverns, Lewisburg, WV",\n                 "Taliesin, County Road C, Spring Green, Wisconsin",\n                 "Yellowstone National Park, WY 82190"]'}, {'identified': 'None', 'updated_code': '# The three players\' heights, in meters:\nklay =  2.01 # Klay Thompson is 6\'7"\nsteph = 1.91 # Steph Curry is 6\'3"\nkevin = 2.06 # Kevin Durant is officially 6\'9", but many suspect that he is taller.\n             # (Further complicating matters, membership of the "Splash Triplets" \n             #  is disputed, since it was originally used in reference to \n             #  Klay Thompson, Steph Curry, and Draymond Green.)\n\n# We\'d like to look at all 3 pairs of heights, compute the absolute\n# difference between each pair, and then find the smallest of those\n# 3 absolute differences.  This is left to you!  If you\'re stuck,\n# try computing the value for each step of the process (like the\n# difference between Klay\'s height and Steph\'s height) on a separate\n# line and giving it a name (like klay_steph_height_diff).\n\nklay_steph_height_diff = abs(klay - steph)\nklay_kevin_height_diff = abs(klay - kevin)\nsteph_kevin_height_diff = abs(steph - kevin)\n\nsmallest_height_diff = min(klay_steph_height_diff, klay_kevin_height_diff, steph_kevin_height_diff)\n\nprint(smallest_height_diff)'}, {'identified': 'None', 'updated_code': 'threshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\nclip_weights = tf.assign(weights, clipped_weights)'}, {'identified': "['arg']", 'updated_code': '@print_log("Awesome function")\ndef do_something(_, **other_args):\n    print("Doing something")\n    return'}, {'identified': 'None', 'updated_code': '#Setup Axis\nstate=df_county_data[\'State\']\ncounty = df_county_data["County Name"]\ngrad_rate = df_county_data["Graduation Rate"]\nx_axis = np.arange(len(state))\n\n# Create a bar chart based upon the above data\n# tick_locations = [value for value in x_axis]\n# plt.xticks(tick_locations, county, rotation= 45)\nplt.bar(x_axis, grad_rate, color="b", align="center")\nplt.title("County Graduation Rates")\nplt.xlabel("Counties")\nplt.ylabel("Graduation Rates")\nplt.text(140, 0.6, "Note:\\nGraduation Rates for all counties in NJ, NY, & PA.")\n\n# Save an image of the chart and print it to the screen\nplt.savefig("Images/County_Graduation_Rates.png", bbox_inches = "tight")\n# plt.zoom=10\nplt.show()'}, {'identified': "['ax', 'fig']", 'updated_code': 'losses = np.array(losses)\nplt.plot(losses.T[0], label=\'Discriminator\')\nplt.plot(losses.T[1], label=\'Generator\')\nplt.title("Training Losses")\nplt.legend()'}, {'identified': 'None', 'updated_code': 'from helpers import *\ndef test_your_least_squares():\n    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n    x, mean_x, std_x = standardize(height)\n    y, tx = build_model_data(x, weight)\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # least square or grid search: TODO\n    # this code should compare the optimal weights obtained \n    # by least squares vs. grid search\n    # ***************************************************\n    raise NotImplementedError'}, {'identified': "['a']", 'updated_code': "sms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Valid).tconfint_mean()"}, {'identified': "['result_test', 'result_train', 'tot']", 'updated_code': "#train.drop(['sentiment','seven_days'],axis=1,inplace=True)\n#test.drop(['sentiment','seven_days'],axis=1,inplace=True)\n\nfor string in ['share','comment','zan','content_len','链接','//@','@','#','【','《','\\[']:\n    temp = []\n    for i in test[string+'_histogram']:\n        if isinstance(i,int):\n            temp.append(np.zeros(shape=8))\n        else:\n            temp.append(i[0])\n    temp = np.asarray(temp)\n    train.drop(string+'_histogram',axis=1,inplace=True)\n    test.drop(string+'_histogram',axis=1,inplace=True)\n\ntrain.drop(['pid','uid'],inplace=True,axis=1)\ntest.drop(['pid','uid'],inplace=True,axis=1)\n\ntrain_y = train[['share','comment','zan']].values\ntrain.drop(['share','comment','zan'],axis=1,inplace=True)\ntrain_x = train.values\ntest_x  = test.values\n\nfor i in result_train:\n    train_x = np.c_[train_x,i]\nfor i in result_test:\n    test_x = np.c_[test_x,i]\n\nnp.save('processed_data/train3_np',train_x)\nnp.save('processed_data/test3_np',test_x)\nnp.save('processed_data/target3_np',train_y)"}, {'identified': 'None', 'updated_code': 'reset_graph()\n\nX = tf.placeholder(tf.float32, shape=(None, height, width, 1))\nfeature_maps = tf.constant(fmap)\nconvolution = tf.nn.conv2d(X, feature_maps, strides=[1,1,1,1], padding="SAME")'}, {'identified': 'None', 'updated_code': "fig, ax = plt.subplots(figsize=(8, 6))\n\nn_bins = 20\nax.hist(old_faithful_df.std_waiting, bins=n_bins, color=blue, lw=0, alpha=0.5);\n\nax.set_xlabel('Standardized waiting time between eruptions');\nax.set_ylabel('Number of eruptions');"}, {'identified': 'None', 'updated_code': 'reset_graph()\n\nimport tensorflow as tf\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\n\ntraining = tf.placeholder_with_default(False, shape=(), name=\'training\')\n\nhidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")\nbn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\nbn1_act = tf.nn.elu(bn1)\n\nhidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")\nbn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\nbn2_act = tf.nn.elu(bn2)\n\nlogits_before_bn = tf.layers.dense(bn2_act, n_outputs, name="outputs")\nlogits = tf.layers.batch_normalization(logits_before_bn, training=training,\n                                       momentum=0.9)'}, {'identified': 'None', 'updated_code': "xmin = 0\nxmax = x_test.shape[1]\nxindex = np.arange(xmin, xmax)\nnrows=3\nfig, ax = plt.subplots(nrows=nrows, ncols=1, figsize=(15, 3*nrows))\n\nax[0].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=y_true_colors)\nax[0].set_title('epoch = {}'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\n\nax[1].scatter(\n    xindex, \n    x_test[0, :, 0], \n    marker='.', \n    c=y_pred_colors)\nax[1].set_title('loss = {:<6.4f}, accuracy = {:<.2%}'.format(*score))\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].grid(True)\n\nlegend_labels = []\nfor wave in msig.waves:\n    ax[2].plot(\n        xindex,\n        wave.sample_full, \n        color=wave.color,\n        zorder=1)\n    legend_labels.append(wave.name)\n\nax[2].scatter(\n    xindex[i_fail], \n    x_test[0, i_fail, 0], \n    marker='o', \n    c=y_pred_colors[i_fail],\n    zorder=2)\n\nax[2].set_xlim((xmin, xmax))\nax[2].grid(True)\nax[2].legend(legend_labels)\n\n# plt.draw()\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'prediction_analysis.png'), bbox_inches='tight')\n# plt.show()"}, {'identified': "['an_integer']", 'updated_code': "int('hello')"}, {'identified': "['f', 'u']", 'updated_code': "from math import sqrt\nfrom numpy.random import randn\n\ndef univariate_filter(x0, P, R, Q):\n    f = KalmanFilter(dim_x=1, dim_z=1, dim_u=1)\n    f.x = np.array([[x0]])\n    f.P *= P\n    f.H = np.array([[1.]])\n    f.F = np.array([[1.]])\n    f.B = np.array([[1.]])\n    f.Q *= Q\n    f.R *= R\n    return f\n\ndef plot_1d_2d(xs, xs1d, xs2d):\n    plt.plot(xs1d, label='1D Filter')\n    plt.scatter(range(len(xs2d)), xs2d, c='r', alpha=0.7, label='2D Filter')\n    plt.plot(xs, ls='--', color='k', lw=1, label='track')\n    plt.title('State')\n    plt.legend(loc=4)\n    plt.show()\n    \ndef compare_1D_2D(x0, P, R, Q, vel):\n    # storage for filter output\n    xs, xs1, xs2 = [], [], []\n\n    # 1d KalmanFilter\n    univariate_filter(x0, P, R, Q)\n\n    #2D Kalman filter\n    f2D = pos_vel_filter(x=(x0, vel), P=P, R=R, Q=0)\n    pos = 0 # true position\n    for i in range(100):\n        pos += vel\n        xs.append(pos)\n\n        # control input u - discussed below\n        f1D.predict(u=u)\n        f2D.predict()\n        \n        z = pos + randn()*sqrt(R) # measurement\n        f1D.update(z)\n        f2D.update(z)\n        \n        xs1.append(f1D.x[0])\n        xs2.append(f2D.x[0])\n    plt.figure()\n    plot_1d_2d(xs, xs1, xs2)\n\ncompare_1D_2D(x0=0, P=50., R=5., Q=.02, vel=1.) "}, {'identified': "['P', 'eig', 'us_np', 'xs_np']", 'updated_code': 'with tf.Session() as sess:\n    A_np, B_np, Q_np, R_np = sess.run([A, B, Q, R])\n\nK, _, _ = controlpy.synthesis.controller_lqr_discrete_time(A_np, B_np, Q_np, R_np)\nx_np = x0\nu_np = (-K@x_np)\nloss_np = 0\nfor i in range(T):\n    loss_np += x_np.T@Q_np@x_np + u_np.T*R_np*u_np\n    x_np = A_np@x_np + B_np@u_np\n    u_np = (-K@x_np)'}, {'identified': 'None', 'updated_code': "from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')/255\ntest_tensors  = paths_to_tensor(test_files).astype('float32')/255"}, {'identified': 'None', 'updated_code': 'np.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)'}, {'identified': "['c']", 'updated_code': 'data = np.array([[1, 3, 5], [2, 2, 3], [5, 3, 1]])'}, {'identified': 'None', 'updated_code': "df = pd.merge(df1, df2, on=['item', 'time'])   #df1とdf2を項目'item'と'time'でマージ"}, {'identified': 'None', 'updated_code': "_, (l_ax, r_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(16, 6))\n\nK = 50\nalpha = 10.\n\nbeta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)\n\nl_ax.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n          label='DP sample CDFs')\nl_ax.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75)\nl_ax.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF')\n\nl_ax.set_title(r'$\\alpha = {}$'.format(alpha))\nl_ax.legend(loc=2)\n\nK = 200\nalpha = 50.\n\nbeta = sp.stats.beta.rvs(1, alpha, size=(N, K))\nw = np.empty_like(beta)\nw[:, 0] = beta[:, 0]\nw[:, 1:] = beta[:, 1:] * (1 - beta[:, :-1]).cumprod(axis=1)\n\nomega = P0.rvs(size=(N, K))\n\nsample_cdfs = (w[..., np.newaxis] * np.less.outer(omega, x_plot)).sum(axis=1)\n\nr_ax.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n          label='DP sample CDFs')\nr_ax.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75)\nr_ax.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF')\n\nr_ax.set_title(r'$\\alpha = {}$'.format(alpha))\nr_ax.legend(loc=2)"}, {'identified': 'None', 'updated_code': "# Loop over features (polygons) in the shapefile\nfor f in tqdm(feats):\n    # Rasterize the polygon into an array\n    rasterized_image = features.rasterize([(shape(f['geometry']),1)],\n                                          out_shape=out_shape,\n                                          transform=new_aff,\n                                          fill=0,\n                                          all_touched=True)\n\n    # Extract from the xarray where the rasterized polygon is\n    region = data.where(rasterized_image == 1)\n    \n    # Combine x and y into a new dimension called allpoints and calculate the mean over it\n    # and then convert to a dataframe with an appropriate name\n    res = region.stack(allpoints=['x','y']).mean(dim='allpoints').to_dataframe(name=f['properties']['LSOA11CD'])\n    \n    # Append to the list of data frames so we can concatenate them all at the end\n    dfs.append(res)\n    \nstats = pd.concat(dfs, axis=1)"}, {'identified': 'None', 'updated_code': "fig = plt.figure(figsize=(16,5))\nax = fig.add_subplot(121); ax.axis('off')\nay = fig.add_subplot(122); ay.axis('off')\n\nP1 = Ps[0]; P2 = Ps[1]\nK1 = Ks[0]; K2 = Ks[1]\nRt1 = Rts[0]; Rt2 = Rts[1]\n\nannot1 = Annotations[0][0]\nannot2 = Annotations[1][0]\n\nindv_left = annot1[0]\nindv_right = annot2[1]\n\nlefthand_left = indv_left[1][0]\nlefthand_right = indv_right[1][0]\n\n# -------------\nax.imshow(X[0,0])\nax.scatter(lefthand_left[0], lefthand_left[1], color='red')\n\n# -------------\nay.imshow(X[1,0])\nay.scatter(lefthand_right[0], lefthand_right[1], color='red')\n\nplt.show()"}, {'identified': 'None', 'updated_code': 'n_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = saver.save(sess, "./my_model_final.ckpt")'}, {'identified': "['clf_df']", 'updated_code': '# Create DensityForest instance\nDensityForest(max_depth=2, min_subset=.1, n_trees=100,\n                       subsample_pct=.1, n_jobs=-1, verbose=10,\n                       ig_improvement=.4)'}, {'identified': 'None', 'updated_code': "images_seq, labels_seq = loadGapData('data/gapdet/large/',\n                                     slider=slider_size,\n                                     seq=True)"}, {'identified': 'None', 'updated_code': '# Train the model\nAdaModel = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                             n_estimators=100, learning_rate=0.05)\nAdaModel = AdaModel.fit(X_train, y_train)\ny_pred = AdaModel.predict(X_train)'}, {'identified': "['twitter_archive']", 'updated_code': "# Read in csv file as a Pandas DataFrame\ntwitter_archive = pd.read_csv('twitter-archive-enhanced.csv')"}, {'identified': 'None', 'updated_code': "#Plot visualized correlation matrix\ndef plot_corr(data,size=20):\n    corr = data.corr()\n    fig, ax = plt.subplots(figsize=(25, 25))\n    cax = ax.matshow(corr)\n    fig.colorbar(cax)\n    plt.xticks(range(len(corr.columns)), corr.columns,rotation='vertical');\n    plt.yticks(range(len(corr.columns)), corr.columns);\n    plt.show()\nplot_corr(data2)"}, {'identified': "['accuracies_training', 'accuracies_validation', 'aucs_training', 'aucs_validation']", 'updated_code': 'arbol = MiClasificadorArbol(max_depth=3)\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test))\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train))\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train\ndf["Accuracy (validación)"] = accuracy_validation\ndf["AUC ROC (training)"]    = roc_train\ndf["AUC ROC (validación)"]  = roc_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()'}, {'identified': "['gb']", 'updated_code': 'y = pd.DataFrame(y_train)\ny.columns = [\'class\']\ny[\'index\'] = y.index\n\n#print(gb[\'index\'].agg([np.min,np.max]))\n\nnum_image = 5\nfor c in range(n_classes):\n    # filter \n    yf = y[y[\'class\']== c].sample(num_image)\n    idx = (yf[\'index\'])\n    fig = plt.figure()\n    for i in range(num_image):\n        fig.add_subplot(1,num_image,i+1)\n        plt.imshow(X_train[idx][i])\n        plt.text(0,1,signnames[str(c)],color=\'w\',backgroundcolor=\'r\', fontsize=5, weight="bold") \n        plt.axis(\'off\')\n    plt.tight_layout()\n    plt.show()'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': "X = Variable(name='X', num_states=2)\nX_prior = Factor(name='p(X)',\n                 f=np.array([0.95, 0.05]),\n                 neighbours=[X])\n\nZ = Variable(name='Z', num_states=2)\nZ_prior = Factor(name='p(Z)',\n                 f=np.array([0.8, 0.2]),\n                 neighbours=[Z])\n                 \nY = Variable(name='Y', num_states=2)\nf_Y_cond = [\n    [ #Y = 0\n        [ # X = 0\n          0.9999, # Z = 0\n          0.3     # Z = 1\n        ],\n        [ # X = 1\n          0.1,  # Z = 0\n          0.01  # Z = 1\n        ]\n    ],\n    [  #Y = 1\n        [ # X = 0\n          0.0001, # Z = 0\n          0.7     # Z = 1\n        ],\n        [ # X = 1\n          0.9,  # Z = 0\n          0.99  # Z = 1\n        ]\n    ]\n]\nY_cond = Factor(name='p(Y |X, Z)',\n                 f=np.array(f_Y_cond),\n                 neighbours=[Y, X, Z])"}, {'identified': "['scale']", 'updated_code': "for i in range(len(multiplier)):\n    \n    imageToTest = cv.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv.INTER_CUBIC)\n    imageToTest_padded, pad = padRightDownCorner(imageToTest, 8, 128)\n\n    transposeImage = np.transpose(np.float32(imageToTest_padded[:,:,:]), (2,0,1))/256 - 0.5\n    testimage = transposeImage\n    cmodel = mx.mod.Module(symbol=sym, label_names=[])\n    cmodel.bind(data_shapes=[('data', (1,3,\n                                   testimage.shape[1],testimage.shape[2]))])\n    cmodel.init_params(arg_params=arg_params, aux_params=aux_params)\n    onedata = DataBatch(mx.nd.array([testimage[:,:,:]]), 0)\n    \n    cmodel.forward(onedata)\n    result=cmodel.get_outputs()\n    heatmap = np.moveaxis(result[1].asnumpy()[0], 0, -1)\n\n\n    heatmap = cv.resize(heatmap, (0,0), fx=model['stride'], fy=model['stride'], interpolation=cv.INTER_CUBIC)\n    heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n    heatmap = cv.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv.INTER_CUBIC)\n        \n    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n    \n   \n    f = plt.figure(i)\n    plt.imshow(oriImg[:,:,[2,1,0]])\n    ax2 = plt.imshow(heatmap[:,:,18], alpha=.5)\n    f.show()"}, {'identified': "['X_dev_dog', 'X_train_dog']", 'updated_code': "X_train_dog = X_train_feature.drop('AnimalType_Dog', 1)\nX_dev_dog = X_dev_feature.drop('AnimalType_Dog', 1)\n\nmodel3 = Sequential([\n    Dense(32, input_shape=(12,)),\n    Dropout(0.1),  \n    Activation('sigmoid'),\n    Dense(5),\n    Activation('softmax'),\n])\n\nmodel3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel3.fit(np.array(X_train_dog), y_train_hot, epochs=10, batch_size=32)"}, {'identified': 'None', 'updated_code': 'logits = LeNetTrafficSign(x, n_classes)\nprint(logits)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)'}, {'identified': 'None', 'updated_code': "negative_samples = pd.merge(\n    pd.merge(\n        tracks_df, playlist_map_df_negative, left_index=True, right_on='track_uri'),\n    playlist_df,\n    on='playlist_pid')"}, {'identified': 'None', 'updated_code': 'with pm.Model() as model:\n    theta = pm.Exponential("theta", 2.0)\n    data_generator = pm.Poisson("data_generator", theta)'}, {'identified': 'None', 'updated_code': '# construct pipeline\npipe_dt = make_pipeline(\n    MinMaxScaler(), # used to normalize data onto a similar scale\n    SelectPercentile(), # used to filter out features that add noise\n    DecisionTreeRegressor()\n)\n\n# create the parameter grid for hyperparameter tuning\nparam_grid_dt = {\n    \'selectpercentile__percentile\':range(5, 30, 5), # what upper percentile of features to take\n    \'decisiontreeregressor__max_features\':["auto", "sqrt", "log2", None], # the number of features to conside when splitting\n    \'decisiontreeregressor__max_depth\':range(1, 10), # maximum depth of the decision tree\n    \'decisiontreeregressor__min_samples_leaf\':range(1, 4) # minimum number of samples required to be at a leaf node\n}\n\n# perform grid search of pipeline\ndt_grid = GridSearchCV(pipe_dt, param_grid_dt)\n\n# use results to create model on training data\ndt_grid.fit(train_features, train_outcome)\n\n# find the best parameters from the grid search\ndt_best_params = dt_grid.best_params_\n\n# find the score of our model on the test data\ndt_grid_score = dt_grid.score(test_features, test_outcome)\n\n# find the mean absolute error of our model on the test data\ndt_mae = mean_absolute_error(dt_grid.predict(test_features), test_outcome)\n\n# find the explained variance score of our model on the test data\ndt_evs = explained_variance_score(dt_grid.predict(test_features), test_outcome)'}, {'identified': 'None', 'updated_code': '# Separa o dataset em conjunto treino e teste\ntrain_data = keystrokes[:split_data_idx]\ntest_data = keystrokes[split_data_idx:]'}, {'identified': "['root']", 'updated_code': 'create_decision_tree(dataset)'}, {'identified': 'None', 'updated_code': 'n_outputs = len(flower_classes)\n\nwith tf.name_scope("new_output_layer"):\n    flower_logits = tf.layers.dense(prelogits, n_outputs, name="flower_logits")\n    Y_proba = tf.nn.softmax(flower_logits, name="Y_proba")'}, {'identified': 'None', 'updated_code': "colunasTeste=[\n    'PassageiroId',\n    'Idade',\n    'FaixaEtaria',\n    'Classe',\n    'PortoEmbarqueNum',\n    'ParentesIrmao',\n    'ParentesFilhos',\n    'SexoNum',\n    'SaudacaoNum',\n    'PassagemPreco',\n    'FamiliaQtde',\n    'EstaSozinho',\n    'TemCabine',\n    'FamiliaQtde'\n]"}, {'identified': 'None', 'updated_code': "# TODO add features for polar coordinate values where the nose is the origin\n# Name these 'polar-rr', 'polar-rtheta', 'polar-lr', and 'polar-ltheta'\n# Note that 'polar-rr' and 'polar-rtheta' refer to the radius and angle\n\nfeatures_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']"}, {'identified': "['degree_df', 'degree_frequencies1', 'degree_frequencies2', 'degree_frequencies3', 'gig_bet_degree', 'gig_comp_degree', 'gig_degree_degree']", 'updated_code': "df_hues = ['Todos','Sin distinguidos Betweenness', 'Sin distinguidos Grado']\ndegree_info = []\nfor k,v in degree_frequencies1.iteritems(): degree_info.append((k,v,df_hues[0]))\nfor k,v in degree_frequencies2.iteritems(): degree_info.append((k,v,df_hues[1]))\nfor k,v in degree_frequencies3.iteritems(): degree_info.append((k,v,df_hues[2]))"}, {'identified': "['cov', 'idx', 'inverse', 'pca11', 'pca12', 'pca21', 'pca22']", 'updated_code': "# Do EVD to see the projection matrix\nval, vec = np.linalg.eigh(X.T @ X /(X.shape[0] - 1))\nval = val[np.argsort(val)[::-1]]\nvec = vec[:,np.argsort(val)[::-1]]\nproject_X = X @ vec\nproject_V = vec.T @ vec\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(141); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True); \n\nplt.subplot(142); \nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *project_V[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *project_V[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True); \n\ninverse = np.linalg.inv(vec)\ninverse = inverse - inverse.mean(0,keepdims=True) \nrevert_X = project_X @ inverse\nrevertedV = project_V @ inverse.T\n\nplt.subplot(143); \nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True); \n\ninverse = np.linalg.inv(vec)\ninverse = inverse - inverse*inverse.std(0,keepdims=True)\nrevert_X = project_X @ inverse\nrevertedV = project_V @ inverse.T\n\nplt.subplot(144); \nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *revertedV[:,0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nplt.arrow(0, 0, *revertedV[:,1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True); \n\nplt.show()"}, {'identified': 'None', 'updated_code': "print('\\n============== LSTM MODEL ==============\\n')\ntext, hiddens = generate(model_lstm, '\\n\\n', 500, 0.8, True)\nprint(text)\n\nprint('\\n============== GRU MODEL ===============\\n')\ntext, hiddens = generate(model_gru, '\\n\\n', 500, 0.8, True)\nprint(text)"}, {'identified': 'None', 'updated_code': '# To be explained in Chapter 3!\nwith model:\n    step = pm.Metropolis(vars=[p])\n    trace = pm.sample(40000, step=step)\n    burned_trace = trace[15000:]'}, {'identified': 'None', 'updated_code': '# Create datapoints between X_min and X_max to visualize the line of best fit\nX_best_fit = np.arange(X.numpy().min(), X.numpy().max(), 0.001)[:,None]'}, {'identified': 'None', 'updated_code': 'dfBabies = dfBabyDirt[(dfBabyDirt["bwt"] != 999) & (dfBabyDirt["gestation"] != 999) & (dfBabyDirt["parity"] != 9) & (dfBabyDirt["height"] != 99) & (dfBabyDirt["weight"] != 999) & (dfBabyDirt["smoke"] != 9)]'}, {'identified': 'None', 'updated_code': "# perform D reduction\ncov        = X.T @ X /(X.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\nidx        = np.argsort(val)[::-1]; val = val[idx]; vec = vec[:,idx]\n\nvec_reduced= np.zeros_like(vec)\nvec_reduced[:,:1] = vec[:,:1]\nval_reduced= val.copy()\nval_reduced[-1:]= 0\n\nproject_X  = X   @ vec_reduced;                      project_V  = vec_reduced.T @ vec_reduced\nrevert_X   = project_X @ np.linalg.inv(vec_reduced+0.0001) ;revertedV  = project_V.T @ np.linalg.inv(vec_reduced+0.0001)\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(131); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.subplot(132); \nplt.scatter(project_X[y==0, 0], project_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(project_X[y==1, 0], project_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *project_V[:,0] * val_reduced[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *project_V[:,1] * val_reduced[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.subplot(133); \nplt.scatter(revert_X[y==0, 0], revert_X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(revert_X[y==1, 0], revert_X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow   (0, 0, *revertedV[:,0] * val_reduced[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow   (0, 0, *revertedV[:,1] * val_reduced[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()"}, {'identified': 'None', 'updated_code': None}, {'identified': "['cases_bestfit_10000', 'cases_bestfit_15000', 'cases_bestfit_15000_allgenes']", 'updated_code': 'print(\'Loading case data ...\')\n\n#cases_800                    = pd.read_csv("pancancer_case_features_800.csv")\n#cases_1000                   = pd.read_csv("pancancer_case_features_1000.csv")\n#cases_1500                   = pd.read_csv("pancancer_case_features_1500.csv")\n#cases_bestfit_8000           = pd.read_csv("pancancer_case_features_bestfit_8000_topgenes_1000.csv")\n#cases_bestfit_10000          = pd.read_csv("pancancer_case_features_bestfit_10000_topgenes_2000.csv")\n#cases_bestfit_15000          = pd.read_csv("pancancer_case_features_bestfit_15000_topgenes_3000.csv")\n#cases_bestfit_15000_allgenes = pd.read_csv("pancancer_case_features_bestfit_15000_topgenes_None.csv")\ncases_allgenes               = pd.read_csv("pancancer_case_features_all.csv")\nall_data = {\n    #\'800\':                     getDataAndLabels(cases_800),\n    #\'1000\':                    getDataAndLabels(cases_1000),\n    #\'1500\':                    getDataAndLabels(cases_1500),\n    #\'best_fit_10000\':          getDataAndLabels(cases_bestfit_10000),\n    #\'best_fit_15000\':          getDataAndLabels(cases_bestfit_15000),\n    #\'best_fit_15000_allgenes\': getDataAndLabels(cases_bestfit_15000_allgenes),\n    \'genes_all\':               getDataAndLabels(cases_allgenes)\n}\nprint("done.")'}, {'identified': 'None', 'updated_code': "train_csv = pd.read_csv('train.csv', sep = ',', encoding = 'UTF-8')\ntest_csv = pd.read_csv('test.csv', sep = ',', encoding = 'UTF-8')"}, {'identified': 'None', 'updated_code': "from sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english',\n                        max_df=.1,\n                        max_features=5000)\nX = count.fit_transform(df['review'].values)"}, {'identified': "['mdl']", 'updated_code': "from docplex.mp.model import Model\n\nModel(name='portfolio_miqp')"}, {'identified': "['j', 's0', 's1']", 'updated_code': "# Code specific to window_type == sliding\n\n# j = 10\n# s0 = slice(0, j)\n# s1 = slice(-j, -1)\n# s0 = s1\n# print(y_hat[s0, s1])\n# print(y_true[s0, s1])\n# print(y_true_value[s0, s1])\n# print(y_pred[s0, s1])\n# print(y_pred_value[s0, s1])\n# print(y_penalty[s0, s1])\n# print(y_score[s0, s1])\n# print(np.min(y_score), np.max(y_score))\n\nif msig.sequence_type == 'many2many':\n    y_score_mean = y_score.sum(axis=1) / y_score.shape[1]\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\n    y_score_unshifted_clipped_mean = y_score_unshifted_clipped.sum(axis=1) / y_score.shape[1]\nelse:\n    y_score_mean = y_score.sum(axis=1) / y_score.shape[1]\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size-1:]\n    y_score_unshifted_clipped_mean = y_score_unshifted_clipped.sum(axis=1) / y_score.shape[1]"}, {'identified': "['grads_and_vars']", 'updated_code': "####### Defining network #######\n# input: state x\n# output: control u\n\ninput_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\nfc1 = tf.layers.dense(inputs=input_layer, units=1, activation=tf.nn.tanh, name='fc1', reuse=tf.AUTO_REUSE)\nu = tf.layers.dense(inputs=fc1, units=1, activation=tf.nn.tanh, name='fc_out', reuse=tf.AUTO_REUSE)\n# u = tf.layers.dense(inputs=input_layer, units=1, name='u_out_layer', reuse=tf.AUTO_REUSE)\n\n### LOSS FUNCTION ### \nloss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n\n# xs = tf.identity(x, name='xs')\n# us = tf.constant(0, name='us')\nxs = x\nus = u\n\n# cond = lambda i, x, l, xs, us: i < T\n\n# def body(i, x, l, xs, us):\n#     next_i = i+1\n#     next_x = tf.add(tf.matmul(A, x), tf.multiply(u,B))\n#     next_l = tf.add(l,\n#                     tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x),\n#                            tf.matmul(tf.transpose(u), tf.multiply(R, u))))\n#     next_xs = tf.concat(xs, next_x)\n#     next_us = tf.concat(us, u)\n#     return (next_i, next_x, next_l, next_xs, next_us)\n\n# i, xloss_f, traj_f = tf.while_loop(cond, body, \n#                                    loop_vars=[tf.constant(0), x, loss, xs, us],\n#                                    shape_invariants=[tf.TensorShape([1,]), tf.TensorShape([2, 1]), \n#                                                      tf.TensorShape([1,]) , tf.TensorShape([2, None]), \n#                                                      tf.TensorShape([1, None])])\n# train = tf.train.GradientDescentOptimizer(0.01).minimize(xloss_f.loss)\n\nfor i in range(T):\n    # LQR loss \n#     x_term = tf.matmul(tf.matmul(tf.transpose(x), Q), x, name='x_term')\n#     u_term = tf.matmul(tf.transpose(u), tf.multiply(R, u), name='u_term')\n#     loss = tf.add(loss, tf.add(x_term, u_term), name='loss')  # accumulate loss\n    \n    # Dynamics: advancing the system dynamics\n    Ax = tf.matmul(A, x, name='Ax'+str(i))\n    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n\n    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n    \n#     u = tf.layers.dense(inputs=tf.transpose(x), units=1, name='u_out_layer', reuse=True)\n    \n    fc1 = tf.layers.dense(inputs=tf.transpose(x), units=1, name='fc1', reuse=True)\n    u = tf.layers.dense(inputs=fc1, units=1, name='fc_out', reuse=True)\n    \n    xs = tf.concat([xs, x], 1)\n    us = tf.concat([us, u], 1)\n    \nopt = tf.train.GradientDescentOptimizer(0.0001)\ntrain = opt.minimize(loss)"}, {'identified': "['x_plot']", 'updated_code': ''}, {'identified': "['detNames']", 'updated_code': 'gROOT.ProcessLine(\'HistogramOperations ops\')\ngROOT.ProcessLine(\'lightTables.setBirksParams(1.0,6.90)\')\n\nfor detNum, detName in detNames.iteritems():\n    params = CalibParams(calPath+calNames[detNum])\n    gROOT.ProcessLine(\'vector<TH1*> phs{1} = ops.loadHistograms("33MeVTa_{0}_ls_{1}_fittedPSDCut.root")\'.format(runNum,detNum))\n    gROOT.ProcessLine(\'ops.applyCalibration(phs{0}[1],{1},{2})\'.format(detNum, params.a, params.b))\n    gROOT.ProcessLine(\'TFile *tgt{0} = new TFile("33MeVTa_{0}_ls_{1}_calibFittedPSDCut.root","recreate")\'.format(runNum,detNum))\n    gROOT.ProcessLine(\'phs{0}[1]->Rebin(3)\'.format(detNum))\n    gROOT.ProcessLine(\'phs{0}[1]->Draw()\'.format(detNum))    \n    gROOT.ProcessLine(\'phs{0}[1]->Write()\'.format(str(detNum)))\n    pause()'}, {'identified': 'None', 'updated_code': None}, {'identified': "['x']", 'updated_code': ''}, {'identified': "['batch_size', 'n_epochs']", 'updated_code': ''}, {'identified': "['times']", 'updated_code': "# Create the mfusg headfile object\nheadfile = os.path.join(modelpth, '{0}.hds'.format(modelname))\nheadobj = flopy.utils.HeadFile(headfile)\nmfusghead = headobj.get_data(totim=times[-1])"}, {'identified': "['predictions_F40K']", 'updated_code': '# Re-run the model with ONLY female runners, and  features of the 5K, 10K 15K 20K 25K, half and 35K split times to predict 40K time\n\n### set up data for modeling\nX_F40K = boston_females[[\'Bib\', \'Age\',\'Official Time Duration\', \'F\', \'M\', \'Temp (F)\', \'5K Duration\', \'10K Duration\', \'15K Duration\', \'20K Duration\',\'Half Duration\', \'25K Duration\', \'30K Duration\', \'35K Duration\']]\ny_F40K = boston_females[\'40K Duration\'].values.reshape(-1, 1)\nprint(X_F40K.shape, y_F40K.shape)\n\n# split the data into test and train subsets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_F40K, X_test_F40K, y_train_F40K, y_test_F40K = train_test_split(X_F40K, y_F40K, random_state=29)\n# X_train_F40K.head()\n\n# Create a linear regression model and fit it to the training data\n\nfrom sklearn.linear_model import LinearRegression\nmodel_F40K = LinearRegression()\nmodel_F40K.fit(X_train_F40K, y_train_F40K)\n\n# Plot the residuals\n\nplt.scatter(model_F40K.predict(X_train_F40K), model_F40K.predict(X_train_F40K) - y_train_F40K, c="blue", label="Training Data")\nplt.scatter(model_F40K.predict(X_test_F40K), model_F40K.predict(X_test_F40K) - y_test_F40K, c="orange", label="Testing Data")\nplt.legend()\nplt.hlines(y=0, xmin=y_test_F40K.min(), xmax=y_test_F40K.max())\nplt.title("Residual Plot Female Runners 40K")\nplt.savefig(\'model_F40K.png\')\nplt.show()'}, {'identified': "['adpc_2014', 'cov_2014', 'parms']", 'updated_code': '[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2014, test_diag_fun(x)[1] - adpc_2014], \n    [0.09, 0.25] \n    )\ninc = incsol\nscr = scrsol\n\n# solve, 2013-2014\nsol_13_14 = odeint(dydt, \n       sol_12_13[999,:], \n       linspace(0,10,1000), \n       args = (parms,)\n      )'}, {'identified': "['Y']", 'updated_code': "# tranform each class\nY1 = np.matmul(X1,v2)\nY2 = np.matmul(X2,v2)\n\n## show me dont tell me\nax = plt.gca()\nax.hist(Y1,color='blue', alpha=0.5, label='1')\nax.hist(Y2,color='yellow', alpha=0.5, label='2')\nplt.legend(loc='upper right')\nplt.xlabel('y')"}, {'identified': "['scale']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'class MyScaler():\n\n    def __init__(self, cols):\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        self.ss = StandardScaler()\n        self.ss.fit(X[self.cols])\n        return self\n    \n    def transform(self, X):\n        return self.ss.transform(X[self.cols])'}, {'identified': "['test_uid', 'train_uid']", 'updated_code': "train_corpus, test_corpus = preprocessing.clean_corpus()\ntest_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\ntrain_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n\ntrain_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\ntest_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n\n_, _ = preprocessing.encode_label()\n\npreprocessing.bag_of_word(train_corpus['corpus'].values, test_corpus['corpus'].values, min_df=10)"}, {'identified': 'None', 'updated_code': 'x = df_county_data["Speak a language other than English"]\ny = df_county_data["Graduation Rate"] \nplt.scatter(x, y, color="g", marker="o", alpha=0.9) \n\n#Calculate and add R2 value\nmask = ~np.isnan(x) & ~np.isnan(y)\n\n\n#Add regression line\nsns.regplot(df_county_data["Speak a language other than English"], \n              df_county_data["Graduation Rate"], color=\'r\',label = "Speak a language other than English" )\n\n# Incorporate the other graph properties\nplt.title("High School Graduation Rates and ESL by County")\nplt.ylabel("Graduation Rate")\nplt.xlabel("Speak a language other than English") \n\nplt.legend(loc=\'best\')\nplt.grid(True)\nsns.set_style(\'whitegrid\')\nplt.text(65, 0.925, "Note:\\nAreas with one or more Foreign languages beside English language \\ntend to have a Lower graduation rate.")\n\nplt.savefig("Images/County_Grad_Speak a language other than English3.png", bbox_inches = "tight")\nplt.show()'}, {'identified': "['atoms_D']", 'updated_code': 'if ld is not None:\n    dictenc(ae.D)'}, {'identified': 'None', 'updated_code': 'with tf.Session() as sess:\n    saver.restore(sess, "./my_model_final.ckpt")\n\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, "검증 세트 정확도:", accuracy_val)\n\n    save_path = saver.save(sess, "./my_new_model_final.ckpt")    '}, {'identified': "['betweenness_dict']", 'updated_code': 'nx.algorithms.centrality.betweenness_centrality(gig_comp_graph)'}, {'identified': 'None', 'updated_code': 'predictions_5K = model_5K.predict(X_test_5K)'}, {'identified': "['m']", 'updated_code': 'plt.figure(figsize=(11, 4))\nfor subplot, learning_rate in ((121, 1), (122, 0.5)):\n    sample_weights = np.ones(m)\n    for i in range(5):\n        plt.subplot(subplot)\n        svm_clf = SVC(kernel="rbf", C=0.05, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n        y_pred = svm_clf.predict(X_train)\n        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n        plt.title("learning_rate = {}".format(learning_rate), fontsize=16)\n\nplt.subplot(121)\nplt.text(-0.7, -0.65, "1", fontsize=14)\nplt.text(-0.6, -0.10, "2", fontsize=14)\nplt.text(-0.5,  0.10, "3", fontsize=14)\nplt.text(-0.4,  0.55, "4", fontsize=14)\nplt.text(-0.3,  0.90, "5", fontsize=14)\nsave_fig("boosting_plot")\nplt.show()'}, {'identified': 'None', 'updated_code': '# Devemos especificar o dispositivo que vai rodar as operações da rede.\nwith tf.device("/device:GPU:0"):\n    # As variáveis x e y são placeholders para os vetores de entrada e\n    # labels (classes) respectivamente.\n    x = tf.placeholder(tf.float32, shape=[None, 31])\n    y = tf.placeholder(tf.float32, shape=[None, 51])\n\n    # W1 é a matriz que vai levar o input para um estado intermedirário\n    # de processamento da rede.\n    # b1 é o vetor responsável pela translação nesse novo espaço.\n    W1 = tf.get_variable("W1", shape=[31, 64], initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.zeros([64]), name="b1")\n    \n    # A variável layer1 armazena o valor da primeira camada da rede. Isso\n    # significa que layer1 utiliza W1 e b1, bem como uma função de ativação\n    # (não-linearidade) para computar o valor da primeira camada da rede.\n    layer1 = tf.nn.relu(tf.matmul(x,W1) + b1)\n    \n    # Analogamente, W2 e b2 serão utilizados para gerar a segunda camada da\n    # rede (camada final).\n    W2 = tf.get_variable("W2", shape=[64, 51], initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.zeros([51]), name="b2")\n\n    \n    # A variável y_ armazena a última camada da rede utilizando W2 e b2 para\n    # calculá-la.\n    y_ = tf.matmul(layer1,W2) + b2\n\n    # Aplicamos o softmax em y_ e definimos como função de custo a cross-entropy.\n    # O tensorflow tem uma versão otimizada para realizar essas operações em uma\n    # única função (o que ajuda a reduzir o tempo de treino).\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_))\n\n    # Por fim, definimos o otimizador como o Adam.\n    train_step = tf.train.AdamOptimizer().minimize(cross_entropy)'}, {'identified': 'None', 'updated_code': 'fig, ax = plt.subplots(figsize=(7, 3));\nQ1 = [18, 58];\nlabels = [\'Yes\',\'No\']\ncolors = [\'lightcoral\', \'lightskyblue\'];\npatches, texts = plt.pie(Q1, colors=colors, startangle=90)\nplt.legend(patches, labels, loc=\'lower left\')\nplt.title(\'Q1: Noticed Any Regularities, Sequences, or Pairs?\', fontsize=17,fontweight="bold");\nplt.axis(\'equal\');\nplt.show()'}, {'identified': "['column_count', 'df_2', 'dff', 'fig', 'l']", 'updated_code': 'def plot_categorical_count(df, column, title=\'\', limit=2, xtick_rotation=\'horizontal\'):\n    sns.barplot(x=df[column].value_counts()[:limit].index, y=df[column].value_counts()[:limit].values, palette=\'Paired\')\n    sns.despine(left=True)\n    plt.title(title, fontsize=16)\n    plt.xticks(rotation=xtick_rotation)\n\ndef plot_heatmap(df, variable):\n    for name, group in top10_df.groupby(\'Country\'):\n        dff = pd.DataFrame(group[variable].value_counts() / group[variable].count()) \n        dff[\'Country\'] = name\n        dff[\'rate\'] = dff.index\n        sns.heatmap(dff.pivot_table(index=\'Country\', columns=\'rate\'), cmap="YlGnBu", linewidths=.3)'}, {'identified': "['a']", 'updated_code': "sms.DescrStatsW(RTrunanalysis.loc[RTrunanalysis['Run'] == 0].Invalid).tconfint_mean()"}, {'identified': 'None', 'updated_code': 'reset_graph()\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 50\nn_outputs = 10\n\nlearning_rate = 0.01\nmomentum = 0.9\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")\n\nwith tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")\n\nwith tf.name_scope("loss"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name="loss")\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n    training_op = optimizer.minimize(loss)    \n\nwith tf.name_scope("eval"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))'}, {'identified': 'None', 'updated_code': 'moviesavg_std = moviesstd.mean(axis=1).round(decimals=5)\nuseravg_std = moviesstd.mean().round(decimals=5)\n\nuseravg_std'}, {'identified': "['allowed_tels', 'dl1_parameters_filename', 'infile', 'max_events']", 'updated_code': 'cal = CameraCalibrator()'}, {'identified': 'None', 'updated_code': '# Plot the revenues for different months and years\nax = sns.lineplot(y=\'Revenue\', x=df.index.year, hue=df.index.month, data=df, palette="ch:r=-.5,l=.75", legend=False)'}, {'identified': 'None', 'updated_code': 'test_ratio = 0.2\ntrain_size = int(len(flower_paths_and_classes) * (1 - test_ratio))\n\nnp.random.shuffle(flower_paths_and_classes)\n\nflower_paths_and_classes_train = flower_paths_and_classes[:train_size]\nflower_paths_and_classes_test = flower_paths_and_classes[train_size:]'}, {'identified': "['batch_size', 'n_epochs']", 'updated_code': ''}, {'identified': "['band1', 'band2', 'band3']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'he_init = tf.variance_scaling_initializer()\nhidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                          kernel_initializer=he_init, name="hidden1")'}, {'identified': "['incsol', 'scrsol']", 'updated_code': '# Dudley\n# find steady state based on 2012 data\n\ncov_2012 = 0.0750667240187\nadpc_2012 = 0.0057129570304\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\ncov_2013 = 0.238873910562\nadpc_2013 = 0.0199612670162\n[_, _] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nsol_dudley = odeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )'}, {'identified': "['random_knn', 'tiempo_random_KNN']", 'updated_code': "from scipy.stats import uniform\n\nparametersKNN = {\n    'n_neighbors' : randint(1, 360),\n    'weights'     : ['uniform', 'distance']\n}\n\ncorrer_randomized_y_mostrar(KNeighborsClassifier(), parametersKNN, 5, 10, 200)\n\nverTiempo(tiempo_KNN, tiempo_random_KNN)"}, {'identified': 'None', 'updated_code': 'fig, ax = plt.subplots(figsize=(8,4))\ndfTitanic.hist(column="Age", ax=ax);\nprint(dfTitanic["Age"].describe())'}, {'identified': "['kernel_sums', 'pairwise_kernel_', 'r']", 'updated_code': "scores = []\nfor rep in range(number_of_replicates):\n    env.reset()\n    done = False\n    episode_score = 0.0\n    while not done:\n        action, _, _ = \\\n            model_smoothed_fitted_q(env, gamma, RandomForestRegressor, number_of_value_iterations, transition_model_fitter,\n                                    pairwise_kernels_=None, kernel_sums=None, smoothing_method='mse')\n        _, r, done = env.step(action)\n        episode_score += r\n    scores.append(episode_score)\n    print('score: {}'.format(score))\nprint('mean score: {} se: {}'.format(np.mean(scores), np.std(scores) / np.sqrt(number_of_replicates)))"}, {'identified': 'None', 'updated_code': '# Load pickled data\nimport pickle\n\n# TODO: Fill this in based on where you saved the training and testing data\n\ntraining_file = "../traffic-signs-data/train.p"\nvalidation_file="../traffic-signs-data/valid.p"\ntesting_file = "../traffic-signs-data/test.p"\n\nwith open(training_file, mode=\'rb\') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode=\'rb\') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode=\'rb\') as f:\n    test = pickle.load(f)\n    \nX_train, y_train = train[\'features\'], train[\'labels\']\nX_valid, y_valid = valid[\'features\'], valid[\'labels\']\nX_test, y_test = test[\'features\'], test[\'labels\']'}, {'identified': 'None', 'updated_code': "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3))\n\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes#[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(\n            vsig.timestamps[vsig.window_size-1:], \n            vsig.mixed_signal[vsig.window_size-1:], \n            marker='o', \n            c=y_pred_colors, \n            cmap=plt.get_cmap('coolwarm'), \n            vmin=-1, \n            vmax=1\n        )\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n        \n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, 'output', g + 1))\n#     plt.savefig(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, 'gen', str(g + 1)]) + '.png'))\n\nplt.show()"}, {'identified': 'None', 'updated_code': "import numpy as np\n\ndef word2embedding(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n            \n    if word in model.vocab:\n        worde = word\n    else:\n        worde = 'null'\n    \n    res = model[worde]\n     \n    if i > 0:\n        word1 = sent[i-1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n    \n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n  \n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n    \n    res.shape = (1,900)\n    return res\n        \ndef sent2embedding(sent):\n    rese = []\n    for  i in range(len(sent)):\n        line = word2embedding(sent,i)\n        rese.append(line)\n        \n    resee = np.vstack(rese)\n    return resee"}, {'identified': "['a']", 'updated_code': ''}, {'identified': "['BASE', 'DATA']", 'updated_code': ''}, {'identified': "['im_shape', 'num_examples']", 'updated_code': ''}, {'identified': "['df_gme']", 'updated_code': '#dataframe om alle gediplomeerden in Categorie "Meerdere" te selecteren\ndf_me = output.loc[output[\'Categorie\'] == \'Meerdere\']\ndf_me'}, {'identified': 'None', 'updated_code': 'basic_model = HiddenMarkovModel(name="base-hmm-tagger")\n\ntags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\n\n# TODO: create states with emission probability distributions P(word | tag) and add to the model\n# (Hint: you may need to loop & create/add new states)\n#basic_model.add_states()\n\nemission_counts = pair_counts(tags, words)\nstates = {}\nfor tag, word_dict in emission_counts.items():\n    emission_dict = defaultdict(float)\n    for word in word_dict.keys():\n        emission_dict[word] = emission_counts[tag][word] / tag_unigrams[tag] \n    state_emission = DiscreteDistribution(dict(emission_dict))\n    states[tag] = State(state_emission, name=tag)\n    \nbasic_model.add_states(list(states.values()))\n\n# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n# (Hint: you may need to loop & add transitions\n#basic_model.add_transition()\nfor tag in data.training_set.tagset:\n    state = states[tag]\n    basic_model.add_transition(basic_model.start, state, tag_starts[tag]/len(data.training_set))\n    basic_model.add_transition(state, basic_model.end, tag_ends[tag]/tag_unigrams[tag])\n    for next_tag in data.training_set.tagset:\n        next_state = states[next_tag]\n        basic_model.add_transition(state, next_state, tag_bigrams[(tag, next_tag)]/tag_unigrams[tag])\n\n# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n# finalize the model\nbasic_model.bake()\n\nassert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n       "Every state in your network should use the name of the associated tag, which must be one of the training set tags."\nassert basic_model.edge_count() == 168, \\\n       ("Your network should have an edge from the start node to each state, one edge between every " +\n        "pair of tags (states), and an edge from each state to the end node.")\nHTML(\'<div class="alert alert-block alert-success">Your HMM network topology looks good!</div>\')'}, {'identified': "['circuit', 'classical_r', 'quantum_r']", 'updated_code': '# Get the components.'}, {'identified': "['visualizer']", 'updated_code': 'from yellowbrick.text import FreqDistVisualizer\n\nplt.figure(figsize=(15,20))\n#visualizer = FreqDistVisualizer(n=100, features = cv_description.get_feature_names())\n#visualizer.fit(dt_mat_description)\n#visualizer.poof()'}, {'identified': "['step_size', 'window_size']", 'updated_code': '# use your function\n_ = 100\n_ = 5\nX, y = encode_io_pairs(text, _, _)'}, {'identified': "['d']", 'updated_code': 'DotPlot(gepard_command, out_dir)'}, {'identified': 'None', 'updated_code': "RTanalysis = pd.DataFrame()\nlists = [[] for list in range(0,5)]\n\nfor ID in range(10,86):\n    sub = cdat[cdat.subject == ID]\n    lists[0].append(ID)\n    validRT_trials = sub[sub.TrialType == 'Valid'].RT.mean()\n    invalidRT_trials = sub[sub.TrialType == 'Invalid'].RT.mean()\n    lists[1].append(validRT_trials)\n    lists[2].append(invalidRT_trials)\n    \nRTanalysis['SubjectID'] = lists[0]\nRTanalysis['Valid'] = lists[1]\nRTanalysis['Invalid'] = lists[2]"}, {'identified': "['pd.options.display.max_colwidth', 'schema']", 'updated_code': ''}, {'identified': 'None', 'updated_code': '# Set up the pymc3 model. Again assume Uniform priors for p_A and p_B.\nwith pm.Model() as model:\n    p_A = pm.Uniform("p_A", 0, 1)\n    p_B = pm.Uniform("p_B", 0, 1)\n    \n    # Define the deterministic delta function. This is our unknown of interest.\n    delta = pm.Deterministic("delta", p_A - p_B)\n\n    \n    # Set of observations, in this case we have two observation datasets.\n    obs_A = pm.Bernoulli("obs_A", p_A, observed=observations_A)\n    obs_B = pm.Bernoulli("obs_B", p_B, observed=observations_B)\n\n    # To be explained in chapter 3.\n    step = pm.Metropolis()\n    trace = pm.sample(20000, step=step)\n    burned_trace = trace[1000:]'}, {'identified': "['river']", 'updated_code': '## get river name\ndata.iloc[0]["River"]'}, {'identified': "['data']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'with model:\n    yes_responses = pm.Binomial("number_cheaters", 100, p_skewed, observed=35)'}, {'identified': 'None', 'updated_code': 'X, y = make_regression(n_samples=100, n_features=1, n_informative=1, noise=30, \n                       random_state=2018)'}, {'identified': 'None', 'updated_code': 'X_test = vectorizer.transform(data_test.values)'}, {'identified': 'None', 'updated_code': 'with tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope="hidden[34]|outputs")\n    training_op = optimizer.minimize(loss, var_list=train_vars)'}, {'identified': "['sch']", 'updated_code': ''}, {'identified': "['random_average_min_path_length']", 'updated_code': 'np.average([nx.average_shortest_path_length(C) for C in nx.connected_component_subgraphs(random_degree_graph)])'}, {'identified': 'None', 'updated_code': "import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer()\ndocs = np.array([\n        'The sun is shining',\n        'The weather is sweet',\n        'The sun is shining, the weather is sweet, and one and one is two'])\nbag = count.fit_transform(docs)"}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'n_epochs = 10\nbatch_size = 100\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)\n\n        save_path = saver.save(sess, "./my_mnist_model")'}, {'identified': 'None', 'updated_code': 'bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(splitter="random", max_leaf_nodes=16, random_state=42),\n    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)'}, {'identified': "['adpc_2012', 'adpc_2013', 'cov_2012', 'cov_2013']", 'updated_code': '# North Lincolnshire\n# find steady state based on 2012 data\n\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - 0.100807801953, test_diag_fun(x)[1] - 0.0111652211547], \n    [0.09, 0.25] \n    )\n\nU_2012 = U_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nA_2012 = A_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\nS_2012 = S_fun(\n    incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos\n    )\n\n\n# find incidence and screening based on 2013 data\n[incsol, scrsol] = fsolve(\n    lambda x: [test_diag_fun(x)[0] - 0.173269822929, test_diag_fun(x)[1] - 0.0216211803756], \n    [0.09, 0.25] \n    )\n\n# solve, 2012-2013\ninc = incsol\nscr = scrsol\nparms = \\\n    [incsol*p_asymp, sc + scrsol*p_true_pos, incsol*(1-p_asymp), scrsol*p_true_pos + att_symp*p_true_pos]\n\nsol_n_lincs = odeint(dydt, \n       [U_2012,A_2012,S_2012], \n       linspace(0,10,1000), \n       args = (parms,)\n      )'}, {'identified': 'None', 'updated_code': "def fit_lstm(train,batch_size,nb_epoch,neurons):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    #define the loss function and optimization algorithm here\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model"}, {'identified': 'None', 'updated_code': 'def error_line_trace(x_values, y_values, m, b, x):\n    pass'}, {'identified': 'None', 'updated_code': "from keras.applications.resnet50 import ResNet50\n\n# define ResNet50 model\nResNet50_model = ResNet50(weights='imagenet')"}, {'identified': 'None', 'updated_code': 'def broad_less_than_50_meters_port():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 201)\n        distance_bw_left_endpoint = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = distance_bw_left_endpoint + light_width\n        running_light_start = tall_masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if distance_bw_left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(distance_bw_left_endpoint - 20, distance_bw_left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(25, distance_bw_left_endpoint + 20)\n        running_light_area = running_light_loc + light_width\n        new_array[taller_masthead_light:tall_masthead_height, distance_bw_left_endpoint:tall_masthead_width] = white\n        new_array[running_light_start:running_light_width, running_light_loc: running_light_area] = red\n        new_array = new_array.flatten()\n        all_broad_images[i] = new_array\n\n    return all_broad_images'}, {'identified': 'None', 'updated_code': '##  Ask for input file, full path\n##  i.e. Y:\\LRMF\\R_tables\\columbia_river_orig.csv\ninputFile = input("Please provide a full-path input file:")\n##inputFile = "Y:\\\\LRMF\\\\R_tables\\\\colorado_river_orig.csv"\n\ndata = pd.read_csv(inputFile, header=0)'}, {'identified': 'None', 'updated_code': 't = np.linspace(temperature.min() - 5, temperature.max()+5, 50)[:, None]\np_t = logistic(t.T, beta_samples, alpha_samples)\n\nmean_prob_t = p_t.mean(axis=0)'}, {'identified': 'None', 'updated_code': 'import numpy as np\n\ndata = np.array([[1., 2., 0.],\n                 [0., 0., 0.],\n                 [2., 2., 0.]])\n\ncentroids = np.array([[0.5, 0.5, 0.],\n                      [0., -0.5, 0.]])'}, {'identified': 'None', 'updated_code': "first, last = 0, 1000 # Sample interval to plot\n\nfig1, ax1 = plt.subplots()\nax1.plot(t[first:last], traj_data_us[first:last],  label='Angles',  marker='o',linewidth=0.3, markersize=1.5)\nax1.legend();\nax1.set_ylabel('Angles [deg]')\nax1.set_xlabel('Time')\nax1.set_title('Angles');\n\nfig2, ax2 = plt.subplots()\nax2.plot(t[first:last], vel_us[first:last],  label='Angular velocity',  marker='o',linewidth=0.3, markersize=1.5)\nax2.legend();\nax2.set_ylabel('Angular velocity [deg/s]')\nax2.set_xlabel('Time')\nax2.set_title('Velocity');\n\nfig3, ax3 = plt.subplots()\nax3.plot(t[first:last], acc_us[first:last],  label='Angular acceleration',  marker='o',linewidth=0.3, markersize=1.5)\nax3.legend();\nax3.set_ylabel('Angular acceleration [deg/s^2]')\nax3.set_xlabel('Time')\nax3.set_title('Acceleration');\n\nfig4, ax4 = plt.subplots()\nax4.plot(t[first:last], np.sum(np.abs(acc_us[first:last]), axis=1),  label='Summed acc',  marker='o',linewidth=0.3, markersize=1.5)\nax4.legend();\nax4.set_ylabel('Summed accelerations [deg/s^2]')\nax4.set_xlabel('Time')\nax4.set_title('Summed accelerations');\n\nfig5, ax5 = plt.subplots()\nax5.plot(t[first:last], psi_us[first:last],  label='Pseudo power',  marker='o',linewidth=0.3, markersize=1.5)\nax5.legend();\nax5.set_ylabel('Pseudo power [deg/s^3]')\nax5.set_xlabel('Time')\nax5.set_title('Pseudo power');\n\nfig6, ax6 = plt.subplots()\nax6.plot(power_time_ds[first:last], power_data_250[first:last],  label='Power',  marker='o',linewidth=0.3, markersize=1.5)\nax6.legend();\nax6.set_ylabel('Power [W]')\nax6.set_xlabel('Time')\nax6.set_title('Power');"}, {'identified': "['div']", 'updated_code': '# Shuffle data\nimages_seq, labels_seq = correspondingShuffle([images_seq, labels_seq])\n\n    \n# Split data on train and test dataset\n_ = int(train_set * len(images_seq))\n\ntrainImages = images_seq[0:div]\ntestImages = images_seq[div:]\n\ntrainLabels = labels_seq[0:div]\ntestLabels = labels_seq[div:]\n\nprint("Training images:", div)\nprint("Testing images:", len(images_seq) - div)'}, {'identified': "['dx']", 'updated_code': '# 2 1d ODEs\ndef ode(state,t):\n    # x and y are first two components of state vector\n    x = state[0]\n\n    # Compute state derivatives.  Mess around here! \n    dx = .4 * np.square(x) - 2\n\n    # Return the state derivatives\n    return [dx]'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n\n    gray = grayscale(image)\n\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray, kernel_size)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0, imshape[0]), (imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1], imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges, vertices)\n\n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    rho = 1  # distance resolution in pixels of the Hough grid\n    theta = np.pi/180  # angular resolution in radians of the Hough grid\n    threshold = 90  # minimum number of votes (intersections in Hough grid cell)\n    min_line_length = 30  # minimum number of pixels making up a line\n    max_line_gap = 30  # maximum gap in pixels between connectable line segments\n    line_image = np.copy(image)*0  # creating a blank to draw lines on\n\n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap, vertices)\n\n    # Draw the lines on the edge image\n    result = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n\n    return result'}, {'identified': 'None', 'updated_code': 'min_error = np.min(errors)'}, {'identified': 'None', 'updated_code': "from sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_topics=10,\n                                random_state=123,\n                                learning_method='batch')\nX_topics = lda.fit_transform(X)"}, {'identified': "['r']", 'updated_code': ''}, {'identified': "['P', 'count']", 'updated_code': "from numpy.random import seed\n\nseed(8923)\n\nf = pos_vel_filter(x=(0., 0.), R=3., Q=.02, P=np.diag([500., 49.]))\ntrack, zs = compute_dog_data(3., .02, count)\nXs, Covs, _, _ = f.batch_filter(zs)\nMs, Ps, _, _ = f.rts_smoother(Xs, Covs)\n\nbook_plots.plot_measurements(zs)\nplt.plot(Xs[:, 0], ls='--', label='Kalman Position')\nplt.plot(Ms[:, 0], label='RTS Position')\nplt.legend(loc=4);"}, {'identified': 'None', 'updated_code': 'def bow_lights_gt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    \n    Ships greater than 50m in length.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 126)\n        tall_mh_height = taller_masthead_light + light_width\n        shorter_masthead_light = np.random.randint(tall_mh_height + 2, tall_mh_height + 16)\n        short_mast_head_height = shorter_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(15, 56)\n        running_light_dist_vert = np.random.randint(short_mast_head_height, short_mast_head_height + 51)\n        new_view[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        new_view[shorter_masthead_light:short_mast_head_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_view = new_view.flatten()\n        all_bow_images[i] = new_view\n    \n    return all_bow_images'}, {'identified': "['a']", 'updated_code': "sms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 3].Valid).tconfint_mean()"}, {'identified': 'None', 'updated_code': 'T = 1000  # number of samples\nwith tf.name_scope("posterior"):\n    qpi = Empirical(tf.get_variable("qpi/params", [T, K],initializer=tf.constant_initializer(1.0/K)))\n    qmu = Empirical(tf.get_variable("qmu/params", [T, K, D],initializer=tf.zeros_initializer()))\n    qsigma = Empirical(tf.get_variable("qsigma/params", [T, K, D],initializer=tf.ones_initializer()))\n    qz = Empirical(tf.get_variable("qz/params", [T, N],initializer=tf.zeros_initializer(),dtype=tf.int32))'}, {'identified': "['f', 'key']", 'updated_code': "reload(slope)\nplt.close()\n\nle = .12\nre = .02\nte = .1\nbe = .11\nh_gap = .13\n\nw = .5\nh = 1. - te - be\n\nax_lines = f.add_axes([le, be, w, h])\nax_slopes = f.add_axes([le + w + h_gap, be, 1. - w - h_gap - le - re, h])\n\nslope.plot_cv_slope(subjects, deep_all, linear_all, chance[0], training_size, fracs, (ax_lines, ax_slopes),\n                    legend=True, normalize_chance=False)\n\nx0 = .05\ny0 = 1. - te + .02\nx1 = le + w + h_gap - .075\n\nf.text(x0, y0, 'A', **letter_fontstyle)\nf.text(x1, y0, 'B', **letter_fontstyle)\n\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.eps'), dpi=300)\nplt.savefig(os.path.join(os.environ['HOME'], 'Downloads/slope2.png'), dpi=300)"}, {'identified': "['data_full']", 'updated_code': "# This looks much better, so let's replace the SalePrice with the log-transformed version (will need to exponentiate predictions)\ntrain['SalePrice'] = np.log1p(train['SalePrice'])"}, {'identified': "['y']", 'updated_code': 'X, _ = train[0:-1], train[-1]\nX'}, {'identified': 'None', 'updated_code': "fig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x_plot, sample_cdfs[0], c='gray', alpha=0.75,\n        label='DP sample CDFs')\nax.plot(x_plot, sample_cdfs[1:].T, c='gray', alpha=0.75)\nax.plot(x_plot, P0.cdf(x_plot), c='k', label='Base CDF')\n\nax.set_title(r'$\\alpha = {}$'.format(alpha))\nax.legend(loc=2)"}, {'identified': "['BUCKET', 'PROJECT', 'REGION']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "for feature_set, slices in storage.get_slices().items():\n    output = slices.to_output()\n    print(output, '\\n')"}, {'identified': "['save_file']", 'updated_code': "saver = tf.train.Saver()\n\nwith tf.Session() as session:\n    saver.restore(session, save_file)\n    weights_layer_1 = session.run(weights['layer_1'])\n\n    # min/max values of weights\n    wmin = np.min(weights_layer_1)\n    wmax = np.max(weights_layer_1)\n\n    fig, axes = plt.subplots(7,7)\n\n    for i, ax in enumerate(axes.flat):\n        if i < 48:\n            image = weights_layer_1[:,:,0,i].reshape([3,3]) \n            ax.imshow(image, vmin=wmin, vmax=wmax, cmap='seismic')\n            ax.axis('off')\n\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()"}, {'identified': "['randomForest']", 'updated_code': 'parametrosRF = {\n    "max_features": ["sqrt", "log2", None] + [1, 20, 50, 100, 150, 200],\n    "max_depth": [3, 6, 12],\n    "min_samples_split": [2, 6, 12],\n    "n_estimators": [10, 50, 120, 200]\n}\n\n(tiempo_random_forest, grid_random_forest) = correr_y_mostrar(\n    RandomForestClassifier(),\n    parametrosRF,\n    5,\n    5\n)\n\nplot_learning_curve(grid_random_forest.best_estimator_, "Learning Curve Random Forest Mejor segun GridSearch", X_dev_np, y_dev_np, cv=5)'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")\ny = tf.placeholder(tf.int32, shape=(None), name="y")'}, {'identified': "['centers', 'clusterer', 'preds', 'sample_preds', 'score']", 'updated_code': '# TODO: Apply your clustering algorithm of choice to the reduced data \n\n# TODO: Predict the cluster for each data point\n\n# TODO: Find the cluster centers\n\n# TODO: Predict the cluster for each transformed sample data point\n\n# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen'}, {'identified': "['BATCH_SIZE', 'EPOCHS', 'rate']", 'updated_code': ''}, {'identified': "['pca11', 'pca12', 'pca21', 'pca22']", 'updated_code': "new_pc = eigvecs[:,-2:]\n\nplt.figure(figsize=(15,5)); \n\nplt.subplot(121); \nplt.scatter(X[y==0, 0], X[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.05, head_length=0.05,color='Green',  label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.05, head_length=0.05,color='magenta',label='Second PC')\nplt.grid(True); \n\nnew_pc_cen = new_pc - new_pc.mean(0,keepdims=True)\ncov        = new_pc_cen.T @ new_pc_cen /(new_pc_cen.shape[0] - 1)\nval,vec    = np.linalg.eigh(cov)\n\nplt.subplot(122); \nplt.scatter(new_pc[y==0, 0], new_pc[y==0, 1], color='red',  alpha=0.5)\nplt.scatter(new_pc[y==1, 0], new_pc[y==1, 1], color='blue', alpha=0.5)\nplt.arrow(0, 0, *vec[:,0] * val[0], head_width=0.005, head_length=0.005,color='Green',  label='First PC')\nplt.arrow(0, 0, *vec[:,1] * val[1], head_width=0.005, head_length=0.005,color='magenta',label='Second PC')\nplt.grid(True); \n\nplt.show()"}, {'identified': "['c']", 'updated_code': 'data = ["acacag", "acacgg", "aaaaaacgg"]\nClusteringSeq(data)'}, {'identified': 'None', 'updated_code': 'output_dir = "nmt"\nen_vocab_file = os.path.join(output_dir, "en_vocab")\nzh_vocab_file = os.path.join(output_dir, "zh_vocab")\ncheckpoint_path = os.path.join(output_dir, "checkpoints")\nlog_dir = os.path.join(output_dir, \'logs\')\ndownload_dir = "tensorflow-datasets/downloads"\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)'}, {'identified': 'None', 'updated_code': '# Edward Model\nwith tf.name_scope("model"):\n    pi_ed = Dirichlet(concentration=tf.constant([1.0] * K, name="pi/weights"), name= "pi")\n    mu_ed = Normal(loc= tf.ones(D, name="centroids/loc") * 127, \n                scale= tf.ones(D, name="centroids/scale") * 80, sample_shape=K, name= "centroids")\n    sigmasq_ed = InverseGamma(concentration=tf.ones(D, name="variability/concentration"), \n                         rate=tf.ones(D, name="variability/rate"), sample_shape=K, name= "variability")\n    x_ed = ParamMixture(pi_ed, {\'loc\': mu_ed, \'scale_diag\': tf.sqrt(sigmasq_ed)},\n                     MultivariateNormalDiag, sample_shape=N, name= "mixture")\n    z_ed = x_ed.cat'}, {'identified': "['YOUR_BUCKET_NAME']", 'updated_code': ''}, {'identified': "['posttest']", 'updated_code': "pd.read_csv('posttest.csv')"}, {'identified': "['estimated_distance_m', 'time']", 'updated_code': ''}, {'identified': "['a']", 'updated_code': 'sms.DescrStatsW(ACCanalysis.Invalid).tconfint_mean()'}, {'identified': 'None', 'updated_code': 'y_predict = trainer.predict(Xt)'}, {'identified': "['gmaps']", 'updated_code': 'import googlemaps\n\ngooglemaps.Client(key="PASTE YOUR API KEY HERE")'}, {'identified': "['dump_every']", 'updated_code': "n_samples_train = 2048\nn_samples_valid = 512\nepochs = 200\nstatus_update_every = 50\n\nx_valid, y_valid, *_ = msig.generate_samples(n_samples_valid, 'tf_tc')\n\nfor i in range(epochs):\n    x_train, y_train, *_ = msig.generate_samples(n_samples_train, 'tf_tc')\n    model.fit(\n        x_train, y_train,\n        epochs=1, \n        validation_data=(x_valid, y_valid),\n        batch_size=batch_size,\n        verbose=1, \n        callbacks=[\n            csvlogger,\n            checkpointer\n        ],\n    )\n    if stateful:\n        model.reset_states()\n\n# Code specific to window_type == sliding\n\n    if (i + 1) % status_update_every == 0:\n        print('#' * 50)\n        print(f'Epoch: {(i + 1)}/{epochs}')\n        print('#' * 50)\n#         model.save(msig.model_filename)"}, {'identified': "['union_cat', 'union_dog']", 'updated_code': "#Set the pipelines for categorical variables\ndiscrete_pipe_dog = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['dog'], hashing=None))])\ndiscrete_pipe_cat = Pipeline(steps=[('Vectorizer', MyVectorizer(cols=discrete['cat'], hashing=None))])\n\n#Set the pipelines for continuous variables\ncontinuous_pipe_cat = Pipeline(steps=[('Scale', MyScaler(continuous['cat']))])\ncontinuous_pipe_dog = Pipeline(steps=[('Scale', MyScaler(continuous['dog']))])"}, {'identified': 'None', 'updated_code': 'array_1 = band1.ReadAsArray().flatten()\narray_2 = band2.ReadAsArray().flatten()\narray_3 = band3.ReadAsArray().flatten()'}, {'identified': "['both_data_file', 'brent_data_file', 'corpus', 'labels', 'providence_data_file', 'x_train', 'y_train']", 'updated_code': '# define x-fields (column IDs) to keep at 1    = word itself\n#                                         2    = pos\n#                                         3    = length (letters)\n#                                         4    = frequency\n#                                         5-93 = egemaps prosody features\nfeatures  = list(range(1,93))\n\npos_filter = None  #[[\'pos\', \'nouns\',\'function_words\']]\n\n# define name (col header) of y-variable in data file\ny         = \'y\'\n\n# load data, x-fields, y-field, train/dev/test split from input file\nprint("extracting providence...")\nprovidence_x_train, providence_y_train, _, _, _, _, _ = get_data_from_tsv(providence_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\nprint("extracting brent...")\nbrent_x_train, brent_y_train, _, _, _, _, _ = get_data_from_tsv(brent_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\n\nprint("extracting brentprovidence...")\nboth_x_train, both_y_train, _, _, _, _, _ = get_data_from_tsv(both_data_file, \n                                                                      x_fields=features, \n                                                                      y_field=y, \n                                                                      x_filter=pos_filter,\n                                                                      train_portion=1.0,\n                                                                      shuffle=False)\n\nfirst_numeric_feature = x_train.columns.tolist().index(\'log_length\')\nfirst_egemaps_feature = x_train.columns.tolist().index(\'F0semitoneFrom27.5Hz_sma3nz_amean\')\n\nprint(first_numeric_feature,first_egemaps_feature)'}, {'identified': 'None', 'updated_code': '#ignore\n# @pysnooper.snoop()\ndef plot_2d(value, ax=None, group=None, mask=None, matrix_id=0, mat_as_group=False, \n            group_id=None, linewidths=0, is_string=False, fmt="d", square=False):\n  \n  if hasattr(value, "numpy"):\n    value = value.numpy()\n  if group is not None and hasattr(group, "numpy"):\n    group = group.numpy()\n  if mask is not None and hasattr(mask, "numpy"):\n    mask = tf.squeeze(mask)\n    mask = tf.ones_like(value) * mask\n    mask = mask.numpy()\n    \n\n  cmaps = [\'PuOr\', \'tab20b\', \'RdBu\']\n  group_id = int(group[0][0])\n  cmap = cmaps[group_id]\n  \n  if is_string:\n    fmt = \'\'\n  \n  sns.heatmap(group, \n              fmt=fmt,\n#               cmap=cmap,\n              cmap=cmap,\n              annot=value, \n              cbar=False, \n              xticklabels=False, \n              yticklabels=False, \n              square=square,\n              mask=mask,\n              linewidths=linewidths,\n              ax=ax)'}, {'identified': "['max_norm_reg']", 'updated_code': 'with tf.name_scope("dnn"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_regularizer(threshold=1.0), name="hidden1")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n                              kernel_regularizer=max_norm_regularizer(threshold=1.0), name="hidden2")\n    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'ridge_preds = np.expm1(model_ridge.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\nelastic_preds = np.expm1(model_elastic.predict(X_test))'}, {'identified': 'None', 'updated_code': "# make the flopy model\nmf = flopy.modflow.Modflow(modelname=modelname, exe_name=mfexe, model_ws=modelpth)\ndis = flopy.modflow.ModflowDis(mf, nlay, nrow, ncol,\n                               delr=delr, delc=delc, \n                               top=botm[0, :, :], botm=botm[1:, :, :], \n                               perlen=1, nstp=1, steady=True)\nbas = flopy.modflow.ModflowBas(mf, ibound=ibound, strt=strt)\nlpf = flopy.modflow.ModflowLpf(mf, hk=0.0001, laytyp=4)\noc = flopy.modflow.ModflowOc(mf, \n                             stress_period_data={(0,0): ['print budget', 'print head', \n                                                         'save head', 'save budget']})\nsms = flopy.modflow.ModflowSms(mf, nonlinmeth=1, linmeth=1,\n                               numtrack=50, btol=1.1, breduc=0.70, reslim = 0.0,\n                               theta=0.85, akappa=0.0001, gamma=0., amomentum=0.1,\n                               iacl=2, norder=0, level=5, north=7, iredsys=0, rrctol=0.,\n                               idroptol=1, epsrn=1.e-5,\n                               mxiter=500, hclose=1.e-3, hiclose=1.e-3, iter1=50)\nmf.write_input()\n\n# remove any existing head files\ntry:\n    os.remove(os.path.join(model_ws, '{0}.hds'.format(modelname)))\nexcept:\n    pass\n\n# run the model\nmf.run_model()"}, {'identified': "['errors_for_regression']", 'updated_code': 'error_line_traces(x_values, y_values, 1.7, 10)'}, {'identified': 'None', 'updated_code': "im = Model(name='integer_programming')\nb = im.binary_var(name='boolean_var')\nijk = im.integer_var(name='int_var')\nim.print_information()"}, {'identified': 'None', 'updated_code': 'X_batch, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size=4)'}, {'identified': "['f_M_Nm']", 'updated_code': ''}, {'identified': "['modelname', 'modelpth']", 'updated_code': "#Set name of MODFLOW exe\n#  assumes executable is in users path statement\nexe_name = 'mfusg'\nif platform.system() == 'Windows':\n    exe_name += '.exe'\nmfexe = exe_name\n\n#make sure modelpth directory exists\nif not os.path.exists(modelpth):\n    os.makedirs(modelpth)"}, {'identified': 'None', 'updated_code': 'lambdas = np.logspace(-5, 0, 15)\n\n# bluid poly\ntx_tr = build_poly(Strain1_z, 1)\ntx_te = build_poly(Stest1_z, 1)\n\n# ridge regression with different lambda\nrmse_tr = []\nrmse_te = []\nfor ind, lambda_ in enumerate(lambdas):\n    # ridge regression\n    weight = ridge_regression(Strain1_y, tx_tr, lambda_)\n    rmse_tr.append(np.sqrt(2 * compute_mse(Strain1_y, tx_tr, weight)))\n    rmse_te.append(np.sqrt(2 * compute_mse(Stest1_y, tx_te, weight)))\nprint(rmse_tr, rmse_te)\n#print("last weight:",weight)'}, {'identified': 'None', 'updated_code': 'learning_rate = 0.01\n\nwith tf.name_scope("train"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)'}, {'identified': "['line_image', 'mask', 'result']", 'updated_code': '# TODO: Build your pipeline that will draw lane lines on the test_images\n# then save them to the test_images directory.\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport cv2\n\n\n# Read in and grayscale the image\nimage = mpimg.imread(\'test_images/whiteCarLaneSwitch.jpg\')\ngray = grayscale(image)\n\n# Define a kernel size and apply Gaussian smoothing\nkernel_size = 5\nblur_gray = gaussian_blur(gray,kernel_size)\n\n# Define our parameters for Canny and apply\nlow_threshold = 60\nhigh_threshold = 100\nedges = canny(blur_gray, low_threshold, high_threshold)\n\n# Next we\'ll create a masked edges image using cv2.fillPoly()\nimshape = image.shape\nvertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\nmasked_edges = region_of_interest(edges,vertices)\n \n# Define the Hough transform parameters\n# Make a blank the same size as our image to draw on\nrho = 1 # distance resolution in pixels of the Hough grid\ntheta = np.pi/180 # angular resolution in radians of the Hough grid\nthreshold = 41     # minimum number of votes (intersections in Hough grid cell)\nmin_line_length = 30 #minimum number of pixels making up a line\nmax_line_gap = 15    # maximum gap in pixels between connectable line segments\n\n# Run Hough on edge detected image\n# Output "lines" is an array containing endpoints of detected line segments\nlines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap,vertices)\n\n# Draw the lines on the edge image\nresult = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n \nplt.imshow(lines)\nplt.figure()\nplt.imshow(result)'}, {'identified': "['beta1', 'l']", 'updated_code': '#print(beta1)\nres1 = definitive_res(Stest1_z.dot(beta1))\n#print(res1)\nlen(Stest1_y[Stest1_y==res1])/len(Stest1_y)'}, {'identified': "['i']", 'updated_code': 'for _ in range(10):\n    print(repr(mutate("A quick brown fox")))'}, {'identified': 'None', 'updated_code': 'num_layers = 4 \nd_model = 128\ndff = 512\nnum_heads = 8\n\ninput_vocab_size = subword_encoder_en.vocab_size + 2\ntarget_vocab_size = subword_encoder_zh.vocab_size + 2\ndropout_rate = 0.1  # 預設值\n\nprint("input_vocab_size:", input_vocab_size)\nprint("target_vocab_size:", target_vocab_size)'}, {'identified': "['rects']", 'updated_code': '# Plot Frequency again\nsign_frequencies = get_frequencies(y_train_augmented, sign_dict)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nclasses = list(sign_dict.values())\nind = np.arange(len(classes))\nwidth = 0.8\n\nax.bar(ind, sign_frequencies.values(), width, align="edge", alpha=0.5)\nax.set_ylabel(\'Frequency\')\nax.set_title(\'Traffic Sign Classes\')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(sign_frequencies.keys(), rotation=90)\nplt.show()'}, {'identified': 'None', 'updated_code': "# TODO add features for normalized by speaker values of left, right, x, y\n# Name these 'norm-rx', 'norm-ry', 'norm-lx', and 'norm-ly'\n# using Z-score scaling (X-Xmean)/Xstd\n\nfeatures_norm = ['norm-rx', 'norm-ry', 'norm-lx','norm-ly']"}, {'identified': 'None', 'updated_code': 'bivar = sorted(filter(lambda r: len(r[0]) == 1, storage.relevancies.relevancy.iteritems()), \n               key=lambda r: r[1], reverse=True)'}, {'identified': "['b', 'graph', 'num_nodes', 'w']", 'updated_code': None}, {'identified': 'None', 'updated_code': "t_min_max = (msig.timestamps[0], msig.timestamps[-1])\ny_pred_colors = np.hstack([msig.waves[i].color for i in y_pred])\nprint(y_pred_colors[:3])\nprint(y_pred.shape, y_pred_colors.shape)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 3))\nax.plot(msig.timestamps, msig.mixed_signal, color='grey', alpha=0.3)\n# ax.scatter(msig.timestamps[msig.window_size-1:], x_test[:, -1, 0], marker='.', c=y_pred_colors)\n# ax.scatter(msig.timestamps, x_test[0, :, 0], marker='.', c=y_pred_colors)\nax.scatter(msig.timestamps[msig.window_size-1:], msig.mixed_signal[msig.window_size-1:], marker='.', c=y_pred_colors)\nax.set_xlabel('time')\nax.set_xlim(t_min_max)\nplt.grid(True)\nplt.show()"}, {'identified': "['center', 'fn', 'orientation']", 'updated_code': 'with fiona.open(path.join(DATA,\'field-orientations.geojson\')) as ds:\n    for i,item in ds.items():\n        p = item[\'properties\']\n        if p[\'planeType\'].strip() != \'Bedding\':\n            continue\n\n        asm = p.get("aster_smoothed")\n        alt = asm\n\n        alt -= 40 # Global datum is higher than local\n\n        err = 0.1*N.pi/180\n        a = ReconstructedPlane(p[\'strike\'], p[\'dip\'],0,err,err)\n        orientation = a.to_mapping(\n            center=(*item[\'geometry\'][\'coordinates\'],alt),\n            color=\'#444\', type=\'in-situ\')\n        collection.append(orientation)\n\nremovedUIDs = ["89636280","6031fd6f"]\ncollection = [c for c in collection if 1600 < c[\'center\'][2] < 1680]\ncollection = [c for c in collection if c[\'uid\'] not in removedUIDs]'}, {'identified': 'None', 'updated_code': 'x = linspace(0, 5, 10)\ny = x ** 2'}, {'identified': 'None', 'updated_code': "redundancy = storage.redundancies.redundancy\nsubset = ('Col43',)\nfeature = 'Col89'"}, {'identified': 'None', 'updated_code': "GCS_PATTERN = 'gs://flowers-public/*/*.jpg'\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # flower labels (folder names in the data)"}, {'identified': 'None', 'updated_code': "# variables for total production\ndesk = tm2.integer_var(name='desk', lb=100)\ncell = tm2.continuous_var(name='cell', lb=100)\n\n# two variables per machine type:\ndesk1 = tm2.integer_var(name='desk1')\ncell1 = tm2.integer_var(name='cell1')\n\ndesk2 = tm2.integer_var(name='desk2')\ncell2 = tm2.integer_var(name='cell2')\n\n# yes no variable\nz = tm2.binary_var(name='z')"}, {'identified': "['i']", 'updated_code': 'for _ in range(10):\n    print(repr(flip_random_character(seed_input)))'}, {'identified': 'None', 'updated_code': 'p_A_samples = burned_trace["p_A"]\np_B_samples = burned_trace["p_B"]\ndelta_samples = burned_trace["delta"]'}, {'identified': 'None', 'updated_code': 'letter, bigram = get_probabilities()'}, {'identified': 'None', 'updated_code': '# Find clusters in projected data\ny_kmeans_proj = []\ncenters_kmeans_proj = []\nfor i, x in enumerate(X_proj):\n    kmeans = KMeans(n_clusters=5)#, init=np.array([(i*200/6.0, 25) for i in range(1,6)]))\n    kmeans.fit(x)\n    centers_kmeans_proj.append(kmeans.cluster_centers_)\n    y_kmeans_proj.append(kmeans.predict(x))'}, {'identified': "['yhat']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'weights2 = tf.get_default_graph().get_tensor_by_name("hidden2/kernel:0")\nclipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\nclip_weights2 = tf.assign(weights2, clipped_weights2)'}, {'identified': "['a']", 'updated_code': "sms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 1].Valid).tconfint_mean()"}, {'identified': "['ax2', 'f']", 'updated_code': 'plt.imshow(oriImg[:,:,[2,1,0]])\nplt.imshow(heatmap_avg[:,:,1], alpha=.5)\nplt.show()'}, {'identified': "['channels', 'height', 'width']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "try:\n    university_canton_dict = json.loads(open('university_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for universities has not been saved yet. Let''s create a new dictionary.')\n    university_canton_dict = {}\n    \ntry:\n    institution_canton_dict = json.loads(open('institution_canton_dict.json').read())\nexcept FileNotFoundError:\n    print('The dictionary for institutions has not been saved yet. Let''s create a new dictionary.')\n    institution_canton_dict = {}"}, {'identified': "['m']", 'updated_code': 'z_obs = info.get_sn_info("SN2007uy")["z_obs"]\n\nprint(z_obs)'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': 'def expectedFit(x, a, b):\n    """\n    Returns the expected fit for the histogram\n    \n    Arguments: x - the x value in the equation\n               a - the first fit parameter\n               b - the second fit paramter\n               \n    Returned: The expected fit function\n    """\n    return a * np.exp(-b * x)\n\noccurenceRegion = bigOccurences[7:14] # Only fits region of interest\nmagnitudeRegion = bigMagnitudes[7:14]\n\nparameters, covariance = curve_fit(expectedFit, magnitudeRegion, occurenceRegion)\n\nfitX = magnitudeRegion\n\naFit = parameters[0]\nbFit = parameters[1]\n\nfitY = aFit * np.exp(-bFit * magnitudeRegion)\n\nprint("A = ", aFit, "b = ", bFit)'}]