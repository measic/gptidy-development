[{'identified': "['absolute', 'squared']", 'updated_code': '#Example\n#pandas.concat([Trial_data[Protein[:4]],squared(Trial_data[Protein[:4]]),absolute(Trial_data[Protein[:4]])],axis=1).T'}, {'identified': "['insert_random_character']", 'updated_code': ''}, {'identified': "['systemcall', 'systemcall_pipe']", 'updated_code': 'import subprocess \nimport shlex'}, {'identified': "['delete_random_character']", 'updated_code': ''}, {'identified': "['myfunc']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "def value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4):\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    \n    while delta > epsilon:\n\n        delta = 0\n        \n        Q = {0: {0: 0,   # state0, action0\n                 1: 0},  # state0, action1\n             1: {2: 0}}  # state1, action2\n        for s in states:\n            v = 0\n            for a in actions[s]:\n                for n in next_states:\n                \n                    # Bellman's optimality update rule\n                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n\n            # This step replaces the poilicy improvement step (gets the maximal value)\n            V_kplus1[s] = max(Q[s].items(), key=operator.itemgetter(1))[1]\n            \n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n    \n    # Updates the policy to be greedy with respect to the value function\n    for s in states:\n        PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n    \n    return V_k, k, PI "}, {'identified': "['plotImg']", 'updated_code': 'def plotAll(array2D, **kwargs):\n    """\n    Input:  \'array2D\': 2D image array\n    Output: show plot of histogram and image\n    """\n    \n    arr = array2D.flatten()\n    \n    avg = mean(arr)\n    std = stdev(arr)\n    med = np.median(arr)\n    Npix = len(arr)    \n    \n    sigma = kwargs.get(\'sigma\', 2)\n    low = int(np.round((avg-sigma*std)))\n    high = int(np.round((avg+sigma*std)))\n    rng = kwargs.get(\'rng\', [low, high])\n    exp = kwargs.get(\'exp\')\n    if \'nbins\' in kwargs:\n        nbins = kwargs.get(\'nbins\')\n        bin_size = (rng[1]-rng[0])/nbins\n    else:\n        bin_size = kwargs.get(\'bin_size\', 1)\n    \n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=[18,6])\n    \n    # Histogram\n    #===========\n    hr = np.arange(rng[0], rng[1]+1, bin_size)\n    hist = []\n    for i in range(len(hr)):\n        try:\n            counts = len(np.where((arr >= hr[i]) & (arr < hr[i+1]))[0])\n        except:\n            counts = 0\n        hist.append(counts)\n    ax1.step(hr, hist, color=\'k\')\n\n    #mean and median lines\n    ax1.axvline(avg, color=\'b\', label=r\'$\\bar{x}=%s$\'%(np.round(avg,2)))\n#     ax1.axvline(med, color=\'b\', label=r\'$\\tilde{x}=%s$\'%(np.round(med,2)), linestyle=\'dashed\')\n    \n    #sigma levels\n    if kwargs.get(\'show_level\', True) == True:\n        for i in np.arange(1,sigma+1):\n            if i == 1:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05, label=r\'$s=\\pm %s$\'%(np.round(std,2)))\n            else:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05)\n                \n                \n    #poisson distribution\n    xarray = np.arange(rng[0]-10, rng[1]+10, 1)\n    pdist = poisson_approx(xarray, avg)\n    pdist = max(hist)/max(pdist)*pdist\n    ax1.plot(xarray, pdist, color=\'r\', label=r\'$P_{Poisson}(\\bar{x})$\')\n    std_expected = math.sqrt(avg)\n    ax1.axvspan(avg - std_expected, avg + std_expected, facecolor=\'r\', alpha=0.05, \\\n                label=r\'$\\sigma=\\pm %s$\'%(np.round(std_expected,2)))\n    \n    #gaussian distribution\n    gdist = gaussian(xarray, avg, std)\n    gdist = max(hist)/max(gdist)*gdist\n    ax1.plot(xarray, gdist, color=\'c\', label=r\'$P_{Gaussian}(\\bar{x}, s)$\')\n    \n    ax1.legend(loc=\'upper left\')\n    ax1.set_xlabel(\'Counts (ADU)\')\n    ax1.set_ylabel(\'Frequency\')\n    \n    if \'exp\' in kwargs:\n        ax1.set_title(\'Combined Histogram (Exposure Time: %s sec)\'%(exp))\n    ax1.set_xlim(rng)\n    \n    # Image\n    #===========\n    hrng = kwargs.get(\'hrng\', [np.percentile(arr, 10), np.percentile(arr, 90)])\n    pl = ax2.imshow(array2D, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', vmin=hrng[0], vmax=hrng[1])\n    fig.colorbar(pl, ax=ax2, fraction=0.046, pad=0.04).set_label(\'Detector Value (ADU)\')\n    \n    ax2.set_xlabel(\'pixels(x)\')\n    ax2.set_ylabel(\'pixels(y)\')\n    ax2.set_title(\'Combined Image\')\n    \n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'exposure%s.png\'%(exp))\n    plt.show()'}, {'identified': "['regression_formula']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def Lectura(ux,uy,uz):\n    Ux = np.loadtxt(ux, unpack = True)\n    Uy = np.loadtxt(uy, unpack = True)\n    Uz = np.loadtxt(uz, unpack = True)\n    return Ux,Uy,Uz\n    \n\n\ndef Grafica(udat,vdat,xdat,ydat,linea):\n    uesc = 1.0#dx/dt\n    u = np.loadtxt(udat, unpack = True)\n    v = np.loadtxt(vdat, unpack = True)\n    x = np.loadtxt(xdat, unpack = True)\n    y = np.loadtxt(ydat, unpack = True)\n    #rho = np.loadtxt(rhodat, unpack = True)\n    X = np.linspace(0,1,50)\n\n    f, axarr = plt.subplots(1,2, figsize=(24,6))\n    st = f.suptitle("$\\\\tau = 0.6$", fontsize=25)\n    st.set_y(1.0)\n    pasos = 12\n    M= np.hypot(u, v)\n    #axarr[0].streamplot(x,y,u,v, color="k",linewidth=0.8,density=1.0, arrowstyle=\'->\', arrowsize=1.5)\n    im=axarr[0].quiver(x,y,u,v,M , cmap=plt.cm.jet,width=0.022,scale=1/0.1)\n    axarr[0].set_title("Campos",fontsize = 20)\n    axarr[0].set_xlim(-0.01,1)\n    axarr[0].set_xlabel("$x[m]$",fontsize = 20)\n    axarr[0].set_ylabel("$y[m]$",fontsize = 20)\n    axarr[0].tick_params(axis="x", labelsize=20)\n    axarr[0].tick_params(axis="y", labelsize=20)\n    \n    axarr[1].plot(x,v[linea,:],"b", label = "Simulacion")\n    #axarr[1].plot(X,uesc*uy(X,nul,gl,Pl),"r+", label = "Teorica")\n    #axarr[1].set_ylim(-0.1*uesc,0)\n    axarr[1].legend()\n    axarr[1].grid(True)\n    axarr[1].set_title(\'Perfil de Velocidad\',fontsize = 20,y=1.0)\n    axarr[1].set_xlabel("$x[m]$",fontsize = 20)\n    axarr[1].set_ylabel("$v[m/s]$",fontsize =20)\n    axarr[1].tick_params(axis="x", labelsize=20)\n    axarr[1].tick_params(axis="y", labelsize=20)\n    \n    cbar = f.colorbar(im, ax=axarr, shrink = 1.0)\n    cbar.set_label(\'$v[m/s]$\',fontsize =20)\n    cbar.ax.tick_params(labelsize=20)'}, {'identified': "['plot_color_image', 'plot_image']", 'updated_code': ''}, {'identified': "['to_appropriate_column_name']", 'updated_code': ''}, {'identified': "['create_xgb_target']", 'updated_code': ''}, {'identified': "['add_noise', 'true_mean_function']", 'updated_code': 'def generate_t(x, sigma):\n    return add_noise(true_mean_function(x), sigma)'}, {'identified': "['blockMotion']", 'updated_code': ''}, {'identified': "['glance_at_tensor']", 'updated_code': 'def highlight_column_matches(data, column=\'\', color=\'yellow\'):\n    \'\'\'\n    highlight the maximum in a Series or DataFrame\n    \'\'\'\n    attr = \'background-color: {}\'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_mixed = data == data[column]\n        return [attr if v else \'\' for v in is_mixed]\n    else:  # from .apply(axis=None)\n        is_mixed = data == data[column]\n        return pd.DataFrame(np.where(is_mixed, attr, \'\'), index=data.index, columns=data.columns)\n\ndef plot_stats(csv_filename, columns=[\'total_reward\'], **kwargs):\n    """Plot specified columns from CSV file."""\n    df_stats = pd.read_csv(csv_filename)\n    df_stats[columns].plot(**kwargs)\n\ndef save_rnn_layers(hidden_layers, output_layers):\n    for i, layer in hidden_layers.items():\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_output\'), hidden_layers[i][\'output\'])\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_state\'), hidden_layers[i][\'state\'])\n#     np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_2_output\'), hidden_layers[\'2\'][\'output\'])\n#     np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_2_state\'), hidden_layers[\'2\'][\'state\'])\n    np.save(os.path.join(vsig.out_dir, \'valid_output_layer\'), output_layers)\n    \ndef save_mlp_layers(hidden_layers, output_layers):\n    for i, layer in hidden_layers.items():\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_output\'), layer)\n    np.save(os.path.join(vsig.out_dir, \'valid_output_layer\'), output_layers)\n    \nclassifier_activation = {\'binary\': \'sigmoid\', \'categorical\': \'softmax\'}'}, {'identified': "['fit_lstm', 'inverse_difference', 'invert_scale', 'scale']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def recall(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n    tp = tf.diag_part(conf_matrix)\n    recall_scores = tp/(tp_and_fn)\n    if weights:\n        recall_score = tf.multiply(recall_scores, weights)/tf.reduce_sum(weights)\n    else:\n        recall_score = tf.reduce_mean(recall_scores)\n    return recall_score'}, {'identified': 'None', 'updated_code': 'class two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)\n    \n    def fit(self, input_data, target, optimizer, num_epochs=500, \n            verbose=50, track_accuracy=True):\n        """ Function to train the model, using the selected optimizer and\n            for the desired number of epochs. It also stores the accuracy\n            of the model after each epoch.\n        """   \n        \n        if track_accuracy:\n            # Initialize list to store the accuracy of the model\n            self.hist_accuracy = []     \n            # Initialize class to compute the accuracy metric\n            accuracy = tfe.metrics.Accuracy()\n\n        for i in range(num_epochs):\n            # Take a step of gradient descent\n            grads = self.grads_fn(input_data, target)\n            optimizer.apply_gradients(zip(grads, self.variables))\n            if track_accuracy:\n                # Predict targets after taking a step of gradient descent\n                logits = self.predict(X)\n                preds = tf.argmax(logits, axis=1)\n                # Compute the accuracy\n                accuracy(preds, target)\n                # Get the actual result and add it to our list\n                self.hist_accuracy.append(accuracy.result())\n                # Reset accuracy value (we don\'t want to track the running mean accuracy)\n                accuracy.init_variables()'}, {'identified': 'None', 'updated_code': 'accuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\ndef get_positive_class_probabilities(arr):\n    arr_aux = []\n    for entry in arr:\n        arr_aux.append(entry[1])\n    return arr_aux\n\ndef get_accuracy(y_pred, y_eval_np):\n    return np.mean(y_pred == y_eval_np)\n    \ndef show_prediction_accuracy(y_pred, y_eval_np, x_eval_np):\n    print("Predicciones sobre el test set:\\n {}".format(y_pred))\n    print("Score sobre el test set: {:.2f}".format(np.mean(y_pred == y_eval_np))) # A mano\n    print("Score sobre el test set: {:.2f}".format(arbol.score(x_eval_np, y_eval_np))) # Usando el método score.\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test) )\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train      # cambiar por accuracies_training\ndf["Accuracy (validación)"] = accuracy_validation # cambiar por accuracies_validation\ndf["AUC ROC (training)"]    = roc_train           # cambiar por aucs_training\ndf["AUC ROC (validación)"]  = roc_validation      # cambiar por aucs_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()'}, {'identified': "['draw_id\\n\\nNext, we remove the identified unused function definitions from the code. We remove all the unused function definitions and the code inside these function definitions. We leave everything else as is. We output the full code below with the function definitions removed.', 'draw_ids', 'draw_ids_avg', 'draw_ids_diff', 'draw_week_id', 'draw_week_ids']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "def value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n    while delta > epsilon:\n\n        # Only one sweep of evaluation before taking the max\n        delta = 0\n        for i, j in states:\n            # Now evaluates the value function for each state for every possible action (not just with respect to current policy)\n            for a_idx in range(4): # actions\n\n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Update rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n            # This step replaces the poilicy improvement step\n            V_kplus1[i,j] = np.max(Q[i,j,:])\n\n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n        \n    # Updates the policy to be greedy with respect to the value function\n    for i, j in states:\n        PI[i,j] = np.argmax(Q[i,j,:])\n    \n    return V_k, k, PI "}, {'identified': "['sframe_to_scipy']", 'updated_code': ''}, {'identified': "['forward_pass']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def get_Ks(x_new, x, kernel, theta):\n    """\n    Generates K, KS, and KSS by using the given kernel and theta\n    :param x_new: numpy array of new data\n    :param x: numpy array of data where we have sampled\n    :param theta: list (params + [sigma_n]), this is a list that has the params (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    :return: np.array(np.array) K, np.array(np.array) KS, np.array(np.array) KSS  all as described above.\n    """\n    K = kernel(x, x, theta[:-1], theta[-1]) # K #-\n    KS = kernel(x_new, x, theta[:-1], theta[-1]) # K* #-\n    KSS = kernel(x_new, x_new, theta[:-1], theta[-1]) # K** #-\n    return K, KS, KSS #-'}, {'identified': 'None', 'updated_code': 'def regression_optimize_theta(x, y, sigma_n, kernel, params_0=[0.1, 0.1]):\n    \n    """\n    Optimizes parameters for the data given by maximizing logp(data|parameters)\n    :param x: numpy array of data where we have sampled\n    :param y: numpy array of y values for data where we have sampled\n    :sigma_n: float sigma_n\n    :param kernal: the kernel function which we will be using\n    :param params_0: list params_0 this is a list that has the initial params (must be length\n    of params which your kernel is expecting) from this point the optimizer will run.\n    :return: list (optimal_params + [sigma_n]), this is a list that has the optimal parameterss (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    """\n    \n    def log_pY(theta):\n        """\n        Calculates the - log(p(y|parameters))\n        :param theta: list params this is a list that has the params (must be length\n        of params which your kernel is expecting)\n        :return: float - log(p(y|parameters)) (using negative because our optimizer is a minimizer)\n        """\n        K = kernel(x, x, theta, sigma_n) #+\n        log_k = np.linalg.slogdet(K)[1] #+\n        output = 0.5 * np.matmul(np.matmul(y.T, np.linalg.inv(K)),y) #-\n        output += 0.5 * log_k #-\n        return output #-\n\n    res = minimize(log_pY, params_0, method=\'nelder-mead\', options={\'xtol\': 1e-8, \'disp\': False}) #+\n    return list(res.x) + [sigma_n] # +'}, {'identified': "['calc_arc_scores', 'calc_climbing_scores', 'calc_gym_scores', 'calc_hangboard_scores', 'calc_power_scores', 'calc_score', 'calc_technique_scores', 'calc_yoga_scores']", 'updated_code': "boulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,   \n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n    \n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n    \n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n    \n    # yoga\n    'yoga': boulder_6b / 5,\n    \n    # ARC\n    'arc': boulder_6b_plus / 5,\n    \n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n    \n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%' : boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n    \n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n    \n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n    \n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n    \n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit',]\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+',]"}, {'identified': 'None', 'updated_code': '#Auxiliares para correr randomized search\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\n\ndef correr_randomized_y_mostrar(estimator, parameters, folds, top,  iteraciones=None):\n    random_search = None\n    \n    if(iteraciones is None):\n        random_search = RandomizedSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\')\n    else:\n        random_search = RandomizedSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\', n_iter=iteraciones)\n        \n    time_before = time.time()\n    random_search.fit(X_dev_np, y_dev_np)\n    time_after = time.time()\n    runtime = (time_after - time_before) * 1000.0\n    \n    top_resultados(random_search, top)\n    bot_resultados(random_search, top)\n    \n    return (runtime, random_search)\n\ndef verTiempo(original, random):\n    display("########### Timepos ###########")\n    display("original: {:f}".format(original))\n    display("random: {:f}".format(random))\n    display("diferencia: {:f}".format( np.absolute(original-random) ))'}, {'identified': "['predict_next_chars']", 'updated_code': '# Empty code (no function definitions)'}, {'identified': "['show_results']", 'updated_code': ''}, {'identified': "['word2embedding']", 'updated_code': 'import numpy as np\n\ndef sent2embedding(sent):\n        rese = []\n        for  i in range(len(sent)):\n            line = word2embedding(sent,i)\n            rese.append(line)\n        \n        resee = np.vstack(rese)\n                #rese.extend(word2embedding(sent, i))\n        #print(resee.shape)\n        return resee'}, {'identified': "['extractPolyFromKML', 'getNodes', 'linksPerRegion', 'nodesPerRegion', 'visualizeLinks', 'visualizeNodes', 'visualizePoly', 'writeToCSV']", 'updated_code': '# Given kml file path, extracts boundary points in regions\n# No longer used: extractPolyFromKML\n# No longer used: visualizePoly\n# No longer used: visualizeNodes\n# No longer used: visualizeLinks\n# No longer used: nodesPerRegion\n# No longer used: linksPerRegion\n# No longer used: getNodes\n# No longer used: writeToCSV'}, {'identified': 'None', 'updated_code': "from scipy.optimize import fsolve\n\ntsym, dsym, ssym, test_sym = symbols('tsym dsym ssym test_sym')\n\nmodel_test_diag = [\n    tsym - ( ssym + (1 - A - U)*test_sym ),\n    dsym - ( A*ssym*p_true_pos + U*ssym*p_false_pos + (1 - A - U)*test_sym*p_true_pos )\n    ]\n\nsol_test_diag = solve(model_test_diag, tsym, dsym)\ntest_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[tsym])\ndiag_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[dsym])\n\ndef test_diag_fun(parms):\n    # parms = (incidence, screening rate)\n    inc = parms[0]\n    scr = parms[1]\n    \n    A = A_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1 - p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n    U = U_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1 - p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n    return [test_fun(A, U, scr, att_symp), diag_fun(A, U, scr, att_symp)]\n\n\n# set up a function to simulate system dynamics when perturbed from steady state\nfrom scipy.integrate import odeint\n\ndef dydt(y, t, parms):\n    return([\n    parms[1]*y[1] + parms[3]*y[2] - (parms[0] + parms[2])*y[0],\n    parms[0]*y[0] - parms[1]*y[1],\n    parms[2]*y[0] - parms[3]*y[2]\n    ])"}, {'identified': "['eval_grads', 'flatten_grads', 'full', 'process_weights']", 'updated_code': 'import tensorflow as tf\nimport numpy as np\n\ndef seven():\n    return 7'}, {'identified': "['nCr']", 'updated_code': 'import numpy as np'}, {'identified': "['getBestParamsSVM']", 'updated_code': "#mini_train_data, mini_test_data, mini_train_labels, mini_test_labels = train_test_split(train_data, train_labels,\n#                                    stratify=train_labels, \n#                                    test_size=0.55)\n\n\n#\n# SVM\n#\nclassifier = LinearSVC(penalty='l2')\n\nparams = {'C': [0.01, 0.1, 0.5]}\nsvm = GridSearchCV(classifier, params, cv=4, \n                   scoring='accuracy', return_train_score=True)\n\n# Fit  training data\nsvm.fit(train_data, train_labels)  \n# Show the best C parameter to use and the expected accuracy\nprint('\\nSVM Classifier')\nprint(' Best param:', svm.best_params_)\nprint(' Accuracy:  ', np.round(svm.best_score_, 4) )\n\nreturn svm.best_params_"}, {'identified': "['gp_plot']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C=None):\n    \n    if C is None:\n        K = computeK(x_train, x_train, theta)\n        C = K + (1/beta)*np.eye(len(K))\n    \n    C_inv = np.linalg.inv(C)\n    \n    mean_test = np.zeros(len(x_test))\n    covar_test = np.zeros((len(x_test), len(x_test)))\n    \n    for i, x_t in enumerate(x_test):\n        # kernel evaluated between all training points and the new x_test \n        k = np.array([k_n_m(x_n, x_t, theta) for x_n in x_train])   \n        # kernel evaluated in the x_test with itself added of variance\n        c = k_n_m(x_t, x_t, theta) + 1/beta        \n        k_dot_C = np.matmul(k.T, C_inv) \n        \n        mean_test[i] = np.matmul(k_dot_C,t_train)\n        covar_test[i][i] = c - np.matmul(k_dot_C, k)\n\n    return mean_test, covar_test, C'}, {'identified': 'None', 'updated_code': 'def showPrecisionRecallPairByLabel(precision_by_label, recall_by_label, label_encoder, classifier_name, colors):\n    labels = []\n    for i in range(len(precision_by_label)):\n        label = label_encoder.inverse_transform([i])[0]\n        labels.append(label)\n    \n    y_pos = np.arange(len(labels))    \n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n\n    ax1.invert_xaxis()\n    ax1.yaxis.tick_right()\n    \n    ax1.set_yticks(y_pos)\n    ax1.set_yticklabels(labels)\n    \n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels(labels)\n        \n    ax1.barh(y_pos, precision_by_label, color=colors[0] , label="precision")\n    ax2.barh(y_pos, recall_by_label,    color=colors[1],  label=\'recall\')\n\n    ax1.set_title(\'Precision( \' + classifier_name + \')\')\n    ax2.set_title(\'Recall (\' + classifier_name + \')\')\n    \n    plt.grid()\n    plt.show()'}, {'identified': "['getInflunet', 'padInflunet']", 'updated_code': 'def importInflunet(path):\n    \'\'\'\n    Reads the Influnet data and creates a unique multiindex dataframe of the format\n    \n    (year,week) - incidence\n    \n    :param path: location of the influnet folder\n    :return: compacted version of \n    \'\'\'\n    \n    df = pd.concat([pd.read_csv(path+t, names=["time", "incidence"], sep=" ", header=1, usecols=[0,4], decimal=",") for t in listdir(path)], ignore_index=True)\n    df[["year","week"]] = df["time"].str.split("-", expand=True).astype(int)\n    df.drop(["time"], axis=1, inplace=True)\n    df = df.set_index(["year","week"])\n    df.sortlevel(inplace=True)\n    df = df.astype(float)\n    df = df.loc[2008:]\n    return df'}, {'identified': "['update_nueron']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def backward_pass(x, y, W, b):\n    # Responsável por calcular as derivadas parciais para W e b.\n    # Funciona com batches também, nesse caso retornando a média\n    # dos gradientes.\n    y_, z = forward_pass(x, W, b)\n    dLdb = (-y/(y_ + 1e-8) + (1-y)/(1-y_ + 1e-8)) * sigmoid(z, True)\n    dLdW = np.zeros((dLdb.shape[0], 31))\n    for i, el in enumerate(dLdb):\n        dLdW[i] = x[i] * el\n    dLdW = np.mean(dLdW, axis=0)\n    dLdb = np.mean(dLdb, axis=0)\n    return dLdW, dLdb'}, {'identified': 'None', 'updated_code': "# robust pca\nfrom __future__ import division, print_function\n\nimport numpy as np\n\ntry:\n    from pylab import plt\nexcept ImportError:\n    print('Unable to import pylab. R_pca.plot_fit() will not work.')\n\ntry:\n    # Python 2: 'xrange' is the iterative version\n    range = xrange\nexcept NameError:\n    # Python 3: 'range' is iterative - no need for 'xrange'\n    pass\n\n\nclass R_pca:\n\n    def __init__(self, D, mu=None, lmbda=None):\n        self.D = D\n        self.S = np.zeros(self.D.shape)\n        self.Y = np.zeros(self.D.shape)\n\n        if mu:\n            self.mu = mu\n        else:\n            self.mu = np.prod(self.D.shape) / (4 * self.norm_p(self.D, 2))\n\n        self.mu_inv = 1 / self.mu\n\n        if lmbda:\n            self.lmbda = lmbda\n        else:\n            self.lmbda = 1 / np.sqrt(np.max(self.D.shape))\n\n    @staticmethod\n    def norm_p(M, p):\n        return np.sum(np.power(M, p))\n\n    @staticmethod\n    def shrink(M, tau):\n        return np.sign(M) * np.maximum((np.abs(M) - tau), np.zeros(M.shape))\n\n    def svd_threshold(self, M, tau):\n        U, S, V = np.linalg.svd(M, full_matrices=False)\n        return np.dot(U, np.dot(np.diag(self.shrink(S, tau)), V))\n\n    def fit(self, tol=None, max_iter=1000, iter_print=100):\n        iter = 0\n        err = np.Inf\n        Sk = self.S\n        Yk = self.Y\n        Lk = np.zeros(self.D.shape)\n\n        if tol:\n            _tol = tol\n        else:\n            _tol = 1E-7 * self.norm_p(np.abs(self.D), 2)\n\n        while (err > _tol) and iter < max_iter:\n            Lk = self.svd_threshold(\n                self.D - Sk + self.mu_inv * Yk, self.mu_inv)\n            Sk = self.shrink(\n                self.D - Lk + (self.mu_inv * Yk), self.mu_inv * self.lmbda)\n            Yk = Yk + self.mu * (self.D - Lk - Sk)\n            err = self.norm_p(np.abs(self.D - Lk - Sk), 2)\n            iter += 1\n            if (iter % iter_print) == 0 or iter == 1 or iter > max_iter or err <= _tol:\n                print('iteration: {0}, error: {1}'.format(iter, err))\n\n        self.L = Lk\n        self.S = Sk\n        return Lk, Sk\n\n    def plot_fit(self, size=None, tol=0.1, axis_on=True):\n\n        n, d = self.D.shape\n\n        if size:\n            nrows, ncols = size\n        else:\n            sq = np.ceil(np.sqrt(n))\n            nrows = int(sq)\n            ncols = int(sq)\n\n        ymin = np.nanmin(self.D)\n        ymax = np.nanmax(self.D)\n        print('ymin: {0}, ymax: {1}'.format(ymin, ymax))\n\n        numplots = np.min([n, nrows * ncols])\n        plt.figure()\n\n        for n in range(numplots):\n            plt.subplot(nrows, ncols, n + 1)\n            plt.ylim((ymin - tol, ymax + tol))\n            plt.plot(self.L[n, :] + self.S[n, :], 'r')\n            plt.plot(self.L[n, :], 'b')\n            if not axis_on:\n                plt.axis('off')"}, {'identified': "['_test_loop', 'test_bsd100', 'test_set14', 'test_set5']", 'updated_code': 'from keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport sys\nsys.path.append("..")\n\n#import models\nfrom loss import PSNRLoss, psnr\n\nimport os\nimport time\nimport numpy as np\nfrom imageio import imwrite as imsave\nfrom scipy.misc import imresize\nfrom scipy.ndimage.filters import gaussian_filter\n\nbase_weights_path = "weights/"\nbase_val_images_path = "val_images/"\nbase_test_images = "test_images/"\n\nset5_path = "tests/set5"\nset14_path = "tests/set14"\nbsd100_path = "tests/bsd100"\n\nif not os.path.exists(base_weights_path):\n    os.makedirs(base_weights_path)\n\nif not os.path.exists(base_val_images_path):\n    os.makedirs(base_val_images_path)\n\nif not os.path.exists(base_test_images):\n    os.makedirs(base_test_images)'}, {'identified': 'None', 'updated_code': 'def buildIncomesExpenses(bankdata):\n    data = []\n\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        movements = bankdata[CURRENT][bank][\'movements\']\n\n        incomes = {}\n        expenses = {}\n        for date, movement in zip(dates, movements):\n            key = str(date.month) + \'/\' + str(date.year)\n\n            if float(movement) > 0:\n                if key in incomes:\n                    incomes[key] += float(movement)\n                else:\n                    incomes[key] = float(movement)\n            else:\n                if key in expenses:\n                    expenses[key] += float(movement)\n                else:\n                    expenses[key] = float(movement)\n            \n        months_x = []\n        incomes_y = []\n        for key, value in incomes.items():\n            months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            incomes_y.append(value)\n        \n        trace = go.Bar(\n            x = months_x,\n            y = incomes_y,\n            name = "Incomes for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n        months_x = []\n        expenses_y = []\n        for key, value in expenses.items():\n            months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            expenses_y.append(value)\n        \n        trace = go.Bar(\n            x = months_x,\n            y = expenses_y,\n            name = "Expenses for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n    return data'}, {'identified': 'None', 'updated_code': 'def precision(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n    tp = tf.diag_part(conf_matrix)\n    precision_scores = tp / (tp_and_fp)\n    if weights:\n        precision_score = tf.multiply(precision_scores, weights) / tf.reduce_sum(weights)\n    else:\n        precision_score = tf.reduce_mean(precision_scores)\n    return precision_score'}, {'identified': "['accuracy']", 'updated_code': ''}, {'identified': "['get_data_from_tsv']", 'updated_code': ''}, {'identified': "['get_neighbour_messages']", 'updated_code': ''}, {'identified': "['show_history_graph']", 'updated_code': '## Show history'}, {'identified': "['NRM1', 'SCL1', 'TRSH']", 'updated_code': '# Scaling Functions'}, {'identified': "['print_policy']", 'updated_code': ''}, {'identified': "['delta_f']", 'updated_code': ''}, {'identified': "['get_fpy_az']", 'updated_code': "'''Function for creating a FP yield(A,Z) list container'''\n\nimport pandas as pd\nfrom collections import namedtuple\nfrom mendeleev import element\n\ndef get_fpy_az( df ):\n    nuclides = list()\n    FPY = namedtuple('FPY', ['name','element_name','Z','A','yield_percent'])\n\n    total_yield = 0.0 # sum total yield\n    for row in df.itertuples(index=False):\n        z = int(row[0])\n        for j in range(1,len(row)-1):\n            if row[j] < 1.e-10: # this will eliminate many zeros\n                continue\n            a_str = df.columns[j] # index column is not part of the columns\n            symbol = element(z).symbol\n            name = name=symbol+'-'+a_str\n            element_name = element(z).name\n            yield_value = row[j]\n            total_yield += yield_value\n            nuc = FPY( name=name, element_name=element_name, Z=z, A=int(a_str), yield_percent=yield_value )\n        \n            nuclides.append(nuc)\n            \n    print('Sum of yield values in data file = ',round(total_yield,2))\n    return nuclides"}, {'identified': "['paths_to_tensor']", 'updated_code': "from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path, height=224, width=224):\n    ''' Loads RGB image as PIL.Image.Image type of given Height x Width dimensions\n    '''\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(height, width))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)"}, {'identified': "['load_coords']", 'updated_code': 'from astropy import units as u\nfrom astropy.coordinates import SkyCoord\n\nfrom astropy.table import Table'}, {'identified': "['draw_annotation', 'save_annotations']", 'updated_code': 'from IPython.display import clear_output\n\ndef create_anottations(lista, save=True):\n    """Use dumb walk heuristic to create anottations\n    Args: \n    \n        lista: list of images\n        save: if true, save on current directory a csv <annottations_timestamp.csv>\n    \n    Returns: \n        \n        a dict with name of image: (xleft, ytop, xright, ytop) coordinates\n    """\n    cont = 0\n    result = {}\n    for img in lista:\n        try:\n            result[img] = find_conteiner(img)\n        except ValueError:\n            pass\n        cont += 1\n        if cont % 100 == 0:\n            clear_output()\n            print(\'...\', cont, \'...\')\n    if save:\n        save_annotations(result)\n    return result'}, {'identified': "['bow_lights_gt_50m']", 'updated_code': ''}, {'identified': 'None', 'updated_code': '# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\ndef evaluate(inp_sentence):\n  \n  # 準備英文句子前後會加上的 <start>, <end>\n  start_token = [subword_encoder_en.vocab_size]\n  end_token = [subword_encoder_en.vocab_size + 1]\n  \n  # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n  # 並在前後加上 BOS / EOS\n  inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n  # 是一個只包含一個中文 <start> token 的序列\n  decoder_input = [subword_encoder_zh.vocab_size]\n  output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n  \n  # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n  for i in range(MAX_LENGTH):\n    # 每多一個生成的字就得產生新的遮罩\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n\n    # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n    predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n    if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n    # 下個中文字的時候關注到最新的 `predicted_id`\n    output = tf.concat([output, predicted_id], axis=-1)\n\n  # 將 batch 的維度去掉後回傳預測的中文索引序列\n  return tf.squeeze(output, axis=0), attention_weights'}, {'identified': "['clean_data']", 'updated_code': ''}, {'identified': "['timeseries_to_supervised']", 'updated_code': '# No code remaining'}, {'identified': "['read_table']", 'updated_code': "'''Function to read the blank-spaced column data into a Pandas data frame (table)'''\n\nimport pandas as pd\n\n# read the data into a data frame (or table)   \ndf = pd.read_csv(file_name, delim_whitespace=True)\n\n# to avoid frustrations, set explicitly the data types of each column\ndf = df.astype({'A':int,'Element':str,'Z':int,'N':int,'T1/2-(seconds)':float},copy=False)\nprint(df.dtypes)"}, {'identified': "['shuffle_batch']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "def rv_plot2(r1, v1, r2, v2, t0, t1, epsilon=500, title1='', title2='', redrange=None):\n    #0. r1とv2はそれぞれ対応する膝屈曲角とそれの角速度, t0とt1は第１と第２の極小値近傍時刻, itimeオプションは後のため\n    ti = t0 - epsilon\n    tf = t1 + epsilon\n    tm = ti + (tf - ti) // 2    #tm は ti ~ tf の中間インデックス\n    xi = np.where(r1==np.min(r1[ti:tm]))[0][0]    #xi は ti ~ tm までの r の最小値のインデックス\n    xf = np.where(r1==np.min(r1[tm:tf]))[0][0]    #xf は tm ~ tf までの r の最小値のインデックス\n    xm = xi + (xf - xi) // 2    #xm は xi ~ xf の中間インデックス\n    \n    #1. 膝屈曲角プロット\n    plt.subplot(1, 2, 1)\n    plt.plot(r1[xi:xm], v1[xi:xm], c='black')\n    plt.plot(r1[xm:xf], v1[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r1[redrange[0]:redrange[1]], v1[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた\n    plt.title(f'{title1}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()\n    \n    #2. 大腿(下腿)傾き角プロット\n    plt.subplot(1, 2, 2)\n    plt.plot(r2[xi:xm], v2[xi:xm], c='black')\n    plt.plot(r2[xm:xf], v2[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r2[redrange[0]:redrange[1]], v2[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた       \n    plt.title(f'{title2}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()"}, {'identified': "['LeNet6']", 'updated_code': '### Train your model here.\n### Calculate and report the accuracy on the training and validation set.\n### Once a final model architecture is selected, \n### the accuracy on the test set should be calculated and reported as well.'}, {'identified': "['stick_breaking']", 'updated_code': ''}, {'identified': 'None', 'updated_code': '# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # TODO: Create a decision tree regressor object\n    regressor = None\n\n    # TODO: Create a dictionary for the parameter \'max_depth\' with a range from 1 to 10\n    params = {}\n\n    # TODO: Transform \'performance_metric\' into a scoring function using \'make_scorer\' \n    scoring_fnc = None\n\n    # TODO: Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values \'regressor\', \'params\', \'scoring_fnc\', and \'cv_sets\' respectively.\n    grid = None\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_'}, {'identified': 'None', 'updated_code': None}, {'identified': 'None', 'updated_code': "# Define a function to convert from image coords to rover coords\ndef rover_coords(binary_img):\n    # Identify nonzero pixels\n    ypos, xpos = binary_img.nonzero()\n    # Calculate pixel positions with reference to the rover position being at the \n    # center bottom of the image.  \n    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n    return x_pixel, y_pixel\n\n# Define a function to convert to radial coords in rover space\ndef to_polar_coords(x_pixel, y_pixel):\n    # Convert (x_pixel, y_pixel) to (distance, angle) \n    # in polar coordinates in rover space\n    # Calculate distance to each pixel\n    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n    # Calculate angle away from vertical for each pixel\n    angles = np.arctan2(y_pixel, x_pixel)\n    return dist, angles\n\n# Define a function to map rover space pixels to world space\ndef rotate_pix(xpix, ypix, yaw):\n    # Convert yaw to radians\n    yaw_rad = yaw * np.pi / 180\n    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n                            \n    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n    # Return the result  \n    return xpix_rotated, ypix_rotated\n\ndef translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n    # Apply a scaling and a translation\n    xpix_translated = (xpix_rot / scale) + xpos\n    ypix_translated = (ypix_rot / scale) + ypos\n    # Return the result  \n    return xpix_translated, ypix_translated\n\n\n# Define a function to apply rotation and translation (and clipping)\n# Once you define the two functions above this function should work\ndef pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n    # Apply rotation\n    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n    # Apply translation\n    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n    # Perform rotation, translation and clipping all at once\n    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n    # Return the result\n    return x_pix_world, y_pix_world\n\n# Grab another random image\nidx = np.random.randint(0, len(img_list)-1)\nimage = mpimg.imread(img_list[idx])\nwarped, mask = perspect_transform(image, source, destination)\nthreshed = color_thresh(warped)\n\n# Calculate pixel values in rover-centric coords and distance/angle to all pixels\nxpix, ypix = rover_coords(threshed)\ndist, angles = to_polar_coords(xpix, ypix)\nmean_dir = np.mean(angles)\n\n# Do some plotting\nfig = plt.figure(figsize=(12,9))\nplt.subplot(221)\nplt.imshow(image)\nplt.subplot(222)\nplt.imshow(warped)\nplt.subplot(223)\nplt.imshow(threshed, cmap='gray')\nplt.subplot(224)\nplt.plot(xpix, ypix, '.')\nplt.ylim(-160, 160)\nplt.xlim(0, 160)\narrow_length = 100\nx_arrow = arrow_length * np.cos(mean_dir)\ny_arrow = arrow_length * np.sin(mean_dir)\nplt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)"}, {'identified': "['gaussian', 'poisson_approx']", 'updated_code': ''}, {'identified': "['_residual_block', '_upscale_block', 'create_sr_model', 'get_generator_output', 'set_trainable']", 'updated_code': 'class GenerativeNetwork:\n\n    def __init__(self, img_width=96, img_height=96, batch_size=16, num_upscales=2, small_model=False,\n                 content_weight=1, tv_weight=2e5, gen_channels=64):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.batch_size = batch_size\n        self.small_model = small_model\n        self.num_scales = num_upscales\n\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n\n        self.filters = gen_channels\n        self.mode = 2\n        self.init = \'glorot_uniform\'\n\n        self.sr_res_layers = None\n        self.sr_weights_path = "weights/SRGAN.h5"\n\n        self.output_func = None'}, {'identified': "['qda_accracy']", 'updated_code': ''}, {'identified': "['compare_phot_specphot']", 'updated_code': 'from scipy.integrate import simps\n\ndef calc_spectrum_filter_flux(filter_name, SpecClass):\n    filter_object = pcc.functions.load_filter("/Users/berto/Code/CoCo/data/filters/" + filter_name + ".dat")\n    filter_object.resample_response(new_wavelength = SpecClass.wavelength)\n    filter_area = simps(filter_object.throughput, filter_object.wavelength)\n    \n    transmitted_spec = filter_object.throughput * SpecClass.flux\n\n    integrated_flux = simps(transmitted_spec, SpecClass.wavelength)\n    \n    return  integrated_flux/filter_area\n\ndef calc_specphot(sn, filtername):\n\n    specphot = np.array([])\n    specepoch = np.array([])\n\n    for spec in sn.mangledspec:\n        specphot = np.append(specphot, calc_spectrum_filter_flux(filtername, sn.mangledspec[spec]))\n        specepoch = np.append(specepoch, sn.mangledspec[spec].mjd_obs)\n    \n    return specepoch, specphot'}, {'identified': "['processCSV']", 'updated_code': ''}, {'identified': "['stern_light']", 'updated_code': ''}, {'identified': "['get_augmented_images_by_class']", 'updated_code': "def get_augmented_images(X,y,img_ct_per_class = 2500):\n    '''\n    This function creates the necessary number of images so that each class\n    has the same number of images (img_ct_per_class)\n    '''\n    aug_images = []\n    aug_labels = []\n    for cl in range(0,n_classes):\n        y_class = np.where(y == cl)\n        idx_class = y_class[0]\n        X_class = X[idx_class]\n        tmp_x,tmp_y = get_augmented_images_by_class(X=X_class, cl=cl, final_length = img_ct_per_class)\n\n        aug_images.extend(tmp_x)\n        aug_labels.extend(tmp_y)\n        \n    X_aug = np.stack(aug_images, axis = 0)\n    Y_aug = np.stack(aug_labels, axis = 0)\n    \n    return (X_aug,Y_aug)"}, {'identified': "['daterange']", 'updated_code': 'def get_trace_sum_balances(bankdata):\n    sum_balances = []\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        balances = bankdata[CURRENT][bank][\'balance\']\n        sum_account = {}\n        for date, balance in zip(dates, balances):\n            sum_account[date] = balance\n                \n        sum_balances.append(sum_account)\n    \n    total = {}\n    (ini, fin) = getIntervalDates(bankdata)\n    last = 0\n    max_amount = 0\n    for b in sum_balances:\n        for d in daterange(ini, fin):\n            if d in b:\n                last = b[d]                    \n                if d in total:\n                    total[d] += b[d]\n                    if total[d] > max_amount:\n                        max_amount = total[d]\n                else:\n                    total[d] = b[d]\n            else:\n                if d in total:\n                    total[d] += last\n                else:\n                    total[d] = last\n                \n    \n    dates = total.keys()\n    balances = total.values()\n    \n    (dates, balances) = zip(*sorted(zip(dates, balances)))\n    \n    trace = go.Scatter(\n        x = dates,\n        y = balances,\n        name = "All Accounts - Amount: " + format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".") + CURRENCY,\n        mode = \'lines\',\n        line = dict ( width = 4 )\n    )\n\n    return (max_amount, trace)'}, {'identified': "['Error_CM']", 'updated_code': 'import pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv(\'data/exoplanet.eu_catalog.csv\', \n                      usecols=[\'mass\',\'mass_error_min\',\'mass_error_max\',\n                               \'semi_major_axis\',\'semi_major_axis_error_min\',\'semi_major_axis_error_max\',\'star_name\'])\n\nclass System:\n    def __init__(self, data):\n        self.data=data\n        self.system = list(self.data.groupby("star_name").groups.keys())\n        self.Number()\n        self.Mass()\n        self.CenterOfMass()\n        #self.Error_CM()\n        \n    def Number(self):\n        sys = self.data.groupby("star_name")\n        self.N_total = len(sys["mass"])\n        \n    def Mass(self):\n        sys = self.data.groupby("star_name")\n        self.M_total = sys["mass"].sum()\n    \n    def CenterOfMass(self):\n        self.rm_i = self.data["mass"].multiply(self.data["semi_major_axis"])\n        self.data_i = self.data.assign(CM_i = self.rm_i.values) \n        p_system = self.data_i.groupby("star_name")\n        sum_rm = p_system[\'CM_i\'].sum()#.tolist()\n        self.CM = sum_rm.divide(self.M_total)   '}, {'identified': 'None', 'updated_code': 'class Point:\n    """\n    Simple class for representing a point in a Cartesian coordinate system.\n    """\n    \n    def __init__(self, x, y):\n        """\n        Create a new Point at x, y.\n        """\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        """\n        Translate the point by dx and dy in the x and y direction.\n        """\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return("Point at [%f, %f]" % (self.x, self.y))'}, {'identified': "['make_export_table']", 'updated_code': ''}, {'identified': 'None', 'updated_code': '### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n    # Here make sure to preprocess your image_input in a way your network expects\n    # with size, normalization, ect if needed\n    # image_input =\n    # Note: x should be the same name as your network\'s tensorflow data placeholder variable\n    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15,15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap)) # displays the feature map number\n        if activation_min != -1 & activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min !=-1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")'}, {'identified': 'None', 'updated_code': 'def runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n    l1 = LogisticRegression(penalty=\'l1\', tol=.01, \n                            solver="liblinear", multi_class="ovr",\n                            max_iter=500, C=c_param)\n    # Fit model\n    l1.fit(train_data, train_labels) \n    # Predict\n    predict = l1.predict(dev_data)\n    # Get precision, recall, f1 scores\n    scores = precision_recall_fscore_support(dev_labels, predict, average=\'weighted\')  \n    \n    # Get the features with non-zero coefficients.  We will use\n    # this list to reduce the features for the\n    # following logistic regression with L2 regularization\n    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n    names = np.array(list(train_data.columns))\n    non_zero_names = names[non_zero_sums] \n    \n    return {\'scores\': scores, \'non_zero_genes\': non_zero_names}'}, {'identified': "['make_document', 'update']", 'updated_code': '# voorbeeld data, laad in om werking te zien! Wel even de volgende paragraaf skippen...\n\nimport random\n\nsource = ColumnDataSource({\'x\': [], \'y\': [], \'color\': []})\n\nfig = figure(title=\'Streaming Circle Plot!\', sizing_mode=\'scale_width\',\n             x_range=[0, 1], y_range=[0, 1])\nfig.circle(source=source, x=\'x\', y=\'y\', color=\'color\', size=10)\n\ndoc.title = "Now with live updating!"\ndoc.add_root(fig)'}, {'identified': 'None', 'updated_code': 'class FunctionRunner(Runner):\n    def __init__(self, function):\n        """Initialize.  `function` is a function to be executed"""\n        self.function = function\n\n    def run_function(self, inp):\n        return self.function(inp)\n\n    def run(self, inp):\n        try:\n            result = self.run_function(inp)\n            outcome = self.PASS\n        except Exception:\n            result = None\n            outcome = self.FAIL\n\n        return result, outcome'}, {'identified': "['replace_unknown']", 'updated_code': 'def simplify_decoding(X, model):\n    """X should be a 1-D sequence of observations for the model to predict"""\n    _, state_path = model.viterbi(replace_unknown(X))\n    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions'}, {'identified': 'None', 'updated_code': "def fit_lstm(train,batch_size,nb_epoch,neurons):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    #define the loss function and optimization algorithm here\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model"}, {'identified': 'None', 'updated_code': 'class Preprocessor:\n    \n    def __init__(self, train_data_file, train_label_file, train_ids_file,\n                 instr_file, test_data_file=None, test_ids_file=None):\n        """A class to process and reformat data\n        for use in learning models"""\n        \n        # initialize the data the data filenames\n        self.train_data_file = train_data_file\n        self.train_label_file = train_label_file\n        self.train_ids_file = train_ids_file\n        self.instr_file = instr_file\n        \n        # test data is optional\n        self.test_data_file = test_data_file\n        self.test_ids_file = test_ids_file\n        \n    def read_data(self):\n        """Reads in data from the files passed to constructor"""\n        \n        # read in the data\n        train_X_df = pd.read_csv(self.train_data_file)\n        train_y_df = pd.read_csv(self.train_label_file)\n        train_ids_df = pd.read_csv(self.train_ids_file)\n        self.instr_df = pd.read_csv(self.instr_file)\n        \n        self.feature_names = [feature for feature in train_X_df]\n        self.original_feature_names = [feature for feature in train_X_df]\n        self.label_names = [feature for feature in train_y_df]\n        self.id_names = [feature for feature in train_ids_df]\n        \n        # create cross validation data\n        self.cv_X_df = pd.DataFrame(train_X_df)\n        self.cv_y_df = pd.DataFrame(train_y_df)\n        self.cv_ids_df = pd.DataFrame(train_ids_df)\n        \n        # read in the test data if it exists\n        if self.test_data_file != None:\n            self.test_X_df = pd.read_csv(self.test_data_file)\n            self.test_ids_df = pd.read_csv(self.test_ids_file)\n            self.all_X_df = train_X_df.append(self.test_X_df)\n        else:\n            self.test_X_df = None\n            self.test_ids_df = None\n            self.all_X_df = pd.DataFrame(train_X_df)\n        \n        # determine the shape of the input data\n        self.train_X_shape = train_X_df.shape\n        self.train_y_shape = train_y_df.shape\n        self.train_ids_shape = train_ids_df.shape\n        self.instr_shape = self.instr_df.shape\n        self.all_shape = self.all_X_df.shape\n        \n        # get size of test data if it exists\n        if self.test_data_file != None:\n            self.test_X_shape = self.test_X_df.shape\n            self.test_ids_shape = self.test_ids_df.shape\n        else:\n            self.test_X_shape = None\n            self.test_ids_shape = None\n\n        \n    def process(self, shuffle_train_data=False):\n        """Performs the processing on cross validation and train/test data"""\n        \n        # ADD OPTION TO SHUFFLE DATA HERE\n        \n        # processing on all data - remember to include cv_X and all_X for each condition\n        for col in self.original_feature_names:\n            print(col)\n            \n            # determine what to perform at each of the steps\n            col_instr = self.instr_df[col].values\n            col_enc = col_instr[1]\n            col_scl = col_instr[2]\n            col_imp = col_instr[3]\n\n            # impute values\n            # imputed first so that other functions will not use nan values in calculations\n            if col_imp == \'UNIQ\':\n                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n            if col_imp == \'MEAN\':\n                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n                self.all_X_df[col] = MEAN(self.all_X_df[col])\n            if col_imp == \'MODE\':\n                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n                self.all_X_df[col] = MODE(self.all_X_df[col])\n            if col_imp == \'MED\':\n                self.cv_X_df[col] = MED(self.cv_X_df[col])\n                self.all_X_df[col] = MED(self.all_X_df[col])\n            if is_int(col_imp):\n                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n            if col_imp == \'DEL\':\n                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n            \n            \n            # perform encoding of data\n            if col_enc == \'MAP\':\n                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n                self.all_X_df[col] = MAP(self.all_X_df[col])\n            if col_enc == \'OHE\':\n                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n                    feature_names=self.feature_names)\n            if col_enc == \'LOO\':\n                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n                self.all_X_df[col] = LOO(self.all_X_df[col])\n            \n\n            # perform scaling\n            if col_scl == \'NRM1\':\n                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n                self.all_X_df[col] = NRM1(self.all_X_df[col])\n            if col_scl == \'SCL1\':\n                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n                self.all_X_df[col] = SCL1(self.all_X_df[col])\n            if col_scl == \'TRSH\':\n                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n                self.all_X_df[col] = TRSH(self.all_X_df[col])\n\n        \n        # get the values from the dataframes\n        self.cv_X = self.cv_X_df.values\n        self.cv_y = self.cv_y_df.values\n        self.cv_ids = self.cv_ids_df.values\n        \n        all_X = self.all_X_df.values\n        self.train_X = all_X[:self.train_X_shape[0], :]\n        self.train_y = self.cv_y_df.values\n        self.train_ids = self.cv_ids_df.values\n        \n        if self.test_data_file != None:\n            self.test_X = all_X[self.train_X_shape[0]:, :]\n            self.test_ids = self.test_ids_df.values\n        else:\n            self.test_X = None\n            self.test_ids = None\n        \n    def write_data(self, out_dir=\'./processed_data/\'):\n        """Writes all of the data to output files"""\n        \n        # create the output directory if it does not'}, {'identified': "['vm']", 'updated_code': ''}, {'identified': "['add_degrees_isotypic', 'add_degrees_symmetric', 'add_degrees_test']", 'updated_code': ''}, {'identified': "['add_airports_name', 'get_airports_arrival_sorted', 'get_df_cols', 'get_name', 'print_top_n_arrival_airport']", 'updated_code': '#METHOD PART'}, {'identified': "['return_itself']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def evaluate(X_data, y_data, b, lr):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        summary, accuracy = sess.run([summary_op,accuracy_operation], feed_dict={tf_train_dataset : batch_x, tf_train_labels : batch_y, \n                         tf_beta: b, tf_keep_prob : 1, tf_learning_rate : lr})\n        total_accuracy += (accuracy * len(batch_x))\n    return summary, total_accuracy / num_examples'}, {'identified': "['featurize_state']", 'updated_code': ''}, {'identified': "['logprob', 'random_distribution', 'sample', 'sample_distribution']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def is_valid_url(url):\n    try:\n        result = http_program(url)\n        return True\n    except ValueError:\n        return False'}, {'identified': 'None', 'updated_code': 'def build_index(non_empty_days, from_time, to_time):\n    date_ranges = []\n    for date in non_empty_days:\n        yyyy, mm, dd = date.split(\'-\')\n        from_hour, from_min = from_time.split(\':\')\n        to_hour, to_min = to_time.split(\':\')    \n        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n        date_ranges.append(pd.DataFrame({"OrganizedDateTime": pd.date_range(t1, t2, freq=\'1Min\').values}))\n    agg = pd.concat(date_ranges, axis=0) \n    agg.index = agg["OrganizedDateTime"]\n    return agg'}, {'identified': "['error']", 'updated_code': ''}, {'identified': "['create_model', 'create_model']", 'updated_code': "# Configure model presets\nfrom examples.seismic import demo_model\n\n# Enable model presets here:\npreset = 'layers-isotropic'  # A simple but cheap model (recommended)\n# preset = 'marmousi2d-isotropic'  # A larger more realistic model\n\n# Standard preset with a simple two-layer model\nif preset == 'layers-isotropic':\n    filter_sigma = (1, 1)\n    nshots = 21\n    nreceivers = 101\n    t0 = 0.\n    tn = 1000.  # Simulation last 1 second (1000 ms)\n    f0 = 0.010  # Source peak frequency is 10Hz (0.010 kHz)\n\n\n# A more computationally demanding preset based on the 2D Marmousi model\nif preset == 'marmousi2d-isotropic':\n    filter_sigma = (6, 6)\n    nshots = 301  # Need good covergae in shots, one every two grid points\n    nreceivers = 601  # One recevier every grid point\n    t0 = 0.\n    tn = 3500.  # Simulation last 3.5 second (3500 ms)\n    f0 = 0.025  # Source peak frequency is 25Hz (0.025 kHz)"}, {'identified': "['ode']", 'updated_code': '# 2 1d ODEs'}, {'identified': "['window_bounds']", 'updated_code': ''}, {'identified': "['calc_other_neighbour_msg_prod']", 'updated_code': ''}, {'identified': "['neuron']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "def get_wide_deep():\n    # define column types\n    is_male,mother_age,plurality,gestation_weeks = \\\n        [\\\n            tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n                        ['True', 'False', 'Unknown']),\n            tf.feature_column.numeric_column('mother_age'),\n            tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n                        ['Single(1)', 'Twins(2)', 'Triplets(3)',\n                         'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),\n            tf.feature_column.numeric_column('gestation_weeks')\n        ]\n\n    # discretize\n    age_buckets = tf.feature_column.bucketized_column(mother_age, \n                        boundaries=np.arange(15,45,1).tolist())\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n                        boundaries=np.arange(17,47,1).tolist())\n      \n    # sparse columns are wide \n    wide = [is_male,\n            plurality,\n            age_buckets,\n            gestation_buckets]\n    \n    # feature cross all the wide columns and embed into a lower dimension\n    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n    embed = tf.feature_column.embedding_column(crossed, 3)\n    \n    # continuous columns are deep\n    deep = [mother_age,\n            gestation_weeks,\n            embed]\n    return wide, deep"}, {'identified': "['fill_nulls']", 'updated_code': '# The code is now empty as the only function definition has been removed.'}, {'identified': "['canny', 'draw_lines', 'gaussian_blur', 'grayscale', 'hough_lines', 'region_of_interest', 'weighted_img']", 'updated_code': 'import math'}, {'identified': "['view_samples']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'class EuropeanOption(object):\n    """ Abstract Class for European options. Partially implemented.\n    S0 : float : initial stock/index level\n    strike : float : strike price\n    T : float : time to maturity (in year fractions)\n    r : float : constant risk-free short rate\n    div :    float : dividend yield\n    sigma :  float : volatility factor in diffusion term\n    model: str: name of the model for the pricing"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, option_type, S0, strike, T, r, div, sigma, model):\n        try:\n            self.option_type = option_type\n            assert isinstance(option_type, str)\n            self.S0 = float(S0)\n            self.strike = float(strike)\n            self.T = float(T)\n            self.r = float(r)\n            self.div = float(div)\n            self.sigma = float(sigma)\n            self.model = str(model)\n        except ValueError:\n            print(\'Error passing Options parameters\')\n\n        models = [\'BlackScholes\', \'MonteCarlo\', \n                  \'BinomialTree\', \'TrinomialTree\',\n                  \'FFT\', \'PDE\']\n        \n        if model not in models:\n            raise Exception(\'Error: Model unknown\')\n            \n        option_types = [\'call\', \'put\']\n        \n        if option_type not in option_types:\n            raise ValueError("Error: Option type not valid. Enter \'call\' or \'put\'")\n        if S0 < 0 or strike < 0 or T <= 0 or r < 0 or div < 0 or sigma < 0:\n            raise ValueError(\'Error: Negative inputs not allowed\')\n            \n        self.discount = np.exp(-self.r * self.T)\n\n    def getmodel(self):\n        return self.model\n\n    def __str__(self):\n        return "This European Option is priced using {0}".format(self.getmodel())\n\n    @abstractmethod\n    def value(self):\n        pass\n    \n    @abstractmethod\n    def delta(self):\n        pass'}, {'identified': "['make_input_fn']", 'updated_code': ''}, {'identified': "['insert_and_time_filter_cuckoo_filter']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'import ipywidgets as widgets\nimport json\nimport time\nfrom threading import Thread\nfrom traitlets import Int, Unicode, Tuple, CInt, Dict, validate, observe\n\n\nclass newcyjsWidget(widgets.DOMWidget):\n    \n    _view_name = Unicode(\'CyjsView\').tag(sync=True)\n    _view_module = Unicode(\'cyjs\').tag(sync=True)\n    frameWidth = Int(400).tag(sync=True)\n    frameHeight = Int(300).tag(sync=True)\n    msgFromKernel = Unicode("{}").tag(sync=True)\n    msgToKernel = Unicode("{}").tag(sync=True)\n    status = "initial status message\\n"\n    selectedNodes = [];\n    incomingMessageArrivedAndParsed = False;\n    globalStatus = "blank"\n\n    #------------------------------------------------------------------------------\n    class MyThread(Thread):\n       owner = None\n       def __init__(self, owner, group=None, target=None, name=None,\n                    args=(), kwargs=None, *, daemon=None):\n          Thread.__init__(self, group, target, name, args, kwargs, daemon=daemon)\n          self.owner = owner\n\n       def run(self):\n          maxLoops = 5\n          counter = 0\n          while((self.owner.incomingMessageArrivedAndParsed == False) and (counter < maxLoops)):\n             counter += 1\n             print("thread, no message yet, sleeping, loop %d" % counter)\n             time.sleep(1.0)\n          self.owner.status += "thread owner\'s selectedNodes: %s\\n" % self.owner.selectedNodes;\n          self.owner.status += "MyThread ending loop\\n";\n\n       def result(self):\n          #while(not self.owner.incomingMessageArrivedAndParsed):\n          #   time.sleep(0.1)\n          return("imaap? %s" % self.owner.incomingMessageArrivedAndParsed)\n            \n    #------------------------------------------------------------------------------\n\n    def testThread(self):\n      for i in range(4):\n         threadName = "Thread-%s" % (i + 1)\n         mythread = self.MyThread(name=threadName, owner=self)\n         mythread.start()\n    \n    def setSize(self, width, height):\n      self.status += "setSize(%d, %d)\\n" % (width, height)\n      self.frameWidth = width\n      self.frameHeight = height\n        \n    def fit(self, margin=50):\n      self.status += "entering fit (%d)\\n" % margin\n      self.msgFromKernel = json.dumps({"cmd": "fit", "status": "request",\n                                       "callback": "", "payload": margin});\n    def getSelectedNodes(self):\n      #self.selectedNodes = [];\n      self.incomingMessageArrivedAndParsed = False;\n      self.status += "entering getSelectedNodes\\n";\n      self.msgFromKernel = json.dumps({"cmd": "cleanSlate", "status": "request", "callback": "", "payload":""});\n      self.msgFromKernel = json.dumps({"cmd": "getSelectedNodes", "status": "request",\n                                       "callback": "", "payload": ""});\n      \n      observingThread = self.MyThread(name="getSelectedNodes-thread", owner=self)\n      print("getSelectedNodes about to start observingThread")\n      #observingThread.start()\n      #observingThread.join()\n      #while(self.incomingMessageArrivedAndParsed == False):\n      #   time.sleep(0.5)\n      observingThread.start()\n      self.status += "getSelectedNodes, observingThread now started: %s\\n" %  self.selectedNodes\n      self.status += "getSelectedNodes, incomingMessageArrivedAndParsed? %s\\n" % self.incomingMessageArrivedAndParsed\n      return(observingThread.result())\n        #return(self.status)\n        \n    def selectNodes(self, nodes):\n      self.msgFromKernel = json.dumps({"cmd": "selectNodes", "status": "request",\n                                       "callback": "", "payload": nodes});\n       \n    def clearSelection(self):\n      self. msgFromKernel = json.dumps({"cmd": "clearSelection", "status": "request",\n                                        "callback": "", "payload": ""});\n        \n    @observe(\'msgToKernel\')\n    def msg_arrived(self, change):\n        self.status += "---- python - msg arrived\\n"\n        tmp = change[\'new\']\n        self.status += "len of tmp: %d\\n" % len(tmp)\n        self.status += "type of tmp: %s\\n" % type(tmp)\n        self.msgToKernel = tmp\n        self.status += "%s\\n" % tmp\n        self.incomingMessageArrived = True\n        self.dispatch(self.msgToKernel)\n \n    def dispatch(self, msgRaw):\n        self.msg = json.loads(msgRaw)\n        self.status += "entering dispatch\\n"\n        self.status += "dispatch this msg: %s\\n" % self.msg\n        self.status += "msg.cmd: %s\\n" % self.msg["cmd"]\n        if self.msg["cmd"] == \'storeSelectedNodes\':\n            self.status += "storing selected nodes to self.selectedNodes %s\\n" % msg["payload"]\n            self.selectedNodes = msg["payload"]\n        elif self.msg["cmd"] == \'clearCircles\':\n            self.circles = []\n        else:\n          print("unknown cmd: %s" % self.msg["cmd"])\n        self.incomingMessageArrivedAndParsed = True\n        \n    def getResponse(self):\n        return(self.msg["payload"])'}, {'identified': "['term_output']", 'updated_code': ''}, {'identified': "['getWiki']", 'updated_code': 'import pandas as pd\n\ndf = pd.DataFrame()\nfor wikiPage in wikiPages:\n    wiki2 = pd.read_csv(path+wikiPage+".csv", usecols=[0,1], parse_dates=[0], index_col=[0], header=None)\n    wiki2 = wiki2.resample("W-Sun").sum()\n    df[wikipage] = wiki2\nreturn df'}, {'identified': "['ResNet50_predict_labels']", 'updated_code': 'from keras.applications.resnet50 import preprocess_input, decode_predictions'}, {'identified': "['inverse_difference']", 'updated_code': ''}, {'identified': "['getBestParams']", 'updated_code': "#mini_train_data, mini_test_data, mini_train_labels, mini_test_labels = train_test_split(train_data, train_labels,\n#                                    stratify=train_labels, \n#                                    test_size=0.55)\n\n\n#\n# Logistic Regression\n#\nlr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\nparams = {'C': [0.1, 0.25,  0.5,]}\nlogit = GridSearchCV(lr, params, cv=5,\n                     scoring='accuracy', return_train_score=True)\n\n# Fit  training data\nlogit.fit(train_data, train_labels)  \n# Show the best C parameter to use and the expected accuracy\nprint('\\nLogistic Regression Classifier, L2 regularization')\nprint(' Best param:', logit.best_params_)\nprint(' Accuracy:  ', np.round(logit.best_score_, 4) )"}, {'identified': "['convert_column_string_encoding', 'get_mjdmax_BessellV']", 'updated_code': ''}, {'identified': "['window_bounds']", 'updated_code': ''}, {'identified': "['forecast']", 'updated_code': ''}, {'identified': "['func1']", 'updated_code': ''}, {'identified': "['get_nuclides']", 'updated_code': "'''Function for creating a nuclide container'''"}, {'identified': "['get_model_params', 'restore_model_params']", 'updated_code': ''}, {'identified': "['accuracy']", 'updated_code': 'def simplify_decoding(observations, model):\n    # Implementation of simplify_decoding goes here\n    pass'}, {'identified': "['read_dataset']", 'updated_code': "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\nLABEL_COLUMN = 'weight_pounds'\nKEY_COLUMN = 'key'\nDEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\nTRAIN_STEPS = 1000"}, {'identified': "['hypothesis_capswords', 'hypothesis_inlinecounter', 'hypothesis_nouns', 'hypothesis_pos', 'hypothesis_verbs']", 'updated_code': ''}, {'identified': "['build_poly']", 'updated_code': ''}, {'identified': "['calc_count_xy_y', 'calc_transition_count', 'train']", 'updated_code': 'class Hmm(object):\n    """\n    Stores counts for n-grams and emissions. \n    """\n\n    def __init__(self, n=3):\n        assert n>=2, "Expecting n>=2."\n        self.n = n\n        self.emission_counts = defaultdict(int)\n        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n        self.all_states = set()\n        self.count_y = dict() \n        #[(\'O\', 0), (\'I-MISC\', 0), (\'I-PER\', 0), (\'I-ORG\', 0), (\'I-LOC\', 0), (\'B-MISC\', 0), (\'B-PER\', 0), (\'B-ORG\', 0), (\'B-LOC\', 0)])\n        self.count_xy = dict()\n        self.trigram_counts = dict()\n        self.bigram_counts = dict()'}, {'identified': 'None', 'updated_code': 'def coords_of_max(theArray, n):\n    # Flatten the 2D array\n    flat = theArray.flatten()\n    # Partition so that the we know the sort order for\n    # the cells with the highest values.  We just\n    # care about the top n highest values.  So for example,\n    # if n = 3, get return 3 indices.\n    indices = np.argpartition(flat, -n)[-n:]\n    # Reverse so that we show index of highest value first\n    # (descending)\n    indices = indices[np.argsort(-flat[indices])]\n    # Now return the coordinates for these indices\n    # for a 2D array.  This will return 2 arrays,\n    # the first for the row index, the second for the\n    # column index.  The row index represents the\n    # actual digit, the column index represents\n    # the confused digit\n    return np.unravel_index(indices, theArray.shape)'}, {'identified': "['VGG16_predict_breed', 'extract_VGG16']", 'updated_code': 'from extract_bottleneck_features import *'}, {'identified': 'None', 'updated_code': '### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n    # Here make sure to preprocess your image_input in a way your network expects\n    # with size, normalization, ect if needed\n    # image_input =\n    # Note: x should be the same name as your network\'s tensorflow data placeholder variable\n    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15,15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap)) # displays the feature map number\n        if activation_min != -1 & activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min !=-1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")'}, {'identified': 'None', 'updated_code': '#ignore\n# @pysnooper.snoop()\ndef plot_2d(value, ax=None, group=None, mask=None, matrix_id=0, mat_as_group=False, \n            group_id=None, linewidths=0, is_string=False, fmt="d", square=False):\n  \n  if hasattr(value, "numpy"):\n    value = value.numpy()\n  if group is not None and hasattr(group, "numpy"):\n    group = group.numpy()\n  if mask is not None and hasattr(mask, "numpy"):\n    mask = tf.squeeze(mask)\n    mask = tf.ones_like(value) * mask\n    mask = mask.numpy()\n    \n\n  cmaps = [\'PuOr\', \'tab20b\', \'RdBu\']\n  group_id = int(group[0][0])\n  cmap = cmaps[group_id]\n  \n  if is_string:\n    fmt = \'\'\n  \n  sns.heatmap(group, \n              fmt=fmt,\n#               cmap=cmap,\n              cmap=cmap,\n              annot=value, \n              cbar=False, \n              xticklabels=False, \n              yticklabels=False, \n              square=square,\n              mask=mask,\n              linewidths=linewidths,\n              ax=ax)'}, {'identified': 'None', 'updated_code': 'def buildProfit(bankdata):\n    data = []\n\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        movements = bankdata[CURRENT][bank][\'movements\']\n\n        profit = {}\n        for date, movement in zip(dates, movements):\n            key = str(date.month) + \'/\' + str(date.year)\n\n            if key in profit:\n                profit[key] += float(movement)\n            else:\n                profit[key] = float(movement)\n            \n        months = []\n        profits = []\n        for key, value in profit.items():\n            months.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            profits.append(value)\n        \n        trace = go.Bar(\n            x = months,\n            y = profits,\n            name = "Profit for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n    return data'}, {'identified': 'None', 'updated_code': '#ignore\n# @pysnooper.snoop()\ndef plot(value, group=None, group_dim=0, mask=None, labels=[], square=False, shape_desc=\'\', width_prec=None, bottom_prec=None, \n         is_string=False, name=\'\', fmt="d", single_plot_size=4, h_dist_ratio=0.7, w_dist_ratio=0.7, linewidths=None):\n\n  shape = value.shape\n  num_groups = shape[0]\n  \n  if hasattr(value, \'numpy\'):\n    if value.dtype in [tf.int32, tf.int64]:\n      value = tf.cast(value, tf.int32)\n    elif value.dtype in [tf.float32, tf.float64] and fmt == "d":\n      fmt = ".2g"\n    \n    value = value.numpy()\n\n  if hasattr(value, \'ndim\'):\n    ndim = value.ndim\n  else:\n    ndim = len(value)\n    \n    \n  if ndim == 2:\n    value = value[np.newaxis, np.newaxis, :, :]\n  if ndim == 3:\n    value = value[np.newaxis, :, :, :]\n  if ndim == 4:\n    pass\n  \n  # decide how to group sub-tensors smartly\n  if not group_dim:\n    group_dim = ndim - 1\n    \n  # generate group identifier tensor for differentiating between\n  # different bactch / sentence\n  if group is None:\n    group_ids = tf.range(num_groups, dtype=tf.int64).numpy()\n    if group_dim == 1:\n      group = group_ids[:, tf.newaxis]\n    elif group_dim == 2:\n      group = group_ids[:, tf.newaxis, tf.newaxis]\n    elif group_dim == 3:\n      group = group_ids[:, tf.newaxis, tf.newaxis, tf.newaxis]\n\n    # broadcast to all groups    \n    group = tf.ones(shape=value.shape) * group\n\n  d0, d1, d2, d3 = value.shape\n\n  # set figure size based on tensor dimensions\n  fig_width = (d3 * 1.0 / 4) * d0 * single_plot_size\n  fig_height = (d2 * 1.0 / 4) * single_plot_size\n  figsize = (fig_width, fig_height)\n  \n  if width_prec is None:\n    width_prec = 1.0 / d0\n  \n  if bottom_prec is None:\n    bottom_prec = 1.0 / d1\n\n  fig = plt.figure(figsize=figsize)\n  fig_title = f\'name: {name}, shape: {shape}\' if name else f\'shape: {shape}\'\n  \n  if shape_desc:\n    fig_title = fig_title + \' = \' + shape_desc\n    \n  for e0 in range(d0):\n\n    # plot 2d array in reverse order since the earlier plot will be covered\n    for e1 in reversed(range(d1)):\n      annot = value[e0, e1]\n\n      # select corresponding matplotlib axis      \n      cur_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3) * w_dist_ratio, \n                             e1 / d2 * h_dist_ratio, \n                             width_prec, \n                             bottom_prec]) \n\n      matrix_id = e0 + e1 * 2\n      \n      if mask is not None:\n        if ndim == 2:\n          mask_idx = e0\n        elif ndim == 3:\n          mask_idx = e1\n        elif ndim ==4:\n          mask_idx = e0\n          \n        # mimic broadcasting\n        if mask.shape[0] == 1:\n          mask_idx = 0\n          \n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, \n                is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n      else:\n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, is_string=is_string, fmt=fmt, square=square)\n      \n      # minic shadowing for each 2d matrix\n      width_delta_prec = 0.0005\n      height_delta_prec = width_delta_prec * d2 / d3\n      \n      for k in range(1, 3):\n        shadow_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3)  * w_dist_ratio - width_delta_prec * k, \n                                  e1 / d2 * h_dist_ratio - height_delta_prec * k, \n                                  width_prec, \n                                  bottom_prec])  \n        \n        if k == 2:\n          linewidths = 1\n        else:\n          linewidths = 0\n          \n        if mask is not None:\n          if ndim == 2:\n            mask_idx = e0\n          elif ndim == 3:\n            mask_idx = e1\n          elif ndim ==4:\n            mask_idx = e0\n            \n          # mimic broadcasting\n          if mask.shape[0] == 1:\n            mask_idx = 0  \n            \n            \n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n        else:\n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, square=square)\n\n      if e0 == 0 and e1 == 0:\n        ax1 = cur_ax\n        \n        if labels:\n            plt.ylabel(labels[-2])\n            plt.xlabel(labels[-1] + \'\\n\' + fig_title)\n        else:\n          plt.xlabel(fig_title)\n\n        # 4D 中的 axis1 說明 label\n#           if len(labels) >= 3:\n#             plt.text(d3 + 2, 1 + 0.5, labels[-3],\n#                      rotation=0, rotation_mode=\'anchor\')\n\n      if e1 == d0 - 1:\n        ax2 = cur_ax\n        \n\n#       transFigure = fig.transFigure.inverted()\n#       coord1 = transFigure.transform(ax1.transData.transform([d3 + 2 + 0.5, 0]))\n#       coord2 = transFigure.transform(ax2.transData.transform([d3 + 0.5, d2]))\n\n\n#       line = mpl.lines.Line2D((coord1[0],coord2[0]),(coord1[1],coord2[1]), \n#                               transform=fig.transFigure, \n#                               linestyle=\'--\',\n#                               color=\'black\')\n#       fig.lines.append(line)'}, {'identified': 'None', 'updated_code': 'def calc_sum_product_variable_to_factor_msg(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    if len(neighbour_msg_prod) > 0:\n        return np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        return np.ones(variable.num_states)'}, {'identified': "['new_loglike_model', 'new_prior_transform_model']", 'updated_code': 'def sample_withplot(loglike_model, prior_transform_model, datafile, priorRange):\n    data_file = io.get_data_file_path(datafile)\n    data_x, data_xerr, data_y, data_yerr = io.load_data(data_file)\n    \n    # n: number of parameters, len(priorRange)\n    n = len(priorRange)\n    \n    result = nestle.sample(loglike_model, prior_transform_model, n)\n    \n    print(\'log evidence\')\n    print(result.logz)\n    \n    print(\'numerical (sampling) error on logz\')\n    print(result.logzerr)\n    \n    print(\'array of sample parameters\')\n    print(result.samples)\n    \n    print(\'array of weights associated with each sample\')\n    print(result.weights)\n    \n    import matplotlib.pyplot as plt\n    import corner\n    \n    p_fit, cov_fit = nestle.mean_and_cov(result.samples, result.weights)\n    \n    plt.figure()\n    plt.errorbar(data_x, data_y, yerr=data_yerr, fmt=\'*\')\n    plt.xlabel("r (kpc)")\n    plt.ylabel(\'V (km/s)\')\n    plt.title("Results of using the model to fit the DM rotational velocity distribution")\n    xplot = [5 + 5 * i for i in range(40)]\n    yplot = [model.model_NFW(xplot[i], p_fit) for i in range(40)]\n    plt.plot(xplot, yplot)\n    plt.show()\n    \n    fig = corner.corner(result.samples, weights=result.weights, labels=[\'a\', \'rho0\'],\n                        range=[0.99999, 0.99999], bins=30)\n    plt.show()\n    \n    return result'}, {'identified': 'None', 'updated_code': 'def delete_from_bloom_filter_and_time(capacity, percent_to_fill=0.9, percent_delete=1.0):\n    b_filter, _ = insert_and_time_filter_bloom_filter(capacity, percent_fill=percent_to_fill)\n    \n    total_size = b_filter.get_size()\n    number_of_items_to_delete = total_size*percent_delete\n    \n    now = time.time()\n    for i in range(int(number_of_items_to_delete)):\n        b_filter.remove(str(i))\n    elapsed_time = time.time() - now\n        \n    return b_filter, elapsed_time'}, {'identified': 'None', 'updated_code': 'def read_s3_csv(dates):\n    s3 = boto3.resource(\'s3\')\n    deutsche_boerse_bucket = \'deutsche-boerse-xetra-pds\'\n\n    bucket = s3.Bucket(deutsche_boerse_bucket)\n\n    dataframes = []\n\n    for date in dates:\n        objs_count = 0\n        csv_objects = bucket.objects.filter(Prefix=date)\n        for csv_obj in csv_objects:\n            csv_key = csv_obj.key\n            if csv_key[-4:] == \'.csv\':\n                objs_count += 1\n                csv_body = csv_obj.get()[\'Body\']\n                df = pd.read_csv(csv_body)\n                dataframes.append(df)\n\n        print("Loaded {} data objects for {}".format(objs_count, date))\n    return pd.concat(dataframes)'}, {'identified': "['squared_error']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def clean_election_data():\n    \'\'\'\n    Function to clean election data \n    \'\'\'\n    import math\n        \n    # read in dirty data \n    df = pd.read_csv("2014_election_results.csv")\n    dfClean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\n    for i in range(len(dfClean)):\n        row = dfClean.iloc[i]  \n        row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n        if(pd.isnull(row["CANDIDATE NAME"]) or (row["CANDIDATE NAME"] == \'Scattered\')):\n            if(pd.isnull(row["CANDIDATE NAME (Last)"]) or (row["CANDIDATE NAME (Last)"] == \'Scattered\')):\n                row["CANDIDATE NAME"] = "UNKNOWN" \n            else:\n                row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"]\n    \n    dfClean = dfClean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\n    return dfClean'}, {'identified': 'None', 'updated_code': '# returns "True" if face is detected in image stored at img_path\ndef face_detector(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray)\n    return len(faces) > 0'}, {'identified': "['convert_col_type']", 'updated_code': "# By pasting the above list into a spreadsheet and cross checking with the data dictionary, we can \n# see which category each field should be\n\n# statsmodel requires all fieldsnames to begin with letters, so let's sort this out now.\ntrain = train.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ntest = test.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ndata_full = pd.concat([train, test], keys = ['train', 'test'])\n\n# Makes lists of each type\ncategories = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n              'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'YearBuilt', \n              'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', \n              'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'Heating', \n              'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', \n              'GarageFinish', 'GarageCars', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', \n              'SaleCondition']\nfloats = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n          'FirstFlrSF', 'SecondFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n          'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nints = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \n         'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n\nbools = ['CentralAir']\n\nfeature_names = categories + floats + ints + bools"}, {'identified': "['disp_image']", 'updated_code': 'import matplotlib.image as mpimg'}, {'identified': "['get_feature_names']", 'updated_code': 'class ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {\'a\': [1, 5, 2, 5, 2, 8],\n               \'b\': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key=\'a\')\n    >> data[\'a\'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        # if self.key == \'playlist_pid\': from IPython.core.debugger import set_trace; set_trace()\n        return data_dict[:,[self.key]].astype(np.int64)'}, {'identified': "['make_feature_cols']", 'updated_code': ''}, {'identified': "['dateRange', 'date_to_week', 'week_to_date']", 'updated_code': ''}, {'identified': "['tokenizer']", 'updated_code': 'from nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]'}, {'identified': "['square']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def buildTraces(bankdata, min_trace = False, mean_trace = False, max_trace = False):\n    data = []\n    min_amount = 0\n    max_amount = 0\n    \n    for accountType in ACCOUNTTYPES: # Account type\n        for bank in bankdata[accountType]: # Bank name\n            dates = np.array(bankdata[accountType][bank][\'date\'])\n            balances = np.array(bankdata[accountType][bank][\'balance\'])\n            \n            bankName = \'(\' + accountType + \') \' + SUPPORTED_BANKS[bank]\n\n            trace_main = go.Scatter(\n                x = dates,\n                y = balances,\n                name = bankName + \': Saldo \' + str(format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".")) + CURRENCY,\n                #line = dict(\n                #    color = \'green\'\n                #),\n                mode = \'lines\'\n            )\n            data.append(trace_main)\n\n            if max_trace:\n                trace_max = go.Scatter(\n                    x = dates,\n                    y = [balances.max() for f in dates],\n                    name = bankName + \': Saldo máximo\',\n                    #visible = \'legendonly\',\n                    #hoverinfo = \'name\',\n                    line = dict(\n                        #color = \'cyan\',\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_max)\n\n            if mean_trace:\n                trace_mean = go.Scatter(\n                    x = dates,\n                    y = [balances.mean() for f in dates],\n                    name = bankName + \': Saldo medio\',\n                    #hoverinfo = \'none\',\n                    line = dict(\n                        #color = \'magenta\',\n                        width = 4,\n                        dash = \'dashdot\'\n                    )\n                )\n                data.append(trace_mean)\n\n            if min_trace:\n                trace_min = go.Scatter(\n                    x = dates,\n                    y = [balances.min() for f in dates],\n                    name = bankName + \': Saldo mínimo\',\n                    line = dict(\n                        #color = \'red\',\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_min)\n                \n            # Extra\n            if balances.max() > max_amount:\n                max_amount = balances.max()\n    \n    max_amount, sum_trace = get_trace_sum_balances(bankdata)\n    data.append(sum_trace)\n    \n    return (data, min_amount, max_amount)'}, {'identified': "['get_translation']", 'updated_code': '#helper in getting top features and making visual\n#convert relevant dataframe columns to lowercase so we can compare with top feature output\n\ntrack_artist_lower_df = tracks_df["track_artist_uri"].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df["track_album_uri"].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged["track_uri"].apply(lambda x: x.lower())'}, {'identified': 'None', 'updated_code': 'def IRLS(data):\n    length        = len(data)\n    not_converged = True\n    w             = np.array([0, 0, 0])\n    p             = np.empty(length)\n    s             = np.empty(length)\n    z             = np.empty(length)\n\n    ones          = np.ones(200)\n    ones[100:]    = ones[100:]*-1\n    \n    temp_var      = np.hstack((ones[None].T,data))\n    np.random.shuffle(temp_var)\n    \n    y             = temp_var[:,0]\n    data          = temp_var[:,1:]\n    print(data.shape)\n    \n    while not_converged:\n        w_prev = w\n        for i in range(length):\n            p[i] = np.exp(w_prev.dot(data[i])) / (1 + np.exp(w_prev.dot(data[i])))\n            s[i] = p[i]*(1-p[i])\n            \n            if math.isnan(s[i]): s[i]=1\n            \n            z[i] = w_prev.dot(data[i]) + (y[i]-p[i])/(max(s[i],0.00001))\n            \n        diag_s = np.diag(s)\n#         print(\'diag:\', diag_s.shape)\n        t1     = np.linalg.inv(np.dot(np.dot(data.T, diag_s), data))\n        t2     = np.dot(np.dot(data.T, diag_s), z)\n        w      = np.dot(t1, t2)\n#         print("t1, t2",t1.shape, t2.shape)\n        w      = w/np.linalg.norm(w)\n        \n        print(\'Iterations\',w)\n        \n        if abs(sum(w-w_prev)) < 0.000001:\n            \n            print("Converged!!")\n            not_converged = False\n            return w\n            \n        elif sum(abs(w)) > 900 or math.isnan(w[0]):\n            \n            print("Not converging!!!")\n            return w\n            \n    return w'}, {'identified': "['pos_vel_filter']", 'updated_code': 'from filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\ndef Q_discrete_white_noise(dim, dt, var):\n    n = int(dim / 2)\n    G = np.zeros((dim, n))\n    G[:n, :] = 0.5 * dt**2 * np.eye(n)\n    G[n:, :] = dt * np.eye(n)\n    Q = np.dot(G, G.T) * var\n    return Q'}, {'identified': 'None', 'updated_code': 'def removal_evolution(removal_list, G):\n    disting_bet_average_degree = []\n    hist = nx.degree_histogram(G)\n    last_average_degree = float(sum(hist[i] * i for i in range(len(hist)))) / float(sum(hist))\n    idx_node = 0\n    while last_average_degree > 0 and idx_node < len(removal_list):\n        disting_bet_average_degree.append(last_average_degree)\n        hist = nx.degree_histogram(G)\n        last_average_degree = float(sum(hist[i] * i for i in range(len(hist)))) / float(sum(hist))\n        G.remove_node(removal_list[idx_node][0])\n        idx_node = idx_node + 1\n    return disting_bet_average_degree'}, {'identified': "['regression_and_rss']", 'updated_code': 'import plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go'}, {'identified': 'None', 'updated_code': None}, {'identified': "['highlight_max']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "def kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):\n    heterogeneity = {}\n    \n    min_heterogeneity_achieved = float('inf')\n    best_seed = None\n    final_centroids = None\n    final_cluster_assignment = None\n    \n    for i in xrange(num_runs):\n        \n        # Use UTC time if no seeds are provided \n        if seed_list is not None: \n            seed = seed_list[i]\n            np.random.seed(seed)\n        else: \n            seed = int(time.time())\n            np.random.seed(seed)\n        \n        # Use k-means++ initialization\n        # YOUR CODE HERE\n        initial_centroids = get_initial_centroids(data, k, seed=0)\n        \n        # Run k-means\n        # YOUR CODE HERE\n        centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter=400,\n                                       record_heterogeneity=None, verbose=True)\n        \n        # To save time, compute heterogeneity only once in the end\n        # YOUR CODE HERE\n        heterogeneity[seed] = compute_heterogeneity(data, k, centroids, cluster_assignment)\n        \n        if verbose:\n            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n            sys.stdout.flush()\n        \n        # if current measurement of heterogeneity is lower than previously seen,\n        # update the minimum record of heterogeneity.\n        if heterogeneity[seed] < min_heterogeneity_achieved:\n            min_heterogeneity_achieved = heterogeneity[seed]\n            best_seed = seed\n            final_centroids = centroids\n            final_cluster_assignment = cluster_assignment\n    \n    # Return the centroids and cluster assignments that minimize heterogeneity.\n    return final_centroids, final_cluster_assignment"}, {'identified': 'None', 'updated_code': "# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars"}, {'identified': "['preprocess_inception_input']", 'updated_code': "### TODO: Write a function that takes a path to an image as input\n### and returns the dog breed that is predicted by the model.\n\nfrom keras.applications.inception_v3 import InceptionV3\n\ndef detect_dog_breed(img_path, given_model, use_bottleneck=True, img_H=224, img_W=224):\n    ''' Detect dog breed given image in the img_path,\n        using given model, using either bottleneck features (or not)\n        with given img Height and Width\n        \n        @return: Dog breed (str)\n    '''\n    print('Detecting dog breed...')\n    tensor = path_to_tensor(img_path, img_H, img_W)\n    \n    # using given image, extract its bottleneck features by running thru InceptionV3 n/w first\n    if use_bottleneck: \n        tensor = extract_InceptionV3(tensor)\n    \n    # print('  [input tensor shape: {}]'.format(tensor.shape))\n    # make predictions (probabilities)\n    predicted_vector = given_model.predict(tensor)\n    # get max index\n    y_hat = np.argmax(predicted_vector)\n    chance = 100. * predicted_vector[0][y_hat]  # probability of correctness\n    # print('  [y_hat:{}]'.format(y_hat))\n    # print('  prob:{:.2f}%'.format(chance))\n\n    # return dog breed and probability \n    return dog_names[y_hat], chance"}, {'identified': "['parser']", 'updated_code': 'filename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)\n\n# If the parsed data only contains one column then return a Series'}, {'identified': 'None', 'updated_code': 'class DiscriminatorNetwork:\n\n    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.adversarial_loss_weight = adversarial_loss_weight\n        self.small_model = small_model\n\n        self.k = 3\n        self.mode = 2\n        self.weights_path = "weights/Discriminator weights.h5"\n\n        self.gan_layers = None\n\n    def append_gan_network(self, true_X_input):\n\n        # Normalize the inputs via custom VGG Normalization layer\n        x = Normalize(type="gan", value=127.5, name="gan_normalize")(true_X_input)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_1\', kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name="gan_lrelu1_1")(x)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_2\', strides=(2, 2), kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu1_2\')(x)\n        x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm1_1\')(x)\n\n        filters = [128, 256] if self.small_model else [128, 256, 512]\n\n        for i, num_filters in enumerate(filters):\n            for j in range(2):\n                strides = (2, 2) if j == 1 else (1, 1)\n            \n                x = Conv2D(num_filters, (self.k, self.k), padding=\'same\', strides=strides,\n                                  name=\'gan_conv%d_%d\' % (i + 2, j + 1), kernel_initializer="glorot_uniform")(x)\n                x = LeakyReLU(0.3, name=\'gan_lrelu_%d_%d\' % (i + 2, j + 1))(x)\n                x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm%d_%d\' % (i + 2, j + 1))(x)\n\n        x = Flatten(name=\'gan_flatten\')(x)\n\n        output_dim = 128 if self.small_model else 1024\n\n        x = Dense(output_dim, name=\'gan_dense1\')(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu5\')(x)\n\n        gan_regulrizer = AdversarialLossRegularizer(weight=self.adversarial_loss_weight)\n        x = Dense(2, activation="softmax", activity_regularizer=gan_regulrizer, name=\'gan_output\')(x)\n\n        return x\n\n    def set_trainable(self, model, value=True):\n        if self.gan_layers is None:\n            disc_model = [layer for layer in model.layers\n                          if \'model\' in layer.name][0] # Only disc model is an inner model\n\n            self.gan_layers = [layer for layer in disc_model.layers\n                               if \'gan_\' in layer.name]\n\n        for layer in self.gan_layers:\n            layer.trainable = value\n\n    def load_gan_weights(self, model):\n        f = h5py.File(self.weights_path)\n\n        layer_names = [name for name in f.attrs[\'layer_names\']]\n        layer_names = layer_names[1:] # First is an input layer. Not needed.\n\n        if self.gan_layers is None:\n            self.gan_layers = [layer for layer in model.layers\n                                if \'gan_\' in layer.name]\n\n        for i, layer in enumerate(self.gan_layers):\n            g = f[layer_names[i]]\n            weights = [g[name] for name in g.attrs[\'weight_names\']]\n            layer.set_weights(weights)\n\n        print("GAN Model weights loaded.")\n        return model\n\n    def save_gan_weights(self, model):\n        print(\'GAN Weights are being saved.\')\n        model.save_weights(self.weights_path, overwrite=True)\n        print(\'GAN Weights saved.\')'}, {'identified': 'None', 'updated_code': 'import ipyleaflet as leaflet\nimport ipywidgets as widgets\n\nclass USMap:\n    def __init__(self):\n        self.us = USZIPCodeRepository()\n\n        self.center = [47.621795, -122.334958]\n        self.zoom = 8\n        self.height = \'500px\'\n        self.progress_bar_width = \'500px\'\n        self.area_style = {\'color\':\'#0000ff\', \'weight\': .5, \'fillColor\':\'#000077\', \'fillOpacity\':0.2}\n\n        self.progress_bar = widgets.IntProgress(bar_style=\'info\', layout=widgets.Layout(width=self.progress_bar_width))\n        self.label = widgets.Label()\n        self.progress_label = widgets.Label()\n        self.info_box = widgets.HBox([self.progress_label, self.progress_bar])\n\n        self.basemap = leaflet.basemaps.OpenMapSurfer.Roads\n        self.basemap[\'name\'] = \'basemap\'\n        self.heatmap_data = leaflet.basemaps.Strava.All\n        self.heatmap_data[\'name\'] = \'heatmap\'\n        self.heatmap = leaflet.basemap_to_tiles(self.heatmap_data)\n        self.layers_control = leaflet.LayersControl()\n        self.map_layout = widgets.Layout(height=self.height)\n\n        self.map = None\n\n\n    def enable_heatmap(self):\n        self.map.add_layer(self.heatmap)\n\n\n    def disable_heatmap(self):\n        self.map.remove_layer(self.heatmap)\n\n\n    def handle_interaction(self, **kwargs):\n        \'\'\'mouse interaction handling\'\'\'\n        if kwargs.get(\'type\') == \'mousemove\':\n            self.label.value = str(kwargs.get(\'coordinates\'))\n\n    def fetch_zipcode(self, zipcode):\n        d, err = self.us.fetch_zipcode(zipcode)\n        if err is not None:\n            print(err)\n        return d\n\n\n    def add_point(self, lat, lng, name=\'\', popup=None):\n        feature = {"type": "Feature", "properties": {}, "geometry": {"type": "Point", "coordinates": [lng, lat]}}\n        self.add_geojson(feature, name, popup)\n\n\n    def add_geojson(self, geojson, name=\'\', popup=None):\n        g = leaflet.GeoJSON(data=geojson, hover_style={\'fillColor\': \'#00aaff\'}, name=name)\n\n        if popup is not None:\n            g.popup = popup\n\n        self.map += g\n\n\n    def add_geojsons(self, geojsons, name=\'\'):\n        d = {"type": "FeatureCollection", "features": list(geojsons)}\n\n        self.add_geojson(d, name)\n\n\n    def add_zipcode(self, zipcode):\n        d = self.fetch_zipcode(zipcode)\n        if d is None:\n            print(\'failed to add \' + zipcode + \'.\')\n            return\n\n        d[\'properties\'][\'style\'] = self.area_style\n\n        text_template = Template(\'\'\'<div>ZIP Code\n                                        <ul class=\'list-group\'>\n                                            <li class=\'list-group-item\'>$zipcode</li>\n                                        </ul>\n                                    </div>\'\'\')\n        popup_text = text_template.substitute(zipcode=zipcode)\n        popup = widgets.HTML(value=popup_text, placeholder=\'\', description=\'\')\n\n        self.add_geojson(d, name=zipcode, popup=popup)\n\n\n    def progressive_iter(self, iterable, n=None, label_on_finish=\'\'):\n        display(self.info_box)\n        \n        if n is None:\n            n = len(iterable)\n\n        self.progress_bar.value = self.progress_bar.min\n        self.progress_bar.max = n\n\n        for v in iterable:\n            yield v\n            self.progress_label.value = v\n            self.progress_bar.value += 1\n\n        self.progress_label.value = label_on_finish\n\n\n    def add_zipcodes_no_check(self, zipcodes, show_progress=False):\n        zipcode_gen = self.progressive_iter(zipcodes) if show_progress else zipcodes\n\n        for z in zipcode_gen:\n            self.add_zipcode(z)\n\n        return zipcodes\n\n\n    def add_zipcodes(self, zipcodes, show_progress=False):\n        zipcodes = set(zipcodes)\n        available_zipcodes = list(zipcodes & self.us.zipcode_set)\n        available_zipcodes.sort()\n\n        return self.add_zipcodes_no_check(available_zipcodes, show_progress)\n\n\n    def display(self):\n        if self.map is None:\n            self.map = leaflet.Map( center=self.center, zoom=self.zoom, basemap=self.basemap, layout=self.map_layout)\n            self.map.on_interaction(self.handle_interaction)\n            self.map.add_control(self.layers_control)\n\n        display(self.map)\n        display(self.label)'}, {'identified': 'None', 'updated_code': '# Set up parameters:    \nk_size = 3\nvertex_ratio_h = .45\nvertex_ratio_v = .60\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline(image, \n                                   k_size = k_size,\n                                   low_thresh = low_thresh,\n                                   high_thresh = high_thresh,\n                                   L2gradient = L2gradient,\n                                   rho = rho,\n                                   theta = theta,\n                                   min_votes = min_votes,\n                                   min_line_len = min_line_len,\n                                   max_line_gap = max_line_gap,\n                                   angle = angle,\n                                   angle_thresh = angle_threshold,\n                                   debug = False)\n    return result'}, {'identified': "['is_int']", 'updated_code': ''}, {'identified': "['sigmoid']", 'updated_code': ''}, {'identified': "['prepare_image']", 'updated_code': 'from scipy.misc import imresize'}, {'identified': 'None', 'updated_code': 'def find_f(K, y, iterations=100):\n    """\n    Finds f using a iterative aproach also finds p(y|f)\n    :param K: np.array(np.array) covariance matrix of data points we have observed\n    :param y: numpy array of the y value of the data points we have observed\n    :iterations: int optional default = 100 number of iterations we will preform to optimize f\n    :return: (numpy array <float> f, numpy array <float> p(y|f)) f is the latent function value for each of the \n    sampled data points, p(y|f) is the probability of y given the latent function we calculated\n    make sure to calculate p(y|f) after having approximated f_hat.\n    """\n    n = len(y)\n    f = np.zeros(n)\n    grad = np.zeros(n)\n    for i in range(iterations):\n        for j in range(n):\n            sigmoid_v = sigmoid(f[j]*y[j])\n            grad[j] = (1-sigmoid_v)*y[j]\n        f = np.array(np.matmul(K, grad)).flatten()\n        \n    y_giv_f = np.zeros(n)\n    for j in range(n):\n        y_giv_f[j] = sigmoid(f[j]*y[j])\n        \n    return f, y_giv_f'}, {'identified': "['splitData']", 'updated_code': ''}, {'identified': "['max_norm_regularizer']", 'updated_code': 'def max_norm(weights):\n    clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n    clip_weights = tf.assign(weights, clipped, name=name)\n    tf.add_to_collection(collection, clip_weights)\n    return None # 규제 손실을 위한 항이 없습니다'}, {'identified': 'None', 'updated_code': 'def f(gamma):\n    v = vandermonde(gamma)\n    n = gamma.size()\n    deg_v = v.degree()\n    generator = {v.multidegree() : [v]}\n    list_op = partial_derivatives(v.parent())\n    W1 = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n\n    r = 1\n    op_pol = polarization_operators(r, deg_v, row_symmetry="permutation")\n    show(IsotypicComponent(W1, Partition([1 for i in range(n)])).basis())\n    show(IsotypicComponent(W1, gamma).basis())\n    W1 = PolarizedSpace(IsotypicComponent(W1, n, use_antisymmetry=True), op_pol)\n    return character(W1, row_symmetry="permutation")'}, {'identified': "['calc_other_neighbour_msg_prod']", 'updated_code': 'def calc_sum_product_factor_to_variable_msg(factor, variable):\n    neighbour_msg_prod = calc_other_neighbour_msg_prod(factor, variable)\n    \n    f_neighb_first = move_dimension_first(factor.f, factor.neighbours.index(variable))\n    \n    return marginalize(calculate_factor(f_neighb_first, neighbour_msg_prod), 0)'}, {'identified': 'None', 'updated_code': 'def calc_sum_product_variable_to_factor_msg(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    \n    if len(neighbour_msg_prod) > 0:\n        message = np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        message = np.ones(variable.num_states)\n    \n    message = message * variable.observed_state\n    \n    return message\n    '}, {'identified': "['delete_random_character', 'flip_random_character', 'insert_random_character']", 'updated_code': 'import random\n\ndef mutate(s):\n    """Return s with a random mutation applied"""\n    mutators = [\n        # delete_random_character,\n        # insert_random_character,\n        # flip_random_character\n    ]\n    mutator = random.choice(mutators)\n    # print(mutator)\n    return mutator(s)'}, {'identified': 'None', 'updated_code': "def max_sum(node_list):\n    \n    for n in node_list:\n#         print(n, ':', *list(n.pending))\n        while len(n.pending) > 0:            \n            f = next(iter(n.pending))\n            n.send_ms_msg(f)\n            \n    for n in reversed(node_list):\n        \n        while len(n.pending) > 0:\n            f = next(iter(n.pending))\n            n.send_ms_msg(f)"}, {'identified': 'None', 'updated_code': "def PreProcess(table,Dream9):\n    #Select all variables that are not Categorical\n    Tables=[table[[v for v in table.keys() if v not in Categorical]]]\n    \n    #Convert yes/no to 1/0\n    Alias_Dict={'SEX':{'F':1},'PRIOR.MAL':{'YES':1},'PRIOR.CHEMO':{'YES':1},'PRIOR.XRT':{'YES':1},\n                'Infection':{'Yes':1},'ITD':{'POS':1,'ND':numpy.nan},'D835':{'POS':1,'ND':numpy.nan},\n                'Ras.Stat':{'POS':1,'NotDone':numpy.nan},'resp.simple':{'CR':1},'Relapse':{'Yes':1},\n                'vital.status':{'A':1}}\n    \n    Tables+=[alias(table,Alias_Dict)]\n    \n    #Split data that has multiple values\n    Tables+=[split(table['cyto.cat'],Dream9)]\n    \n    #Create new data for protein\n    Tables+=[squared(table[Protein])]\n    Tables+=[absolute(table[Protein])]\n    Tables+=[bin_independent(table[Protein],Dream9,2)]\n    Tables+=[bin_independent(table[Protein],Dream9,3)]\n    Tables+=[bin_independent(table[Protein],Dream9,4)]\n    Tables+=[bin_independent(table[Protein],Dream9,5)]\n    \n    #Make PCA axis\n    Tables+=[make_pca(table[Protein],Dream9,200,name='PCA')]\n    Tables+=[make_pca(table[Protein],Dream9,200,name='Whiten_PCA',whiten=True)]\n    Tables+=[make_pca(squared(table[Protein]),squared(Dream9[Protein]),200,name='PCA_Sq')]\n    \n    #Bin dependent variables\n    try:\n        Tables+=[cutoff(table[['Overall_Survival','Remission_Duration']],130)]\n        Tables+=[binned(table[['Overall_Survival','Remission_Duration']])]\n    except KeyError:\n        pass        \n    \n    #Join everything\n    return pandas.concat(Tables,axis=1)"}, {'identified': "['make_pca']", 'updated_code': "#pandas.concat([Trial_data[Protein[:4]],make_pca(Trial_data[Protein[:4]],Dream9,3),make_pca(Trial_data[Protein[:4]],Dream9,2,'Whiten_PCA',True)],axis=1).T"}, {'identified': "['powers']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "def get_initial_centroids(data, k, seed=None):\n    '''Randomly choose k data points as initial centroids'''\n    if seed is not None: # useful for obtaining consistent results\n        np.random.seed(seed)\n    n = data.shape[0] # number of data points\n    # Pick K indices from range [0, N).\n    rand_indices = np.random.randint(0, n, k)\n    \n    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n    # As long as at least one document in a cluster contains a word,\n    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n    centroids = data[rand_indices,:].toarray()\n    \n    return centroids"}, {'identified': "['insert_and_time_filter_bloom_filter']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def delete_from_cuckoo_filter_and_time(capacity, percent_to_fill=0.9, percent_delete=1.0):\n    c_filter, _, _ = insert_and_time_filter_cuckoo_filter(capacity, percent_fill=percent_to_fill)\n    \n    total_size = c_filter.get_size()\n    number_of_items_to_delete = total_size*percent_delete\n    \n    now = time.time()\n    for i in range(int(number_of_items_to_delete)):\n        c_filter.remove(str(i))\n    elapsed_time = time.time() - now\n        \n    return c_filter, elapsed_time'}, {'identified': "['flip_random_character']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def plotMetrics(precision_l1, recall_l1, \n                accuracy, precision, recall, \n                precision_by_label, recall_by_label, \n                svm_accuracy, svm_precision, svm_recall, \n                svm_precision_by_label, svm_recall_by_label, \n                confusion, feature_size, label_encoder):\n    labels = [ \'\\n\'.join(wrap(l, 8)) for l in feature_size ]       \n    \n    plt.rcParams["figure.figsize"] = (20,6)\n\n    plt.plot(labels, precision, color=\'olivedrab\', \n             linewidth=3, label=\'Precision (Logistic regression) \', marker=\'o\' )\n    plt.plot(labels, recall, color=\'olivedrab\', linestyle=\'dashed\',\n             linewidth=3, label=\'Recall (Logistic regression)\', marker=\'o\' )\n    #plt.plot(labels, accuracy, color=\'darkolivegreen\', linestyle=\':\',\n    #         linewidth=3, label=\'Accuracy (Logistic regression)\', marker=\'v\' )\n\n    plt.plot(labels, svm_precision, color=\'slateblue\', \n             linewidth=3, label=\'Precision (SVM)\', marker=\'o\' )\n    plt.plot(labels, svm_recall, color=\'slateblue\', linestyle=\'dashed\',\n             linewidth=3, label=\'Recall (SVM)\', marker=\'o\' )\n    #plt.plot(labels, svm_accuracy, color=\'darkslateblue\', linestyle=\':\',\n    #         linewidth=3, label=\'Accuracy (SVM)\', marker=\'v\' )\n\n    #plt.plot(labels, precision_l1, color=\'gray\',alpha=.4,\n    #         linewidth=3, label=\'L1 precision at different C values\', marker=\'o\' )\n    #plt.plot(labels, recall_l1, color=\'gray\', linestyle=\'dashed\',alpha=.6,\n    #         linewidth=3, label=\'L1 at different C values\', marker=\'o\' )\n    \n    \n    plt.yticks(np.arange(.42, .65, .01))\n    plt.ylabel(\'Precision, Recall\', fontsize=20)\n    plt.xlabel(\'Feature size with L1 regularization at different C parameters\', fontsize=20, labelpad=20)\n    plt.legend()\n    plt.grid()\n    plt.show()\n    \n    # find optimal f1\n    best_idx = np.argmax(precision)\n    \n    # Show precision and recall across different labels\n    showPrecisionRecallPairByLabel(precision_by_label[best_idx], recall_by_label[best_idx], label_encoder,\n                                  \'Logistic Regression\', [\'olivedrab\', \'darkolivegreen\'])\n    showPrecisionRecallPairByLabel(svm_precision_by_label[best_idx], svm_recall_by_label[best_idx], label_encoder,\n                                  \'SVM\', [\'slateblue\', \'darkslateblue\'])\n    \n    \n    # Get the confusion matrix for the optimal precision\n    # Show the labels that have the highest error rate\n    conf_mx = confusion[best_idx]\n    showTopConfused(conf_mx, label_encoder)'}, {'identified': "['test']", 'updated_code': ''}, {'identified': 'None', 'updated_code': "# checks if line is in iambic pentameter (i.e. 0101010101 stress pattern)\ndef check_iambic_pentameter(line):\n    # get the stresses from cmu dict \n    # if word is 1 syllable, then have the option for it to be stressed or unstressed\n    stresses = []\n    for i in line.split(' '):\n        stress = poetrytools.stress(i)\n        if len(stress) == 1:\n            stresses.append(['0','1'])\n        else:\n            stresses.append([stress])\n    \n    # make combination of all possible stress patterns\n    result = [[]]\n    final = []\n    for pool in stresses:\n        result = [x+[y] for x in result for y in pool]\n    final = [''.join(i) for i in result]\n    \n    # return if any pattern fits iambic pentameter \n    return ('0101010101' in final)"}, {'identified': 'None', 'updated_code': 'def eulerCoupledAdaptive (f, n, blockPositions, vBlock, blockNum, h1, h2, maxPoints, minBlockV, *args, verbose = False):\n    """ \n    Solve a coupled system of ODEs by Euler method with fixed number of steps.\n\n    Arguements: f - function giving ODE as y\'=f(x,y)\n                n - the number of ODEs\n                blockPositions - the array containging the initial block positions\n                vBlock - initial block velocity\n                blockNum - the number of blocks\n                interval - tuple region (a,b) on which to solve ODE\n                steps - number of steps\n    \n    Returned: An array containing the positions and velocies of the blocks over time\n    """\n    \n    points = np.zeros((maxPoints, blockNum*2 + 1)) # array to contain all the block positions and velocities over time\n\n    # set up initial point\n    t = 0\n    points[0,0] = t\n    count = 0\n    \n    # Sets up first row of poitns\n    for l in range(1, blockNum + 1):\n        points[0,l] = blockPositions[count]   \n        count += 1\n    for m in range(blockNum + 1, blockNum * 2 + 1):\n        points[0,m] = vBlock\n    \n    # initializes count, dv and r\n    count = 1\n    dv = 0\n    r = (0,0)\n    \n    while points[maxPoints - 1, 0] == 0: # Repeats until points array is filled up                   \n        \n        # Proceeds if none of the blocks are moving\n        if dv < minBlockV:\n            h = h1 # Long timestep\n            \n            # Repeats Euler calculation until one of the blocks moves or points is full\n            while dv < minBlockV and points[maxPoints - 1, 0] == 0:\n                \n                if verbose == True:\n                    print("h1:", t)\n                \n                # Saves block positions and updates timestep\n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n                \n                # Repeats Euler calculation for each block\n                for i in range(0, blockNum): \n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n                        \n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n                \n        # Proceeds if any of the blocks are moving        \n        elif points[maxPoints - 1, 0] == 0:\n            # Resets to previous timestep\n            h = h2 # Short timestep\n            t -= h1\n            count -= 1\n            blockPositions = oldBlockPositions\n            dv = 0\n\n            # Repeats Euler calculation until one of the blocks moves or points is full\n            # This is necessary, because the timestep was reset, so dv will still be zero\n            while dv < minBlockV and points[maxPoints - 1, 0] == 0: \n                \n                if verbose == True:\n                    print("h2:", t)\n                \n                # Saves block positions and updates timestep                \n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n\n                # Repeats Euler calculation for each block                \n                for i in range(0, blockNum):\n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new            \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n\n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n                \n            # Repeats Euler calculation until all of the blocks are still or points is full                \n            while dv >= minBlockV and points[maxPoints - 1, 0] == 0: \n                \n                if verbose == True:\n                    print("h2:", t)\n                \n                # Saves block positions and updates timestep\n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n\n                # Repeats Euler calculation for each block  \n                for i in range(0, blockNum):\n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new            \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n\n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n\n    return points'}, {'identified': "['rmse_cv']", 'updated_code': '# Invoke Ridge regularisation\nmodel_ridge = Ridge()'}, {'identified': 'None', 'updated_code': 'f1 = lambda x: x**2\n    \n# is equivalent to \n\ndef f2(x):\n    return x**2'}, {'identified': 'None', 'updated_code': '# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # TODO: Create a decision tree regressor object\n    regressor = None\n\n    # TODO: Create a dictionary for the parameter \'max_depth\' with a range from 1 to 10\n    params = {}\n\n    # TODO: Transform \'performance_metric\' into a scoring function using \'make_scorer\' \n    scoring_fnc = None\n\n    # TODO: Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values \'regressor\', \'params\', \'scoring_fnc\', and \'cv_sets\' respectively.\n    grid = None\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_'}, {'identified': 'None', 'updated_code': 'def fillEmpty(bankdata, maxDate):\n    for accountType in ACCOUNTTYPES:\n        for bank in bankdata[accountType]:\n            if bankdata[accountType][bank][\'date\'][-1] != maxDate:\n                bankdata[accountType][bank]["balance"] = bankdata[accountType][bank]["balance"] + (bankdata[accountType][bank]["balance"][-1],)\n                bankdata[accountType][bank]["date"] = bankdata[accountType][bank]["date"] + (maxDate,)\n                \n    return bankdata'}, {'identified': 'None', 'updated_code': 'class ValueEstimator():\n    """\n    Value Function approximator. \n    """\n    \n    def __init__(self, learning_rate=0.1, scope="value_estimator"):\n        with tf.variable_scope(scope):\n            self.state = tf.placeholder(tf.float32, [400], "state")\n            self.target = tf.placeholder(dtype=tf.float32, name="target")\n\n            # This is just linear classifier\n            self.output_layer = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n\n            self.value_estimate = tf.squeeze(self.output_layer)\n            self.loss = tf.squared_difference(self.value_estimate, self.target)\n\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            self.train_op = self.optimizer.minimize(\n                self.loss, global_step=tf.contrib.framework.get_global_step())        \n    \n    def predict(self, state, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        return sess.run(self.value_estimate, { self.state: state })\n\n    def update(self, state, target, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        feed_dict = { self.state: state, self.target: target }\n        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n        return loss'}, {'identified': "['sample']", 'updated_code': ''}, {'identified': "['convert_line', 'generate_10_rhyming_and_meter_sonnets', 'generate_line', 'generate_pair', 'generate_rhyming_and_meter_sonnet']", 'updated_code': ''}, {'identified': "['sum_product']", 'updated_code': ''}, {'identified': "['move_dimension_first']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'import unittest\n# import numpy as np\n\nclass TestFeatures(unittest.TestCase):\n\n    def test_features_ground(self):\n        sample = (asl.df.ix[98, 1][features_ground]).tolist()\n        self.assertEqual(sample, [9, 113, -12, 119])\n\n    def test_features_norm(self):\n        sample = (asl.df.ix[98, 1][features_norm]).tolist()\n        np.testing.assert_almost_equal(sample, [ 1.153,  1.663, -0.891,  0.742], 3)\n\n    def test_features_polar(self):\n        sample = (asl.df.ix[98,1][features_polar]).tolist()\n        np.testing.assert_almost_equal(sample, [113.3578, 0.0794, 119.603, -0.1005], 3)\n\n    def test_features_delta(self):\n        sample = (asl.df.ix[98, 0][features_delta]).tolist()\n        self.assertEqual(sample, [0, 0, 0, 0])\n        sample = (asl.df.ix[98, 18][features_delta]).tolist()\n        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], "Sample value found was {}".format(sample))\n                         \nsuite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\nunittest.TextTestRunner().run(suite)'}, {'identified': 'None', 'updated_code': "def policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4, modified=False):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    policy_stable = False\n    all_k = []\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n\n    while not policy_stable:\n        \n        # 2. POLICY EVALUATION (iterates until V_k converges)\n        k = 0\n        V_kplus1 = copy.deepcopy(V_k)\n        delta = epsilon + 1\n        \n        while delta > epsilon and (k < 5 or not modified):\n\n            delta = 0\n            for i, j in states:\n                \n                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n                a = idx_to_a[PI[i,j]]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Bellman's update rule\n                V_kplus1[i, j] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n                # Keeps biggest difference seen so far\n                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n            # Updates our current estimate\n            V_k = copy.deepcopy(V_kplus1)\n            k += 1\n        all_k.append(k)\n\n        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n        \n        policy_stable = True\n        old_PI = copy.deepcopy(PI)\n        \n        for i, j in states:\n            for a_idx in range(4): # actions\n                    \n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Policy Improvement rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n                    \n            PI[i,j] = np.argmax(Q[i,j,:])\n                    \n            if old_PI[i,j] != PI[i,j]:\n                policy_stable = False\n    \n    return V_k, all_k, PI "}, {'identified': "['path_to_tensor', 'paths_to_tensor']", 'updated_code': 'from keras.preprocessing import image                  \nfrom tqdm import tqdm'}, {'identified': "['bot_resultados', 'correr_y_mostrar', 'top_resultados']", 'updated_code': 'import time\nfrom sklearn.model_selection import GridSearchCV\n\n##############################################\n################# Auxiliares #################\n##############################################\n\ndef top_resultados(grid, top=5):\n    print("Top {} combinaciones".format(top))\n    df = pd.DataFrame(grid.cv_results_["params"])\n    df["mean_score_validation"] = grid.cv_results_["mean_test_score"]\n    df["mean_score_training"] = grid.cv_results_["mean_train_score"]\n    display(df.sort_values(by="mean_score_validation", ascending=False).head(top))\n\ndef bot_resultados(grid, bot=5):\n    print("Bot {} combinaciones".format(bot))\n    df = pd.DataFrame(grid.cv_results_["params"])\n    df["mean_score_validation"] = grid.cv_results_["mean_test_score"]\n    df["mean_score_training"] = grid.cv_results_["mean_train_score"]\n    display(df.sort_values(by="mean_score_validation", ascending=True).head(bot))\n\ndef correr_y_mostrar(estimator, parameters, folds, top):\n    grid = GridSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\')\n    time_before = time.time()\n    grid.fit(X_dev_np, y_dev_np)\n    time_after = time.time()\n    top_resultados(grid, top)\n    bot_resultados(grid, top)\n    runtime = (time_after - time_before) * 1000.0 \n    return (runtime, grid)\n\n# Para usar en caso de tener probabilidades a priori\npriors = [(0.1,0.9),(0.2,0.8),(0.3,0.7),(0.4,0.6),(0.5,0.5),(0.6,0.4),(0.7,0.3),(0.8,0.2),(0.9,0.1)]'}, {'identified': "['next_batch']", 'updated_code': ''}, {'identified': 'None', 'updated_code': '# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\nclass EncoderLayer(tf.keras.layers.Layer):\n  # Transformer 論文內預設 dropout rate 為 0.1\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    # 一樣，一個 sub-layer 一個 dropout layer\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n  def call(self, x, training, mask):\n    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n    \n    # sub-layer 1: MHA\n    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n    attn_output, attn = self.mha(x, x, x, mask)  \n    attn_output = self.dropout1(attn_output, training=training) \n    out1 = self.layernorm1(x + attn_output)  \n    \n    # sub-layer 2: FFN\n    ffn_output = self.ffn(out1) \n    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n    out2 = self.layernorm2(out1 + ffn_output)\n    \n    return out2'}, {'identified': "['vandermonde']", 'updated_code': ''}, {'identified': 'None', 'updated_code': 'def sgd_iter(x_train, t_train, W, b):\n    \n    #every day I am shufflin`\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    lr = 1e-4\n    \n    logp = np.zeros(len(x_train))\n    for i in indices:\n        x = x_train[i:i+1]\n        t = t_train[i]\n        logp[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\n        W = W + lr*grad_w #grad ascent\n        b = b + lr*grad_b\n    \n    logp_train = logp.mean()\n    return logp_train, W, b'}, {'identified': "['LeNet', 'model_arc']", 'updated_code': "from tensorflow.contrib.layers import flatten\n\n# Removed unused functions 'LeNet' and 'model_arc'"}, {'identified': "['derivative_of']", 'updated_code': ''}, {'identified': "['process_image']", 'updated_code': '# Define a function to pass stored images to\n# reading rover position and yaw angle from csv file\n# This function will be used by moviepy to create an output video'}, {'identified': 'None', 'updated_code': 'def broad_less_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible starboard orientation.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        masthead_light = np.random.randint(50, 201)\n        mh_horiz = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        masthead_height = masthead_light + light_width\n        masthead_width = mh_horiz + light_width\n        running_light_start = masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if mh_horiz < 2 * light_width:\n            running_light_loc = np.random.randint(mh_horiz - 20, mh_horiz + 21)\n        else:\n            running_light_loc = np.random.randint(mh_horiz - 20, 211)\n        running_light_area = running_light_loc + light_width\n        new_view[masthead_light:masthead_height, mh_horiz:masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images'}, {'identified': 'None', 'updated_code': "### TODO: Write your algorithm.\n### Feel free to use as many code cells as needed.\n\ndef whose_a_good_doggy(img_path):\n    ''' Using the given image (in img_path), returns either:\n        - Dog breed (if it's a dog)\n        - Dog breed that resembles a human (if it's a human face)\n        \n        Uses the transfer-learned CNN model from Step 5\n    '''\n    print('.'*60)\n    print('Given image:', img_path)\n    \n    # human face?\n    human_found = face_detector(img_path)\n    print('Found human:', human_found)\n    \n    # doggy ?\n    # dog_found   = dog_detector(img_path)\n    # print('Found dog:  ', dog_found)\n    \n    # find breed of dog\n    breed, chance = detect_dog_breed(img_path, inception_bneck, use_bottleneck=True, img_H=229, img_W=229)\n    print()\n    print('Image is dog breed: {} ({:.2f}% prob)'.format(breed, chance))\n    print('🐶 Woof!') if not human_found else print('Hellooo, 🐱👩🏻👦🏻👧🏻 animal 🤔')\n    print('='*60)"}, {'identified': 'None', 'updated_code': "def draw_rule_and_reg_id(id):\n    plt.title(str(id))\n    list_result = dateRange('2016-06-06','2016-10-31')\n    for i in range(1,15):\n        list_result.append('predict_day_'+str(i))\n    xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n    ax = plt.subplot(111) \n    ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id ,list_result].T.plot(figsize=(16,9),ax=ax)\n    \n\n    list_result = dateRange('2016-06-06','2016-10-31')\n    for i in range(1,8):\n        list_result.append('predict_'+str(i))\n\n    xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n    ax = plt.subplot(111) \n    ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id,list_result].T.plot(figsize=(16,9),ax=ax)\n    plt.show()"}, {'identified': "['nparray_to_list']", 'updated_code': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\n# Make sure the input and output formats are the same for X and y\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    X_np = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(X_np, y_np):\n        X_train, X_valid = np.asarray(X_np[train_index]).tolist(), np.asarray(X_np[valid_index]).tolist()\n        y_train, y_valid = np.asarray(y_np[train_index]).tolist(), np.asarray(y_np[valid_index]).tolist()\n    train = X_train, y_train\n    valid = X_valid, y_valid\n    return train, valid'}, {'identified': 'None', 'updated_code': 'def compute_character(mu, use_antisymmetry=True, row_symmetry="permutation"):\n    n = Integer(mu.size())\n    # Determinant computation\n    v = vandermonde(mu)\n    # Span by derivatives\n    generator = {v.multidegree() : [v]}\n    list_op = partial_derivatives(v.parent())\n    V = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n    # Projection on isotypic components\n    V_iso = IsotypicComponent(V, n, use_antisymmetry=use_antisymmetry)\n    # Polarization\n    r = n-1\n    deg = v.degree()\n    if deg == 0:\n        deg = 1\n    op_pol = polarization_operators(r, deg, row_symmetry=row_symmetry)\n    V_pol = PolarizedSpace(V_iso, op_pol)\n    \n    # character\n    return character(V_pol, row_symmetry=row_symmetry)'}, {'identified': "['canny', 'draw_lines', 'draw_lines_roi', 'gaussian_blur', 'grayscale', 'hough_lines', 'region_of_interest', 'weighted_img']", 'updated_code': 'import math\n\nleftline=[(0,0,0,0)]\nrightline=[(0,0,0,0)]\n\n# Removed function definitions'}, {'identified': "['convert_invalid_values']", 'updated_code': '# No functions remaining'}]