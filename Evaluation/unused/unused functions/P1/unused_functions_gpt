[{'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n#Example\n#pandas.concat([Trial_data[Protein[:4]],squared(Trial_data[Protein[:4]]),absolute(Trial_data[Protein[:4]])],axis=1).T\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef insert_random_character(s):\n    """Returns s with a random character inserted"""\n    pos = random.randint(0, len(s))\n    random_character = chr(random.randrange(32, 127))\n    # print("Inserting", repr(random_character), "at", pos)\n    return s[:pos] + random_character + s[pos:]\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nimport subprocess \nimport shlex\n\ndef systemcall ( cmdstr ):\n    \'\'\' System call to execute command string in a shell. \'\'\'\n    try:\n        retcode = subprocess.call( cmdstr, shell=True)\n        if retcode != 0:\n            print ("Error code:", retcode)\n        return retcode\n    except OSError as e:\n        print ("Execution failed:", e )\n        \ndef systemcall_pipe( cmdstr, allow=None, disp=True ):\n    \'\'\' System call to execute command string, to get stderr and stdout output in variable proc. \'\'\'\n    # this function is superior to systemcall for use with Spyder where otherwise stdout/stderr are not visible.\n    # it is also needed if your main program needs to capture this output instead of only print it to terminal.\n    args = shlex.split(cmdstr)\n    try:\n        proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        #stdout and stderr from your process\n        out, err = proc.communicate()\n        retcode = proc.returncode\n        if err:\n            #decode the standard errors to readable form\n            str_err = err.decode("utf-8")\n            #Exclude error messages in allow list which are expected.\n            bShow = True\n            if allow:\n                for allowstr in allow:\n                    if allowstr in str_err:\n                        bShow = False\n            if bShow:\n                print ("System command \'{0}\' produced stderr message:\\n{1}".format(cmdstr, str_err))\n\n        if disp:\n            str_out = out.decode("utf-8")\n            if str_out:\n                print ("System command \'{0}\' produced stdout message:\\n{1}".format(cmdstr, str_out))\n\n        return retcode, out\n    except OSError as e:\n        print ("Execution failed:", e )\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef delete_random_character(s):\n    """Returns s with a random character deleted"""\n    if s == "":\n        return s\n\n    pos = random.randint(0, len(s) - 1)\n    return s[:pos] + s[pos + 1:]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef myfunc(x, p=2, debug=False):\n    if debug:\n        print("evaluating myfunc for x = " + str(x) + " using exponent p = " + str(p))\n    return x**p\n```'}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4):\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    \n    while delta > epsilon:\n\n        delta = 0\n        \n        Q = {0: {0: 0,   # state0, action0\n                 1: 0},  # state0, action1\n             1: {2: 0}}  # state1, action2\n        for s in states:\n            v = 0\n            for a in actions[s]:\n                for n in next_states:\n                \n                    # Bellman's optimality update rule\n                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n\n            # This step replaces the poilicy improvement step (gets the maximal value)\n            V_kplus1[s] = max(Q[s].items(), key=operator.itemgetter(1))[1]\n            \n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n    \n    # Updates the policy to be greedy with respect to the value function\n    for s in states:\n        PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n    \n    return V_k, k, PI \n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef plotImg(arr, **kwargs):\n    \n    avg = mean(arr.flatten())\n    std = stdev(arr.flatten())\n    \n    fig,ax = plt.subplots(1)\n    pl = plt.imshow(arr, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', \\\n                    vmin=np.percentile(arr, 10), vmax=np.percentile(arr, 90))\n    if \'window\' in kwargs:\n        window = kwargs.get(\'window\')\n        rect = patches.Rectangle(window[0], window[1][1], window[1][0], edgecolor=\'r\', facecolor=\'none\')\n        ax.add_patch(rect)\n    plt.colorbar().set_label(\'Detector Value (ADU)\')\n    plt.xlabel(\'pixels(x)\')\n    plt.ylabel(\'pixels(y)\')\n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'selection_window.png\')\n    plt.show()\n\ndef plotAll(array2D, **kwargs):\n    """\n    Input:  \'array2D\': 2D image array\n    Output: show plot of histogram and image\n    """\n    \n    arr = array2D.flatten()\n    \n    avg = mean(arr)\n    std = stdev(arr)\n    med = np.median(arr)\n    Npix = len(arr)    \n    \n    sigma = kwargs.get(\'sigma\', 2)\n    low = int(np.round((avg-sigma*std)))\n    high = int(np.round((avg+sigma*std)))\n    rng = kwargs.get(\'rng\', [low, high])\n    exp = kwargs.get(\'exp\')\n    if \'nbins\' in kwargs:\n        nbins = kwargs.get(\'nbins\')\n        bin_size = (rng[1]-rng[0])/nbins\n    else:\n        bin_size = kwargs.get(\'bin_size\', 1)\n    \n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=[18,6])\n    \n    # Histogram\n    #===========\n    hr = np.arange(rng[0], rng[1]+1, bin_size)\n    hist = []\n    for i in range(len(hr)):\n        try:\n            counts = len(np.where((arr >= hr[i]) & (arr < hr[i+1]))[0])\n        except:\n            counts = 0\n        hist.append(counts)\n    ax1.step(hr, hist, color=\'k\')\n\n    #mean and median lines\n    ax1.axvline(avg, color=\'b\', label=r\'$\\bar{x}=%s$\'%(np.round(avg,2)))\n#     ax1.axvline(med, color=\'b\', label=r\'$\\tilde{x}=%s$\'%(np.round(med,2)), linestyle=\'dashed\')\n    \n    #sigma levels\n    if kwargs.get(\'show_level\', True) == True:\n        for i in np.arange(1,sigma+1):\n            if i == 1:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05, label=r\'$s=\\pm %s$\'%(np.round(std,2)))\n            else:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05)\n                \n                \n    #poisson distribution\n    xarray = np.arange(rng[0]-10, rng[1]+10, 1)\n    pdist = poisson_approx(xarray, avg)\n    pdist = max(hist)/max(pdist)*pdist\n    ax1.plot(xarray, pdist, color=\'r\', label=r\'$P_{Poisson}(\\bar{x})$\')\n    std_expected = math.sqrt(avg)\n    ax1.axvspan(avg - std_expected, avg + std_expected, facecolor=\'r\', alpha=0.05, \\\n                label=r\'$\\sigma=\\pm %s$\'%(np.round(std_expected,2)))\n    \n    #gaussian distribution\n    gdist = gaussian(xarray, avg, std)\n    gdist = max(hist)/max(gdist)*gdist\n    ax1.plot(xarray, gdist, color=\'c\', label=r\'$P_{Gaussian}(\\bar{x}, s)$\')\n    \n    ax1.legend(loc=\'upper left\')\n    ax1.set_xlabel(\'Counts (ADU)\')\n    ax1.set_ylabel(\'Frequency\')\n    \n    if \'exp\' in kwargs:\n        ax1.set_title(\'Combined Histogram (Exposure Time: %s sec)\'%(exp))\n    ax1.set_xlim(rng)\n    \n    # Image\n    #===========\n    hrng = kwargs.get(\'hrng\', [np.percentile(arr, 10), np.percentile(arr, 90)])\n    pl = ax2.imshow(array2D, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', vmin=hrng[0], vmax=hrng[1])\n    fig.colorbar(pl, ax=ax2, fraction=0.046, pad=0.04).set_label(\'Detector Value (ADU)\')\n    \n    ax2.set_xlabel(\'pixels(x)\')\n    ax2.set_ylabel(\'pixels(y)\')\n    ax2.set_title(\'Combined Image\')\n    \n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'exposure%s.png\'%(exp))\n    plt.show()\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- regression_formula\n\nUpdated code:\n```python\n# No changes made to the code\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef Grafica(udat,vdat,xdat,ydat,linea):\n    uesc = 1.0#dx/dt\n    u = np.loadtxt(udat, unpack = True)\n    v = np.loadtxt(vdat, unpack = True)\n    x = np.loadtxt(xdat, unpack = True)\n    y = np.loadtxt(ydat, unpack = True)\n    #rho = np.loadtxt(rhodat, unpack = True)\n    X = np.linspace(0,1,50)\n\n    f, axarr = plt.subplots(1,2, figsize=(24,6))\n    st = f.suptitle("$\\\\tau = 0.6$", fontsize=25)\n    st.set_y(1.0)\n    pasos = 12\n    M= np.hypot(u, v)\n    #axarr[0].streamplot(x,y,u,v, color="k",linewidth=0.8,density=1.0, arrowstyle=\'->\', arrowsize=1.5)\n    im=axarr[0].quiver(x,y,u,v,M , cmap=plt.cm.jet,width=0.022,scale=1/0.1)\n    axarr[0].set_title("Campos",fontsize = 20)\n    axarr[0].set_xlim(-0.01,1)\n    axarr[0].set_xlabel("$x[m]$",fontsize = 20)\n    axarr[0].set_ylabel("$y[m]$",fontsize = 20)\n    axarr[0].tick_params(axis="x", labelsize=20)\n    axarr[0].tick_params(axis="y", labelsize=20)\n    \n    axarr[1].plot(x,v[linea,:],"b", label = "Simulacion")\n    #axarr[1].plot(X,uesc*uy(X,nul,gl,Pl),"r+", label = "Teorica")\n    #axarr[1].set_ylim(-0.1*uesc,0)\n    axarr[1].legend()\n    axarr[1].grid(True)\n    axarr[1].set_title(\'Perfil de Velocidad\',fontsize = 20,y=1.0)\n    axarr[1].set_xlabel("$x[m]$",fontsize = 20)\n    axarr[1].set_ylabel("$v[m/s]$",fontsize =20)\n    axarr[1].tick_params(axis="x", labelsize=20)\n    axarr[1].tick_params(axis="y", labelsize=20)\n    \n    cbar = f.colorbar(im, ax=axarr, shrink = 1.0)\n    cbar.set_label(\'$v[m/s]$\',fontsize =20)\n    cbar.ax.tick_params(labelsize=20)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# No unused functions to remove\ndef plot_image(image):\n    plt.imshow(image, cmap="gray", interpolation="nearest")\n    plt.axis("off")\n\ndef plot_color_image(image):\n    plt.imshow(image.astype(np.uint8),interpolation="nearest")\n    plt.axis("off")\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef to_appropriate_column_name(s):\n    s = s.lower().replace(' ', '_')\n    s = ''.join([ch for ch in s if ch.isalnum() or ch == '_'])\n    return s\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n# No changes made\ndef create_xgb_target (df):\n    return df.MaxPrice.shift(-1).fillna (method='ffill')\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef true_mean_function(x):\n    return np.sin(2*pi*(x+1))\n\ndef add_noise(y, sigma):\n    return y + sigma*np.random.randn(len(y))\n\ndef generate_t(x, sigma):\n    return add_noise(true_mean_function(x), sigma)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef blockMotion (t, blockPositions, vBlock, i, blockNum, kp, kc, mass, F0, v0, vf):\n    """\n    Returns the differential equation that models the motion of the blocks\n    \n    Arguments:  t - time\n                blockPositions - the positions of the blocks\n                vBlock - the velocity of the block\n                i - the index of the current block\n                blockNum - the number of blocks\n                kp - spring constant of leaf springs\n                kc - spring constant of springs between blocks\n                mass - mass of individual block\n                F0 - the static friction force\n                v0 - initial velocity of top plate\n                vf - the friction coefficient\n                \n    Returned: The differential equation modeling the motion of the individual blocks\n    \n    Examples:\n    \n    >>> blockMotion (0, (0, 1, 2, 3, 4), 0, 2, 5, 0, 0, 1, 0, 1, 20)\n    array([ 0.,  0.])\n    \n    """\n    # Sets position and velocity of the block\n    xi = blockPositions[i] - i\n    vi = vBlock\n    \n    # Block motion for the first block, connected to a block one distance unit away\n    if i == 0:\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc*(xiP - xi) + kp * (v0 * t - xi)\n    \n    # Block motion for the last block, connected to a block one distance unit away\n    elif i == blockNum - 1:\n        xiM = blockPositions[i - 1] - (i - 1)\n        springForce = kc*(xiM - xi) + kp * (v0 * t - xi)\n   \n    # Block motion for all the middle blocks, connected to their neighbors\n    else:\n        xiM = blockPositions[i - 1] - (i - 1)\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc*(xiP + xiM - 2 * xi) + kp * (v0 * t - xi)\n    \n    frictionForce = friction (vi, vf, F0)\n    \n    # If spring force is large enough to overpower friction, change velocity\n    if abs(springForce) <= abs(frictionForce):\n        dv = -vi\n        vi = 0\n        dx = vi\n    \n    else: \n        totalForce = (springForce + frictionForce) / mass\n        dx = vi\n        dv = totalForce\n        \n    return np.array([dx, dv], float)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- save_rnn_layers\n- save_mlp_layers\n- glance_at_tensor\n\nUpdated code:\n```python\ndef highlight_column_matches(data, column=\'\', color=\'yellow\'):\n    \'\'\'\n    highlight the maximum in a Series or DataFrame\n    \'\'\'\n    attr = \'background-color: {}\'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_mixed = data == data[column]\n        return [attr if v else \'\' for v in is_mixed]\n    else:  # from .apply(axis=None)\n        is_mixed = data == data[column]\n        return pd.DataFrame(np.where(is_mixed, attr, \'\'), index=data.index, columns=data.columns)\n\ndef plot_stats(csv_filename, columns=[\'total_reward\'], **kwargs):\n    """Plot specified columns from CSV file."""\n    df_stats = pd.read_csv(csv_filename)\n    df_stats[columns].plot(**kwargs)\n    \nclassifier_activation = {\'binary\': \'sigmoid\', \'categorical\': \'softmax\'}\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n# scale train and test data to [-1,1]\ndef scale(train,test):\n    # fit scaler\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler=scaler.fit(train)\n    # transform train\n    train = train.reshape(train.shape[0],train.shape[1])\n    train_scaled=scaler.transform(train)\n    # transform test\n    test = test.reshape(test.shape[0],test.shape[1])\n    test_scaled=scaler.transform(test)\n    return scaler, train_scaled, test_scaled\n\n# inverse scaling for a forecasted value\ndef invert_scale(scaler,X,value):\n    new_row=[x for x in X] +[value]\n    array=numpy.array(new_row)\n    array=array.reshape(1,len(array))\n    inverted=scaler.inverse_transform(array)\n    return inverted[0,-1]\n\n# fit an LSTM network to training data\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n    X,y=train[:,0:-1],train[:,-1]\n    X=X.reshape(X.shape[0],1,X.shape[1])\n    model=Sequential()\n    model.add(LSTM(neurons,batch_input_shape=(batch_size,X.shape[1],X.shape[2]),stateful=True))\n    model.add(Dense(1))\n    model.compile(loss="mean_squared_error",optimizer="adam")\n    for i in range(nb_epoch):\n        model.fit(X,y,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)\n        model.reset_states()\n    return model\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef recall(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n    tp = tf.diag_part(conf_matrix)\n    recall_scores = tp/(tp_and_fn)\n    if weights:\n        recall_score = tf.multiply(recall_scores, weights)/tf.reduce_sum(weights)\n    else:\n        recall_score = tf.reduce_mean(recall_scores)        \n    return recall_score\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)\n    \n    def fit(self, input_data, target, optimizer, num_epochs=500, \n            verbose=50, track_accuracy=True):\n        """ Function to train the model, using the selected optimizer and\n            for the desired number of epochs. It also stores the accuracy\n            of the model after each epoch.\n        """   \n        \n        if track_accuracy:\n            # Initialize list to store the accuracy of the model\n            self.hist_accuracy = []     \n            # Initialize class to compute the accuracy metric\n            accuracy = tfe.metrics.Accuracy()\n\n        for i in range(num_epochs):\n            # Take a step of gradient descent\n            grads = self.grads_fn(input_data, target)\n            optimizer.apply_gradients(zip(grads, self.variables))\n            if track_accuracy:\n                # Predict targets after taking a step of gradient descent\n                logits = self.predict(X)\n                preds = tf.argmax(logits, axis=1)\n                # Compute the accuracy\n                accuracy(preds, target)\n                # Get the actual result and add it to our list\n                self.hist_accuracy.append(accuracy.result())\n                # Reset accuracy value (we don\'t want to track the running mean accuracy)\n                accuracy.init_variables()\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\naccuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test) )\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train      # cambiar por accuracies_training\ndf["Accuracy (validación)"] = accuracy_validation # cambiar por accuracies_validation\ndf["AUC ROC (training)"]    = roc_train           # cambiar por aucs_training\ndf["AUC ROC (validación)"]  = roc_validation      # cambiar por aucs_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\ndef draw_week_id(id,start_date='wk_0',end_date='wk_68',figsieze=(16,9)):\n    shop_info.loc[id,start_date:end_date].T.plot(figsize=(16,9))\ndef draw_week_ids(ids,start_date='wk_0',end_date='wk_68',figsieze=(16,9)):\n    shop_info.loc[ids,start_date:end_date].T.plot(figsize=(16,9))\ndef draw_ids(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):\n    if by_week:\n        xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n        shop_info.loc[ids,start_date:end_date].T.plot(figsize=figsize,ax=ax)\n    else: shop_info.loc[ids,start_date:end_date].T.plot(figsize=figsize)\n    plt.show()\ndef draw_ids_avg(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=True,figsize=(70,10)):\n    if by_week:\n        xmajorLocator   = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[ids,start_date:end_date].mean(axis=0).plot(figsize=figsize)\n    plt.show()\ndef draw_ids_diff(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):  \n    if by_week:\n        xmajorLocator   = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    (shop_info.loc[ids[0],start_date:end_date]-shop_info.loc[ids[1],start_date:end_date]).plot(figsize=figsize)\ndef draw_id(id,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):\n    if by_week:\n        xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id,start_date:end_date].plot(legend=True,figsize=figsize)\n```"}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n    while delta > epsilon:\n\n        # Only one sweep of evaluation before taking the max\n        delta = 0\n        for i, j in states:\n            # Now evaluates the value function for each state for every possible action (not just with respect to current policy)\n            for a_idx in range(4): # actions\n\n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Update rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n            # This step replaces the poilicy improvement step\n            V_kplus1[i,j] = np.max(Q[i,j,:])\n\n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n        \n    # Updates the policy to be greedy with respect to the value function\n    for i, j in states:\n        PI[i,j] = np.argmax(Q[i,j,:])\n    \n    return V_k, k, PI \n```"}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef sframe_to_scipy(x, column_name):\n    '''\n    Convert a dictionary column of an SFrame into a sparse matrix format where\n    each (row_id, column_id, value) triple corresponds to the value of\n    x[row_id][column_id], where column_id is a key in the dictionary.\n       \n    Example\n    >>> sparse_matrix, map_key_to_index = sframe_to_scipy(sframe, column_name)\n    '''\n    assert x[column_name].dtype() == dict, \\\n        'The chosen column must be dict type, representing sparse data.'\n        \n    # Create triples of (row_id, feature_id, count).\n    # 1. Add a row number.\n    x = x.add_row_number()\n    # 2. Stack will transform x to have a row for each unique (row, key) pair.\n    x = x.stack(column_name, ['feature', 'value'])\n\n    # Map words into integers using a OneHotEncoder feature transformation.\n    f = graphlab.feature_engineering.OneHotEncoder(features=['feature'])\n    # 1. Fit the transformer using the above data.\n    f.fit(x)\n    # 2. The transform takes 'feature' column and adds a new column 'feature_encoding'.\n    x = f.transform(x)\n    # 3. Get the feature mapping.\n    mapping = f['feature_encoding']\n    # 4. Get the feature id to use for each key.\n    x['feature_id'] = x['encoded_features'].dict_keys().apply(lambda x: x[0])\n\n    # Create numpy arrays that contain the data for the sparse matrix.\n    i = np.array(x['id'])\n    j = np.array(x['feature_id'])\n    v = np.array(x['value'])\n    width = x['id'].max() + 1\n    height = x['feature_id'].max() + 1\n\n    # Create a sparse matrix.\n    mat = csr_matrix((v, (i, j)), shape=(width, height))\n\n    return mat, mapping\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef forward_pass(x, W, b):\n    # Calcula a classe baseado em um vetor de pesos e um escalar de víes\n    # Note que nessa função aceitam-se matrizes para W e vetores para b,\n    # afinal estamos trabalhando com batches e não com um exemplo de\n    # cada vez.\n    z = x.dot(W) + b\n    y_ = sigmoid(z)\n    return y_, z\n```'}, {'reason': 'stop', 'result': 'Unused functions:\nNone\n\nUpdated code:\n```python\ndef get_Ks(x_new, x, kernel, theta):\n    """\n    Generates K, KS, and KSS by using the given kernel and theta\n    :param x_new: numpy array of new data\n    :param x: numpy array of data where we have sampled\n    :param theta: list (params + [sigma_n]), this is a list that has the params (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    :return: np.array(np.array) K, np.array(np.array) KS, np.array(np.array) KSS  all as described above.\n    """\n    K = kernel(x, x, theta[:-1], theta[-1]) # K #-\n    KS = kernel(x_new, x, theta[:-1], theta[-1]) # K* #-\n    KSS = kernel(x_new, x_new, theta[:-1], theta[-1]) # K** #-\n    return K, KS, KSS #-\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef regression_optimize_theta(x, y, sigma_n, kernel, params_0=[0.1, 0.1]):\n    \n    """\n    Optimizes parameters for the data given by maximizing logp(data|parameters)\n    :param x: numpy array of data where we have sampled\n    :param y: numpy array of y values for data where we have sampled\n    :sigma_n: float sigma_n\n    :param kernal: the kernel function which we will be using\n    :param params_0: list params_0 this is a list that has the initial params (must be length\n    of params which your kernel is expecting) from this point the optimizer will run.\n    :return: list (optimal_params + [sigma_n]), this is a list that has the optimal parameterss (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    """\n    \n    def log_pY(theta):\n        """\n        Calculates the - log(p(y|parameters))\n        :param theta: list params this is a list that has the params (must be length\n        of params which your kernel is expecting)\n        :return: float - log(p(y|parameters)) (using negative because our optimizer is a minimizer)\n        """\n        K = kernel(x, x, theta, sigma_n) #+\n        log_k = np.linalg.slogdet(K)[1] #+\n        output = 0.5 * np.matmul(np.matmul(y.T, np.linalg.inv(K)),y) #-\n        output += 0.5 * log_k #-\n        return output #-\n\n    res = minimize(log_pY, params_0, method=\'nelder-mead\', options={\'xtol\': 1e-8, \'disp\': False}) #+\n    return list(res.x) + [sigma_n] # +\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- calc_arc_scores\n- calc_technique_scores\n- calc_power_scores\n- calc_yoga_scores\n\nUpdated code:\n```python\nboulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,   \n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n    \n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n    \n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n    \n    # yoga\n    'yoga': boulder_6b / 5,\n    \n    # ARC\n    'arc': boulder_6b_plus / 5,\n    \n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n    \n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%' : boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n    \n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n    \n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n    \n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n    \n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit',]\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+',]\n\n\ndef calc_score(row):\n    s = [row[key] * scores_translator[key] for key in scores_translator]\n    return sum(s)\n\n\ndef calc_hangboard_scores(row):\n    s = [row[key] * scores_translator[key] for key in hangboard]\n    return sum(s)\n        \n    \ndef calc_gym_scores(row):\n    s = [row[key] * scores_translator[key] for key in gymnastics]\n    return sum(s)\n\ndef calc_climbing_scores(row):\n    s = [row[key] * scores_translator[key] for key in climbing]\n    return sum(s)"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n#Auxiliares para correr randomized search\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\n\ndef correr_randomized_y_mostrar(estimator, parameters, folds, top,  iteraciones=None):\n    random_search = None\n    \n    if(iteraciones is None):\n        random_search = RandomizedSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\')\n    else:\n        random_search = RandomizedSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\', n_iter=iteraciones)\n        \n    time_before = time.time()\n    random_search.fit(X_dev_np, y_dev_np)\n    time_after = time.time()\n    runtime = (time_after - time_before) * 1000.0\n    \n    top_resultados(random_search, top)\n    bot_resultados(random_search, top)\n    \n    return (runtime, random_search)\n\ndef verTiempo(original, random):\n    display("########### Timepos ###########")\n    display("original: {:f}".format(original))\n    display("random: {:f}".format(random))\n    display("diferencia: {:f}".format( np.absolute(original-random) ))\n```'}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\n# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef show_results(label):\n    loss, mae, mse = models[label].evaluate(normed_test_data, test_labels[label], verbose=0)\n\n    print("Testing set Mean Abs Error: {:5.2f} um".format(mae))\n    print("Testing set RMS: {:5.2f} um".format(np.sqrt(mse)))\n\n    test_predictions = models[label].predict(normed_test_data).flatten()\n\n    plt.scatter(test_labels[label], test_labels[label] - test_predictions)\n    plt.xlabel(\'True Values [um]\')\n    plt.ylabel(\'Residuals [um]\')\n    minx, maxx = min(test_labels[label]), max(test_labels[label])\n    plt.plot([minx, maxx], [0, 0])\n    plt.show()\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\nimport numpy as np\n\ndef word2embedding(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n            \n    if word in model.vocab:\n        worde = word\n    else:\n        worde = 'null'\n    \n    res = model[worde]\n     \n    if i > 0:\n        word1 = sent[i-1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n    \n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n  \n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n    \n    res.shape = (1,900)\n    return res\n        \ndef sent2embedding(sent):\n    rese = []\n    for  i in range(len(sent)):\n        line = word2embedding(sent,i)\n        rese.append(line)\n        \n    resee = np.vstack(rese)\n    return resee\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- writeToCSV\n\nUpdated code:\n```python\n#Given kml file path, extracts boundary points in regions\ndef extractPolyFromKML(kmlpath):\n    root = parser.fromstring(open(kmlpath, 'r').read())\n    regions = []\n    placemark = root.Document.Placemark\n    for i in range(len(placemark)):\n        block = []\n        r = str(root.Document.Placemark[i].Polygon.outerBoundaryIs.LinearRing.coordinates).split()\n        if r is not None:\n            for point in r:\n                loc = point.split(',')[0:2]\n                block.append(tuple([float(loc[0]),float(loc[1])]))\n            regions.append(block)\n    return regions\n\n#visualizes each of the regions with a list of colors onto map m\ndef visualizePoly(regions, colors, m):\n    for region in range(len(regions)):\n        for i in range(len(regions[region])):\n            if( i < len(regions[region]) -1):\n                point1 = regions[region][i]\n                point2 = regions[region][i+1]\n                plotRoad(point1[1], point2[1], point1[0], point2[0], m, color = colors[region], width = 3)\n    return m\n#visualizes each of the nodes in inter with a list of colors onto a map m\n#nodes should be passed into nodes\ndef visualizeNodes(inter, colors, nodes, m):\n    for region in range(len(inter)):\n        c = colors[region]\n        for idd in inter[region]:\n            p = nodes[nodes['node_id'] == idd]\n            x = p['xcoord'].values[0]\n            y = p['ycoord'].values[0]\n            plotNode(y,x,m,sides = 4, radius = 2, color = c)\n    return m\n\n#plots a list of lists containing linkIds with different colors\ndef visualizeLinks(_regionNodes, _regionLinks, colors, links, m):\n    for i in range(len(_regionNodes)):\n        plotRoads(m, _regionLinks[i], links, color = colors[i], width = 3)\n    return m\n\n#given\ndef nodesPerRegion(regions,nodes):\n    regionNodes = []\n    for i in range(len(regions)):\n        print(i)\n        regionNodes.append(getNodes(regions[i],nodes))\n    return regionNodes\n\ndef linksPerRegion(regionNodes, links):\n    regionLinks = [[] for i in range(len(regionNodes))]\n    for i in range(len(links)):\n        link = links.iloc[i]\n        beginNode = link['begin_node_id']\n        endNode = link['end_node_id']\n        for i in range(len(regionLinks)):\n            if beginNode in regionNodes[i] and endNode in regionNodes[i]:\n                regionLinks[i].append(link['link_id'])\n    return regionLinks\n   \n\n\ndef getNodes(region, nodes):\n    inter = []\n    verts = region\n    codes = [mplPath.Path.LINETO] * len(verts)\n    codes[0] = mplPath.Path.MOVETO\n    codes[-1] = mplPath.Path.CLOSEPOLY\n    poly = mplPath.Path(verts,codes)\n    for i in range(len(nodes)):\n        p = nodes.iloc[i]\n        x = p['xcoord']\n        y = p['ycoord']\n        if(poly.contains_point((x,y))):\n            inter.append(p['node_id'])\n    inter = [int(i) for i in inter]\n    return inter\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- test_diag_fun\n\nUpdated code:\n```python\nfrom scipy.optimize import fsolve\n\ntsym, dsym, ssym, test_sym = symbols('tsym dsym ssym test_sym')\n\nmodel_test_diag = [\n    tsym - ( ssym + (1 - A - U)*test_sym ),\n    dsym - ( A*ssym*p_true_pos + U*ssym*p_false_pos + (1 - A - U)*test_sym*p_true_pos )\n    ]\n\nsol_test_diag = solve(model_test_diag, tsym, dsym)\ntest_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[tsym])\ndiag_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[dsym])\n\n# set up a function to simulate system dynamics when perturbed from steady state\nfrom scipy.integrate import odeint\n\ndef dydt(y, t, parms):\n    return([\n    parms[1]*y[1] + parms[3]*y[2] - (parms[0] + parms[2])*y[0],\n    parms[0]*y[0] - parms[1]*y[1],\n    parms[2]*y[0] - parms[3]*y[2]\n    ])\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef eval_grads(new_weights):\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    variables = TensorFlowVariables(loss, sess)\n    variables.set_weights(new_weights)\n    grads = {}\n    for g, v in grads_and_vars:\n        grads[v.name] = sess.run(g, feed_dict={input_layer : x0.T})\n        # print(str(sess.run(g, feed_dict={input_layer : x0.T})) + " - " + v.name)\n    sess.close()\n    return grads\n\ndef process_weights(w):\n    nw = dict()\n    nw[\'fc1/kernel\'] = w[\'fc1/weights\']\n    nw[\'fc1/bias\'] = w[\'fc1/biases\']\n    nw[\'fc_out/kernel\'] = w[\'fc_out/weights\']\n    nw[\'fc_out/bias\'] = w[\'fc_out/biases\']\n    return nw        \n\ndef flatten_grads(g):\n    arr = np.array([])\n    arr = np.append(arr, g[\'fc1/kernel:0\'])  # weights\n    arr = np.append(arr, g[\'fc1/bias:0\'])\n    arr = np.append(arr, g[\'fc_out/kernel:0\'])  # weights\n    arr = np.append(arr, g[\'fc_out/bias:0\'])\n    return arr\n\ndef full(iteration):\n    true_grad = flatten_grads(eval_grads(process_weights(iteration[\'weights\'])))\n    return np.corrcoef(true_grad, iteration[\'grad\'])\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nimport numpy as np\n```'}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef getBestParamsSVM(train_data, train_labels):\n    classifier = LinearSVC(penalty='l2')\n\n    params = {'C': [0.01, 0.1, 0.5]}\n    svm = GridSearchCV(classifier, params, cv=4, \n                       scoring='accuracy', return_train_score=True)\n\n    svm.fit(train_data, train_labels)  \n    print('\\nSVM Classifier')\n    print(' Best param:', svm.best_params_)\n    print(' Accuracy:  ', np.round(svm.best_score_, 4) )\n    \n    return svm.best_params_\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef gp_plot( x_test, y_test, mean_test, covar_test, x_train, t_train, theta, beta ):\n    # x_test: \n    # y_test:    the true function at x_test\n    # mean_test: predictive mean at x_test\n    # var_test:  predictive covariance at x_test \n    # t_train:   the training values\n    # theta:     the kernel parameters\n    # beta:      the precision (known)\n    \n    # the reason for the manipulation is to allow plots separating model and data stddevs.\n    std_total = np.sqrt(np.diag(covar_test))       # includes all uncertainty, model and target noise \n    std_model = np.sqrt(std_total**2 - 1.0/beta) # remove data noise to get model uncertainty in stddev\n    std_combo = std_model + np.sqrt(1.0/beta)    # add stddev (note: not the same as full)\n    \n    plt.plot(x_test, y_test, 'b', lw=3)\n    plt.plot(x_test, mean_test, 'k--', lw=2)\n    plt.fill_between(x_test, mean_test+2*std_combo,mean_test-2*std_combo, color='k', alpha=0.25)\n    plt.fill_between(x_test, mean_test+2*std_model,mean_test-2*std_model, color='r', alpha=0.25)\n    plt.plot(x_train, t_train, 'ro', ms=10)\n```"}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C=None):\n    \n    if C is None:\n        K = computeK(x_train, x_train, theta)\n        C = K + (1/beta)*np.eye(len(K))\n    \n    C_inv = np.linalg.inv(C)\n    \n    mean_test = np.zeros(len(x_test))\n    covar_test = np.zeros((len(x_test), len(x_test)))\n    \n    for i, x_t in enumerate(x_test):\n        # kernel evaluated between all training points and the new x_test \n        k = np.array([k_n_m(x_n, x_t, theta) for x_n in x_train])   \n        # kernel evaluated in the x_test with itself added of variance\n        c = k_n_m(x_t, x_t, theta) + 1/beta        \n        k_dot_C = np.matmul(k.T, C_inv) \n        \n        mean_test[i] = np.matmul(k_dot_C,t_train)\n        covar_test[i][i] = c - np.matmul(k_dot_C, k)\n\n    return mean_test, covar_test, C\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef showPrecisionRecallPairByLabel(precision_by_label, recall_by_label, label_encoder, classifier_name, colors):\n    labels = []\n    for i in range(len(precision_by_label)):\n        label = label_encoder.inverse_transform([i])[0]\n        labels.append(label)\n    \n    y_pos = np.arange(len(labels))    \n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n\n    ax1.invert_xaxis()\n    ax1.yaxis.tick_right()\n    \n    ax1.set_yticks(y_pos)\n    ax1.set_yticklabels(labels)\n    \n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels(labels)\n        \n    ax1.barh(y_pos, precision_by_label, color=colors[0] , label="precision")\n    ax2.barh(y_pos, recall_by_label,    color=colors[1],  label=\'recall\')\n\n    ax1.set_title(\'Precision( \' + classifier_name + \')\')\n    ax2.set_title(\'Recall (\' + classifier_name + \')\')\n    \n    plt.grid()\n    plt.show()\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- `padInflunet`\n\nUpdated code:\n```python\ndef importInflunet(path):\n    \'\'\'\n    Reads the Influnet data and creates a unique multiindex dataframe of the format\n    \n    (year,week) - incidence\n    \n    :param path: location of the influnet folder\n    :return: compacted version of \n    \'\'\'\n    \n    df = pd.concat([pd.read_csv(path+t, names=["time", "incidence"], sep=" ", header=1, usecols=[0,4], decimal=",") for t in listdir(path)], ignore_index=True)\n    df[["year","week"]] = df["time"].str.split("-", expand=True).astype(int)\n    df.drop(["time"], axis=1, inplace=True)\n    df = df.set_index(["year","week"])\n    df.sortlevel(inplace=True)\n    df = df.astype(float)\n    df = df.loc[2008:]\n    return df\n\n\ndef getInflunet(path = "/home/aalto/Desktop/DE/hw2/influnet/data/"):\n    \'\'\'\n    import and reformat the original Influnet dataset\n    \n    :param path: \n    :return: clean and padded version of the Influnet dataset\n    \'\'\'\n    \n    df = importInflunet();\n    previous = None\n    for x,y in df.index.values:\n        if previous == None:\n            df2 = reindexDF(df.loc[x], x)\n        elif x != previous:\n            df2 = df2.append(reindexDF(df.loc[x], x))\n        previous = x\n    return df\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef update_nueron(w, x, label):\n    '''\n    Update McCulloch + Pitts Nueron using Rosenblatt's Perceptron Algorithms\n    w = neuron weights\n    x = (n dimensional numpy array example)\n    label = binary label\n    '''\n    if neuron(x, w) == 0 and label == 1:\n        w = w + x\n        print('incorrect!, New weights = ' + str(w))\n\n    elif neuron(x, w) == 1 and label == 0:\n        w = w - x\n        print('incorrect!, New weights = ' + str(w))\n\n    else:\n        print('correct!')  \n    \n    return w\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef backward_pass(x, y, W, b):\n    # Responsável por calcular as derivadas parciais para W e b.\n    # Funciona com batches também, nesse caso retornando a média\n    # dos gradientes.\n    y_, z = forward_pass(x, W, b)\n    dLdb = (-y/(y_ + 1e-8) + (1-y)/(1-y_ + 1e-8)) * sigmoid(z, True)\n    dLdW = np.zeros((dLdb.shape[0], 31))\n    for i, el in enumerate(dLdb):\n        dLdW[i] = x[i] * el\n    dLdW = np.mean(dLdW, axis=0)\n    dLdb = np.mean(dLdb, axis=0)\n    return dLdW, dLdb\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n# robust pca\nfrom __future__ import division, print_function\n\nimport numpy as np\n\ntry:\n    from pylab import plt\nexcept ImportError:\n    print('Unable to import pylab. R_pca.plot_fit() will not work.')\n\ntry:\n    # Python 2: 'xrange' is the iterative version\n    range = xrange\nexcept NameError:\n    # Python 3: 'range' is iterative - no need for 'xrange'\n    pass\n\n\nclass R_pca:\n\n    def __init__(self, D, mu=None, lmbda=None):\n        self.D = D\n        self.S = np.zeros(self.D.shape)\n        self.Y = np.zeros(self.D.shape)\n\n        if mu:\n            self.mu = mu\n        else:\n            self.mu = np.prod(self.D.shape) / (4 * self.norm_p(self.D, 2))\n\n        self.mu_inv = 1 / self.mu\n\n        if lmbda:\n            self.lmbda = lmbda\n        else:\n            self.lmbda = 1 / np.sqrt(np.max(self.D.shape))\n\n    @staticmethod\n    def norm_p(M, p):\n        return np.sum(np.power(M, p))\n\n    @staticmethod\n    def shrink(M, tau):\n        return np.sign(M) * np.maximum((np.abs(M) - tau), np.zeros(M.shape))\n\n    def svd_threshold(self, M, tau):\n        U, S, V = np.linalg.svd(M, full_matrices=False)\n        return np.dot(U, np.dot(np.diag(self.shrink(S, tau)), V))\n\n    def fit(self, tol=None, max_iter=1000, iter_print=100):\n        iter = 0\n        err = np.Inf\n        Sk = self.S\n        Yk = self.Y\n        Lk = np.zeros(self.D.shape)\n\n        if tol:\n            _tol = tol\n        else:\n            _tol = 1E-7 * self.norm_p(np.abs(self.D), 2)\n\n        while (err > _tol) and iter < max_iter:\n            Lk = self.svd_threshold(\n                self.D - Sk + self.mu_inv * Yk, self.mu_inv)\n            Sk = self.shrink(\n                self.D - Lk + (self.mu_inv * Yk), self.mu_inv * self.lmbda)\n            Yk = Yk + self.mu * (self.D - Lk - Sk)\n            err = self.norm_p(np.abs(self.D - Lk - Sk), 2)\n            iter += 1\n            if (iter % iter_print) == 0 or iter == 1 or iter > max_iter or err <= _tol:\n                print('iteration: {0}, error: {1}'.format(iter, err))\n\n        self.L = Lk\n        self.S = Sk\n        return Lk, Sk\n\n    def plot_fit(self, size=None, tol=0.1, axis_on=True):\n\n        n, d = self.D.shape\n\n        if size:\n            nrows, ncols = size\n        else:\n            sq = np.ceil(np.sqrt(n))\n            nrows = int(sq)\n            ncols = int(sq)\n\n        ymin = np.nanmin(self.D)\n        ymax = np.nanmax(self.D)\n        print('ymin: {0}, ymax: {1}'.format(ymin, ymax))\n\n        numplots = np.min([n, nrows * ncols])\n        plt.figure()\n\n        for n in range(numplots):\n            plt.subplot(nrows, ncols, n + 1)\n            plt.ylim((ymin - tol, ymax + tol))\n            plt.plot(self.L[n, :] + self.S[n, :], 'r')\n            plt.plot(self.L[n, :], 'b')\n            if not axis_on:\n                plt.axis('off')\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport sys\nsys.path.append("..")\n\n#import models\nfrom loss import PSNRLoss, psnr\n\nimport os\nimport time\nimport numpy as np\nfrom imageio import imwrite as imsave\nfrom scipy.misc import imresize\nfrom scipy.ndimage.filters import gaussian_filter\n\nbase_weights_path = "weights/"\nbase_val_images_path = "val_images/"\nbase_test_images = "test_images/"\n\nset5_path = "tests/set5"\nset14_path = "tests/set14"\nbsd100_path = "tests/bsd100"\n\nif not os.path.exists(base_weights_path):\n    os.makedirs(base_weights_path)\n\nif not os.path.exists(base_val_images_path):\n    os.makedirs(base_val_images_path)\n\nif not os.path.exists(base_test_images):\n    os.makedirs(base_test_images)\n\ndef test_set5(model : Model, img_width=32, img_height=32, batch_size=1):\n    datagen = ImageDataGenerator(rescale=1. / 255)\n    large_img_width = img_width * 4\n    large_img_height = img_height * 4\n\n    iteration = 0\n    total_psnr = 0.0\n\n    print("Testing model on Set 5 Validation images")\n    total_psnr = _test_loop(set5_path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width,\n                            model, total_psnr, "Set5", 5)\n\n    print("Average PSNR of Set5 validation images : ", total_psnr / 5)\n    print()\n\n\ndef test_set14(model : Model, img_width=32, img_height=32, batch_size=1):\n    datagen = ImageDataGenerator(rescale=1. / 255)\n    large_img_width = img_width * 4\n    large_img_height = img_height * 4\n\n    iteration = 0\n    total_psnr = 0.0\n\n    print("Testing model on Set 14 Validation images")\n    total_psnr = _test_loop(set14_path, batch_size, datagen, img_height, img_width, iteration, large_img_height,\n                            large_img_width, model, total_psnr, "Set14", 14)\n\n    print("Average PSNR of Set5 validation images : ", total_psnr / 14)\n    print()\n\ndef test_bsd100(model : Model, img_width=32, img_height=32, batch_size=1):\n    datagen = ImageDataGenerator(rescale=1. / 255)\n    large_img_width = img_width * 4\n    large_img_height = img_height * 4\n\n    iteration = 0\n    total_psnr = 0.0\n\n    print("Testing model on BSD 100 Validation images")\n    total_psnr = _test_loop(bsd100_path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width,\n                            model, total_psnr, "bsd100", 100)\n\n    print("Average PSNR of BSD100 validation images : ", total_psnr / 100)\n    print()\n\n\ndef _test_loop(path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width, model,\n               total_psnr, prefix, num_images):\n    print("Path " + path)\n    print("Height " + str(img_height))\n    print("img_width " + str(img_width))\n    print("large_img_height " + str(large_img_height))\n    print("large_img_width " + str(large_img_width))\n    print("num_images " + str(num_images))\n    for x in datagen.flow_from_directory(path, class_mode=None, batch_size=batch_size,\n                                         target_size=(large_img_width, large_img_height)):\n        t1 = time.time()\n\n        # resize images\n        x_temp = x.copy()\n        x_temp = x_temp.transpose((0, 2, 3, 1))\n\n        x_generator = np.empty((batch_size, img_width, img_height, 3))\n\n        for j in range(batch_size):\n            img = imresize(x_temp[j], (img_width, img_height))\n            x_generator[j, :, :, :] = img\n\n        #x_generator = x_generator.transpose((0, 3, 1, 2))\n\n        output_image_batch = model.predict_on_batch(x_generator)\n\n        average_psnr = 0.0\n        for x_i in range(batch_size):\n            average_psnr += psnr(x[x_i], output_image_batch[x_i] / 255.)\n            total_psnr += average_psnr\n\n        average_psnr /= batch_size\n\n        iteration += batch_size\n        t2 = time.time()\n\n        print("Time required : %0.2f. Average validation PSNR over %d samples = %0.2f" %\n              (t2 - t1, batch_size, average_psnr))\n\n        for x_i in range(batch_size):\n            real_path = base_test_images + prefix + "_iteration_%d_num_%d_real_.png" % (iteration, x_i + 1)\n            generated_path = base_test_images + prefix + "_iteration_%d_num_%d_generated.png" % (iteration, x_i + 1)\n\n            val_x = x[x_i].copy() * 255.\n            #val_x = val_x.transpose((1, 2, 0))\n            val_x = np.clip(val_x, 0, 255).astype(\'uint8\')\n\n            output_image = output_image_batch[x_i]\n            #output_image = output_image.transpose((1, 2, 0))\n            output_image = np.clip(output_image, 0, 255).astype(\'uint8\')\n            \n            imsave(real_path, val_x[:,:,0])\n            imsave(generated_path, output_image[:,:,0])\n\n        if iteration >= num_images:\n            break\n    return total_psnr\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef buildIncomesExpenses(bankdata):\n    data = []\n\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        movements = bankdata[CURRENT][bank][\'movements\']\n\n        incomes = {}\n        expenses = {}\n        for date, movement in zip(dates, movements):\n            key = str(date.month) + \'/\' + str(date.year)\n\n            if float(movement) > 0:\n                if key in incomes:\n                    incomes[key] += float(movement)\n                else:\n                    incomes[key] = float(movement)\n            else:\n                if key in expenses:\n                    expenses[key] += float(movement)\n                else:\n                    expenses[key] = float(movement)\n            \n        months_x = []\n        incomes_y = []\n        for key, value in incomes.items():\n            months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            incomes_y.append(value)\n        \n        trace = go.Bar(\n            x = months_x,\n            y = incomes_y,\n            name = "Incomes for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n        months_x = []\n        expenses_y = []\n        for key, value in expenses.items():\n            months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            expenses_y.append(value)\n        \n        trace = go.Bar(\n            x = months_x,\n            y = expenses_y,\n            name = "Expenses for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n    return data\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef precision(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n    tp = tf.diag_part(conf_matrix)\n    precision_scores = tp/(tp_and_fp)\n    if weights:\n        precision_score = tf.multiply(precision_scores, weights)/tf.reduce_sum(weights)\n    else:\n        precision_score = tf.reduce_mean(precision_scores)        \n    return precision_score\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef accuracy(predictions, labels):\n    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n            / predictions.shape[0])\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef get_data_from_tsv(feature_file, x_fields, y_field, x_filter=None, seed=0, as_np_array=False, scale=False, shuffle=False, train_portion=0.6, test_portion=0.2):\n\n    # Load the dataset\n    my_data = pandas.read_csv(feature_file)\n\n\n    # Drop missing values\n    print(my_data.shape)\n    my_data = my_data.dropna()\n    my_data = my_data.reset_index(drop=True)\n    print(my_data.shape)\n    \n    # Filter responses if len(x_filter) > 0\n    if not x_filter == None:\n        for ff in x_filter:\n            my_data = my_data[my_data[ff[0]].isin(ff[1:])]\n            print("filtering %s by " % ff[0], ff[1:], "num datapoints left: ", len(my_data))\n        my_data = my_data.reset_index(drop=True)\n    labels = my_data[\'word\']\n        \n    # turn frequencies into log frequencies\n    if \'freq\' in x_fields or my_data.columns.get_loc(\'freq\') in x_fields:\n        idx = my_data.columns.get_loc(\'freq\')\n        my_data.iloc[:,idx] = np.log(my_data.iloc[:,idx])\n        my_data=my_data.rename(columns = {\'freq\':\'log_freq\'})\n    \n    # turn lengths into log lengths\n    if \'length\' in x_fields or my_data.columns.get_loc(\'length\') in x_fields:\n        idx = my_data.columns.get_loc(\'length\')\n        my_data.iloc[:,idx] = np.log(my_data.iloc[:,idx])\n        my_data=my_data.rename(columns = {\'length\':\'log_length\'})\n \n    # Choose the coefficients\n    if type(x_fields[0]) == str:\n        x_fields.append(y_field)\n        my_data = my_data[x_fields]\n    else:\n        x_fields.append(my_data.columns.get_loc(y_field))\n        my_data = my_data.iloc[:,x_fields]\n    del x_fields[-1]\n    \n    \n    # Split the data into training/testing sets\n    x_train, y_train, x_dev, y_dev, x_test, y_test = train_validate_test_split(my_data, \n                                                                               y_field,\n                                                                               train_percent=train_portion,\n                                                                               validate_percent=test_portion,\n                                                                               seed=seed,\n                                                                               shuffle=shuffle)\n\n    if scale:\n        x_train = pandas.DataFrame(preprocessing.scale(x_train), columns=x_train.columns, index=x_train.index)\n        x_dev   = pandas.DataFrame(preprocessing.scale(x_dev)  , columns=x_dev.columns, index=x_dev.index)\n        x_test  = pandas.DataFrame(preprocessing.scale(x_test) , columns=x_test.columns, index=x_test.index)\n        y_train = pandas.DataFrame(preprocessing.scale(y_train), columns=y_train.columns, index=y_train.index)\n        y_dev   = pandas.DataFrame(preprocessing.scale(y_dev)  , columns=y_dev.columns, index=y_dev.index)\n        y_test  = pandas.DataFrame(preprocessing.scale(y_test) , columns=y_test.columns, index=y_test.index)\n\n    \n    if as_np_array:\n        x_train = np.array(x_train).astype(np.float)\n        y_train = np.array(y_train).astype(np.float)\n        x_dev   = np.array(x_dev).astype(np.float)\n        y_dev   = np.array(y_dev).astype(np.float)\n        x_test  = np.array(x_test).astype(np.float)\n        y_test  = np.array(y_test).astype(np.float)\n        \n    return x_train, y_train, x_dev, y_dev, x_test, y_test, labels\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef get_neighbour_messages(sender, receiver):\n    messages = []\n    for n in sender.neighbours:\n        if n is not receiver:\n            messages.append(sender.in_msgs[n])\n    return messages\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n## Show history\n\ndef show_history_graph(history):\n    '''  Graphically show the history of model fitting\n    '''\n    \n    plt.figure(figsize=(8,8))\n    plt.subplot(221)\n    # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    \n    # summarize history for loss\n    plt.subplot(222)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    \n    plt.tight_layout()\n    plt.show() \n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# Scaling Functions\n\ndef NRM1(X):\n    """Scale by dividing by the 1-norm"""\n    norm = np.linalg.norm(X, ord=1)\n    return X / norm\n\ndef SCL1(X):\n    """Scale between (-1, 1)"""\n    mean = X.mean()\n    maximum = X.max()\n    minimum = X.min()\n    return (X - mean) / (maximum - minimum)\n\ndef TRSH(X, threshold_value=20):\n    X = [0 if val < threshold_value else 1 for val in X]\n    return X\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef print_policy(policy, terminal_states):\n    \n    #idx_to_symbol = {0:\'\\u25C0\', 1:\'\\u25BC\', 2:\'\\u25B6\', 3:\'\\u25BC\'}\n    idx_to_symbol = {0:\'\\u2190\', 1:\'\\u2191\', 2:\'\\u2192\', 3:\'\\u2193\'}\n    \n    border_str = "\\u00B7 "\n    for i in range(policy.shape[0]):\n        border_str += "\\u2015 "\n    border_str += "\\u00B7 "\n    #print(border_str)\n    \n    for i in range(policy.shape[0]):\n        \n        string = ""\n        #string = "| "\n        for j in range(policy.shape[1]):\n            \n            if (i,j) in terminal_states:\n                string += \'\\u25A0 \'\n            else:\n                string += idx_to_symbol[policy[i, j]]+" "\n        \n        #string += "|"\n        print(string)\n    \n    #print(border_str)\n    \n    return\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- delta_f\n\nUpdated code:\n```python\n# No changes made as there are no unused functions\ndef delta_f(list_of_terms, x_value, delta_x):\n    pass\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n'''Function for creating a FP yield(A,Z) list container'''\n\ndef get_fpy_az( df ):\n    '''\n    Create a list of named tuple nuclides\n    Parameters\n    ----------\n    df: pandas data frame, required\n        Table of data for nuclides.\n    \n    Returns\n    -------\n    nuclides: list(namedtuple)\n        List of namedtuples. Names: name, element_name, Z, A, yield_percent.\n\n    Examples\n    --------\n    '''\n    \n    nuclides = list()\n\n    # design a container data structure\n    from collections import namedtuple\n    FPY = namedtuple('FPY', ['name','element_name','Z','A','yield_percent'])\n\n    import pandas as pd\n\n    # Use the Mendeleev python package (periodic table of elements)\n    from mendeleev import element\n    \n    total_yield = 0.0 # sum total yield\n    for row in df.itertuples(index=False):\n        z = int(row[0])\n        for j in range(1,len(row)-1):\n            if row[j] < 1.e-10: # this will eliminate many zeros\n                continue\n            a_str = df.columns[j] # index column is not part of the columns\n            symbol = element(z).symbol\n            name = name=symbol+'-'+a_str\n            element_name = element(z).name\n            yield_value = row[j]\n            total_yield += yield_value\n            nuc = FPY( name=name, element_name=element_name, Z=z, A=int(a_str), yield_percent=yield_value )\n        \n            nuclides.append(nuc)\n            \n    print('Sum of yield values in data file = ',round(total_yield,2))\n    return nuclides\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path, height=224, width=224):\n    ''' Loads RGB image as PIL.Image.Image type of given Height x Width dimensions\n    '''\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(height, width))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\n\nfrom astropy.table import Table\ndef load_coords(filename = "sncoordinates.list"):\n    """\n\n    """\n    path = os.path.abspath(os.path.join(pcc.__path__[0], os.path.pardir, filename))\n    coordtable = Table.read(path, format = \'ascii.commented_header\')\n    return coordtable\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- draw_anottation\n\nUpdated code:\n```python\nfrom IPython.display import clear_output\n\ndef save_annotations(adict):\n    timestamp = datetime.datetime.today().strftime(\'%Y%m%d%H%M\')\n    filename = \'annotations_\' + timestamp + \'.csv\'\n    print(filename)\n    with open(os.path.join(\'.\', filename), \'w\', newline=\'\') as out:\n        writer = csv.writer(out)\n        for key, value in adict.items():\n            line = [key, *value]\n            writer.writerow(line)\n\ndef create_anottations(lista, save=True):\n    """Use dumb walk heuristic to create anottations\n    Args: \n    \n        lista: list of images\n        save: if true, save on current directory a csv <annottations_timestamp.csv>\n    \n    Returns: \n        \n        a dict with name of image: (xleft, ytop, xright, ytop) coordinates\n    """\n    cont = 0\n    result = {}\n    for img in lista:\n        try:\n            result[img] = find_conteiner(img)\n        except ValueError:\n            pass\n        cont += 1\n        if cont % 100 == 0:\n            clear_output()\n            print(\'...\', cont, \'...\')\n    if save:\n        save_annotations(result)\n    return result\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef bow_lights_gt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    \n    Ships greater than 50m in length.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 126)\n        tall_mh_height = taller_masthead_light + light_width\n        shorter_masthead_light = np.random.randint(tall_mh_height + 2, tall_mh_height + 16)\n        short_mast_head_height = shorter_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(15, 56)\n        running_light_dist_vert = np.random.randint(short_mast_head_height, short_mast_head_height + 51)\n        new_view[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        new_view[shorter_masthead_light:short_mast_head_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_view = new_view.flatten()\n        all_bow_images[i] = new_view\n    \n    return all_bow_images\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\n# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\ndef evaluate(inp_sentence):\n  \n  # 準備英文句子前後會加上的 <start>, <end>\n  start_token = [subword_encoder_en.vocab_size]\n  end_token = [subword_encoder_en.vocab_size + 1]\n  \n  # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n  # 並在前後加上 BOS / EOS\n  inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n  # 是一個只包含一個中文 <start> token 的序列\n  decoder_input = [subword_encoder_zh.vocab_size]\n  output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n  \n  # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n  for i in range(MAX_LENGTH):\n    # 每多一個生成的字就得產生新的遮罩\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n\n    # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n    predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n    if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n    # 下個中文字的時候關注到最新的 `predicted_id`\n    output = tf.concat([output, predicted_id], axis=-1)\n\n  # 將 batch 的維度去掉後回傳預測的中文索引序列\n  return tf.squeeze(output, axis=0), attention_weights\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef clean_data (df, inplace = False):\n    column_filter = [\'ISIN\', \'Mnemonic\', \'SecurityDesc\', \'SecurityType\', \'Currency\', \'SecurityID\', \'Date\', \'Time\', \'StartPrice\', \'MaxPrice\', \'MinPrice\', \'EndPrice\', \'TradedVolume\', \'NumberOfTrades\']\n    n_df = df[column_filter]\n    if not inplace:\n        n_df = df.copy ()\n        \n    n_df.drop (n_df.Time == \'Time\', inplace = True)\n    # we want the dates to be comparable to datetime.strptime()\n    n_df["CalcTime"] = pd.to_datetime("1900-01-01 " + n_df["Time"], errors=\'coerce\')\n    n_df["CalcDateTime"] = pd.to_datetime(n_df["Date"] + " " + n_df["Time"], errors=\'coerce\')\n\n    # Filter common stock\n    # Filter between trading hours 08:00 and 20:00\n    # Exclude auctions (those are with TradeVolume == 0)\n    only_common_stock = n_df[n_df.SecurityType == \'Common stock\']\n    time_fmt = "%H:%M"\n    opening_hours_str = "08:00"\n    closing_hours_str = "20:00"\n    opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n    closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n\n    cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n                      (only_common_stock.CalcTime >= opening_hours) & \\\n                      (only_common_stock.CalcTime <= closing_hours)]\n    \n    bymnemonic = cleaned_common_stock[[\'Mnemonic\', \'TradedVolume\']].groupby([\'Mnemonic\']).sum()\n    number_of_stocks = 100\n    top = bymnemonic.sort_values([\'TradedVolume\'], ascending=[0]).head(number_of_stocks)\n    top_k_stocks = list(top.index.values)\n    cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n    sorted_by_index = cleaned_common_stock.set_index([\'Mnemonic\', \'CalcDateTime\']).sort_index()\n    non_empty_days = sorted(list(cleaned_common_stock[\'Date\'].unique()))\n    new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)["OrganizedDateTime"].values\n    \n    stocks = []\n    for stock in top_k_stocks:\n        stock = basic_stock_features(sorted_by_index, stock, new_datetime_index, inplace=True)\n        stocks.append(stock)\n    # prepared should contain the numeric features for all top k stocks,\n    # for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n    # for all minutes from 08:00 until 20:00\n    # in minutes without trades the prices from the last available minute are carried forward\n    # trades are filled with zero for such minutes\n    # a new column called HasTrade is introduced to denote the presence of trades\n    prepared = pd.concat(stocks, axis=0).dropna(how=\'any\')\n    prepared.Mnemonic = prepared.Mnemonic.astype(\'category\')\n    return prepared\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# Helper function that takes numpy array of the raw time series and a lag or number of shifted series to create \n# and use as inputs\ndef timeseries_to_supervised(data,lag=1):\n    df=pd.DataFrame(data)\n    columns=[df.shift(i) for i in range(1,lag+1)] # depends on how many lags are required\n    columns.append(df) # Appending list to each other \n    df=pd.concat(columns,axis=1)\n    df.fillna(0,inplace=True) #takes care of the Nan value\n    return df\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n'''Function to read the blank-spaced column data into a Pandas data frame (table)'''\n\ndef read_table(file_name):\n    '''\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    '''\n    import pandas as pd\n\n    # read the data into a data frame (or table)   \n    df = pd.read_csv(file_name, delim_whitespace=True)\n    \n    # to avoid frustrations, set explicitly the data types of each column\n    df = df.astype({'A':int,'Element':str,'Z':int,'N':int,'T1/2-(seconds)':float},copy=False)\n    print(df.dtypes)\n\n    return df\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef shuffle_batch(X, y, batch_size):\n    rnd_idx = np.random.permutation(len(X))\n    n_batches = len(X) // batch_size\n    for batch_idx in np.array_split(rnd_idx, n_batches):\n        X_batch, y_batch = X[batch_idx], y[batch_idx]\n        yield X_batch, y_batch\n```'}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef rv_plot2(r1, v1, r2, v2, t0, t1, epsilon=500, title1='', title2='', redrange=None):\n    #0. r1とv2はそれぞれ対応する膝屈曲角とそれの角速度, t0とt1は第１と第２の極小値近傍時刻, itimeオプションは後のため\n    ti = t0 - epsilon\n    tf = t1 + epsilon\n    tm = ti + (tf - ti) // 2    #tm は ti ~ tf の中間インデックス\n    xi = np.where(r1==np.min(r1[ti:tm]))[0][0]    #xi は ti ~ tm までの r の最小値のインデックス\n    xf = np.where(r1==np.min(r1[tm:tf]))[0][0]    #xf は tm ~ tf までの r の最小値のインデックス\n    xm = xi + (xf - xi) // 2    #xm は xi ~ xf の中間インデックス\n    \n    #1. 膝屈曲角プロット\n    plt.subplot(1, 2, 1)\n    plt.plot(r1[xi:xm], v1[xi:xm], c='black')\n    plt.plot(r1[xm:xf], v1[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r1[redrange[0]:redrange[1]], v1[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた\n    plt.title(f'{title1}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()\n    \n    #2. 大腿(下腿)傾き角プロット\n    plt.subplot(1, 2, 2)\n    plt.plot(r2[xi:xm], v2[xi:xm], c='black')\n    plt.plot(r2[xm:xf], v2[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r2[redrange[0]:redrange[1]], v2[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた       \n    plt.title(f'{title2}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n### Train your model here.\n### Calculate and report the accuracy on the training and validation set.\n### Once a final model architecture is selected, \n### the accuracy on the test set should be calculated and reported as well.\n\ndef LeNet6(x, n_classes):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n    # MODIFIED: Layer 1: Convolutional. Input = 32x32x3 (rgb). Output = 28x28x6. shape (5,5,1,6)->(5,5,3,6)\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    \n    # Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Flatten. Input = 5x5x16. Output = 400.\n    fc0   = flatten(conv2)\n    \n    # Layer 3: Fully Connected. Input = 400. Output = 252.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean = mu, stddev = sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n    \n    # Activation.\n    fc1    = tf.nn.relu(fc1)\n    \n    # Dropout, to prevent overfitting\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n\n    # Layer 4: Fully Connected. Input = 252. Output = 168.\n    fc2_W  = tf.Variable(tf.truncated_normal(shape=(252, 168), mean = mu, stddev = sigma))\n    fc2_b  = tf.Variable(tf.zeros(168))\n    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n    \n    # Activation.\n    fc2    = tf.nn.relu(fc2)\n    \n    # Dropout, to prevent overfitting\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    \n    # Layer 5: Fully Connected. Input = 168. Output = 84.\n    fc3_W  = tf.Variable(tf.truncated_normal(shape=(168, 84), mean = mu, stddev = sigma))\n    fc3_b  = tf.Variable(tf.zeros(84))\n    fc3    = tf.matmul(fc2, fc3_W) + fc3_b\n    \n    # Activation.\n    fc3    = tf.nn.relu(fc3)\n    \n    # Dropout, to prevent overfitting\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n\n    # Layer 6: Fully Connected. Input = 84. Output = n_classes, which is 43.\n    fc4_W  = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n    fc4_b  = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    \n    return logits\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef stick_breaking(beta):\n    portion_remaining = tt.concatenate([[1], tt.extra_ops.cumprod(1 - beta)[:-1]])\n\n    return beta * portion_remaining\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_\n```'}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef basic_stock_features(input_df, mnemonic, new_time_index, inplace=False):\n    stock = input_df.loc[mnemonic]\n    if not inplace:\n        stock = input_df.loc[mnemonic].copy()\n    \n    stock = stock.reindex(new_time_index)\n    \n    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n    for f in features:\n        stock[f] = stock[f].fillna(method='ffill')   \n    \n    features = ['TradedVolume', 'NumberOfTrades']\n    for f in features:\n        stock[f] = stock[f].fillna(0.0)\n        \n    stock['HourOfDay'] = stock.index.hour\n    stock['MinOfHour'] = stock.index.minute\n    stock['MinOfDay'] = stock.index.hour*60 + stock.index.minute\n\n    stock['DayOfWeek'] = stock.index.dayofweek\n    stock['DayOfYear'] = stock.index.dayofyear\n    stock['MonthOfYear'] = stock.index.month\n    stock['WeekOfYear'] = stock.index.weekofyear\n    \n    stock['Mnemonic'] = mnemonic\n    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n    return stock.drop(unwanted_features, axis=1)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n# Define a function to convert from image coords to rover coords\ndef rover_coords(binary_img):\n    # Identify nonzero pixels\n    ypos, xpos = binary_img.nonzero()\n    # Calculate pixel positions with reference to the rover position being at the \n    # center bottom of the image.  \n    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n    return x_pixel, y_pixel\n\n# Define a function to convert to radial coords in rover space\ndef to_polar_coords(x_pixel, y_pixel):\n    # Convert (x_pixel, y_pixel) to (distance, angle) \n    # in polar coordinates in rover space\n    # Calculate distance to each pixel\n    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n    # Calculate angle away from vertical for each pixel\n    angles = np.arctan2(y_pixel, x_pixel)\n    return dist, angles\n\n# Define a function to map rover space pixels to world space\ndef rotate_pix(xpix, ypix, yaw):\n    # Convert yaw to radians\n    yaw_rad = yaw * np.pi / 180\n    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n                            \n    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n    # Return the result  \n    return xpix_rotated, ypix_rotated\n\ndef translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n    # Apply a scaling and a translation\n    xpix_translated = (xpix_rot / scale) + xpos\n    ypix_translated = (ypix_rot / scale) + ypos\n    # Return the result  \n    return xpix_translated, ypix_translated\n\n\n# Define a function to apply rotation and translation (and clipping)\n# Once you define the two functions above this function should work\ndef pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n    # Apply rotation\n    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n    # Apply translation\n    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n    # Perform rotation, translation and clipping all at once\n    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n    # Return the result\n    return x_pix_world, y_pix_world\n\n# Grab another random image\nidx = np.random.randint(0, len(img_list)-1)\nimage = mpimg.imread(img_list[idx])\nwarped, mask = perspect_transform(image, source, destination)\nthreshed = color_thresh(warped)\n\n# Calculate pixel values in rover-centric coords and distance/angle to all pixels\nxpix, ypix = rover_coords(threshed)\ndist, angles = to_polar_coords(xpix, ypix)\nmean_dir = np.mean(angles)\n\n# Do some plotting\nfig = plt.figure(figsize=(12,9))\nplt.subplot(221)\nplt.imshow(image)\nplt.subplot(222)\nplt.imshow(warped)\nplt.subplot(223)\nplt.imshow(threshed, cmap='gray')\nplt.subplot(224)\nplt.plot(xpix, ypix, '.')\nplt.ylim(-160, 160)\nplt.xlim(0, 160)\narrow_length = 100\nx_arrow = arrow_length * np.cos(mean_dir)\ny_arrow = arrow_length * np.sin(mean_dir)\nplt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef poisson_approx(data, mean):\n    pdist = np.array(np.exp((data*np.log(mean))-(data*np.log(data))+data-mean))\n    return pdist\n\ndef gaussian(data,mean,sigma):\n    gdist = np.array((1/(sigma*(2*math.pi)**(.5)))*np.exp(-.5*((data-mean)/sigma)**2))   \n    return gdist\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass GenerativeNetwork:\n\n    def __init__(self, img_width=96, img_height=96, batch_size=16, num_upscales=2, small_model=False,\n                 content_weight=1, tv_weight=2e5, gen_channels=64):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.batch_size = batch_size\n        self.small_model = small_model\n        self.num_scales = num_upscales\n\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n\n        self.filters = gen_channels\n        self.mode = 2\n        self.init = \'glorot_uniform\'\n\n        self.sr_res_layers = None\n        self.sr_weights_path = "weights/SRGAN.h5"\n\n        self.output_func = None\n\n    def create_sr_model(self, ip):\n\n        x = Conv2D(self.filters, (5, 5), activation=\'linear\', padding=\'same\', name=\'sr_res_conv1\',\n                          kernel_initializer=self.init)(ip)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_1\')(x)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_lr1\')(x)\n\n        x = Conv2D(self.filters, (5, 5), activation=\'linear\', padding=\'same\', name=\'sr_res_conv2\', kernel_initializer="glorot_uniform")(x)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_2\')(x)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_lr2\')(x)\n\n        num_residual = 5 if self.small_model else 15\n\n        for i in range(num_residual):\n            x = self._residual_block(x, i + 1)\n\n        for scale in range(self.num_scales):\n            x = self._upscale_block(x, scale + 1)\n    \n        scale = 2 ** self.num_scales\n        tv_regularizer = TVRegularizer(img_width=self.img_width * scale, img_height=self.img_height * scale,\n                                       weight=self.tv_weight) #self.tv_weight)\n        \n        x = Conv2D(3, (5, 5), activation=\'tanh\', padding=\'same\', activity_regularizer=tv_regularizer, \n                   name=\'sr_res_conv_final\', kernel_initializer=self.init)(x)\n        \n        x = Denormalize(name=\'sr_res_conv_denorm\')(x)\n        return x\n\n    def _residual_block(self, ip, id):\n        init = ip\n\n        x = Conv2D(self.filters, (3, 3), activation=\'linear\', padding=\'same\', name=\'sr_res_conv_\' + str(id) + \'_1\',\n                          kernel_initializer=self.init)(ip)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_\' + str(id) + \'_1\')(x)\n        x = LeakyReLU(alpha=0.25, name="sr_res_activation_" + str(id) + "_1")(x)\n\n        x = Conv2D(self.filters, (3, 3), activation=\'linear\', padding=\'same\', name=\'sr_res_conv_\' + str(id) + \'_2\',\n                          kernel_initializer=self.init)(x)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_\' + str(id) + \'_2\')(x)\n\n        m = add([x, init],name="sr_res_merge_" + str(id))\n\n        return m\n\n    def _upscale_block(self, ip, id):\n        \'\'\'\n        As per suggestion from http://distill.pub/2016/deconv-checkerboard/, I am swapping out\n        SubPixelConvolution to simple Nearest Neighbour Upsampling\n        \'\'\'\n        init = ip\n        \n        x = Conv2D(128, (3, 3), activation="linear", padding=\'same\', name=\'sr_res_upconv1_%d\' % id,\n                          kernel_initializer=self.init)(init)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_up_lr_%d_1_1\' % id)(x)\n        x = UpSampling2D(name=\'sr_res_upscale_%d\' % id)(x)\n        #x = SubPixelUpscaling(r=2, channels=32)(x)\n        x = Conv2D(128, (3, 3), activation="linear", padding=\'same\', name=\'sr_res_filter1_%d\' % id,\n                          kernel_initializer=self.init)(x)\n        x = LeakyReLU(alpha=0.3, name=\'sr_res_up_lr_%d_1_2\' % id)(x)\n\n        return x\n\n    def set_trainable(self, model, value=True):\n        if self.sr_res_layers is None:\n            self.sr_res_layers = [layer for layer in model.layers\n                                    if \'sr_res_\' in layer.name]\n\n        for layer in self.sr_res_layers:\n            layer.trainable = value\n\n    def get_generator_output(self, input_img, srgan_model):\n        if self.output_func is None:\n            gen_output_layer = [layer for layer in srgan_model.layers\n                                if layer.name == "sr_res_conv_denorm"][0]\n            self.output_func = K.function([srgan_model.layers[0].input],\n                                          [gen_output_layer.output])\n\n        return self.output_func([input_img])\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef qda_accracy():\n    c1_result = [] \n    c2_result = []\n    miss = []\n    \n    for i in c1.test:\n        c1_result.append(qda(i)<0)\n        if qda(i)>0:\n            miss.append(i)\n    \n    for i in c2.test:\n        c2_result.append(qda(i)>0)\n        if qda(i)<0:\n            miss.append(i)\n    \n    miss = np.array(miss)\n    acc = (sum(c1_result) + sum(c2_result))/200\n    \n    return acc, miss\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nfrom scipy.integrate import simps\n\ndef calc_spectrum_filter_flux(filter_name, SpecClass):\n    filter_object = pcc.functions.load_filter("/Users/berto/Code/CoCo/data/filters/" + filter_name + ".dat")\n    filter_object.resample_response(new_wavelength = SpecClass.wavelength)\n    filter_area = simps(filter_object.throughput, filter_object.wavelength)\n    \n    transmitted_spec = filter_object.throughput * SpecClass.flux\n\n    integrated_flux = simps(transmitted_spec, SpecClass.wavelength)\n    \n    return  integrated_flux/filter_area\n\ndef calc_specphot(sn, filtername):\n\n    specphot = np.array([])\n    specepoch = np.array([])\n\n    for spec in sn.mangledspec:\n        specphot = np.append(specphot, calc_spectrum_filter_flux(filtername, sn.mangledspec[spec]))\n        specepoch = np.append(specepoch, sn.mangledspec[spec].mjd_obs)\n    \n    return specepoch, specphot\n\ndef compare_phot_specphot(sn, filtername):\n    """"""\n    specepoch, specphot =  calc_specphot(sn, filtername)\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    ax.scatter(specepoch, specphot, label = "specphot")\n    ax.scatter(sn.phot.data[filtername]["MJD"], sn.phot.data[filtername]["flux"], label = filtername)\n\n    ax.set_ylim(0, 1.05 * np.nanmax(np.append(sn.phot.data[filtername]["flux"], specphot)))\n    ax.legend()\n#     plt.show()\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef processCSV(file, splitToken, bank): #TODO Move to pandas\n    fecha = []\n    saldo = []\n    movimientos = []\n    first = True\n    \n    if bank == 'cajamar':\n        with open(file) as f:\n            for line in f:\n                if first is False:\n                    tokens = line.split(splitToken)\n                    fecha.append(dt.datetime.strptime(tokens[0], '%d/%m/%Y').date())\n                    saldo.append(float(tokens[len(tokens)-1][:-1].replace('.', '').replace(',', '.')))\n                    movimientos.append(float(tokens[len(tokens)-2].replace('.','').replace(',','.').replace(' ', '')))\n                first = False\n\n    return (saldo, fecha, movimientos)\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# No unused functions to remove\ndef stern_light():\n    """\n    Makes a numpy array of a lot of different arrays meant to behave as an image of a stern light. \n    The color of stern lights is white, so this is fairly straight forward with a bunch of zeros.\n    """\n    min_pixel_bound = 50\n    max_pixel_bound = 200\n    total_gens = np.random.randint(500, 701)\n    all_stern_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        light_width = np.random.randint(10, 51)\n        start_x = np.random.randint(min_pixel_bound, max_pixel_bound)\n        start_y = np.random.randint(min_pixel_bound, max_pixel_bound)\n        new_array = np.zeros((255, 255, 3))\n        aggregate_horiz = start_x + light_width\n        aggregate_vert = start_y + light_width\n        new_array[start_x:aggregate_horiz, start_y:aggregate_vert] = (255, 255, 255)\n        new_array = new_array.flatten()\n        all_stern_images[i] = new_array\n\n    return all_stern_images\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\ndef get_augmented_images(X,y,img_ct_per_class = 2500):\n    '''\n    This function creates the necessary number of images so that each class\n    has the same number of images (img_ct_per_class)\n    '''\n    aug_images = []\n    aug_labels = []\n    for cl in range(0,n_classes):\n        y_class = np.where(y == cl)\n        idx_class = y_class[0]\n        X_class = X[idx_class]\n        tmp_x,tmp_y = get_augmented_images_by_class(X=X_class, cl=cl, final_length = img_ct_per_class)\n\n        aug_images.extend(tmp_x)\n        aug_labels.extend(tmp_y)\n        \n    X_aug = np.stack(aug_images, axis = 0)\n    Y_aug = np.stack(aug_labels, axis = 0)\n    \n    return (X_aug,Y_aug)\n    \n\ndef get_augmented_images_by_class(X,cl,final_length):\n    tmp_x = []\n    for im in X:\n        tmp_x.append(im)\n    original_length = X.shape[0]\n    tmp_y = [cl] * original_length\n    \n    add_length = final_length - original_length\n    for _ in range(add_length):\n        img_to_augment = X[np.random.randint(0,original_length)]\n        new_img = transform_image(img_to_augment, 20, 10, 5)\n        tmp_x.append(new_img)\n    tmp_y.extend([cl] * add_length)\n    return tmp_x, tmp_y\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef get_trace_sum_balances(bankdata):\n    sum_balances = []\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        balances = bankdata[CURRENT][bank][\'balance\']\n        sum_account = {}\n        for date, balance in zip(dates, balances):\n            sum_account[date] = balance\n                \n        sum_balances.append(sum_account)\n    \n    total = {}\n    (ini, fin) = getIntervalDates(bankdata)\n    last = 0\n    max_amount = 0\n    for b in sum_balances:\n        for d in daterange(ini, fin):\n            if d in b:\n                last = b[d]                    \n                if d in total:\n                    total[d] += b[d]\n                    if total[d] > max_amount:\n                        max_amount = total[d]\n                else:\n                    total[d] = b[d]\n            else:\n                if d in total:\n                    total[d] += last\n                else:\n                    total[d] = last\n                \n    \n    dates = total.keys()\n    balances = total.values()\n    \n    (dates, balances) = zip(*sorted(zip(dates, balances)))\n    \n    trace = go.Scatter(\n        x = dates,\n        y = balances,\n        name = "All Accounts - Amount: " + format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".") + CURRENCY,\n        mode = \'lines\',\n        line = dict ( width = 4 )\n    )\n\n    return (max_amount, trace)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- Error_CM\n\nUpdated code:\n```python\nimport pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv(\'data/exoplanet.eu_catalog.csv\', \n                      usecols=[\'mass\',\'mass_error_min\',\'mass_error_max\',\n                               \'semi_major_axis\',\'semi_major_axis_error_min\',\'semi_major_axis_error_max\',\'star_name\'])\n\nclass System:\n    def __init__(self, data):\n        self.data=data\n        self.system = list(self.data.groupby("star_name").groups.keys())\n        self.Number()\n        self.Mass()\n        self.CenterOfMass()\n        \n    def Number(self):\n        sys = self.data.groupby("star_name")\n        self.N_total = len(sys["mass"])\n        \n    def Mass(self):\n        sys = self.data.groupby("star_name")\n        self.M_total = sys["mass"].sum()\n    \n    def CenterOfMass(self):\n        self.rm_i = self.data["mass"].multiply(self.data["semi_major_axis"])\n        self.data_i = self.data.assign(CM_i = self.rm_i.values) \n        p_system = self.data_i.groupby("star_name")\n        sum_rm = p_system[\'CM_i\'].sum()\n        self.CM = sum_rm.divide(self.M_total)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nclass Point:\n    """\n    Simple class for representing a point in a Cartesian coordinate system.\n    """\n    \n    def __init__(self, x, y):\n        """\n        Create a new Point at x, y.\n        """\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        """\n        Translate the point by dx and dy in the x and y direction.\n        """\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return("Point at [%f, %f]" % (self.x, self.y))\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\nkaggle_export = pd.DataFrame({\n    'id': test['Id'],\n    'SalePrice': preds[model]\n},\ncolumns = ['id', 'SalePrice'])\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n    # Here make sure to preprocess your image_input in a way your network expects\n    # with size, normalization, ect if needed\n    # image_input =\n    # Note: x should be the same name as your network\'s tensorflow data placeholder variable\n    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15,15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap)) # displays the feature map number\n        if activation_min != -1 & activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min !=-1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n    l1 = LogisticRegression(penalty=\'l1\', tol=.01, \n                            solver="liblinear", multi_class="ovr",\n                            max_iter=500, C=c_param)\n    # Fit model\n    l1.fit(train_data, train_labels) \n    # Predict\n    predict = l1.predict(dev_data)\n    # Get precision, recall, f1 scores\n    scores = precision_recall_fscore_support(dev_labels, predict, average=\'weighted\')  \n    \n    # Get the features with non-zero coefficients.  We will use\n    # this list to reduce the features for the\n    # following logistic regression with L2 regularization\n    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n    names = np.array(list(train_data.columns))\n    non_zero_names = names[non_zero_sums] \n    \n    return {\'scores\': scores, \'non_zero_genes\': non_zero_names}\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n# voorbeeld data, laad in om werking te zien! Wel even de volgende paragraaf skippen...\n\nimport random\n\ndef make_document(doc):\n    source = ColumnDataSource({\'x\': [], \'y\': [], \'color\': []})\n\n    def update():\n        new = {\'x\': [random.random()],\n               \'y\': [random.random()],\n               \'color\': [random.choice([\'red\', \'blue\', \'green\'])]}\n        source.stream(new)\n\n    doc.add_periodic_callback(update, 100)\n\n    fig = figure(title=\'Streaming Circle Plot!\', sizing_mode=\'scale_width\',\n                 x_range=[0, 1], y_range=[0, 1])\n    fig.circle(source=source, x=\'x\', y=\'y\', color=\'color\', size=10)\n\n    doc.title = "Now with live updating!"\n    doc.add_root(fig)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nclass FunctionRunner(Runner):\n    def __init__(self, function):\n        """Initialize.  `function` is a function to be executed"""\n        self.function = function\n\n    def run_function(self, inp):\n        return self.function(inp)\n\n    def run(self, inp):\n        try:\n            result = self.run_function(inp)\n            outcome = self.PASS\n        except Exception:\n            result = None\n            outcome = self.FAIL\n\n        return result, outcome\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef replace_unknown(sequence):\n    """Return a copy of the input sequence where each unknown word is replaced\n    by the literal string value \'nan\'. Pomegranate will ignore these values\n    during computation.\n    """\n    return [w if w in data.training_set.vocab else \'nan\' for w in sequence]\n\ndef simplify_decoding(X, model):\n    """X should be a 1-D sequence of observations for the model to predict"""\n    _, state_path = model.viterbi(replace_unknown(X))\n    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef fit_lstm(train,batch_size,nb_epoch,neurons):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nclass Preprocessor:\n    \n    def __init__(self, train_data_file, train_label_file, train_ids_file,\n                 instr_file, test_data_file=None, test_ids_file=None):\n        """A class to process and reformat data\n        for use in learning models"""\n        \n        # initialize the data the data filenames\n        self.train_data_file = train_data_file\n        self.train_label_file = train_label_file\n        self.train_ids_file = train_ids_file\n        self.instr_file = instr_file\n        \n        # test data is optional\n        self.test_data_file = test_data_file\n        self.test_ids_file = test_ids_file\n        \n    def read_data(self):\n        """Reads in data from the files passed to constructor"""\n        \n        # read in the data\n        train_X_df = pd.read_csv(self.train_data_file)\n        train_y_df = pd.read_csv(self.train_label_file)\n        train_ids_df = pd.read_csv(self.train_ids_file)\n        self.instr_df = pd.read_csv(self.instr_file)\n        \n        self.feature_names = [feature for feature in train_X_df]\n        self.original_feature_names = [feature for feature in train_X_df]\n        self.label_names = [feature for feature in train_y_df]\n        self.id_names = [feature for feature in train_ids_df]\n        \n        # create cross validation data\n        self.cv_X_df = pd.DataFrame(train_X_df)\n        self.cv_y_df = pd.DataFrame(train_y_df)\n        self.cv_ids_df = pd.DataFrame(train_ids_df)\n        \n        # read in the test data if it exists\n        if self.test_data_file != None:\n            self.test_X_df = pd.read_csv(self.test_data_file)\n            self.test_ids_df = pd.read_csv(self.test_ids_file)\n            self.all_X_df = train_X_df.append(self.test_X_df)\n        else:\n            self.test_X_df = None\n            self.test_ids_df = None\n            self.all_X_df = pd.DataFrame(train_X_df)\n        \n        # determine the shape of the input data\n        self.train_X_shape = train_X_df.shape\n        self.train_y_shape = train_y_df.shape\n        self.train_ids_shape = train_ids_df.shape\n        self.instr_shape = self.instr_df.shape\n        self.all_shape = self.all_X_df.shape\n        \n        # get size of test data if it exists\n        if self.test_data_file != None:\n            self.test_X_shape = self.test_X_df.shape\n            self.test_ids_shape = self.test_ids_df.shape\n        else:\n            self.test_X_shape = None\n            self.test_ids_shape = None\n\n        \n    def process(self, shuffle_train_data=False):\n        """Performs the processing on cross validation and train/test data"""\n        \n        # ADD OPTION TO SHUFFLE DATA HERE\n        \n        # processing on all data - remember to include cv_X and all_X for each condition\n        for col in self.original_feature_names:\n            print(col)\n            \n            # determine what to perform at each of the steps\n            col_instr = self.instr_df[col].values\n            col_enc = col_instr[1]\n            col_scl = col_instr[2]\n            col_imp = col_instr[3]\n\n            # impute values\n            # imputed first so that other functions will not use nan values in calculations\n            if col_imp == \'UNIQ\':\n                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n            if col_imp == \'MEAN\':\n                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n                self.all_X_df[col] = MEAN(self.all_X_df[col])\n            if col_imp == \'MODE\':\n                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n                self.all_X_df[col] = MODE(self.all_X_df[col])\n            if col_imp == \'MED\':\n                self.cv_X_df[col] = MED(self.cv_X_df[col])\n                self.all_X_df[col] = MED(self.all_X_df[col])\n            if is_int(col_imp):\n                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n            if col_imp == \'DEL\':\n                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n            \n            \n            # perform encoding of data\n            if col_enc == \'MAP\':\n                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n                self.all_X_df[col] = MAP(self.all_X_df[col])\n            if col_enc == \'OHE\':\n                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n                    feature_names=self.feature_names)\n            if col_enc == \'LOO\':\n                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n                self.all_X_df[col] = LOO(self.all_X_df[col])\n            \n\n            # perform scaling\n            if col_scl == \'NRM1\':\n                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n                self.all_X_df[col] = NRM1(self.all_X_df[col])\n            if col_scl == \'SCL1\':\n                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n                self.all_X_df[col] = SCL1(self.all_X_df[col])\n            if col_scl == \'TRSH\':\n                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n                self.all_X_df[col] = TRSH(self.all_X_df[col])\n\n        \n        # get the values from the dataframes\n        self.cv_X = self.cv_X_df.values\n        self.cv_y = self.cv_y_df.values\n        self.cv_ids = self.cv_ids_df.values\n        \n        all_X = self.all_X_df.values\n        self.train_X = all_X[:self.train_X_shape[0], :]\n        self.train_y = self.cv_y_df.values\n        self.train_ids = self.cv_ids_df.values\n        \n        if self.test_data_file != None:\n            self.test_X = all_X[self.train_X_shape[0]:, :]\n            self.test_ids = self.test_ids_df.values\n        else:\n            self.test_X = None\n            self.test_ids = None\n        \n    def write_data(self, out_dir=\'./processed_data/\'):\n        """Writes all of the data to output files"""\n        \n        # create the output directory if it does not exist\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n            \n        # convert arrays back into DataFrames\n        cv_X_df = pd.DataFrame(self.cv_X,  columns=self.feature_names)\n        cv_y_df = pd.DataFrame(self.cv_y, columns=self.label_names)\n        cv_ids_df = pd.DataFrame(self.cv_ids, columns=self.id_names)\n        train_X_df = pd.DataFrame(self.train_X, columns=self.feature_names)\n        train_y_df = pd.DataFrame(self.train_y, columns=self.label_names)\n        train_ids_df = pd.DataFrame(self.train_ids, columns=self.id_names)\n        if self.test_data_file != None:\n            test_X_df = pd.DataFrame(self.test_X, columns=self.feature_names)\n            test_ids_df = pd.DataFrame(self.test_ids, columns=self.id_names)\n        \n        # write the dataframes to file\n        cv_X_df.to_csv(out_dir+\'cv_X.csv\', index=False)\n        cv_y_df.to_csv(out_dir+\'cv_y.csv\', index=False)\n        cv_ids_df.to_csv(out_dir+\'cv_ids.csv\', index=False)\n        train_X_df.to_csv(out_dir+\'train_X.csv\', index=False)\n        train_y_df.to_csv(out_dir+\'train_y.csv\', index=False)\n        train_ids_df.to_csv(out_dir+\'train_ids.csv\', index=False)\n        if self.test_data_file != None:\n            test_X_df.to_csv(out_dir+\'test_X.csv\', index=False)\n            test_ids_df.to_csv(out_dir+\'test_ids.csv\', index=False)\n        \n    def select_features(self):\n        """Perform features selection / compression algs like PCA."""\n        """These will be implemented once more has been done."""\n        self.feature_names = self.feature_names\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef vm(n):\n    ## define matrix\n    A = np.array([i**j for i in range(1,n+1) for j in range(0, n)] )\n    A.shape = (n, n)\n    ## determine the solution vector\n    x_target = np.ones(n)\n    b = A @ x_target\n    ## solve SLE\n    x = np.linalg.solve(A, b)\n    \n    return x, A\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- add_degrees_test\n\nUpdated code:\n```python\ndef add_degrees_isotypic(gen_deg, op_deg):\n    D = cartesian_product([ZZ for i in range(len(gen_deg[0]))])\n    return D(gen_deg[0])+D(op_deg), gen_deg[1]\n\ndef add_degrees_symmetric(gen_deg,op_deg):\n    D = cartesian_product([ZZ for i in range(len(gen_deg[0]))])\n    d = D(gen_deg[0])+D(op_deg)\n    return D(sorted(d, reverse=True)), gen_deg[1]\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n#METHOD PART\n\ndef get_df_cols(csvfilename,cols,separator):\n  """\n  Method to get a dataframe from a csv file with specified columns\n  \n  @csvfilename : the name of the file to convert in dataframe\n  @cols        : list of string giving columns name to keep\n  @separator   : character used to delimit fields in the csv file\n  \n  @return      : a dataframe\n  """\n  \n  dataframe = pd.read_csv(BOOKINGS, error_bad_lines=False, encoding=\'UTF8\', sep=separator, usecols=cols)\n  \n  return dataframe\n\n\ndef get_name(IATA_code):\n  """\n    Function to return the name of the airport linked to IATA_code\n    \n    @IATA_code : String object which is a IATA_code\n   \n    @return    : String object which is the name of the airport\n  """\n  \n  #If IATE CODE exists in GEO_O\n  try:\n    result = GEO_O.get(IATA_code.replace(" ",""), \'name\')\n  #Else we just specify that we cannot found the IATA CODE\n  except KeyError as e:\n    result = "NOT FOUND IATA CODE"\n   \n  return result\n  \n\ndef get_airports_arrival_sorted(dataframe):\n  """\n  Method to print the get arrivals airports in 2013 from searches file\n  \n  @dataframe : the dataframe containing the data\n  \n  @return    : a new dataframe\n  """\n  \n  #Created dataframe grouped by \'arr_port\' aggregated by sum\n  result_dataframe = dataframe.groupby([\'arr_port\']).sum()\n  #Sorted the result in a descending way\n  result_dataframe = result_dataframe.sort_values(by=[\'pax\'], ascending=False)\n  \n  return result_dataframe\n\n\ndef add_airports_name(dataframe):\n  """\n  Method to add a column in a dataframe containing the full name of airports\n  thanks to the IATA CODE\n  \n  @dataframe : the dataframe to modify\n  \n  @return    : the dataframe modified\n  """\n  \n  #Reset the index of the dataframe in order to apply a lambda method\n  dataframe = dataframe.reset_index()\n  \n  #Add the column and its values\n  dataframe[\'airport_name\'] = dataframe[\'arr_port\'].apply(lambda x: get_name(x))\n\n  return dataframe\n\ndef print_top_n_arrival_airport(dataframe,n):\n  """\n  Method to print the top n of arrival airports in 2013\n  \n  @dataframe : the preformatted dataframe by columns containing the data\n  @n         : the number of airports to show\n  """\n  \n  df = get_airports_arrival_sorted(dataframe)\n  df = add_airports_name(df)\n  \n  print(df.head(n))\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef return_itself(arg):\n    return arg\n```'}, {'reason': 'stop', 'result': 'Unused functions:\nNone\n\nUpdated code:\n```python\ndef evaluate(X_data, y_data, b, lr):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        summary, accuracy = sess.run([summary_op,accuracy_operation], feed_dict={tf_train_dataset : batch_x, tf_train_labels : batch_y, \n                         tf_beta: b, tf_keep_prob : 1, tf_learning_rate : lr})\n        total_accuracy += (accuracy * len(batch_x))\n    return summary, total_accuracy / num_examples\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef featurize_state(state):\n    """\n    Returns the featurized representation for a state.\n    """\n    scaled = scaler.transform([state])\n    featurized = featurizer.transform(scaled)\n    return featurized[0]\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef logprob(predictions, labels):\n  """Log-probability of the true labels in a predicted batch."""\n  predictions[predictions < 1e-10] = 1e-10\n  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\ndef sample_distribution(distribution):\n  """Sample one element from a distribution assumed to be an array of normalized\n  probabilities.\n  """\n  r = random.uniform(0, 1)\n  s = 0\n  for i in range(len(distribution)):\n    s += distribution[i]\n    if s >= r:\n      return i\n  return len(distribution) - 1\n\ndef sample(prediction):\n  """Turn a (column) prediction into 1-hot encoded samples."""\n  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n  p[0, sample_distribution(prediction[0])] = 1.0\n  return p\n\ndef random_distribution():\n  """Generate a random column of probabilities."""\n  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n  return b/np.sum(b, 1)[:,None]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef is_valid_url(url):\n    try:\n        result = http_program(url)\n        return True\n    except ValueError:\n        return False\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef build_index(non_empty_days, from_time, to_time):\n    date_ranges = []\n    for date in non_empty_days:\n        yyyy, mm, dd = date.split(\'-\')\n        from_hour, from_min = from_time.split(\':\')\n        to_hour, to_min = to_time.split(\':\')    \n        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n        date_ranges.append(pd.DataFrame({"OrganizedDateTime": pd.date_range(t1, t2, freq=\'1Min\').values}))\n    agg = pd.concat(date_ranges, axis=0) \n    agg.index = agg["OrganizedDateTime"]\n    return agg\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- error\n\nUpdated code:\n```python\n# No changes made as the function is unused\ndef error(x_values, y_values, m, b, x):\n    pass\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n# Configure model presets\nfrom examples.seismic import demo_model\n\n# Enable model presets here:\npreset = 'layers-isotropic'  # A simple but cheap model (recommended)\n# preset = 'marmousi2d-isotropic'  # A larger more realistic model\n\n# Standard preset with a simple two-layer model\nif preset == 'layers-isotropic':\n    filter_sigma = (1, 1)\n    nshots = 21\n    nreceivers = 101\n    t0 = 0.\n    tn = 1000.  # Simulation last 1 second (1000 ms)\n    f0 = 0.010  # Source peak frequency is 10Hz (0.010 kHz)\n\n\n# A more computationally demanding preset based on the 2D Marmousi model\nif preset == 'marmousi2d-isotropic':\n    filter_sigma = (6, 6)\n    nshots = 301  # Need good covergae in shots, one every two grid points\n    nreceivers = 601  # One recevier every grid point\n    t0 = 0.\n    tn = 3500.  # Simulation last 3.5 second (3500 ms)\n    f0 = 0.025  # Source peak frequency is 25Hz (0.025 kHz)\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n# 2 1d ODEs\ndef ode(state,t):\n    # x and y are first two components of state vector\n    x = state[0]\n\n    # Compute state derivatives.  Mess around here! \n    dx = .4* np.square(x) - 2\n\n    # Return the state derivatives\n    return [dx]\n```'}, {'reason': 'stop', 'result': 'Unused functions:\nNone\n\nUpdated code:\n```python\ndef window_bounds(window, affine):\n    (row_start, row_stop), (col_start, col_stop) = window\n    w, s = (col_start, row_stop) * affine\n    e, n = (col_stop, row_start) * affine\n    return w, s, e, n\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- calc_other_neighbour_msg_prod\n\nUpdated code:\n```python\n# No changes made to the code\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef neuron(x, w):\n    '''Simple McCulloch-Pitts Neuron model, x and w and are numpy arrays of the same shape'''\n    return np.dot(x, w) > 0\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef get_wide_deep():\n    # define column types\n    is_male,mother_age,plurality,gestation_weeks = \\\n        [\\\n            tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n                        ['True', 'False', 'Unknown']),\n            tf.feature_column.numeric_column('mother_age'),\n            tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n                        ['Single(1)', 'Twins(2)', 'Triplets(3)',\n                         'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),\n            tf.feature_column.numeric_column('gestation_weeks')\n        ]\n\n    # discretize\n    age_buckets = tf.feature_column.bucketized_column(mother_age, \n                        boundaries=np.arange(15,45,1).tolist())\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n                        boundaries=np.arange(17,47,1).tolist())\n      \n    # sparse columns are wide \n    wide = [is_male,\n            plurality,\n            age_buckets,\n            gestation_buckets]\n    \n    # feature cross all the wide columns and embed into a lower dimension\n    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n    embed = tf.feature_column.embedding_column(crossed, 3)\n    \n    # continuous columns are deep\n    deep = [mother_age,\n            gestation_weeks,\n            embed]\n    return wide, deep\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# Define a function to replace nulls for many columns: \ndef fill_nulls(df, col_list, na_val):\n    for col in col_list:\n        df[col].fillna(value = na_val, inplace = True)\n    return df\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nimport math\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    (assuming your grayscaled image is called \'gray\')\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    # Or use BGR2GRAY if you read an image with cv2.imread()\n    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\n# Python 3 has support for cool math symbols.\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n    \n    return fig, axes\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass EuropeanOption(object):\n    """ Abstract Class for European options. Partially implemented.\n    S0 : float : initial stock/index level\n    strike : float : strike price\n    T : float : time to maturity (in year fractions)\n    r : float : constant risk-free short rate\n    div :    float : dividend yield\n    sigma :  float : volatility factor in diffusion term\n    model: str: name of the model for the pricing"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, option_type, S0, strike, T, r, div, sigma, model):\n        try:\n            self.option_type = option_type\n            assert isinstance(option_type, str)\n            self.S0 = float(S0)\n            self.strike = float(strike)\n            self.T = float(T)\n            self.r = float(r)\n            self.div = float(div)\n            self.sigma = float(sigma)\n            self.model = str(model)\n        except ValueError:\n            print(\'Error passing Options parameters\')\n\n        models = [\'BlackScholes\', \'MonteCarlo\', \n                  \'BinomialTree\', \'TrinomialTree\',\n                  \'FFT\', \'PDE\']\n        \n        if model not in models:\n            raise Exception(\'Error: Model unknown\')\n            \n        option_types = [\'call\', \'put\']\n        \n        if option_type not in option_types:\n            raise ValueError("Error: Option type not valid. Enter \'call\' or \'put\'")\n        if S0 < 0 or strike < 0 or T <= 0 or r < 0 or div < 0 or sigma < 0:\n            raise ValueError(\'Error: Negative inputs not allowed\')\n            \n        self.discount = np.exp(-self.r * self.T)\n\n    def getmodel(self):\n        return self.model\n\n    def __str__(self):\n        return "This European Option is priced using {0}".format(self.getmodel())\n\n    @abstractmethod\n    def value(self):\n        pass\n    \n    @abstractmethod\n    def delta(self):\n        pass\n        \n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef make_input_fn(df, num_epochs):\n  return tf.estimator.inputs.pandas_input_fn(\n    x = df,\n    y = df[LABEL],\n    batch_size = 128,\n    num_epochs = num_epochs,\n    shuffle = True,\n    queue_capacity = 1000,\n    num_threads = 1\n  )\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef insert_and_time_filter_cuckoo_filter(capacity, percent_fill=0.9):\n    num_inserted = 0\n    c_filter = CuckooFilter(capacity, 2)\n    now = time.time()\n    for i in range(int(percent_fill*capacity)):\n        try:\n            c_filter.insert(str(i))\n            num_inserted += 1\n        except Exception("CuckooFilter has filled up!"):\n            break\n    elapsed_insertion = time.time() - now\n    return c_filter, elapsed_insertion, num_inserted\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- `setSize`\n- `fit`\n- `selectNodes`\n- `clearSelection`\n- `getResponse`\n\nUpdated code:\n```python\nimport ipywidgets as widgets\nimport json\nimport time\nfrom threading import Thread\nfrom traitlets import Int, Unicode, Tuple, CInt, Dict, validate, observe\n\n\nclass newcyjsWidget(widgets.DOMWidget):\n    \n    _view_name = Unicode(\'CyjsView\').tag(sync=True)\n    _view_module = Unicode(\'cyjs\').tag(sync=True)\n    frameWidth = Int(400).tag(sync=True)\n    frameHeight = Int(300).tag(sync=True)\n    msgFromKernel = Unicode("{}").tag(sync=True)\n    msgToKernel = Unicode("{}").tag(sync=True)\n    status = "initial status message\\n"\n    selectedNodes = [];\n    incomingMessageArrivedAndParsed = False;\n    globalStatus = "blank"\n\n    #------------------------------------------------------------------------------\n    class MyThread(Thread):\n       owner = None\n       def __init__(self, owner, group=None, target=None, name=None,\n                    args=(), kwargs=None, *, daemon=None):\n          Thread.__init__(self, group, target, name, args, kwargs, daemon=daemon)\n          self.owner = owner\n\n       def run(self):\n          maxLoops = 5\n          counter = 0\n          while((self.owner.incomingMessageArrivedAndParsed == False) and (counter < maxLoops)):\n             counter += 1\n             print("thread, no message yet, sleeping, loop %d" % counter)\n             time.sleep(1.0)\n          self.owner.status += "thread owner\'s selectedNodes: %s\\n" % self.owner.selectedNodes;\n          self.owner.status += "MyThread ending loop\\n";\n\n       def result(self):\n          #while(not self.owner.incomingMessageArrivedAndParsed):\n          #   time.sleep(0.1)\n          return("imaap? %s" % self.owner.incomingMessageArrivedAndParsed)\n            \n    #------------------------------------------------------------------------------\n\n    def testThread(self):\n      for i in range(4):\n         threadName = "Thread-%s" % (i + 1)\n         mythread = self.MyThread(name=threadName, owner=self)\n         mythread.start()\n    \n    def getSelectedNodes(self):\n      #self.selectedNodes = [];\n      self.incomingMessageArrivedAndParsed = False;\n      self.status += "entering getSelectedNodes\\n";\n      self.msgFromKernel = json.dumps({"cmd": "cleanSlate", "status": "request", "callback": "", "payload":""});\n      self.msgFromKernel = json.dumps({"cmd": "getSelectedNodes", "status": "request",\n                                       "callback": "", "payload": ""});\n      \n      observingThread = self.MyThread(name="getSelectedNodes-thread", owner=self)\n      print("getSelectedNodes about to start observingThread")\n      #observingThread.start()\n      #observingThread.join()\n      #while(self.incomingMessageArrivedAndParsed == False):\n      #   time.sleep(0.5)\n      observingThread.start()\n      self.status += "getSelectedNodes, observingThread now started: %s\\n" %  self.selectedNodes\n      self.status += "getSelectedNodes, incomingMessageArrivedAndParsed? %s\\n" % self.incomingMessageArrivedAndParsed\n      return(observingThread.result())\n        #return(self.status)\n        \n    @observe(\'msgToKernel\')\n    def msg_arrived(self, change):\n        self.status += "---- python - msg arrived\\n"\n        tmp = change[\'new\']\n        self.status += "len of tmp: %d\\n" % len(tmp)\n        self.status += "type of tmp: %s\\n" % type(tmp)\n        self.msgToKernel = tmp\n        self.status += "%s\\n" % tmp\n        self.incomingMessageArrived = True\n        self.dispatch(self.msgToKernel)\n \n    def dispatch(self, msgRaw):\n        self.msg = json.loads(msgRaw)\n        self.status += "entering dispatch\\n"\n        self.status += "dispatch this msg: %s\\n" % self.msg\n        self.status += "msg.cmd: %s\\n" % self.msg["cmd"]\n        if self.msg["cmd"] == \'storeSelectedNodes\':\n            self.status += "storing selected nodes to self.selectedNodes %s\\n" % msg["payload"]\n            self.selectedNodes = msg["payload"]\n        elif self.msg["cmd"] == \'clearCircles\':\n            self.circles = []\n        else:\n          print("unknown cmd: %s" % self.msg["cmd"])\n        self.incomingMessageArrivedAndParsed = True\n    \n\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- term_output\n\nUpdated code:\n```python\n# No changes made to the code\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef getWiki(wikiPages, path = "/home/aalto/PycharmProjects/digitalepidemiology/data/"):\n    \'\'\'\n    \n    :param wikiPages: list of the wikipages that we want to analyze\n    :param path: location of the downloaded wikipedia pages\n    :return: \n    \'\'\'\n    df = pd.DataFrame()\n    for wikiPage in wikiPages:\n        wiki2 = pd.read_csv(path+wikiPage+".csv", usecols=[0,1], parse_dates=[0], index_col=[0], header=None)\n        wiki2 = wiki2.resample("W-Sun").sum()\n        df[wikipage] = wiki2\n    return df\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\n\ndef ResNet50_predict_labels(img_path):\n    # returns prediction vector for image located at img_path\n    img = preprocess_input(path_to_tensor(img_path))\n    return np.argmax(ResNet50_model.predict(img))\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- getBestParams\n\nUpdated code:\n```python\n#\n# Logistic Regression\n#\nlr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\nparams = {'C': [0.1, 0.25,  0.5,]}\nlogit = GridSearchCV(lr, params, cv=5,\n                     scoring='accuracy', return_train_score=True)\n\n# Fit  training data\nlogit.fit(train_data, train_labels)  \n# Show the best C parameter to use and the expected accuracy\nprint('\\nLogistic Regression Classifier, L2 regularization')\nprint(' Best param:', logit.best_params_)\nprint(' Accuracy:  ', np.round(logit.best_score_, 4) )\n\nreturn logit.best_params_\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef get_mjdmax_BessellV(sn):\n    v = sn.lcfit.spline["BessellV"]    \n    mjd_spline = np.arange(np.nanmin(sn.phot.data["BessellV"]["MJD"]), \n                 np.nanmax(sn.phot.data["BessellV"]["MJD"]),\n                 0.001)\n    w = np.where(v(mjd_spline) == np.nanmax(v(mjd_spline)))\n\n    mjdmax = mjd_spline[w]\n    \n    return mjdmax\n```'}, {'reason': 'stop', 'result': 'Unused functions:\nNone\n\nUpdated code:\n```python\ndef window_bounds(window, affine):\n    (row_start, row_stop), (col_start, col_stop) = window\n    w, s = (col_start, row_stop) * affine\n    e, n = (col_stop, row_start) * affine\n    return w, s, e, n\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef forecast(model, batch_size,row):\n    X=row[0:-1]\n    X=X.reshape(1,1,len(x))\n    yhat=model.predict(X,batch_size=batch_size)\n    return yhat[0,0]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# No changes made, as the function is being used\ndef func1(s):\n    """\n    Print a string \'s\' and tell how many characters it has    \n    """\n    \n    print(s + " has " + str(len(s)) + " characters")\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n'''Function for creating a nuclide container'''\n\ndef get_nuclides( df ):\n    '''\n    Create a dictionary of nuclide name key and named tuple value\n    Parameters\n    ----------\n    df: pandas data frame, required\n        Table of data for nuclides.\n    \n    Returns\n    -------\n    nuclides: dict(name:namedtuple)\n        Dictionary of name key and namedtuple values. Tuple names: element_name, symbol, Z, A, radius, unc.\n\n    Examples\n    --------\n    '''\n    \n    nuclides = dict()\n\n    # design a container data structure\n    from collections import namedtuple\n    Nuclide = namedtuple('Nuclide', ['element_name','symbol','Z','A','half_life'])\n\n    # fill in the list of containers\n    misses = 0 # counter of nuclides without radius data\n    a_max = 0  # maximum A number with radius data present\n    z_max = 0  # maximum Z number with radius data present\n    t_max = 0\n\n    import pandas as pd\n    \n    # Use the Mendeleev python package (periodic table of elements)\n    from mendeleev import element\n    \n    # if df has duplicates, this loop will take care of it\n    for row in df.itertuples(index=False):\n\n        a = int(row[0])\n        symbol = row[1]\n        z = int(row[2])\n        t = row[4]\n        if pd.isnull(t): # missing half-life\n            misses += 1\n            continue\n        if t/3600/24/365 > 50e+6: # remove anything with more than 50 My\n            continue\n\n        a_max = max(a,a_max)\n        z_max = max(z,z_max)\n        \n        name = symbol+'-'+str(a)\n        t_max = max(t,t_max)\n        nuc = Nuclide( element_name=element(z).name, symbol=symbol, Z=z, A=a, half_life=t )\n        \n        if name in nuclides.keys():\n            half_life = nuclides[name].half_life\n            if half_life == t:\n                continue\n            else:\n                name = name+'m'\n        \n        nuclides[name]=nuc\n\n    print('Number of nuclides with    t_1/2 data = ',len(nuclides))\n    print('Number of nuclides without t_1/2 data = ',misses)\n    print('')\n    print('Max Z number with t_1/2 data = ',z_max)\n    print('Max A number with t_1/2 data = ',a_max)\n    print('t_1/2 [Ma] max = ',t_max/3600/24/365/1000000)\n    \n    return nuclides\n```"}, {'reason': 'stop', 'result': 'Unused functions:\nNone\n\nUpdated code:\n```python\ndef get_model_params():\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n\ndef restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + "/Assign")\n                  for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef accuracy(X, Y, model):\n    """Calculate the prediction accuracy by using the model to decode each sequence\n    in the input X and comparing the prediction with the true labels in Y.\n    \n    The X should be an array whose first dimension is the number of sentences to test,\n    and each element of the array should be an iterable of the words in the sequence.\n    The arrays X and Y should have the exact same shape.\n    \n    X = [("See", "Spot", "run"), ("Run", "Spot", "run", "fast"), ...]\n    Y = [(), (), ...]\n    """\n    correct = total_predictions = 0\n    for observations, actual_tags in zip(X, Y):\n        \n        # The model.viterbi call in simplify_decoding will return None if the HMM\n        # raises an error (for example, if a test sentence contains a word that\n        # is out of vocabulary for the training set). Any exception counts the\n        # full sentence as an error (which makes this a conservative estimate).\n        try:\n            most_likely_tags = simplify_decoding(observations, model)\n            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n        except:\n            pass\n        total_predictions += len(observations)\n    return correct / total_predictions\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\nCSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\nLABEL_COLUMN = 'weight_pounds'\nKEY_COLUMN = 'key'\nDEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\nTRAIN_STEPS = 1000\n\ndef read_dataset(prefix, pattern, batch_size=512):\n    # use prefix to create filename\n    filename = 'gs://{}/babyweight/preproc/{}*{}*'.format(BUCKET, prefix, pattern)\n    if prefix == 'train':\n        mode = tf.estimator.ModeKeys.TRAIN\n        num_epochs = None # indefinitely\n    else:\n        mode = tf.estimator.ModeKeys.EVAL\n        num_epochs = 1 # end-of-input after this\n    \n    # the actual input function passed to TensorFlow\n    def _input_fn():\n        # could be a path to one file or a file pattern.\n        input_file_names = tf.train.match_filenames_once(filename)\n        filename_queue = tf.train.string_input_producer(\n            input_file_names, shuffle=True, num_epochs=num_epochs)\n \n        # read CSV\n        reader = tf.TextLineReader()\n        _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n        if mode == tf.estimator.ModeKeys.TRAIN:\n          value = tf.train.shuffle_batch([value], batch_size, capacity=10*batch_size, \n                                         min_after_dequeue=batch_size, enqueue_many=True, \n                                         allow_smaller_final_batch=False)\n        value_column = tf.expand_dims(value, -1)\n        columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n        features = dict(zip(CSV_COLUMNS, columns))\n        features.pop(KEY_COLUMN)\n        label = features.pop(LABEL_COLUMN)\n        return features, label\n  \n    return _input_fn\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\ndef hypothesis_inlinecounter(text):\n    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split('\\n')])[:-1]\n    return hyp\n\ndef hypothesis_capswords(text):\n    hyp = np.concatenate([np.full(len(x)+1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x)+1, -1) for x in text.split('\\n')])[:-1]\n    return hyp\n\ndef hypothesis_pos(text, pos_tag):\n    hyp = text.replace('1', '0')\n    for word, tag in pynlpir.segment(text):\n        if tag == pos_tag:\n            hyp = hyp.replace(word, '1'*len(word), 1)\n        else:\n            hyp = hyp.replace(word, '0'*len(word), 1)\n    hyp = [1 if x=='1' else -1 for x in re.sub('[^1]', '0', hyp)]\n    \n    return hyp\n\ndef hypothesis_verbs(text):\n    return hypothesis_pos(text, 'verb')\n\ndef hypothesis_nouns(text):\n    return hypothesis_pos(text, 'noun')\n```"}, {'reason': 'stop', 'result': "Unused functions: ['build_poly']\n\nUpdated code:\n```python\n# updated code here\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass Hmm(object):\n    """\n    Stores counts for n-grams and emissions. \n    """\n\n    def __init__(self, n=3):\n        assert n>=2, "Expecting n>=2."\n        self.n = n\n        self.emission_counts = defaultdict(int)\n        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n        self.all_states = set()\n        self.count_y = dict() \n        #[(\'O\', 0), (\'I-MISC\', 0), (\'I-PER\', 0), (\'I-ORG\', 0), (\'I-LOC\', 0), (\'B-MISC\', 0), (\'B-PER\', 0), (\'B-ORG\', 0), (\'B-LOC\', 0)])\n        self.count_xy = dict()\n        self.trigram_counts = dict()\n        self.bigram_counts = dict()\n        \n\n    def train(self):\n        """\n        Count n-gram frequencies and emission probabilities from a corpus file.\n        """\n        ngram_iterator = \\\n            get_ngrams(sentence_iterator(simple_conll_corpus_iterator()), self.n)\n\n        for ngram in ngram_iterator:\n            #Sanity check: n-gram we get from the corpus stream needs to have the right length\n            assert len(ngram) == self.n, "ngram in stream is %i, expected %i" % (len(ngram, self.n))\n\n            tagsonly = tuple([ne_tag for word, ne_tag in ngram]) #retrieve only the tags            \n            for i in xrange(2, self.n+1): #Count NE-tag 2-grams..n-grams\n                self.ngram_counts[i-1][tagsonly[-i:]] += 1\n            \n            if ngram[-1][0] is not None: # If this is not the last word in a sentence\n                self.ngram_counts[0][tagsonly[-1:]] += 1 # count 1-gram\n                self.emission_counts[ngram[-1]] += 1 # and emission frequencies\n\n            # Need to count a single n-1-gram of sentence start symbols per sentence\n            if ngram[-2][0] is None: # this is the first n-gram in a sentence\n                self.ngram_counts[self.n - 2][tuple((self.n - 1) * ["*"])] += 1\n    \n    def calc_count_xy_y(self):\n        \n        for word, ne_tag in self.emission_counts:\n            count = self.emission_counts[(word,ne_tag)]\n            label = ne_tag\n            if label in self.count_y:\n                self.count_y[label] = self.count_y[label]+int(float(count))\n            else:\n                self.count_y.update({label:count})\n\n            if word in self.count_xy:\n                self.count_xy[word].update({label : count})\n            else:\n                self.count_xy[word] = {label : count}\n\n    def calc_transition_count(self,printngrams=[1,2,3]):\n        \n        for n in printngrams:            \n                for ngram in self.ngram_counts[n-1]:\n                    count = self.ngram_counts[n-1][ngram]\n                    ngramstr = " ".join(ngram)\n                    if (n == 2):\n                        self.bigram_counts[ngramstr] = count\n                    elif (n == 3):\n                        self.trigram_counts[ngramstr] = count\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef coords_of_max(theArray, n):\n    # Flatten the 2D array\n    flat = theArray.flatten()\n    # Partition so that the we know the sort order for\n    # the cells with the highest values.  We just\n    # care about the top n highest values.  So for example,\n    # if n = 3, get return 3 indices.  \n    indices = np.argpartition(flat, -n)[-n:]\n    # Reverse so that we show index of highest value first\n    # (descending)\n    indices = indices[np.argsort(-flat[indices])]\n    # Now return the coordinates for these indices\n    # for a 2D array.  This will return 2 arrays,\n    # the first for the row index, the second for the\n    # column index.  The row index represents the\n    # actual digit, the column index represents\n    # the confused digit\n    return np.unravel_index(indices, theArray.shape)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nfrom extract_bottleneck_features import *\n\ndef VGG16_predict_breed(img_path):\n    # extract bottleneck features\n    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n    # obtain predicted vector\n    predicted_vector = VGG16_model.predict(bottleneck_feature)\n    # return dog breed that is predicted by the model\n    return dog_names[np.argmax(predicted_vector)]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n    # Here make sure to preprocess your image_input in a way your network expects\n    # with size, normalization, ect if needed\n    # image_input =\n    # Note: x should be the same name as your network\'s tensorflow data placeholder variable\n    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15,15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap)) # displays the feature map number\n        if activation_min != -1 & activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min !=-1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n#ignore\ndef plot_2d(value, ax=None, group=None, mask=None, matrix_id=0, mat_as_group=False, \n            group_id=None, linewidths=0, is_string=False, fmt="d", square=False):\n  \n  if hasattr(value, "numpy"):\n    value = value.numpy()\n  if group is not None and hasattr(group, "numpy"):\n    group = group.numpy()\n  if mask is not None and hasattr(mask, "numpy"):\n    mask = tf.squeeze(mask)\n    mask = tf.ones_like(value) * mask\n    mask = mask.numpy()\n    \n\n  cmaps = [\'PuOr\', \'tab20b\', \'RdBu\']\n  group_id = int(group[0][0])\n  cmap = cmaps[group_id]\n  \n  if is_string:\n    fmt = \'\'\n  \n  sns.heatmap(group, \n              fmt=fmt,\n              cmap=cmap,\n              annot=value, \n              cbar=False, \n              xticklabels=False, \n              yticklabels=False, \n              square=square,\n              mask=mask,\n              linewidths=linewidths,\n              ax=ax)\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef buildProfit(bankdata):\n    data = []\n\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        movements = bankdata[CURRENT][bank][\'movements\']\n\n        profit = {}\n        for date, movement in zip(dates, movements):\n            key = str(date.month) + \'/\' + str(date.year)\n\n            if key in profit:\n                profit[key] += float(movement)\n            else:\n                profit[key] = float(movement)\n            \n        months = []\n        profits = []\n        for key, value in profit.items():\n            months.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            profits.append(value)\n        \n        trace = go.Bar(\n            x = months,\n            y = profits,\n            name = "Profit for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n    return data\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\n#ignore\ndef plot(value, group=None, group_dim=0, mask=None, labels=[], square=False, shape_desc=\'\', width_prec=None, bottom_prec=None, \n         is_string=False, name=\'\', fmt="d", single_plot_size=4, h_dist_ratio=0.7, w_dist_ratio=0.7, linewidths=None):\n\n  shape = value.shape\n  num_groups = shape[0]\n  \n  if hasattr(value, \'numpy\'):\n    if value.dtype in [tf.int32, tf.int64]:\n      value = tf.cast(value, tf.int32)\n    elif value.dtype in [tf.float32, tf.float64] and fmt == "d":\n      fmt = ".2g"\n    \n    value = value.numpy()\n\n  if hasattr(value, \'ndim\'):\n    ndim = value.ndim\n  else:\n    ndim = len(value)\n    \n    \n  if ndim == 2:\n    value = value[np.newaxis, np.newaxis, :, :]\n  if ndim == 3:\n    value = value[np.newaxis, :, :, :]\n  if ndim == 4:\n    pass\n  \n  # decide how to group sub-tensors smartly\n  if not group_dim:\n    group_dim = ndim - 1\n    \n  # generate group identifier tensor for differentiating between\n  # different bactch / sentence\n  if group is None:\n    group_ids = tf.range(num_groups, dtype=tf.int64).numpy()\n    if group_dim == 1:\n      group = group_ids[:, tf.newaxis]\n    elif group_dim == 2:\n      group = group_ids[:, tf.newaxis, tf.newaxis]\n    elif group_dim == 3:\n      group = group_ids[:, tf.newaxis, tf.newaxis, tf.newaxis]\n\n    # broadcast to all groups    \n    group = tf.ones(shape=value.shape) * group\n\n  d0, d1, d2, d3 = value.shape\n\n  # set figure size based on tensor dimensions\n  fig_width = (d3 * 1.0 / 4) * d0 * single_plot_size\n  fig_height = (d2 * 1.0 / 4) * single_plot_size\n  figsize = (fig_width, fig_height)\n  \n  if width_prec is None:\n    width_prec = 1.0 / d0\n  \n  if bottom_prec is None:\n    bottom_prec = 1.0 / d1\n\n  fig = plt.figure(figsize=figsize)\n  fig_title = f\'name: {name}, shape: {shape}\' if name else f\'shape: {shape}\'\n  \n  if shape_desc:\n    fig_title = fig_title + \' = \' + shape_desc\n    \n  for e0 in range(d0):\n\n    # plot 2d array in reverse order since the earlier plot will be covered\n    for e1 in reversed(range(d1)):\n      annot = value[e0, e1]\n\n      # select corresponding matplotlib axis      \n      cur_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3) * w_dist_ratio, \n                             e1 / d2 * h_dist_ratio, \n                             width_prec, \n                             bottom_prec]) \n\n      matrix_id = e0 + e1 * 2\n      \n      if mask is not None:\n        if ndim == 2:\n          mask_idx = e0\n        elif ndim == 3:\n          mask_idx = e1\n        elif ndim ==4:\n          mask_idx = e0\n          \n        # mimic broadcasting\n        if mask.shape[0] == 1:\n          mask_idx = 0\n          \n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, \n                is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n      else:\n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, is_string=is_string, fmt=fmt, square=square)\n      \n      # minic shadowing for each 2d matrix\n      width_delta_prec = 0.0005\n      height_delta_prec = width_delta_prec * d2 / d3\n      \n      for k in range(1, 3):\n        shadow_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3)  * w_dist_ratio - width_delta_prec * k, \n                                  e1 / d2 * h_dist_ratio - height_delta_prec * k, \n                                  width_prec, \n                                  bottom_prec])  \n        \n        if k == 2:\n          linewidths = 1\n        else:\n          linewidths = 0\n          \n        if mask is not None:\n          if ndim == 2:\n            mask_idx = e0\n          elif ndim == 3:\n            mask_idx = e1\n          elif ndim ==4:\n            mask_idx = e0\n            \n          # mimic broadcasting\n          if mask.shape[0] == 1:\n            mask_idx = 0  \n            \n            \n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n        else:\n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, square=square)\n\n      if e0 == 0 and e1 == 0:\n        ax1 = cur_ax\n        \n        if labels:\n            plt.ylabel(labels[-2])\n            plt.xlabel(labels[-1] + \'\\n\' + fig_title)\n        else:\n          plt.xlabel(fig_title)\n\n        # 4D 中的 axis1 說明 label\n#           if len(labels) >= 3:\n#             plt.text(d3 + 2, 1 + 0.5, labels[-3],\n#                      rotation=0, rotation_mode=\'anchor\')\n\n      if e1 == d0 - 1:\n        ax2 = cur_ax\n        \n\n#       transFigure = fig.transFigure.inverted()\n#       coord1 = transFigure.transform(ax1.transData.transform([d3 + 2 + 0.5, 0]))\n#       coord2 = transFigure.transform(ax2.transData.transform([d3 + 0.5, d2]))\n\n\n#       line = mpl.lines.Line2D((coord1[0],coord2[0]),(coord1[1],coord2[1]), \n#                               transform=fig.transFigure, \n#                               linestyle=\'--\',\n#                               color=\'black\')\n#       fig.lines.append(line)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef calc_sum_product_variable_to_factor_msg(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    if len(neighbour_msg_prod) > 0:\n        return np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        return np.ones(variable.num_states)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef sample_withplot (loglike_model, prior_transform_model, datafile,priorRange):\n    data_file = io.get_data_file_path(datafile)\n    data_x,data_xerr,data_y,data_yerr = io.load_data(data_file)\n    \n    #n: number of parameters, len(priorRange)\n    n=len(priorRange) \n\n    def new_loglike_model(theta):\n        return loglike_model(theta, (data_x,data_xerr,data_y,data_yerr))\n        \n    def new_prior_transform_model(theta):\n        return prior_transform_model(theta,priorRange)\n    \n    result = nestle.sample(new_loglike_model, new_prior_transform_model, n)\n    \n    \n    print (\'log evidence\')\n    print (result.logz)\n\n    print (\'numerical (sampling) error on logz\')\n    print (result.logzerr)   \n       \n    print (\'array of sample parameters\')\n    print (result.samples)  \n       \n    print (\'array of weights associated with each sample\')\n    print (result.weights)\n    \n\n    \n    import matplotlib.pyplot as plt\n    import corner\n\n\n    p_fit, cov_fit = nestle.mean_and_cov(result.samples, result.weights)\n\n\n    plt.figure()\n    plt.errorbar(data_x,data_y,yerr=data_yerr,fmt=\'*\')\n    plt.xlabel("r (kpc)")\n    plt.ylabel(\'V (km/s)\')\n    plt.title("Results of using the model to fit the DM rotational velocity distribution")\n    xplot = [5+5*i for i in range(40)]\n    yplot = [model.model_NFW(xplot[i], p_fit) for i in range(40)]\n    plt.plot(xplot,yplot)\n    plt.show()\n\n    fig = corner.corner(result.samples, weights=result.weights, labels=[\'a\', \'rho0\'],\n                            range=[0.99999, 0.99999], bins=30)\n    plt.show()\n\n    \n    return result \n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef delete_from_bloom_filter_and_time(capacity, percent_to_fill=0.9, percent_delete=1.0):\n    b_filter, _ = insert_and_time_filter_bloom_filter(capacity, percent_fill=percent_to_fill)\n    \n    total_size = b_filter.get_size()\n    number_of_items_to_delete = total_size*percent_delete\n    \n    now = time.time()\n    for i in range(int(number_of_items_to_delete)):\n        b_filter.remove(str(i))\n    elapsed_time = time.time() - now\n        \n    return b_filter, elapsed_time\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef read_s3_csv (dates):\n    s3 = boto3.resource(\'s3\')\n    deutsche_boerse_bucket = \'deutsche-boerse-xetra-pds\'\n    \n    bucket = s3.Bucket(deutsche_boerse_bucket)\n    \n    dataframes = []\n    \n    for date in dates:\n        objs_count = 0\n        csv_objects = bucket.objects.filter(Prefix=date)\n        for csv_obj in csv_objects:\n            csv_key = csv_obj.key\n            if csv_key[-4:] == \'.csv\':\n                objs_count += 1\n                csv_body = csv_obj.get()[\'Body\']\n                df = pd.read_csv(csv_body)\n                dataframes.append(df)\n        \n        print ("Loaded {} data objects for {}".format (objs_count, date))\n    return pd.concat(dataframes)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- squared_error\n\nUpdated code:\n```python\n# No changes made as there are no unused functions\ndef squared_error(x_values, y_values, m, b, x):\n    pass\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nimport math\n\n# read in dirty data \ndf = pd.read_csv("2014_election_results.csv")\ndfClean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\nfor i in range(len(dfClean)):\n    row = dfClean.iloc[i]  \n    row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n    if(pd.isnull(row["CANDIDATE NAME"]) or (row["CANDIDATE NAME"] == \'Scattered\')):\n        if(pd.isnull(row["CANDIDATE NAME (Last)"]) or (row["CANDIDATE NAME (Last)"] == \'Scattered\')):\n            row["CANDIDATE NAME"] = "UNKNOWN" \n        else:\n            row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"\n\ndfClean = dfClean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\nreturn dfClean\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# returns "True" if face is detected in image stored at img_path\ndef face_detector(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray)\n    return len(faces) > 0\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n# By pasting the above list into a spreadsheet and cross checking with the data dictionary, we can \n# see which category each field should be\n\n# statsmodel requires all fieldsnames to begin with letters, so let's sort this out now.\ntrain = train.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ntest = test.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ndata_full = pd.concat([train, test], keys = ['train', 'test'])\n\n# Makes lists of each type\ncategories = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n              'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'YearBuilt', \n              'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', \n              'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'Heating', \n              'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', \n              'GarageFinish', 'GarageCars', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', \n              'SaleCondition']\nfloats = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n          'FirstFlrSF', 'SecondFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n          'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nints = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \n         'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n\nbools = ['CentralAir']\n\nfeature_names = categories + floats + ints + bools\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\nimport matplotlib.image as mpimg\n\ndef disp_image(img_path):\n    img = mpimg.imread(img_path)\n    fig = plt.figure()\n    plt.subplot()\n    plt.imshow(img)\n    plt.axis('off')\n    plt.plot()\n    plt.show()\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {\'a\': [1, 5, 2, 5, 2, 8],\n               \'b\': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key=\'a\')\n    >> data[\'a\'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        # if self.key == \'playlist_pid\': from IPython.core.debugger import set_trace; set_trace()\n        return data_dict[:,[self.key]].astype(np.int64)\n\n    def get_feature_names(self):\n        return [dataset.columns[self.key]]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# No changes needed as the function is used\ndef make_feature_cols():\n  input_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n  return input_columns\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef dateRange(start, end):\n    days = (datetime.datetime.strptime(end, "%Y-%m-%d") - datetime.datetime.strptime(start, "%Y-%m-%d")).days + 1\n    return [datetime.datetime.strftime(datetime.datetime.strptime(start, "%Y-%m-%d") + datetime.timedelta(i), "%Y-%m-%d") for i in xrange(days)]\n\ndef date_to_week(date):\n    if type(date) == str:\n        date = pd.to_datetime(date).date()\n    return (date - datetime.date(2015,7,7)).days  / 7\n\ndef week_to_date(week_number,return_str=True):\n    if week_to_date:\n        return [(datetime.date(2015,7,7)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d"),(datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d")]\n    return [datetime.date(2015,7,7)+ datetime.timedelta(week_number*7),datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)]\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- tokenizer\n\nUpdated code:\n```python\nfrom nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef square(x):\n    """\n    Return the square of x.\n    """\n    return x ** 2\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef buildTraces(bankdata, min_trace = False, mean_trace = False, max_trace = False):\n    data = []\n    min_amount = 0\n    max_amount = 0\n    \n    for accountType in ACCOUNTTYPES: # Account type\n        for bank in bankdata[accountType]: # Bank name\n            dates = np.array(bankdata[accountType][bank][\'date\'])\n            balances = np.array(bankdata[accountType][bank][\'balance\'])\n            \n            bankName = \'(\' + accountType + \') \' + SUPPORTED_BANKS[bank]\n\n            trace_main = go.Scatter(\n                x = dates,\n                y = balances,\n                name = bankName + \': Saldo \' + str(format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".")) + CURRENCY,\n                #line = dict(\n                #    color = \'green\'\n                #),\n                mode = \'lines\'\n            )\n            data.append(trace_main)\n\n            if max_trace:\n                trace_max = go.Scatter(\n                    x = dates,\n                    y = [balances.max() for f in dates],\n                    name = bankName + \': Saldo máximo\',\n                    #visible = \'legendonly\',\n                    #hoverinfo = \'name\',\n                    line = dict(\n                        #color = \'cyan\',\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_max)\n\n            if mean_trace:\n                trace_mean = go.Scatter(\n                    x = dates,\n                    y = [balances.mean() for f in dates],\n                    name = bankName + \': Saldo medio\',\n                    #hoverinfo = \'none\',\n                    line = dict(\n                        #color = \'magenta\',\n                        width = 4,\n                        dash = \'dashdot\'\n                    )\n                )\n                data.append(trace_mean)\n\n            if min_trace:\n                trace_min = go.Scatter(\n                    x = dates,\n                    y = [balances.min() for f in dates],\n                    name = bankName + \': Saldo mínimo\',\n                    line = dict(\n                        #color = \'red\',\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_min)\n                \n            # Extra\n            if balances.max() > max_amount:\n                max_amount = balances.max()\n    \n    max_amount, sum_trace = get_trace_sum_balances(bankdata)\n    data.append(sum_trace)\n    \n    return (data, min_amount, max_amount)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n#helper in getting top features and making visual\n#convert relevant dataframe columns to lowercase so we can compare with top feature output\n\ntrack_artist_lower_df = tracks_df["track_artist_uri"].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df["track_album_uri"].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged["track_uri"].apply(lambda x: x.lower())\n\n#Take a uri and return album, artist or song title in plain english\ndef get_translation(uri_type, uri):\n    \n    track = False  #if a single track/song is input as uri, I need to handle it differently\n    if uri_type == "track_artist_uri":\n        df = track_artist_lower_df\n        col = "track_artist_name"\n    elif uri_type == "track_album_uri":\n        df = track_album_lower_df\n        col = "track_album_name"\n    elif uri_type == "track_uri":\n        df = merged_track_uri_lower_df\n        col = "track_name"\n    for i in range(len(tracks_df)):\n        if df[i] == uri:\n            if track == True:\n                return merged.iloc[i][col]\n            return tracks_df.iloc[i][col]\n            break\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef IRLS(data):\n    length        = len(data)\n    not_converged = True\n    w             = np.array([0, 0, 0])\n    p             = np.empty(length)\n    s             = np.empty(length)\n    z             = np.empty(length)\n\n    ones          = np.ones(200)\n    ones[100:]    = ones[100:]*-1\n    \n    temp_var      = np.hstack((ones[None].T,data))\n    np.random.shuffle(temp_var)\n    \n    y             = temp_var[:,0]\n    data          = temp_var[:,1:]\n    print(data.shape)\n    \n    while not_converged:\n        w_prev = w\n        for i in range(length):\n            p[i] = np.exp(w_prev.dot(data[i])) / (1 + np.exp(w_prev.dot(data[i])))\n            s[i] = p[i]*(1-p[i])\n            \n            if math.isnan(s[i]): s[i]=1\n            \n            z[i] = w_prev.dot(data[i]) + (y[i]-p[i])/(max(s[i],0.00001))\n            \n        diag_s = np.diag(s)\n#         print(\'diag:\', diag_s.shape)\n        t1     = np.linalg.inv(np.dot(np.dot(data.T, diag_s), data))\n        t2     = np.dot(np.dot(data.T, diag_s), z)\n        w      = np.dot(t1, t2)\n#         print("t1, t2",t1.shape, t2.shape)\n        w      = w/np.linalg.norm(w)\n        \n        print(\'Iterations\',w)\n        \n        if abs(sum(w-w_prev)) < 0.000001:\n            \n            print("Converged!!")\n            not_converged = False\n            return w\n            \n        elif sum(abs(w)) > 900 or math.isnan(w[0]):\n            \n            print("Not converging!!!")\n            return w\n            \n    return w\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nfrom filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\ndef pos_vel_filter(x, P, R, Q=0., dt=1.0):\n    """ Returns a KalmanFilter which implements a\n    constant velocity model for a state [x dx].T\n    """\n    \n    kf = KalmanFilter(dim_x=2, dim_z=1)\n    kf.x = np.array([x[0], x[1]]) # location and velocity\n    kf.F = np.array([[1., dt],\n                     [0.,  1.]])  # state transition matrix\n    kf.H = np.array([[1., 0]])    # Measurement function\n    kf.R *= R                     # measurement uncertainty\n    if np.isscalar(P):\n        kf.P *= P                 # covariance matrix \n    else:\n        kf.P[:] = P               # [:] makes deep copy\n    if np.isscalar(Q):\n        kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=Q)\n    else:\n        kf.Q[:] = Q\n    return kf\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef removal_evolution(removal_list, G):\n    disting_bet_average_degree = []\n    hist = nx.degree_histogram(G)\n    last_average_degree = float(sum(hist[i] * i for i in range(len(hist)))) / float(sum(hist))\n    idx_node = 0\n    while last_average_degree > 0 and idx_node < len(removal_list):\n        disting_bet_average_degree.append(last_average_degree)\n        hist = nx.degree_histogram(G)\n        last_average_degree = float(sum(hist[i] * i for i in range(len(hist)))) / float(sum(hist))\n        G.remove_node(removal_list[idx_node][0])\n        idx_node = idx_node + 1\n    return disting_bet_average_degree\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef IsotypicComponent(S, arg, use_antisymmetry=False):\n    if isinstance(arg, Partition):\n        list_partitions = [arg]\n    elif isinstance(arg, Integer):\n        list_partitions = Partitions(arg)\n    else : \n        print("Error: arg should be a partition or an integer.")\n    \n    basis = S.basis()\n    result = {}\n    P1 = basis.values().pop()[0].parent()\n    for nu in list_partitions:\n        result_nu = {}\n        if use_antisymmetry == True:\n            antisymmetries = antisymmetries_of_tableau(nu.initial_tableau())\n            P2 = DiagonalAntisymmetricPolynomialRing(P1._R, P1.ncols(), P1.nrows(), \n                                                 P1.ninert(), antisymmetries)\n        for deg, value in basis.iteritems():\n            if use_antisymmetry:\n                gen = []\n                for p in value:\n                    temp = apply_young_idempotent(P2(p), nu)\n                    if temp != 0: \n                        gen += [temp]\n            else:\n                gen = []\n                for p in value:\n                    temp = apply_young_idempotent(p, nu)\n                    if temp != 0:\n                        gen += [temp]\n            if gen != []:\n                result_nu[(deg, tuple(nu))] = Subspace(gen, {}).basis()[0]\n        if result_nu != {}:\n            result[nu] = Subspace(result_nu, operators={})\n                \n    if len(result.keys()) == 1:\n        key = result.keys()[0]\n        return result[key]\n    else:\n        return result\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n#apply style function to highlight max values\ndef highlight_max(s):\n    '''\n    highlight the maximum in a Series yellow.\n    '''\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]\n```"}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):\n    heterogeneity = {}\n    \n    min_heterogeneity_achieved = float('inf')\n    best_seed = None\n    final_centroids = None\n    final_cluster_assignment = None\n    \n    for i in xrange(num_runs):\n        \n        # Use UTC time if no seeds are provided \n        if seed_list is not None: \n            seed = seed_list[i]\n            np.random.seed(seed)\n        else: \n            seed = int(time.time())\n            np.random.seed(seed)\n        \n        # Use k-means++ initialization\n        initial_centroids = get_initial_centroids(data, k, seed=0)\n        \n        # Run k-means\n        centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter=400,\n                                       record_heterogeneity=None, verbose=True)\n        \n        # To save time, compute heterogeneity only once in the end\n        heterogeneity[seed] = compute_heterogeneity(data, k, centroids, cluster_assignment)\n        \n        if verbose:\n            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n            sys.stdout.flush()\n        \n        # if current measurement of heterogeneity is lower than previously seen,\n        # update the minimum record of heterogeneity.\n        if heterogeneity[seed] < min_heterogeneity_achieved:\n            min_heterogeneity_achieved = heterogeneity[seed]\n            best_seed = seed\n            final_centroids = centroids\n            final_cluster_assignment = cluster_assignment\n    \n    # Return the centroids and cluster assignments that minimize heterogeneity.\n    return final_centroids, final_cluster_assignment\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as preprocess_inception_input\n\ndef detect_dog_breed(img_path, given_model, use_bottleneck=True, img_H=224, img_W=224):\n    ''' Detect dog breed given image in the img_path,\n        using given model, using either bottleneck features (or not)\n        with given img Height and Width\n        \n        @return: Dog breed (str)\n    '''\n    print('Detecting dog breed...')\n    tensor = path_to_tensor(img_path, img_H, img_W)\n    \n    # using given image, extract its bottleneck features by running thru InceptionV3 n/w first\n    if use_bottleneck: \n        tensor = extract_InceptionV3(tensor)\n    else:\n        tensor = preprocess_inception_input(tensor)\n    \n    # print('  [input tensor shape: {}]'.format(tensor.shape))\n    # make predictions (probabilities)\n    predicted_vector = given_model.predict(tensor)\n    # get max index\n    y_hat = np.argmax(predicted_vector)\n    chance = 100. * predicted_vector[0][y_hat]  # probability of correctness\n    # print('  [y_hat:{}]'.format(y_hat))\n    # print('  prob:{:.2f}%'.format(chance))\n\n    # return dog breed and probability \n    return dog_names[y_hat], chance\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nfilename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)\n\n# If the parsed data only contains one column then return a Series\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass DiscriminatorNetwork:\n\n    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.adversarial_loss_weight = adversarial_loss_weight\n        self.small_model = small_model\n\n        self.k = 3\n        self.mode = 2\n        self.weights_path = "weights/Discriminator weights.h5"\n\n        self.gan_layers = None\n\n    def append_gan_network(self, true_X_input):\n\n        # Normalize the inputs via custom VGG Normalization layer\n        x = Normalize(type="gan", value=127.5, name="gan_normalize")(true_X_input)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_1\', kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name="gan_lrelu1_1")(x)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_2\', strides=(2, 2), kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu1_2\')(x)\n        x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm1_1\')(x)\n\n        filters = [128, 256] if self.small_model else [128, 256, 512]\n\n        for i, num_filters in enumerate(filters):\n            for j in range(2):\n                strides = (2, 2) if j == 1 else (1, 1)\n            \n                x = Conv2D(num_filters, (self.k, self.k), padding=\'same\', strides=strides,\n                                  name=\'gan_conv%d_%d\' % (i + 2, j + 1), kernel_initializer="glorot_uniform")(x)\n                x = LeakyReLU(0.3, name=\'gan_lrelu_%d_%d\' % (i + 2, j + 1))(x)\n                x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm%d_%d\' % (i + 2, j + 1))(x)\n\n        x = Flatten(name=\'gan_flatten\')(x)\n\n        output_dim = 128 if self.small_model else 1024\n\n        x = Dense(output_dim, name=\'gan_dense1\')(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu5\')(x)\n\n        gan_regulrizer = AdversarialLossRegularizer(weight=self.adversarial_loss_weight)\n        x = Dense(2, activation="softmax", activity_regularizer=gan_regulrizer, name=\'gan_output\')(x)\n\n        return x\n\n    def set_trainable(self, model, value=True):\n        if self.gan_layers is None:\n            disc_model = [layer for layer in model.layers\n                          if \'model\' in layer.name][0] # Only disc model is an inner model\n\n            self.gan_layers = [layer for layer in disc_model.layers\n                               if \'gan_\' in layer.name]\n\n        for layer in self.gan_layers:\n            layer.trainable = value\n\n    def load_gan_weights(self, model):\n        f = h5py.File(self.weights_path)\n\n        layer_names = [name for name in f.attrs[\'layer_names\']]\n        layer_names = layer_names[1:] # First is an input layer. Not needed.\n\n        if self.gan_layers is None:\n            self.gan_layers = [layer for layer in model.layers\n                                if \'gan_\' in layer.name]\n\n        for i, layer in enumerate(self.gan_layers):\n            g = f[layer_names[i]]\n            weights = [g[name] for name in g.attrs[\'weight_names\']]\n            layer.set_weights(weights)\n\n        print("GAN Model weights loaded.")\n        return model\n\n    def save_gan_weights(self, model):\n        print(\'GAN Weights are being saved.\')\n        model.save_weights(self.weights_path, overwrite=True)\n        print(\'GAN Weights saved.\')\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nimport ipyleaflet as leaflet\nimport ipywidgets as widgets\n\nclass USMap:\n    def __init__(self):\n        self.us = USZIPCodeRepository()\n\n        self.center = [47.621795, -122.334958]\n        self.zoom = 8\n        self.height = \'500px\'\n        self.progress_bar_width = \'500px\'\n        self.area_style = {\'color\':\'#0000ff\', \'weight\': .5, \'fillColor\':\'#000077\', \'fillOpacity\':0.2}\n\n        self.progress_bar = widgets.IntProgress(bar_style=\'info\', layout=widgets.Layout(width=self.progress_bar_width))\n        self.label = widgets.Label()\n        self.progress_label = widgets.Label()\n        self.info_box = widgets.HBox([self.progress_label, self.progress_bar])\n\n        self.basemap = leaflet.basemaps.OpenMapSurfer.Roads\n        self.basemap[\'name\'] = \'basemap\'\n        self.heatmap_data = leaflet.basemaps.Strava.All\n        self.heatmap_data[\'name\'] = \'heatmap\'\n        self.heatmap = leaflet.basemap_to_tiles(self.heatmap_data)\n        self.layers_control = leaflet.LayersControl()\n        self.map_layout = widgets.Layout(height=self.height)\n\n        self.map = None\n\n\n    def enable_heatmap(self):\n        self.map.add_layer(self.heatmap)\n\n\n    def disable_heatmap(self):\n        self.map.remove_layer(self.heatmap)\n\n\n    def handle_interaction(self, **kwargs):\n        \'\'\'mouse interaction handling\'\'\'\n        if kwargs.get(\'type\') == \'mousemove\':\n            self.label.value = str(kwargs.get(\'coordinates\'))\n\n    def fetch_zipcode(self, zipcode):\n        d, err = self.us.fetch_zipcode(zipcode)\n        if err is not None:\n            print(err)\n        return d\n\n\n    def add_point(self, lat, lng, name=\'\', popup=None):\n        feature = {"type": "Feature", "properties": {}, "geometry": {"type": "Point", "coordinates": [lng, lat]}}\n        self.add_geojson(feature, name, popup)\n\n\n    def add_geojson(self, geojson, name=\'\', popup=None):\n        g = leaflet.GeoJSON(data=geojson, hover_style={\'fillColor\': \'#00aaff\'}, name=name)\n\n        if popup is not None:\n            g.popup = popup\n\n        self.map += g\n\n\n    def add_geojsons(self, geojsons, name=\'\'):\n        d = {"type": "FeatureCollection", "features": list(geojsons)}\n\n        self.add_geojson(d, name)\n\n\n    def add_zipcode(self, zipcode):\n        d = self.fetch_zipcode(zipcode)\n        if d is None:\n            print(\'failed to add \' + zipcode + \'.\')\n            return\n\n        d[\'properties\'][\'style\'] = self.area_style\n\n        text_template = Template(\'\'\'<div>ZIP Code\n                                        <ul class=\'list-group\'>\n                                            <li class=\'list-group-item\'>$zipcode</li>\n                                        </ul>\n                                    </div>\'\'\')\n        popup_text = text_template.substitute(zipcode=zipcode)\n        popup = widgets.HTML(value=popup_text, placeholder=\'\', description=\'\')\n\n        self.add_geojson(d, name=zipcode, popup=popup)\n\n\n    def progressive_iter(self, iterable, n=None, label_on_finish=\'\'):\n        display(self.info_box)\n        \n        if n is None:\n            n = len(iterable)\n\n        self.progress_bar.value = self.progress_bar.min\n        self.progress_bar.max = n\n\n        for v in iterable:\n            yield v\n            self.progress_label.value = v\n            self.progress_bar.value += 1\n\n        self.progress_label.value = label_on_finish\n\n\n    def add_zipcodes_no_check(self, zipcodes, show_progress=False):\n        zipcode_gen = self.progressive_iter(zipcodes) if show_progress else zipcodes\n\n        for z in zipcode_gen:\n            self.add_zipcode(z)\n\n        return zipcodes\n\n\n    def add_zipcodes(self, zipcodes, show_progress=False):\n        zipcodes = set(zipcodes)\n        available_zipcodes = list(zipcodes & self.us.zipcode_set)\n        available_zipcodes.sort()\n\n        return self.add_zipcodes_no_check(available_zipcodes, show_progress)\n\n\n    def display(self):\n        if self.map is None:\n            self.map = leaflet.Map( center=self.center, zoom=self.zoom, basemap=self.basemap, layout=self.map_layout)\n            self.map.on_interaction(self.handle_interaction)\n            self.map.add_control(self.layers_control)\n\n        display(self.map)\n        display(self.label)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# Set up parameters:    \nk_size = 3\nvertex_ratio_h = .45\nvertex_ratio_v = .60\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline(image, \n                                   k_size = k_size,\n                                   low_thresh = low_thresh,\n                                   high_thresh = high_thresh,\n                                   L2gradient = L2gradient,\n                                   rho = rho,\n                                   theta = theta,\n                                   min_votes = min_votes,\n                                   min_line_len = min_line_len,\n                                   max_line_gap = max_line_gap,\n                                   angle = angle,\n                                   angle_thresh = angle_threshold,\n                                   debug = False)\n    return result\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef is_int(x):\n    try: \n        int(x)\n        return True\n    except ValueError:\n        return False\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef sigmoid(x):\n    """\n    Calculates the Sigmoid(x)\n    :param x: numpy array <dtype=float> points we want to find sigmoid of\n    :return: numpy array <dtype=float> the sigmoid of those points\n    """\n    return 1./(1+np.exp(-x)) #-\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nfrom scipy.misc import imresize\n\ndef prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n    """Zooms and crops the image randomly for data augmentation."""\n\n    # First, let\'s find the largest bounding box with the target size ratio that fits within the image\n    height = image.shape[0]\n    width = image.shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = width if crop_vertically else int(height * target_image_ratio)\n    crop_height = int(width / target_image_ratio) if crop_vertically else height\n        \n    # Now let\'s shrink this bounding box by a random factor (dividing the dimensions by a random number\n    # between 1.0 and 1.0 + `max_zoom`.\n    resize_factor = np.random.rand() * max_zoom + 1.0\n    crop_width = int(crop_width / resize_factor)\n    crop_height = int(crop_height / resize_factor)\n    \n    # Next, we can select a random location on the image for this bounding box.\n    x0 = np.random.randint(0, width - crop_width)\n    y0 = np.random.randint(0, height - crop_height)\n    x1 = x0 + crop_width\n    y1 = y0 + crop_height\n    \n    # Let\'s crop the image using the random bounding box we built.\n    image = image[y0:y1, x0:x1]\n\n    # Let\'s also flip the image horizontally with 50% probability:\n    if np.random.rand() < 0.5:\n        image = np.fliplr(image)\n\n    # Now, let\'s resize the image to the target dimensions.\n    image = imresize(image, (target_width, target_height))\n    \n    # Finally, let\'s ensure that the colors are represented as\n    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n    return image.astype(np.float32) / 255\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef find_f(K, y, iterations=100):\n    """\n    Finds f using a iterative aproach also finds p(y|f)\n    :param K: np.array(np.array) covariance matrix of data points we have observed\n    :param y: numpy array of the y value of the data points we have observed\n    :iterations: int optional default = 100 number of iterations we will preform to optimize f\n    :return: (numpy array <float> f, numpy array <float> p(y|f)) f is the latent function value for each of the \n    sampled data points, p(y|f) is the probability of y given the latent function we calculated\n    make sure to calculate p(y|f) after having approximated f_hat.\n    """\n    n = len(y)\n    f = np.zeros(n)\n    grad = np.zeros(n)\n    for i in range(iterations):\n        for j in range(n):\n            sigmoid_v = sigmoid(f[j]*y[j])\n            grad[j] = (1-sigmoid_v)*y[j]\n        f = np.array(np.matmul(K, grad)).flatten()\n        \n    y_giv_f = np.zeros(n)\n    for j in range(n):\n        y_giv_f[j] = sigmoid(f[j]*y[j])\n        \n    return f, y_giv_f\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef splitData(data, labels):\n    train_data_all, test_data, train_labels_all, test_labels = train_test_split(data, labels,\n                                                               stratify=labels, \n                                                               test_size=0.25)\n\n    train_data, dev_data, train_labels, dev_labels = train_test_split(train_data_all, train_labels_all,\n                                                                     stratify=train_labels_all, \n                                                                     test_size=0.20)\n\n    print("training data:", train_data.shape)\n    print("dev data     :", dev_data.shape)\n    print("test data    :",  test_data.shape)\n    return {\'train_data\': train_data, \'train_labels\': train_labels, \n            \'dev_data\':   dev_data,   \'dev_labels\': dev_labels,\n            \'test_data\':  test_data,  \'test_labels\': test_labels}\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef max_norm_regularizer(threshold, axes=1, name="max_norm",\n                         collection="max_norm"):\n    def max_norm(weights):\n        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n        clip_weights = tf.assign(weights, clipped, name=name)\n        tf.add_to_collection(collection, clip_weights)\n        return None # 규제 손실을 위한 항이 없습니다\n    return max_norm\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef f(gamma):\n    v = vandermonde(gamma)\n    n = gamma.size()\n    deg_v = v.degree()\n    generator = {v.multidegree() : [v]}\n    list_op = partial_derivatives(v.parent())\n    W1 = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n\n    r = 1\n    op_pol = polarization_operators(r, deg_v, row_symmetry="permutation")\n    show(IsotypicComponent(W1, Partition([1 for i in range(n)])).basis())\n    show(IsotypicComponent(W1, gamma).basis())\n    W1 = PolarizedSpace(IsotypicComponent(W1, n, use_antisymmetry=True), op_pol)\n    return character(W1, row_symmetry="permutation")\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef calc_sum_product_factor_to_variable_msg(factor, variable):\n    neighbour_msg_prod = calc_other_neighbour_msg_prod(factor, variable)\n    \n    f_neighb_first = move_dimension_first(factor.f, factor.neighbours.index(variable))\n    \n    return marginalize(calculate_factor(f_neighb_first, neighbour_msg_prod), 0)\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef calc_sum_product_variable_to_factor_msg(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    \n    if len(neighbour_msg_prod) > 0:\n        message = np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        message = np.ones(variable.num_states)\n    \n    message = message * variable.observed_state\n    \n    return message\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef mutate(s):\n    """Return s with a random mutation applied"""\n    mutators = [\n        delete_random_character,\n        insert_random_character,\n        flip_random_character\n    ]\n    mutator = random.choice(mutators)\n    return mutator(s)\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef max_sum(node_list):\n    \n    for n in node_list:\n#         print(n, ':', *list(n.pending))\n        while len(n.pending) > 0:            \n            f = next(iter(n.pending))\n            n.send_ms_msg(f)\n            \n    for n in reversed(node_list):\n        \n        while len(n.pending) > 0:\n            f = next(iter(n.pending))\n            n.send_ms_msg(f)\n```"}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef PreProcess(table,Dream9):\n    #Select all variables that are not Categorical\n    Tables=[table[[v for v in table.keys() if v not in Categorical]]]\n    \n    #Convert yes/no to 1/0\n    Alias_Dict={'SEX':{'F':1},'PRIOR.MAL':{'YES':1},'PRIOR.CHEMO':{'YES':1},'PRIOR.XRT':{'YES':1},\n                'Infection':{'Yes':1},'ITD':{'POS':1,'ND':numpy.nan},'D835':{'POS':1,'ND':numpy.nan},\n                'Ras.Stat':{'POS':1,'NotDone':numpy.nan},'resp.simple':{'CR':1},'Relapse':{'Yes':1},\n                'vital.status':{'A':1}}\n    \n    Tables+=[alias(table,Alias_Dict)]\n    \n    #Split data that has multiple values\n    Tables+=[split(table['cyto.cat'],Dream9)]\n    \n    #Create new data for protein\n    Tables+=[squared(table[Protein])]\n    Tables+=[absolute(table[Protein])]\n    Tables+=[bin_independent(table[Protein],Dream9,2)]\n    Tables+=[bin_independent(table[Protein],Dream9,3)]\n    Tables+=[bin_independent(table[Protein],Dream9,4)]\n    Tables+=[bin_independent(table[Protein],Dream9,5)]\n    \n    #Make PCA axis\n    Tables+=[make_pca(table[Protein],Dream9,200,name='PCA')]\n    Tables+=[make_pca(table[Protein],Dream9,200,name='Whiten_PCA',whiten=True)]\n    Tables+=[make_pca(squared(table[Protein]),squared(Dream9[Protein]),200,name='PCA_Sq')]\n    \n    #Bin dependent variables\n    try:\n        Tables+=[cutoff(table[['Overall_Survival','Remission_Duration']],130)]\n        Tables+=[binned(table[['Overall_Survival','Remission_Duration']])]\n    except KeyError:\n        pass        \n    \n    #Join everything\n    return pandas.concat(Tables,axis=1)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n- None\n\nUpdated code:\n```python\n# No unused functions found\ndef make_pca(Table,All_Data,n,name='PCA_test',whiten=False):\n    pca = PCA(n_components=n,whiten=whiten)\n    pca.fit(All_Data[Table.keys()])\n    trans_PCA=pca.transform(Table)\n    #Saving the components of the PCA\n    components=pca.components_\n    PCA_table=pandas.DataFrame(components,columns=Table.keys(),index=['%s_%i'%(name,i+1) for i in range(n)])\n    PCA_table.to_csv('%s_components.csv'%name)\n    #pca.explained_variance_ratio_##pca.get_params()\n    #Needs to return the PCA transformation\n    return pandas.DataFrame(trans_PCA,columns=['%s_%i'%(name,i+1) for i in range(n)],index=Table.index)\n#pandas.concat([Trial_data[Protein[:4]],make_pca(Trial_data[Protein[:4]],Dream9,3),make_pca(Trial_data[Protein[:4]],Dream9,2,'Whiten_PCA',True)],axis=1).T\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef powers(x):\n    """\n    Return a few powers of x.\n    """\n    return x ** 2, x ** 3, x ** 4\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef get_initial_centroids(data, k, seed=None):\n    '''Randomly choose k data points as initial centroids'''\n    if seed is not None: # useful for obtaining consistent results\n        np.random.seed(seed)\n    n = data.shape[0] # number of data points\n    # Pick K indices from range [0, N).\n    rand_indices = np.random.randint(0, n, k)\n    \n    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n    # As long as at least one document in a cluster contains a word,\n    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n    centroids = data[rand_indices,:].toarray()\n    \n    return centroids\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef insert_and_time_filter_bloom_filter(capacity, percent_fill=0.9):\n    num_inserted = 0\n    b_filter = CountingBloomFilter(capacity)\n    now = time.time()\n    for i in range(int(percent_fill*capacity)):\n        b_filter.add(str(i))\n    elapsed_insertion = time.time() - now\n    return b_filter, elapsed_insertion\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef delete_from_cuckoo_filter_and_time(capacity, percent_to_fill=0.9, percent_delete=1.0):\n    c_filter, _, _ = insert_and_time_filter_cuckoo_filter(capacity, percent_fill=percent_to_fill)\n    \n    total_size = c_filter.get_size()\n    number_of_items_to_delete = total_size*percent_delete\n    \n    now = time.time()\n    for i in range(int(number_of_items_to_delete)):\n        c_filter.remove(str(i))\n    elapsed_time = time.time() - now\n        \n    return c_filter, elapsed_time\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef flip_random_character(s):\n    """Returns s with a random bit flipped in a random position"""\n    if s == "":\n        return s\n\n    pos = random.randint(0, len(s) - 1)\n    c = s[pos]\n    bit = 1 << random.randint(0, 6)\n    new_c = chr(ord(c) ^ bit)\n    # print("Flipping", bit, "in", repr(c) + ", giving", repr(new_c))\n    return s[:pos] + new_c + s[pos + 1:]\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef plotMetrics(precision_l1, recall_l1, \n                accuracy, precision, recall, \n                precision_by_label, recall_by_label, \n                svm_accuracy, svm_precision, svm_recall, \n                svm_precision_by_label, svm_recall_by_label, \n                confusion, feature_size, label_encoder):\n    labels = [ \'\\n\'.join(wrap(l, 8)) for l in feature_size ]       \n    \n    plt.rcParams["figure.figsize"] = (20,6)\n\n    plt.plot(labels, precision, color=\'olivedrab\', \n             linewidth=3, label=\'Precision (Logistic regression) \', marker=\'o\' )\n    plt.plot(labels, recall, color=\'olivedrab\', linestyle=\'dashed\',\n             linewidth=3, label=\'Recall (Logistic regression)\', marker=\'o\' )\n    #plt.plot(labels, accuracy, color=\'darkolivegreen\', linestyle=\':\',\n    #         linewidth=3, label=\'Accuracy (Logistic regression)\', marker=\'v\' )\n\n    plt.plot(labels, svm_precision, color=\'slateblue\', \n             linewidth=3, label=\'Precision (SVM)\', marker=\'o\' )\n    plt.plot(labels, svm_recall, color=\'slateblue\', linestyle=\'dashed\',\n             linewidth=3, label=\'Recall (SVM)\', marker=\'o\' )\n    #plt.plot(labels, svm_accuracy, color=\'darkslateblue\', linestyle=\':\',\n    #         linewidth=3, label=\'Accuracy (SVM)\', marker=\'v\' )\n\n    #plt.plot(labels, precision_l1, color=\'gray\',alpha=.4,\n    #         linewidth=3, label=\'L1 precision at different C values\', marker=\'o\' )\n    #plt.plot(labels, recall_l1, color=\'gray\', linestyle=\'dashed\',alpha=.6,\n    #         linewidth=3, label=\'L1 at different C values\', marker=\'o\' )\n    \n    \n    plt.yticks(np.arange(.42, .65, .01))\n    plt.ylabel(\'Precision, Recall\', fontsize=20)\n    plt.xlabel(\'Feature size with L1 regularization at different C parameters\', fontsize=20, labelpad=20)\n    plt.legend()\n    plt.grid()\n    plt.show()\n    \n    # find optimal f1\n    best_idx = np.argmax(precision)\n    \n    # Show precision and recall across different labels\n    showPrecisionRecallPairByLabel(precision_by_label[best_idx], recall_by_label[best_idx], label_encoder,\n                                  \'Logistic Regression\', [\'olivedrab\', \'darkolivegreen\'])\n    showPrecisionRecallPairByLabel(svm_precision_by_label[best_idx], svm_recall_by_label[best_idx], label_encoder,\n                                  \'SVM\', [\'slateblue\', \'darkslateblue\'])\n    \n    \n    # Get the confusion matrix for the optimal precision\n    # Show the labels that have the highest error rate\n    conf_mx = confusion[best_idx]\n    showTopConfused(conf_mx, label_encoder)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- test\n\nUpdated code:\n```python\n# No changes made\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n# checks if line is in iambic pentameter (i.e. 0101010101 stress pattern)\ndef check_iambic_pentameter(line):\n    # get the stresses from cmu dict \n    # if word is 1 syllable, then have the option for it to be stressed or unstressed\n    stresses = []\n    for i in line.split(' '):\n        stress = poetrytools.stress(i)\n        if len(stress) == 1:\n            stresses.append(['0','1'])\n        else:\n            stresses.append([stress])\n    \n    # make combination of all possible stress patterns\n    result = [[]]\n    final = []\n    for pool in stresses:\n        result = [x+[y] for x in result for y in pool]\n    final = [''.join(i) for i in result]\n    \n    # return if any pattern fits iambic pentameter \n    return ('0101010101' in final)\n```"}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef eulerCoupledAdaptive (f, n, blockPositions, vBlock, blockNum, h1, h2, maxPoints, minBlockV, *args, verbose = False):\n    """ \n    Solve a coupled system of ODEs by Euler method with fixed number of steps.\n\n    Arguements: f - function giving ODE as y\'=f(x,y)\n                n - the number of ODEs\n                blockPositions - the array containging the initial block positions\n                vBlock - initial block velocity\n                blockNum - the number of blocks\n                interval - tuple region (a,b) on which to solve ODE\n                steps - number of steps\n    \n    Returned: An array containing the positions and velocies of the blocks over time\n    """\n    \n    points = np.zeros((maxPoints, blockNum*2 + 1)) # array to contain all the block positions and velocities over time\n\n    # set up initial point\n    t = 0\n    points[0,0] = t\n    count = 0\n    \n    # Sets up first row of poitns\n    for l in range(1, blockNum + 1):\n        points[0,l] = blockPositions[count]   \n        count += 1\n    for m in range(blockNum + 1, blockNum * 2 + 1):\n        points[0,m] = vBlock\n    \n    # initializes count, dv and r\n    count = 1\n    dv = 0\n    r = (0,0)\n    \n    while points[maxPoints - 1, 0] == 0: # Repeats until points array is filled up                   \n        \n        # Proceeds if none of the blocks are moving\n        if dv < minBlockV:\n            h = h1 # Long timestep\n            \n            # Repeats Euler calculation until one of the blocks moves or points is full\n            while dv < minBlockV and points[maxPoints - 1, 0] == 0:\n                \n                if verbose == True:\n                    print("h1:", t)\n                \n                # Saves block positions and updates timestep\n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n                \n                # Repeats Euler calculation for each block\n                for i in range(0, blockNum): \n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n                        \n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n                \n        # Proceeds if any of the blocks are moving        \n        elif points[maxPoints - 1, 0] == 0:\n            # Resets to previous timestep\n            h = h2 # Short timestep\n            t -= h1\n            count -= 1\n            blockPositions = oldBlockPositions\n            dv = 0\n\n            # Repeats Euler calculation until one of the blocks moves or points is full\n            # This is necessary, because the timestep was reset, so dv will still be zero\n            while dv < minBlockV and points[maxPoints - 1, 0] == 0: \n                \n                if verbose == True:\n                    print("h2:", t)\n                \n                # Saves block positions and updates timestep                \n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n\n                # Repeats Euler calculation for each block                \n                for i in range(0, blockNum):\n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new            \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n\n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n                \n            # Repeats Euler calculation until all of the blocks are still or points is full                \n            while dv >= minBlockV and points[maxPoints - 1, 0] == 0: \n                \n                if verbose == True:\n                    print("h2:", t)\n                \n                # Saves block positions and updates timestep\n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n\n                # Repeats Euler calculation for each block  \n                for i in range(0, blockNum):\n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new            \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n\n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n\n    return points\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\n# Invoke Ridge regularisation\nmodel_ridge = Ridge()\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\nf1 = lambda x: x**2\n    \n# is equivalent to \n\ndef f2(x):\n    return x**2\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_\n```'}, {'reason': 'stop', 'result': 'Unused functions: None\n\nUpdated code:\n```python\ndef fillEmpty(bankdata, maxDate):\n    for accountType in ACCOUNTTYPES:\n        for bank in bankdata[accountType]:\n            if bankdata[accountType][bank][\'date\'][-1] != maxDate:\n                bankdata[accountType][bank]["balance"] = bankdata[accountType][bank]["balance"] + (bankdata[accountType][bank]["balance"][-1],)\n                bankdata[accountType][bank]["date"] = bankdata[accountType][bank]["date"] + (maxDate,)\n                \n    return bankdata\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nclass ValueEstimator():\n    """\n    Value Function approximator. \n    """\n    \n    def __init__(self, learning_rate=0.1, scope="value_estimator"):\n        with tf.variable_scope(scope):\n            self.state = tf.placeholder(tf.float32, [400], "state")\n            self.target = tf.placeholder(dtype=tf.float32, name="target")\n\n            # This is just linear classifier\n            self.output_layer = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n\n            self.value_estimate = tf.squeeze(self.output_layer)\n            self.loss = tf.squared_difference(self.value_estimate, self.target)\n\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            self.train_op = self.optimizer.minimize(\n                self.loss, global_step=tf.contrib.framework.get_global_step())        \n    \n    def predict(self, state, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        return sess.run(self.value_estimate, { self.state: state })\n\n    def update(self, state, target, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        feed_dict = { self.state: state, self.target: target }\n        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n        return loss\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef sample(preds, temperature=1.):\n    # helper function to sample an index from a probability array\n    if temperature == 0.:\n        return np.argmax(preds)\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef generate_line(model, words, seed=None, rhymes=None):\n    syllables_left = 10\n    last_stress = 0\n    \n    line= []\n    start_probs = model.startprob_\n    emission_probs = model.emissionprob_\n    transition_probs = model.transmat_\n    \n    start_state = np.random.choice(len(start_probs), p = start_probs)\n    \n    if seed is not None:\n        possible_start_emissions = np.where(rhymes[seed] == 1)\n        probs = np.array(emission_probs[start_state][possible_start_emissions])\n\n        scaled_probs = probs / sum(probs)\n        while True:\n            start_emission = np.random.choice(possible_start_emissions[0], p=scaled_probs)\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1 :\n                break\n    else:\n        while True:\n            start_emission = np.random.choice(len(emission_probs[start_state]), p=emission_probs[start_state])\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1:\n                break\n    \n    line.append(start_emission)\n    start_stress = poetrytools.stress(words[start_emission])\n    syllables_left -= len(start_stress)\n    \n    if len(start_stress) == 1:\n        prev_starting_stress = 1\n    else:\n        prev_starting_stress = int(start_stress[0])\n\n    curr_state = start_state\n    while syllables_left > 0:\n        possible_transitions = transition_probs[curr_state]\n        curr_state = np.random.choice(len(possible_transitions), p=possible_transitions)\n        possible_emissions = emission_probs[curr_state]\n        while True:\n            curr_emission = np.random.choice(len(possible_emissions), p=possible_emissions)\n            curr_stress = poetrytools.stress(words[curr_emission])\n            if len(curr_stress) == 1:\n                prev_starting_stress = 1 - prev_starting_stress\n                syllables_left -= 1\n                break\n            elif len(curr_stress) > syllables_left or int(curr_stress[-1]) == prev_starting_stress:\n                continue\n            else:\n                prev_starting_stress = int(curr_stress[0])\n                syllables_left -= len(curr_stress)\n                break\n        line.append(curr_emission)\n\n    return line\n\ndef convert_line(sample, words):\n    ret = \'\'\n    i = 0\n    for word in reversed(sample):\n        curr_word = words[word]\n        if i == 0 or (curr_word == \'i\'):\n            ret += curr_word.title() + \' \'\n        else:\n            ret += curr_word + \' \'\n        i += 1\n    return ret\n\ndef generate_pair(model, words, rhymes):\n    while True:\n        a_line = generate_line(model, words)\n        seed = a_line[0]\n        if len(np.where(rhymes[seed] == 1)[0]) > 0:\n            b_line = generate_line(model, words, seed, rhymes)\n            return a_line, b_line\n        \ndef generate_rhyming_and_meter_sonnet():\n    sonnet = \'\'\n    a_lines = []\n    b_lines = []\n    \n    for _ in range(4):\n        a_line, b_line = generate_pair(reversed_quatrain_model, quatrain_words, quatrain_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    for i in range(2):\n        sonnet += convert_line(a_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(a_lines[2 * i + 1], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i + 1], quatrain_words) + \'\\n\'\n    \n    a_lines = []\n    b_lines = []\n    \n    for _ in range(2):\n        a_line, b_line = generate_pair(reversed_volta_model, volta_words, volta_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    sonnet += convert_line(a_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(a_lines[1], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[1], volta_words) + \'\\n\'\n    \n    a_line, b_line = generate_pair(reversed_couplet_model, couplet_words, couplet_rhymes)\n    sonnet += convert_line(a_line, couplet_words) + \'\\n\'\n    sonnet += convert_line(b_line, couplet_words) + \'\\n\'\n    \n    return sonnet\n\ndef generate_10_rhyming_and_meter_sonnets():\n    sonnets = \'\'\n    for i in range(10):\n        print(\'Generating Sonnet \' + str(i + 1))\n        sonnets += str(i) + \'\\n\' + generate_rhyming_and_meter_sonnet() + \'\\n\'\n    \n    f = open("project2data/rhyming_and_meter_shakespeare.txt","w")\n    f.write(sonnets)\n    return sonnets\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef sum_product(node_list):\n\n    for n in node_list:\n#         print(n, ':', *list(n.pending))\n        while len(n.pending) > 0:            \n            f = next(iter(n.pending))\n            n.send_sp_msg(f)\n            \n    for n in reversed(node_list):\n        \n        while len(n.pending) > 0:\n            f = next(iter(n.pending))\n            n.send_sp_msg(f)\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- move_dimension_first\n\nUpdated code:\n```python\n# No changes made to the code\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nimport unittest\n\nclass TestFeatures(unittest.TestCase):\n\n    def test_features_ground(self):\n        sample = (asl.df.ix[98, 1][features_ground]).tolist()\n        self.assertEqual(sample, [9, 113, -12, 119])\n\n    def test_features_norm(self):\n        sample = (asl.df.ix[98, 1][features_norm]).tolist()\n        np.testing.assert_almost_equal(sample, [ 1.153,  1.663, -0.891,  0.742], 3)\n\n    def test_features_polar(self):\n        sample = (asl.df.ix[98,1][features_polar]).tolist()\n        np.testing.assert_almost_equal(sample, [113.3578, 0.0794, 119.603, -0.1005], 3)\n\n    def test_features_delta(self):\n        sample = (asl.df.ix[98, 0][features_delta]).tolist()\n        self.assertEqual(sample, [0, 0, 0, 0])\n        sample = (asl.df.ix[98, 18][features_delta]).tolist()\n        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], "Sample value found was {}".format(sample))\n                         \nsuite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\nunittest.TextTestRunner().run(suite)\n```'}, {'reason': 'stop', 'result': "Unused functions: None\n\nUpdated code:\n```python\ndef policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4, modified=False):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    policy_stable = False\n    all_k = []\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n\n    while not policy_stable:\n        \n        # 2. POLICY EVALUATION (iterates until V_k converges)\n        k = 0\n        V_kplus1 = copy.deepcopy(V_k)\n        delta = epsilon + 1\n        \n        while delta > epsilon and (k < 5 or not modified):\n\n            delta = 0\n            for i, j in states:\n                \n                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n                a = idx_to_a[PI[i,j]]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Bellman's update rule\n                V_kplus1[i, j] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n                # Keeps biggest difference seen so far\n                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n            # Updates our current estimate\n            V_k = copy.deepcopy(V_kplus1)\n            k += 1\n        all_k.append(k)\n\n        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n        \n        policy_stable = True\n        old_PI = copy.deepcopy(PI)\n        \n        for i, j in states:\n            for a_idx in range(4): # actions\n                    \n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Policy Improvement rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n                    \n            PI[i,j] = np.argmax(Q[i,j,:])\n                    \n            if old_PI[i,j] != PI[i,j]:\n                policy_stable = False\n    \n    return V_k, all_k, PI \n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nimport time\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Para usar en caso de tener probabilidades a priori\npriors = [(0.1,0.9),(0.2,0.8),(0.3,0.7),(0.4,0.6),(0.5,0.5),(0.6,0.4),(0.7,0.3),(0.8,0.2),(0.9,0.1)]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# No changes made as the function is used\ndef next_batch(size, i):\n    return train_data[i*size:(i+1)*size]\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\nclass EncoderLayer(tf.keras.layers.Layer):\n  # Transformer 論文內預設 dropout rate 為 0.1\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    # 一樣，一個 sub-layer 一個 dropout layer\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n  def call(self, x, training, mask):\n    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n    \n    # sub-layer 1: MHA\n    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n    attn_output, attn = self.mha(x, x, x, mask)  \n    attn_output = self.dropout1(attn_output, training=training) \n    out1 = self.layernorm1(x + attn_output)  \n    \n    # sub-layer 2: FFN\n    ffn_output = self.ffn(out1) \n    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n    out2 = self.layernorm2(out1 + ffn_output)\n    \n    return out2\n```'}, {'reason': 'stop', 'result': 'Unused functions:\nNone\n\nUpdated code:\n```python\ndef vandermonde(gamma, r=0):\n    n = gamma.size()\n    if r == 0:\n        r = 1\n    P = DiagonalPolynomialRing(QQ, n, r, inert=1) \n    X = P.variables()\n    Theta = P.inert_variables()\n    return matrix([[x**i[1]*theta**i[0] for i in gamma.cells()] \n                   for x,theta in zip(X[0],Theta[0])]).determinant()\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\ndef sgd_iter(x_train, t_train, W, b):\n    \n    #every day I am shufflin`\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    lr = 1e-4\n    \n    logp = np.zeros(len(x_train))\n    for i in indices:\n        x = x_train[i:i+1]\n        t = t_train[i]\n        logp[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\n        W = W + lr*grad_w #grad ascent\n        b = b + lr*grad_b\n    \n    logp_train = logp.mean()\n    return logp_train, W, b\n```'}, {'reason': 'stop', 'result': "Unused functions:\n- model_arc\n\nUpdated code:\n```python\n### Define your architecture here.\n### Feel free to use as many code cells as needed.\nfrom tensorflow.contrib.layers import flatten\n\ndef LeNet(x):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # SOLUTION: Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    \n    # SOLUTION: Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n    fc0   = flatten(conv2)\n    \n    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n    fc1_b = tf.Variable(tf.zeros(120))\n    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n    \n    # SOLUTION: Activation.\n    fc1    = tf.nn.relu(fc1)\n\n    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n    fc2_b  = tf.Variable(tf.zeros(84))\n    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n    \n    # SOLUTION: Activation.\n    fc2    = tf.nn.relu(fc2)\n\n    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n    fc3_b  = tf.Variable(tf.zeros(43))\n    logits = tf.matmul(fc2, fc3_W) + fc3_b\n    \n    return logits\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- derivative_of\n\nUpdated code:\n```python\n# No changes made as there are no unused functions\ndef derivative_of(list_of_terms, x_value, delta_x):\n    pass\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n\n# Define a function to pass stored images to\n# reading rover position and yaw angle from csv file\n# This function will be used by moviepy to create an output video\ndef process_image(img):\n    # Example of how to use the Databucket() object defined above\n    # to print the current x, y and yaw values \n    # print(data.xpos[data.count], data.ypos[data.count], data.yaw[data.count])\n\n    # TODO: \n    # 1) Define source and destination points for perspective transform\n    # 2) Apply perspective transform\n    # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n    # 4) Convert thresholded image pixel values to rover-centric coords\n    # 5) Convert rover-centric pixel values to world coords\n    # 6) Update worldmap (to be displayed on right side of screen)\n        # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n        #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n        #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n        \n    # Perspective Transform and mask\n    warped, mask = perspect_transform(img, source, destination)\n    # 2) Apply color threshold to identify navigable terrain/obstacles/rock samples\n    threshed = color_thresh(warped)\n    obs_map = np.absolute(np.float32(threshed) - 1) * mask\n    xpix, ypix = rover_coords(threshed)\n    # 5) Convert rover-centric pixel values to world coords\n    world_size = data.worldmap.shape[0]\n    scale = 2 * dst_size\n    xpos = data.xpos[data.count]\n    ypos = data.ypos[data.count]\n    yaw = data.yaw[data.count]\n    x_world, y_world = pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale)\n    obsxpix, obsypix = rover_coords(obs_map)\n    obs_x_world, obs_y_world = pix_to_world(obsxpix, obsypix, xpos, ypos, yaw, world_size, scale)\n    # 6) Update worldmap (to be displayed on right side of screen)\n        # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n        #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n        #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n    data.worldmap[y_world, x_world, 2] = 255\n    data.worldmap[obs_y_world, obs_x_world, 0] = 255\n    nav_pix = data.worldmap[:,:,2] > 0\n    \n    data.worldmap[nav_pix, 0] = 0\n    # See if we can find some rocks\n    rock_map = find_rocks(warped, levels=(110, 110, 50))\n    if rock_map.any():\n        rock_x, rock_y = rover_coords(rock_map)\n        rock_x_world, rock_y_world = pix_to_world(rock_x, rock_y, xpos, ypos, yaw, world_size, scale)\n        data.worldmap[rock_y_world, rock_x_world, :] = 255\n\n    # 7) Make a mosaic image, below is some example code\n        # First create a blank image (can be whatever shape you like)\n    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n        # Next you can populate regions of the image with various output\n        # Here I\'m putting the original image in the upper left hand corner\n    output_image[0:img.shape[0], 0:img.shape[1]] = img\n\n        # Let\'s create more images to add to the mosaic, first a warped image\n    # warped, mask = perspect_transform(img, source, destination)\n        # Add the warped image in the upper right hand corner\n    output_image[0:img.shape[0], img.shape[1]:] = warped\n\n        # Overlay worldmap with ground truth map\n    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n        # Flip map overlay so y-axis points upward and add to output_image \n    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n\n\n        # Then putting some text over the image\n    cv2.putText(output_image,"Populate this image with your analyses to make a video!", (20, 20), \n                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n    if data.count < len(data.images) - 1:\n        data.count += 1 # Keep track of the index in the Databucket()\n    \n    return output_image\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef broad_less_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible starboard orientation.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        masthead_light = np.random.randint(50, 201)\n        mh_horiz = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        masthead_height = masthead_light + light_width\n        masthead_width = mh_horiz + light_width\n        running_light_start = masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if mh_horiz < 2 * light_width:\n            running_light_loc = np.random.randint(mh_horiz - 20, mh_horiz + 21)\n        else:\n            running_light_loc = np.random.randint(mh_horiz - 20, 211)\n        running_light_area = running_light_loc + light_width\n        new_view[masthead_light:masthead_height, mh_horiz:masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```'}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\n### TODO: Write your algorithm.\n### Feel free to use as many code cells as needed.\n\ndef whose_a_good_doggy(img_path):\n    ''' Using the given image (in img_path), returns either:\n        - Dog breed (if it's a dog)\n        - Dog breed that resembles a human (if it's a human face)\n        \n        Uses the transfer-learned CNN model from Step 5\n    '''\n    print('.'*60)\n    print('Given image:', img_path)\n    \n    # human face?\n    human_found = face_detector(img_path)\n    print('Found human:', human_found)\n    \n    # doggy ?\n    # dog_found   = dog_detector(img_path)\n    # print('Found dog:  ', dog_found)\n    \n    # find breed of dog\n    breed, chance = detect_dog_breed(img_path, inception_bneck, use_bottleneck=True, img_H=229, img_W=229)\n    print()\n    print('Image is dog breed: {} ({:.2f}% prob)'.format(breed, chance))\n    print('🐶 Woof!') if not human_found else print('Hellooo, 🐱👩🏻👦🏻👧🏻 animal 🤔')\n    print('='*60)\n```"}, {'reason': 'stop', 'result': "Unused functions: []\n\nUpdated code:\n```python\ndef draw_rule_and_reg_id(id):\n    plt.title(str(id))\n    list_result = dateRange('2016-06-06','2016-10-31')\n    for i in range(1,15):\n        list_result.append('predict_day_'+str(i))\n    xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n    ax = plt.subplot(111) \n    ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id ,list_result].T.plot(figsize=(16,9),ax=ax)\n    \n\n    list_result = dateRange('2016-06-06','2016-10-31')\n    for i in range(1,8):\n        list_result.append('predict_'+str(i))\n\n    xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n    ax = plt.subplot(111) \n    ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id,list_result].T.plot(figsize=(16,9),ax=ax)\n    plt.show()\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n- None\n\nUpdated code:\n```python\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\n# Make sure the input and output formats are the same for X and y\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    X_np = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(X_np, y_np):\n        X_train, X_valid = nparray_to_list(X_np[train_index]), nparray_to_list(X_np[valid_index])\n        y_train, y_valid = nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index])\n    train = X_train, y_train\n    valid = X_valid, y_valid\n    return train, valid\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\ndef compute_character(mu, use_antisymmetry=True, row_symmetry="permutation"):\n    n = Integer(mu.size())\n    # Determinant computation\n    v = vandermonde(mu)\n    # Span by derivatives\n    generator = {v.multidegree() : [v]}\n    list_op = partial_derivatives(v.parent())\n    V = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n    # Projection on isotypic components\n    V_iso = IsotypicComponent(V, n, use_antisymmetry=use_antisymmetry)\n    # Polarization\n    r = n-1\n    deg = v.degree()\n    if deg == 0:\n        deg = 1\n    op_pol = polarization_operators(r, deg, row_symmetry=row_symmetry)\n    V_pol = PolarizedSpace(V_iso, op_pol)\n    \n    # character\n    return character(V_pol, row_symmetry=row_symmetry)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n- grayscale\n- canny\n- gaussian_blur\n- region_of_interest\n- weighted_img\n\nUpdated code:\n```python\nimport math\n\nleftline=[(0,0,0,0)]\nrightline=[(0,0,0,0)]\n   \ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef draw_lines_roi(img, lines, vertices, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n\n    global leftline\n    global rightline\n    \n    imshape = img.shape\n    y_min = np.int(imshape[0]*0.61)\n    y_max = imshape[0]\n    left_x1 = []\n    left_x2 = []\n    right_x1 = []\n    right_x2 = []\n    left_count=0\n    right_count=0\n    \n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            m = (y2-y1)/(x2-x1)\n            if (m > 0.3) & (m<7):\n                fit = np.polyfit([x1,x2],[y1,y2],1)\n                right_x1.append(np.int(np.float((y_min - fit[1]))/np.float(fit[0])))\n                right_x2.append(np.int(np.float((y_max - fit[1]))/np.float(fit[0])))\n                right_count+=1\n            if (m < -0.3) & (m>-7):\n                fit = np.polyfit([x1,x2],[y1,y2],1)\n                left_x1.append(np.int(np.float((y_min - fit[1]))/np.float(fit[0])))\n                left_x2.append(np.int(np.float((y_max - fit[1]))/np.float(fit[0])))\n                left_count+=1\n    \n    \n    #check we have some detection for this line and also if the line has not moved too much from\n    #last frame. The first time we dont have history to compare. So that is handled separately\n    #checking if anything has ben stored in the left_line, right_line variables\n   \n    if (left_count>0):\n        left_line_x1 = np.int(np.nanmedian(left_x1))\n        left_line_x2 = np.int(np.nanmedian(left_x2))\n                \n        if leftline[0][2]!=0:\n            if (abs(leftline[0][2]-left_line_x2)<100) & (abs(leftline[0][0]-left_line_x1)<50):\n                leftline=[(left_line_x1,y_min,left_line_x2,y_max)]\n        else:\n            leftline=[(left_line_x1,y_min,left_line_x2,y_max)]\n    \n    if (right_count>0):        \n        right_line_x1 = np.int(np.nanmedian(right_x1))\n        right_line_x2 = np.int(np.nanmedian(right_x2))\n    \n        if rightline[0][2]!=0:\n            if (abs(rightline[0][2]-right_line_x2)<100) & (abs(rightline[0][0]-right_line_x1)<50):\n                rightline=[(right_line_x1,y_min,right_line_x2,y_max)]\n        else:\n            rightline=[(right_line_x1,y_min,right_line_x2,y_max)]\n    \n    #These are the new lines\n    newlines = [leftline,rightline]\n           \n    #line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(img, newlines,color,thickness)\n  \n    return img\n                         \ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap,vertices):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n   \n    #draw_lines(line_img,lines)\n    #return line_img\n    \n    result = draw_lines_roi(line_img, lines,vertices,[255,0,0],14)\n    #draw_lines(result,lines,[255, 255, 0],2)\n    return result\n```'}, {'reason': 'stop', 'result': 'Unused functions: []\n\nUpdated code:\n```python\n# will try to convert a value to a float, followed by string,\n# if the float conversion fails\n\ndef convert_invalid_values(value):\n    \n    try:\n        new_value = float(value)\n    except:\n        new_value = str(value)\n\n    return new_value\n```'}]