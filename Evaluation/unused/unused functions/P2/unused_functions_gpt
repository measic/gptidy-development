[{'reason': 'stop', 'result': "Unused functions:\n['squared', 'absolute']\n\nUpdated code:\n```python\n#Example\n#pandas.concat([Trial_data[Protein[:4]],squared(Trial_data[Protein[:4]]),absolute(Trial_data[Protein[:4]])],axis=1).T\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef insert_random_character(s):\n    """Returns s with a random character inserted"""\n    pos = random.randint(0, len(s))\n    random_character = chr(random.randrange(32, 127))\n    # print("Inserting", repr(random_character), "at", pos)\n    return s[:pos] + random_character + s[pos:]\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['systemcall', 'systemcall_pipe']\n\nUpdated code:\n```python\nimport subprocess \nimport shlex\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef delete_random_character(s):\n    """Returns s with a random character deleted"""\n    if s == "":\n        return s\n\n    pos = random.randint(0, len(s) - 1)\n    # print("Deleting", repr(s[pos]), "at", pos)\n    return s[:pos] + s[pos + 1:]\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef myfunc(x, p=2, debug=False):\n    if debug:\n        print("evaluating myfunc for x = " + str(x) + " using exponent p = " + str(p))\n    return x**p\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['value_iteration']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['plotImg', 'plotAll']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['regression_formula']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['Lectura', 'Grafica']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['plot_image', 'plot_color_image']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n[]\n\nUpdated code:\n```python\ndef to_appropriate_column_name(s):\n    s = s.lower().replace(' ', '_')\n    s = ''.join([ch for ch in s if ch.isalnum() or ch == '_'])\n    return s\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['create_xgb_target']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['true_mean_function', 'add_noise', 'generate_t']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['blockMotion']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['highlight_column_matches', 'plot_stats', 'save_rnn_layers', 'save_mlp_layers', 'glance_at_tensor']\n\nUpdated code:\n```python\nclassifier_activation = {'binary': 'sigmoid', 'categorical': 'softmax'}\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['inverse_difference', 'scale', 'invert_scale', 'fit_lstm']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef recall(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n    tp = tf.diag_part(conf_matrix)\n    recall_scores = tp/(tp_and_fn)\n    if weights:\n        recall_score = tf.multiply(recall_scores, weights)/tf.reduce_sum(weights)\n    else:\n        recall_score = tf.reduce_mean(recall_scores)        \n    return recall_score\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'fit\']\n\nUpdated code:\n```python\nclass two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'get_positive_class_probabilities\', \'get_accuracy\', \'show_prediction_accuracy\']\n\nUpdated code:\n```python\naccuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(np.mean(kf_y_pred == kf_y_test))\n    accuracy_train.append(np.mean(kf_y_pred_dev == kf_y_train))\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, kf_y_pred_dev_proba[:, 1]))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, kf_y_pred_proba[:, 1]))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train      # cambiar por accuracies_training\ndf["Accuracy (validación)"] = accuracy_validation # cambiar por accuracies_validation\ndf["AUC ROC (training)"]    = roc_train           # cambiar por aucs_training\ndf["AUC ROC (validación)"]  = roc_validation      # cambiar por aucs_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['draw_week_id', 'draw_week_ids', 'draw_ids', 'draw_ids_avg', 'draw_ids_diff', 'draw_id']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['value_iteration']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['sframe_to_scipy']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef forward_pass(x, W, b):\n    # Calcula a classe baseado em um vetor de pesos e um escalar de víes\n    # Note que nessa função aceitam-se matrizes para W e vetores para b,\n    # afinal estamos trabalhando com batches e não com um exemplo de\n    # cada vez.\n    z = x.dot(W) + b\n    y_ = sigmoid(z)\n    return y_, z\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['get_Ks']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef regression_optimize_theta(x, y, sigma_n, kernel, params_0=[0.1, 0.1]):\n    \n    """\n    Optimizes parameters for the data given by maximizing logp(data|parameters)\n    :param x: numpy array of data where we have sampled\n    :param y: numpy array of y values for data where we have sampled\n    :sigma_n: float sigma_n\n    :param kernal: the kernel function which we will be using\n    :param params_0: list params_0 this is a list that has the initial params (must be length\n    of params which your kernel is expecting) from this point the optimizer will run.\n    :return: list (optimal_params + [sigma_n]), this is a list that has the optimal parameterss (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    """\n    \n    def log_pY(theta):\n        """\n        Calculates the - log(p(y|parameters))\n        :param theta: list params this is a list that has the params (must be length\n        of params which your kernel is expecting)\n        :return: float - log(p(y|parameters)) (using negative because our optimizer is a minimizer)\n        """\n        K = kernel(x, x, theta, sigma_n) #+\n        log_k = np.linalg.slogdet(K)[1] #+\n        output = 0.5 * np.matmul(np.matmul(y.T, np.linalg.inv(K)),y) #-\n        output += 0.5 * log_k #-\n        return output #-\n\n    res = minimize(log_pY, params_0, method=\'nelder-mead\', options={\'xtol\': 1e-8, \'disp\': False}) #+\n    return list(res.x) + [sigma_n] # +\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['calc_score', 'calc_hangboard_scores', 'calc_gym_scores', 'calc_climbing_scores', 'calc_arc_scores', 'calc_technique_scores', 'calc_power_scores', 'calc_yoga_scores']\n\nUpdated code:\n```python\nboulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,   \n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n    \n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n    \n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n    \n    # yoga\n    'yoga': boulder_6b / 5,\n    \n    # ARC\n    'arc': boulder_6b_plus / 5,\n    \n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n    \n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%' : boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n    \n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n    \n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n    \n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n    \n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit',]\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+',]\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['correr_randomized_y_mostrar', 'verTiempo']\n\nUpdated code:\n```python\n#Auxiliares para correr randomized search\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\n```"}, {'reason': 'stop', 'result': "Unused functions:\n[]\n\nUpdated code:\n```python\n# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['show_results']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['word2embedding', 'sent2embedding']\n\nUpdated code:\n```python\nimport numpy as np\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['extractPolyFromKML', 'visualizePoly', 'visualizeNodes', 'visualizeLinks', 'nodesPerRegion', 'linksPerRegion', 'getNodes', 'writeToCSV']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['test_diag_fun', 'dydt']\n\nUpdated code:\n```python\nfrom scipy.optimize import fsolve\n\ntsym, dsym, ssym, test_sym = symbols('tsym dsym ssym test_sym')\n\nmodel_test_diag = [\n    tsym - ( ssym + (1 - A - U)*test_sym ),\n    dsym - ( A*ssym*p_true_pos + U*ssym*p_false_pos + (1 - A - U)*test_sym*p_true_pos )\n    ]\n\nsol_test_diag = solve(model_test_diag, tsym, dsym)\ntest_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[tsym])\ndiag_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[dsym])\n\n\n# set up a function to simulate system dynamics when perturbed from steady state\nfrom scipy.integrate import odeint\n\ndef dydt(y, t, parms):\n    return([\n    parms[1]*y[1] + parms[3]*y[2] - (parms[0] + parms[2])*y[0],\n    parms[0]*y[0] - parms[1]*y[1],\n    parms[2]*y[0] - parms[3]*y[2]\n    ])\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'full\']\n\nUpdated code:\n```python\ndef eval_grads(new_weights):\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    variables = TensorFlowVariables(loss, sess)\n    variables.set_weights(new_weights)\n    grads = {}\n    for g, v in grads_and_vars:\n        grads[v.name] = sess.run(g, feed_dict={input_layer : x0.T})\n        # print(str(sess.run(g, feed_dict={input_layer : x0.T})) + " - " + v.name)\n    sess.close()\n    return grads\n\ndef process_weights(w):\n    nw = dict()\n    nw[\'fc1/kernel\'] = w[\'fc1/weights\']\n    nw[\'fc1/bias\'] = w[\'fc1/biases\']\n    nw[\'fc_out/kernel\'] = w[\'fc_out/weights\']\n    nw[\'fc_out/bias\'] = w[\'fc_out/biases\']\n    return nw        \n\ndef flatten_grads(g):\n    arr = np.array([])\n    arr = np.append(arr, g[\'fc1/kernel:0\'])  # weights\n    arr = np.append(arr, g[\'fc1/bias:0\'])\n    arr = np.append(arr, g[\'fc_out/kernel:0\'])  # weights\n    arr = np.append(arr, g[\'fc_out/bias:0\'])\n    return arr\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['nCr']\n\nUpdated code:\n```python\nimport numpy as np\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['getBestParamsSVM']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['gp_plot']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['gp_predictive_distribution']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['showPrecisionRecallPairByLabel']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'padInflunet\', \'getInflunet\']\n\nUpdated code:\n```python\ndef importInflunet(path):\n    \'\'\'\n    Reads the Influnet data and creates a unique multiindex dataframe of the format\n    \n    (year,week) - incidence\n    \n    :param path: location of the influnet folder\n    :return: compacted version of \n    \'\'\'\n    \n    df = pd.concat([pd.read_csv(path+t, names=["time", "incidence"], sep=" ", header=1, usecols=[0,4], decimal=",") for t in listdir(path)], ignore_index=True)\n    df[["year","week"]] = df["time"].str.split("-", expand=True).astype(int)\n    df.drop(["time"], axis=1, inplace=True)\n    df = df.set_index(["year","week"])\n    df.sortlevel(inplace=True)\n    df = df.astype(float)\n    df = df.loc[2008:]\n    return df\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['update_nueron']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef backward_pass(x, y, W, b):\n    # Responsável por calcular as derivadas parciais para W e b.\n    # Funciona com batches também, nesse caso retornando a média\n    # dos gradientes.\n    y_, z = forward_pass(x, W, b)\n    dLdb = (-y/(y_ + 1e-8) + (1-y)/(1-y_ + 1e-8)) * sigmoid(z, True)\n    dLdW = np.zeros((dLdb.shape[0], 31))\n    for i, el in enumerate(dLdb):\n        dLdW[i] = x[i] * el\n    dLdW = np.mean(dLdW, axis=0)\n    dLdb = np.mean(dLdb, axis=0)\n    return dLdW, dLdb\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['plot_fit']\n\nUpdated code:\n```python\n# robust pca\nfrom __future__ import division, print_function\n\nimport numpy as np\n\ntry:\n    from pylab import plt\nexcept ImportError:\n    print('Unable to import pylab. R_pca.plot_fit() will not work.')\n\ntry:\n    # Python 2: 'xrange' is the iterative version\n    range = xrange\nexcept NameError:\n    # Python 3: 'range' is iterative - no need for 'xrange'\n    pass\n\n\nclass R_pca:\n\n    def __init__(self, D, mu=None, lmbda=None):\n        self.D = D\n        self.S = np.zeros(self.D.shape)\n        self.Y = np.zeros(self.D.shape)\n\n        if mu:\n            self.mu = mu\n        else:\n            self.mu = np.prod(self.D.shape) / (4 * self.norm_p(self.D, 2))\n\n        self.mu_inv = 1 / self.mu\n\n        if lmbda:\n            self.lmbda = lmbda\n        else:\n            self.lmbda = 1 / np.sqrt(np.max(self.D.shape))\n\n    @staticmethod\n    def norm_p(M, p):\n        return np.sum(np.power(M, p))\n\n    @staticmethod\n    def shrink(M, tau):\n        return np.sign(M) * np.maximum((np.abs(M) - tau), np.zeros(M.shape))\n\n    def svd_threshold(self, M, tau):\n        U, S, V = np.linalg.svd(M, full_matrices=False)\n        return np.dot(U, np.dot(np.diag(self.shrink(S, tau)), V))\n\n    def fit(self, tol=None, max_iter=1000, iter_print=100):\n        iter = 0\n        err = np.Inf\n        Sk = self.S\n        Yk = self.Y\n        Lk = np.zeros(self.D.shape)\n\n        if tol:\n            _tol = tol\n        else:\n            _tol = 1E-7 * self.norm_p(np.abs(self.D), 2)\n\n        while (err > _tol) and iter < max_iter:\n            Lk = self.svd_threshold(\n                self.D - Sk + self.mu_inv * Yk, self.mu_inv)\n            Sk = self.shrink(\n                self.D - Lk + (self.mu_inv * Yk), self.mu_inv * self.lmbda)\n            Yk = Yk + self.mu * (self.D - Lk - Sk)\n            err = self.norm_p(np.abs(self.D - Lk - Sk), 2)\n            iter += 1\n            if (iter % iter_print) == 0 or iter == 1 or iter > max_iter or err <= _tol:\n                print('iteration: {0}, error: {1}'.format(iter, err))\n\n        self.L = Lk\n        self.S = Sk\n        return Lk, Sk\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'test_set5\', \'test_set14\', \'test_bsd100\', \'_test_loop\']\n\nUpdated code:\n```python\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport sys\nsys.path.append("..")\n\n#import models\nfrom loss import PSNRLoss, psnr\n\nimport os\nimport time\nimport numpy as np\nfrom imageio import imwrite as imsave\nfrom scipy.misc import imresize\nfrom scipy.ndimage.filters import gaussian_filter\n\nbase_weights_path = "weights/"\nbase_val_images_path = "val_images/"\nbase_test_images = "test_images/"\n\nset5_path = "tests/set5"\nset14_path = "tests/set14"\nbsd100_path = "tests/bsd100"\n\nif not os.path.exists(base_weights_path):\n    os.makedirs(base_weights_path)\n\nif not os.path.exists(base_val_images_path):\n    os.makedirs(base_val_images_path)\n\nif not os.path.exists(base_test_images):\n    os.makedirs(base_test_images)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'buildIncomesExpenses\']\n\nUpdated code:\n```python\ndata = []\n\nfor bank in bankdata[CURRENT]:\n    dates = bankdata[CURRENT][bank][\'date\']\n    movements = bankdata[CURRENT][bank][\'movements\']\n\n    incomes = {}\n    expenses = {}\n    for date, movement in zip(dates, movements):\n        key = str(date.month) + \'/\' + str(date.year)\n\n        if float(movement) > 0:\n            if key in incomes:\n                incomes[key] += float(movement)\n            else:\n                incomes[key] = float(movement)\n        else:\n            if key in expenses:\n                expenses[key] += float(movement)\n            else:\n                expenses[key] = float(movement)\n        \n    months_x = []\n    incomes_y = []\n    for key, value in incomes.items():\n        months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n        incomes_y.append(value)\n    \n    trace = go.Bar(\n        x = months_x,\n        y = incomes_y,\n        name = "Incomes for {}".format(SUPPORTED_BANKS[bank])\n    )\n    data.append(trace)\n    \n    months_x = []\n    expenses_y = []\n    for key, value in expenses.items():\n        months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n        expenses_y.append(value)\n    \n    trace = go.Bar(\n        x = months_x,\n        y = expenses_y,\n        name = "Expenses for {}".format(SUPPORTED_BANKS[bank])\n    )\n    data.append(trace)\n    \ndata\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef precision(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n    tp = tf.diag_part(conf_matrix)\n    precision_scores = tp/(tp_and_fp)\n    if weights:\n        precision_score = tf.multiply(precision_scores, weights)/tf.reduce_sum(weights)\n    else:\n        precision_score = tf.reduce_mean(precision_scores)        \n    return precision_score\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef accuracy(predictions, labels):\n    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n            / predictions.shape[0])\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['get_data_from_tsv']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['get_neighbour_messages']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['show_history_graph']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['NRM1', 'SCL1', 'TRSH']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['print_policy']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['delta_f']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['get_fpy_az']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['paths_to_tensor']\n\nUpdated code:\n```python\nfrom keras.preprocessing import image                  \n\ndef path_to_tensor(img_path, height=224, width=224):\n    ''' Loads RGB image as PIL.Image.Image type of given Height x Width dimensions\n    '''\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(height, width))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['load_coords']\n\nUpdated code:\n```python\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\n\nfrom astropy.table import Table\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['create_anottations', 'draw_anottation']\n\nUpdated code:\n```python\nfrom IPython.display import clear_output\n\ndef save_annotations(adict):\n    timestamp = datetime.datetime.today().strftime('%Y%m%d%H%M')\n    filename = 'annotations_' + timestamp + '.csv'\n    print(filename)\n    with open(os.path.join('.', filename), 'w', newline='') as out:\n        writer = csv.writer(out)\n        for key, value in adict.items():\n            line = [key, *value]\n            writer.writerow(line)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['bow_lights_gt_50m']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['evaluate']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'clean_data\']\n\nUpdated code:\n```python\ncolumn_filter = [\'ISIN\', \'Mnemonic\', \'SecurityDesc\', \'SecurityType\', \'Currency\', \'SecurityID\', \'Date\', \'Time\', \'StartPrice\', \'MaxPrice\', \'MinPrice\', \'EndPrice\', \'TradedVolume\', \'NumberOfTrades\']\nn_df = df[column_filter]\nif not inplace:\n    n_df = df.copy ()\n\nn_df.drop (n_df.Time == \'Time\', inplace = True)\n# we want the dates to be comparable to datetime.strptime()\nn_df["CalcTime"] = pd.to_datetime("1900-01-01 " + n_df["Time"], errors=\'coerce\')\nn_df["CalcDateTime"] = pd.to_datetime(n_df["Date"] + " " + n_df["Time"], errors=\'coerce\')\n\n# Filter common stock\n# Filter between trading hours 08:00 and 20:00\n# Exclude auctions (those are with TradeVolume == 0)\nonly_common_stock = n_df[n_df.SecurityType == \'Common stock\']\ntime_fmt = "%H:%M"\nopening_hours_str = "08:00"\nclosing_hours_str = "20:00"\nopening_hours = datetime.strptime(opening_hours_str, time_fmt)\nclosing_hours = datetime.strptime(closing_hours_str, time_fmt)\n\ncleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n                  (only_common_stock.CalcTime >= opening_hours) & \\\n                  (only_common_stock.CalcTime <= closing_hours)]\n\nbymnemonic = cleaned_common_stock[[\'Mnemonic\', \'TradedVolume\']].groupby([\'Mnemonic\']).sum()\nnumber_of_stocks = 100\ntop = bymnemonic.sort_values([\'TradedVolume\'], ascending=[0]).head(number_of_stocks)\ntop_k_stocks = list(top.index.values)\ncleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\nsorted_by_index = cleaned_common_stock.set_index([\'Mnemonic\', \'CalcDateTime\']).sort_index()\nnon_empty_days = sorted(list(cleaned_common_stock[\'Date\'].unique()))\nnew_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)["OrganizedDateTime"].values\n\nstocks = []\nfor stock in top_k_stocks:\n    stock = basic_stock_features(sorted_by_index, stock, new_datetime_index, inplace=True)\n    stocks.append(stock)\n# prepared should contain the numeric features for all top k stocks,\n# for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n# for all minutes from 08:00 until 20:00\n# in minutes without trades the prices from the last available minute are carried forward\n# trades are filled with zero for such minutes\n# a new column called HasTrade is introduced to denote the presence of trades\nprepared = pd.concat(stocks, axis=0).dropna(how=\'any\')\nprepared.Mnemonic = prepared.Mnemonic.astype(\'category\')\nreturn prepared\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['timeseries_to_supervised']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['read_table']\n\nUpdated code:\n```python\nimport pandas as pd\n\ndef read_table(file_name):\n    '''\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    '''\n    # read the data into a data frame (or table)   \n    df = pd.read_csv(file_name, delim_whitespace=True)\n    \n    # to avoid frustrations, set explicitly the data types of each column\n    df = df.astype({'A':int,'Element':str,'Z':int,'N':int,'T1/2-(seconds)':float},copy=False)\n    print(df.dtypes)\n\n    return df\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef shuffle_batch(X, y, batch_size):\n    rnd_idx = np.random.permutation(len(X))\n    n_batches = len(X) // batch_size\n    for batch_idx in np.array_split(rnd_idx, n_batches):\n        X_batch, y_batch = X[batch_idx], y[batch_idx]\n        yield X_batch, y_batch\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['rv_plot2']\n\nUpdated code:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef rv_plot2(r1, v1, r2, v2, t0, t1, epsilon=500, title1='', title2='', redrange=None):\n    #0. r1とv2はそれぞれ対応する膝屈曲角とそれの角速度, t0とt1は第１と第２の極小値近傍時刻, itimeオプションは後のため\n    ti = t0 - epsilon\n    tf = t1 + epsilon\n    tm = ti + (tf - ti) // 2    #tm は ti ~ tf の中間インデックス\n    xi = np.where(r1==np.min(r1[ti:tm]))[0][0]    #xi は ti ~ tm までの r の最小値のインデックス\n    xf = np.where(r1==np.min(r1[tm:tf]))[0][0]    #xf は tm ~ tf までの r の最小値のインデックス\n    xm = xi + (xf - xi) // 2    #xm は xi ~ xf の中間インデックス\n    \n    #1. 膝屈曲角プロット\n    plt.subplot(1, 2, 1)\n    plt.plot(r1[xi:xm], v1[xi:xm], c='black')\n    plt.plot(r1[xm:xf], v1[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r1[redrange[0]:redrange[1]], v1[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた\n    plt.title(f'{title1}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()\n    \n    #2. 大腿(下腿)傾き角プロット\n    plt.subplot(1, 2, 2)\n    plt.plot(r2[xi:xm], v2[xi:xm], c='black')\n    plt.plot(r2[xm:xf], v2[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r2[redrange[0]:redrange[1]], v2[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた       \n    plt.title(f'{title2}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['LeNet6']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['stick_breaking']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['fit_model']\n\nUpdated code:\n```python\n# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n\n# TODO: Create a decision tree regressor object\nregressor = None\n\n# TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\nparams = {}\n\n# TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \nscoring_fnc = None\n\n# TODO: Create the grid search cv object --> GridSearchCV()\n# Make sure to include the right parameters in the object:\n# (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\ngrid = None\n\n# Fit the grid search object to the data to compute the optimal model\ngrid = grid.fit(X, y)\n\n# Return the optimal model after fitting the data\nreturn grid.best_estimator_\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['basic_stock_features']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['rover_coords', 'to_polar_coords', 'rotate_pix', 'translate_pix', 'pix_to_world']\n\nUpdated code:\n```python\n# Grab another random image\nidx = np.random.randint(0, len(img_list)-1)\nimage = mpimg.imread(img_list[idx])\nwarped, mask = perspect_transform(image, source, destination)\nthreshed = color_thresh(warped)\n\n# Calculate pixel values in rover-centric coords and distance/angle to all pixels\nxpix, ypix = rover_coords(threshed)\ndist, angles = to_polar_coords(xpix, ypix)\nmean_dir = np.mean(angles)\n\n# Do some plotting\nfig = plt.figure(figsize=(12,9))\nplt.subplot(221)\nplt.imshow(image)\nplt.subplot(222)\nplt.imshow(warped)\nplt.subplot(223)\nplt.imshow(threshed, cmap='gray')\nplt.subplot(224)\nplt.plot(xpix, ypix, '.')\nplt.ylim(-160, 160)\nplt.xlim(0, 160)\narrow_length = 100\nx_arrow = arrow_length * np.cos(mean_dir)\ny_arrow = arrow_length * np.sin(mean_dir)\nplt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['poisson_approx', 'gaussian']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'set_trainable\', \'get_generator_output\']\n\nUpdated code:\n```python\nclass GenerativeNetwork:\n\n    def __init__(self, img_width=96, img_height=96, batch_size=16, num_upscales=2, small_model=False,\n                 content_weight=1, tv_weight=2e5, gen_channels=64):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.batch_size = batch_size\n        self.small_model = small_model\n        self.num_scales = num_upscales\n\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n\n        self.filters = gen_channels\n        self.mode = 2\n        self.init = \'glorot_uniform\'\n\n        self.sr_res_layers = None\n        self.sr_weights_path = "weights/SRGAN.h5"\n\n        self.output_func = None\n\n    def create_sr_model(self, ip):\n\n        x = Conv2D(self.filters, (5, 5), activation=\'linear\', padding=\'same\', name=\'sr_res_conv1\',\n                          kernel_initializer=self.init)(ip)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_1\')(x)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_lr1\')(x)\n\n        x = Conv2D(self.filters, (5, 5), activation=\'linear\', padding=\'same\', name=\'sr_res_conv2\', kernel_initializer="glorot_uniform")(x)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_2\')(x)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_lr2\')(x)\n\n        num_residual = 5 if self.small_model else 15\n\n        for i in range(num_residual):\n            x = self._residual_block(x, i + 1)\n\n        for scale in range(self.num_scales):\n            x = self._upscale_block(x, scale + 1)\n    \n        scale = 2 ** self.num_scales\n        tv_regularizer = TVRegularizer(img_width=self.img_width * scale, img_height=self.img_height * scale,\n                                       weight=self.tv_weight) #self.tv_weight)\n        \n        x = Conv2D(3, (5, 5), activation=\'tanh\', padding=\'same\', activity_regularizer=tv_regularizer, \n                   name=\'sr_res_conv_final\', kernel_initializer=self.init)(x)\n        \n        x = Denormalize(name=\'sr_res_conv_denorm\')(x)\n        return x\n\n    def _residual_block(self, ip, id):\n        init = ip\n\n        x = Conv2D(self.filters, (3, 3), activation=\'linear\', padding=\'same\', name=\'sr_res_conv_\' + str(id) + \'_1\',\n                          kernel_initializer=self.init)(ip)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_\' + str(id) + \'_1\')(x)\n        x = LeakyReLU(alpha=0.25, name="sr_res_activation_" + str(id) + "_1")(x)\n\n        x = Conv2D(self.filters, (3, 3), activation=\'linear\', padding=\'same\', name=\'sr_res_conv_\' + str(id) + \'_2\',\n                          kernel_initializer=self.init)(x)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_\' + str(id) + \'_2\')(x)\n\n        m = add([x, init],name="sr_res_merge_" + str(id))\n\n        return m\n\n    def _upscale_block(self, ip, id):\n        \'\'\'\n        As per suggestion from http://distill.pub/2016/deconv-checkerboard/, I am swapping out\n        SubPixelConvolution to simple Nearest Neighbour Upsampling\n        \'\'\'\n        init = ip\n        \n        x = Conv2D(128, (3, 3), activation="linear", padding=\'same\', name=\'sr_res_upconv1_%d\' % id,\n                          kernel_initializer=self.init)(init)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_up_lr_%d_1_1\' % id)(x)\n        x = UpSampling2D(name=\'sr_res_upscale_%d\' % id)(x)\n        #x = SubPixelUpscaling(r=2, channels=32)(x)\n        x = Conv2D(128, (3, 3), activation="linear", padding=\'same\', name=\'sr_res_filter1_%d\' % id,\n                          kernel_initializer=self.init)(x)\n        x = LeakyReLU(alpha=0.3, name=\'sr_res_up_lr_%d_1_2\' % id)(x)\n\n        return x\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['qda_accracy']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'compare_phot_specphot\']\n\nUpdated code:\n```python\nfrom scipy.integrate import simps\n\ndef calc_spectrum_filter_flux(filter_name, SpecClass):\n    filter_object = pcc.functions.load_filter("/Users/berto/Code/CoCo/data/filters/" + filter_name + ".dat")\n    filter_object.resample_response(new_wavelength = SpecClass.wavelength)\n    filter_area = simps(filter_object.throughput, filter_object.wavelength)\n    \n    transmitted_spec = filter_object.throughput * SpecClass.flux\n\n    integrated_flux = simps(transmitted_spec, SpecClass.wavelength)\n    \n    return  integrated_flux/filter_area\n\ndef calc_specphot(sn, filtername):\n\n    specphot = np.array([])\n    specepoch = np.array([])\n\n    for spec in sn.mangledspec:\n        specphot = np.append(specphot, calc_spectrum_filter_flux(filtername, sn.mangledspec[spec]))\n        specepoch = np.append(specepoch, sn.mangledspec[spec].mjd_obs)\n    \n    return specepoch, specphot\n\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['processCSV']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['stern_light']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['get_augmented_images', 'get_augmented_images_by_class']\n\nUpdated code:\n```python\n\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'daterange\']\n\nUpdated code:\n```python\nimport datetime as dt\nimport plotly.graph_objs as go\n\ndef get_trace_sum_balances(bankdata):\n    sum_balances = []\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        balances = bankdata[CURRENT][bank][\'balance\']\n        sum_account = {}\n        for date, balance in zip(dates, balances):\n            sum_account[date] = balance\n                \n        sum_balances.append(sum_account)\n    \n    total = {}\n    (ini, fin) = getIntervalDates(bankdata)\n    last = 0\n    max_amount = 0\n    for b in sum_balances:\n        for d in (ini + dt.timedelta(days=i) for i in range((fin - ini).days + 1)):\n            if d in b:\n                last = b[d]                    \n                if d in total:\n                    total[d] += b[d]\n                    if total[d] > max_amount:\n                        max_amount = total[d]\n                else:\n                    total[d] = b[d]\n            else:\n                if d in total:\n                    total[d] += last\n                else:\n                    total[d] = last\n                \n    \n    dates = total.keys()\n    balances = total.values()\n    \n    (dates, balances) = zip(*sorted(zip(dates, balances)))\n    \n    trace = go.Scatter(\n        x = dates,\n        y = balances,\n        name = "All Accounts - Amount: " + format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".") + CURRENCY,\n        mode = \'lines\',\n        line = dict ( width = 4 )\n    )\n\n    return (max_amount, trace)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'Error_CM\']\n\nUpdated code:\n```python\nimport pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv(\'data/exoplanet.eu_catalog.csv\', \n                      usecols=[\'mass\',\'mass_error_min\',\'mass_error_max\',\n                               \'semi_major_axis\',\'semi_major_axis_error_min\',\'semi_major_axis_error_max\',\'star_name\'])\n\nclass System:\n    def __init__(self, data):\n        self.data=data\n        self.system = list(self.data.groupby("star_name").groups.keys())\n        self.Number()\n        self.Mass()\n        self.CenterOfMass()\n        \n    def Number(self):\n        sys = self.data.groupby("star_name")\n        self.N_total = len(sys["mass"])\n        \n    def Mass(self):\n        sys = self.data.groupby("star_name")\n        self.M_total = sys["mass"].sum()\n    \n    def CenterOfMass(self):\n        self.rm_i = self.data["mass"].multiply(self.data["semi_major_axis"])\n        self.data_i = self.data.assign(CM_i = self.rm_i.values) \n        p_system = self.data_i.groupby("star_name")\n        sum_rm = p_system[\'CM_i\'].sum()\n        self.CM = sum_rm.divide(self.M_total)   \n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nclass Point:\n    """\n    Simple class for representing a point in a Cartesian coordinate system.\n    """\n    \n    def __init__(self, x, y):\n        """\n        Create a new Point at x, y.\n        """\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        """\n        Translate the point by dx and dy in the x and y direction.\n        """\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return("Point at [%f, %f]" % (self.x, self.y))\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['make_export_table']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['outputFeatureMap']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['runLogitL1']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'make_document\']\n\nUpdated code:\n```python\nimport random\n\ndef update():\n    new = {\'x\': [random.random()],\n           \'y\': [random.random()],\n           \'color\': [random.choice([\'red\', \'blue\', \'green\'])]}\n    source.stream(new)\n\ndoc.add_periodic_callback(update, 100)\n\nfig = figure(title=\'Streaming Circle Plot!\', sizing_mode=\'scale_width\',\n             x_range=[0, 1], y_range=[0, 1])\nfig.circle(source=source, x=\'x\', y=\'y\', color=\'color\', size=10)\n\ndoc.title = "Now with live updating!"\ndoc.add_root(fig)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nclass FunctionRunner(Runner):\n    def __init__(self, function):\n        """Initialize.  `function` is a function to be executed"""\n        self.function = function\n\n    def run_function(self, inp):\n        return self.function(inp)\n\n    def run(self, inp):\n        try:\n            result = self.run_function(inp)\n            outcome = self.PASS\n        except Exception:\n            result = None\n            outcome = self.FAIL\n\n        return result, outcome\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef replace_unknown(sequence):\n    """Return a copy of the input sequence where each unknown word is replaced\n    by the literal string value \'nan\'. Pomegranate will ignore these values\n    during computation.\n    """\n    return [w if w in data.training_set.vocab else \'nan\' for w in sequence]\n\ndef simplify_decoding(X, model):\n    """X should be a 1-D sequence of observations for the model to predict"""\n    _, state_path = model.viterbi(replace_unknown(X))\n    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['fit_lstm']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'write_data\', \'select_features\']\n\nUpdated code:\n```python\nclass Preprocessor:\n    \n    def __init__(self, train_data_file, train_label_file, train_ids_file,\n                 instr_file, test_data_file=None, test_ids_file=None):\n        """A class to process and reformat data\n        for use in learning models"""\n        \n        # initialize the data the data filenames\n        self.train_data_file = train_data_file\n        self.train_label_file = train_label_file\n        self.train_ids_file = train_ids_file\n        self.instr_file = instr_file\n        \n        # test data is optional\n        self.test_data_file = test_data_file\n        self.test_ids_file = test_ids_file\n        \n    def read_data(self):\n        """Reads in data from the files passed to constructor"""\n        \n        # read in the data\n        train_X_df = pd.read_csv(self.train_data_file)\n        train_y_df = pd.read_csv(self.train_label_file)\n        train_ids_df = pd.read_csv(self.train_ids_file)\n        self.instr_df = pd.read_csv(self.instr_file)\n        \n        self.feature_names = [feature for feature in train_X_df]\n        self.original_feature_names = [feature for feature in train_X_df]\n        self.label_names = [feature for feature in train_y_df]\n        self.id_names = [feature for feature in train_ids_df]\n        \n        # create cross validation data\n        self.cv_X_df = pd.DataFrame(train_X_df)\n        self.cv_y_df = pd.DataFrame(train_y_df)\n        self.cv_ids_df = pd.DataFrame(train_ids_df)\n        \n        # read in the test data if it exists\n        if self.test_data_file != None:\n            self.test_X_df = pd.read_csv(self.test_data_file)\n            self.test_ids_df = pd.read_csv(self.test_ids_file)\n            self.all_X_df = train_X_df.append(self.test_X_df)\n        else:\n            self.test_X_df = None\n            self.test_ids_df = None\n            self.all_X_df = pd.DataFrame(train_X_df)\n        \n        # determine the shape of the input data\n        self.train_X_shape = train_X_df.shape\n        self.train_y_shape = train_y_df.shape\n        self.train_ids_shape = train_ids_df.shape\n        self.instr_shape = self.instr_df.shape\n        self.all_shape = self.all_X_df.shape\n        \n        # get size of test data if it exists\n        if self.test_data_file != None:\n            self.test_X_shape = self.test_X_df.shape\n            self.test_ids_shape = self.test_ids_df.shape\n        else:\n            self.test_X_shape = None\n            self.test_ids_shape = None\n\n        \n    def process(self, shuffle_train_data=False):\n        """Performs the processing on cross validation and train/test data"""\n        \n        # ADD OPTION TO SHUFFLE DATA HERE\n        \n        # processing on all data - remember to include cv_X and all_X for each condition\n        for col in self.original_feature_names:\n            print(col)\n            \n            # determine what to perform at each of the steps\n            col_instr = self.instr_df[col].values\n            col_enc = col_instr[1]\n            col_scl = col_instr[2]\n            col_imp = col_instr[3]\n\n            # impute values\n            # imputed first so that other functions will not use nan values in calculations\n            if col_imp == \'UNIQ\':\n                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n            if col_imp == \'MEAN\':\n                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n                self.all_X_df[col] = MEAN(self.all_X_df[col])\n            if col_imp == \'MODE\':\n                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n                self.all_X_df[col] = MODE(self.all_X_df[col])\n            if col_imp == \'MED\':\n                self.cv_X_df[col] = MED(self.cv_X_df[col])\n                self.all_X_df[col] = MED(self.all_X_df[col])\n            if is_int(col_imp):\n                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n            if col_imp == \'DEL\':\n                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n            \n            \n            # perform encoding of data\n            if col_enc == \'MAP\':\n                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n                self.all_X_df[col] = MAP(self.all_X_df[col])\n            if col_enc == \'OHE\':\n                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n                    feature_names=self.feature_names)\n            if col_enc == \'LOO\':\n                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n                self.all_X_df[col] = LOO(self.all_X_df[col])\n            \n\n            # perform scaling\n            if col_scl == \'NRM1\':\n                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n                self.all_X_df[col] = NRM1(self.all_X_df[col])\n            if col_scl == \'SCL1\':\n                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n                self.all_X_df[col] = SCL1(self.all_X_df[col])\n            if col_scl == \'TRSH\':\n                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n                self.all_X_df[col] = TRSH(self.all_X_df[col])\n\n        \n        # get the values from the dataframes\n        self.cv_X = self.cv_X_df.values\n        self.cv_y = self.cv_y_df.values\n        self.cv_ids = self.cv_ids_df.values\n        \n        all_X = self.all_X_df.values\n        self.train_X = all_X[:self.train_X_shape[0], :]\n        self.train_y = self.cv_y_df.values\n        self.train_ids = self.cv_ids_df.values\n        \n        if self.test_data_file != None:\n            self.test_X = all_X[self.train_X_shape[0]:, :]\n            self.test_ids = self.test_ids_df.values\n        else:\n            self.test_X = None\n            self.test_ids = None'}, {'reason': 'stop', 'result': "Unused functions:\n['vm']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['add_degrees_isotypic', 'add_degrees_symmetric', 'add_degrees_test']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'get_df_cols\', \'print_top_n_arrival_airport\']\n\nUpdated code:\n```python\nimport pandas as pd\n\ndef get_name(IATA_code):\n    """\n    Function to return the name of the airport linked to IATA_code\n    \n    @IATA_code : String object which is a IATA_code\n   \n    @return    : String object which is the name of the airport\n    """\n  \n    #If IATE CODE exists in GEO_O\n    try:\n        result = GEO_O.get(IATA_code.replace(" ",""), \'name\')\n    #Else we just specify that we cannot found the IATA CODE\n    except KeyError as e:\n        result = "NOT FOUND IATA CODE"\n   \n    return result\n  \n\ndef get_airports_arrival_sorted(dataframe):\n    """\n    Method to print the get arrivals airports in 2013 from searches file\n  \n    @dataframe : the dataframe containing the data\n  \n    @return    : a new dataframe\n    """\n  \n    #Created dataframe grouped by \'arr_port\' aggregated by sum\n    result_dataframe = dataframe.groupby([\'arr_port\']).sum()\n    #Sorted the result in a descending way\n    result_dataframe = result_dataframe.sort_values(by=[\'pax\'], ascending=False)\n  \n    return result_dataframe\n\n\ndef add_airports_name(dataframe):\n    """\n    Method to add a column in a dataframe containing the full name of airports\n    thanks to the IATA CODE\n  \n    @dataframe : the dataframe to modify\n  \n    @return    : the dataframe modified\n    """\n  \n    #Reset the index of the dataframe in order to apply a lambda method\n    dataframe = dataframe.reset_index()\n  \n    #Add the column and its values\n    dataframe[\'airport_name\'] = dataframe[\'arr_port\'].apply(lambda x: get_name(x))\n\n    return dataframe\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['return_itself']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['evaluate']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef featurize_state(state):\n    """\n    Returns the featurized representation for a state.\n    """\n    scaled = scaler.transform([state])\n    featurized = featurizer.transform(scaled)\n    return featurized[0]\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['logprob', 'sample_distribution', 'sample', 'random_distribution']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['is_valid_url']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['build_index']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['error']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['create_model']\n\nUpdated code:\n```python\n# Configure model presets\nfrom examples.seismic import demo_model\n\n# Enable model presets here:\npreset = 'layers-isotropic'  # A simple but cheap model (recommended)\n# preset = 'marmousi2d-isotropic'  # A larger more realistic model\n\n# Standard preset with a simple two-layer model\nif preset == 'layers-isotropic':\n    filter_sigma = (1, 1)\n    nshots = 21\n    nreceivers = 101\n    t0 = 0.\n    tn = 1000.  # Simulation last 1 second (1000 ms)\n    f0 = 0.010  # Source peak frequency is 10Hz (0.010 kHz)\n\n\n# A more computationally demanding preset based on the 2D Marmousi model\nif preset == 'marmousi2d-isotropic':\n    filter_sigma = (6, 6)\n    nshots = 301  # Need good covergae in shots, one every two grid points\n    nreceivers = 601  # One recevier every grid point\n    t0 = 0.\n    tn = 3500.  # Simulation last 3.5 second (3500 ms)\n    f0 = 0.025  # Source peak frequency is 25Hz (0.025 kHz)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['ode']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef window_bounds(window, affine):\n    (row_start, row_stop), (col_start, col_stop) = window\n    w, s = (col_start, row_stop) * affine\n    e, n = (col_stop, row_start) * affine\n    return w, s, e, n\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['calc_other_neighbour_msg_prod']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n[]\n\nUpdated code:\n```python\ndef neuron(x, w):\n    '''Simple McCulloch-Pitts Neuron model, x and w and are numpy arrays of the same shape'''\n    return np.dot(x, w) > 0\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['get_wide_deep']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['fill_nulls']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['grayscale', 'canny', 'gaussian_blur', 'region_of_interest', 'draw_lines', 'hough_lines', 'weighted_img']\n\nUpdated code:\n```python\nimport math\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['view_samples']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'value\', \'delta\']\n\nUpdated code:\n```python\nclass EuropeanOption(object):\n    """ Abstract Class for European options. Partially implemented.\n    S0 : float : initial stock/index level\n    strike : float : strike price\n    T : float : time to maturity (in year fractions)\n    r : float : constant risk-free short rate\n    div :    float : dividend yield\n    sigma :  float : volatility factor in diffusion term\n    model: str: name of the model for the pricing"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, option_type, S0, strike, T, r, div, sigma, model):\n        try:\n            self.option_type = option_type\n            assert isinstance(option_type, str)\n            self.S0 = float(S0)\n            self.strike = float(strike)\n            self.T = float(T)\n            self.r = float(r)\n            self.div = float(div)\n            self.sigma = float(sigma)\n            self.model = str(model)\n        except ValueError:\n            print(\'Error passing Options parameters\')\n\n        models = [\'BlackScholes\', \'MonteCarlo\', \n                  \'BinomialTree\', \'TrinomialTree\',\n                  \'FFT\', \'PDE\']\n        \n        if model not in models:\n            raise Exception(\'Error: Model unknown\')\n            \n        option_types = [\'call\', \'put\']\n        \n        if option_type not in option_types:\n            raise ValueError("Error: Option type not valid. Enter \'call\' or \'put\'")\n        if S0 < 0 or strike < 0 or T <= 0 or r < 0 or div < 0 or sigma < 0:\n            raise ValueError(\'Error: Negative inputs not allowed\')\n            \n        self.discount = np.exp(-self.r * self.T)\n\n    def getmodel(self):\n        return self.model\n\n    def __str__(self):\n        return "This European Option is priced using {0}".format(self.getmodel())\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['make_input_fn']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['insert_and_time_filter_cuckoo_filter']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'setSize\', \'fit\', \'getSelectedNodes\', \'selectNodes\', \'clearSelection\', \'msg_arrived\', \'dispatch\', \'getResponse\']\n\nUpdated code:\n```python\nimport ipywidgets as widgets\nimport json\nimport time\nfrom threading import Thread\nfrom traitlets import Int, Unicode, Tuple, CInt, Dict, validate, observe\n\n\nclass newcyjsWidget(widgets.DOMWidget):\n    \n    _view_name = Unicode(\'CyjsView\').tag(sync=True)\n    _view_module = Unicode(\'cyjs\').tag(sync=True)\n    frameWidth = Int(400).tag(sync=True)\n    frameHeight = Int(300).tag(sync=True)\n    msgFromKernel = Unicode("{}").tag(sync=True)\n    msgToKernel = Unicode("{}").tag(sync=True)\n    status = "initial status message\\n"\n    selectedNodes = [];\n    incomingMessageArrivedAndParsed = False;\n    globalStatus = "blank"\n\n    #------------------------------------------------------------------------------\n    class MyThread(Thread):\n       owner = None\n       def __init__(self, owner, group=None, target=None, name=None,\n                    args=(), kwargs=None, *, daemon=None):\n          Thread.__init__(self, group, target, name, args, kwargs, daemon=daemon)\n          self.owner = owner\n\n       def run(self):\n          maxLoops = 5\n          counter = 0\n          while((self.owner.incomingMessageArrivedAndParsed == False) and (counter < maxLoops)):\n             counter += 1\n             print("thread, no message yet, sleeping, loop %d" % counter)\n             time.sleep(1.0)\n          self.owner.status += "thread owner\'s selectedNodes: %s\\n" % self.owner.selectedNodes;\n          self.owner.status += "MyThread ending loop\\n";\n\n       def result(self):\n          #while(not self.owner.incomingMessageArrivedAndParsed):\n          #   time.sleep(0.1)\n          return("imaap? %s" % self.owner.incomingMessageArrivedAndParsed)\n            \n    #------------------------------------------------------------------------------\n\n    def testThread(self):\n      for i in range(4):\n         threadName = "Thread-%s" % (i + 1)\n         mythread = self.MyThread(name=threadName, owner=self)\n         mythread.start()\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['term_output']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['getWiki']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['ResNet50_predict_labels']\n\nUpdated code:\n```python\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['inverse_difference']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['getBestParams']\n\nUpdated code:\n```python\n#\n# Logistic Regression\n#\nlr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\nparams = {'C': [0.1, 0.25,  0.5,]}\nlogit = GridSearchCV(lr, params, cv=5,\n                     scoring='accuracy', return_train_score=True)\n\n# Fit  training data\nlogit.fit(train_data, train_labels)  \n# Show the best C parameter to use and the expected accuracy\nprint('\\nLogistic Regression Classifier, L2 regularization')\nprint(' Best param:', logit.best_params_)\nprint(' Accuracy:  ', np.round(logit.best_score_, 4) )\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['convert_column_string_encoding', 'get_mjdmax_BessellV']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef window_bounds(window, affine):\n    (row_start, row_stop), (col_start, col_stop) = window\n    w, s = (col_start, row_stop) * affine\n    e, n = (col_stop, row_start) * affine\n    return w, s, e, n\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['forecast']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['func1']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['get_nuclides']\n\nUpdated code:\n```python\n'''Function for creating a nuclide container'''\n\ndef get_nuclides( df ):\n    '''\n    Create a dictionary of nuclide name key and named tuple value\n    Parameters\n    ----------\n    df: pandas data frame, required\n        Table of data for nuclides.\n    \n    Returns\n    -------\n    nuclides: dict(name:namedtuple)\n        Dictionary of name key and namedtuple values. Tuple names: element_name, symbol, Z, A, radius, unc.\n\n    Examples\n    --------\n    '''\n    \n    nuclides = dict()\n\n    # design a container data structure\n    from collections import namedtuple\n    Nuclide = namedtuple('Nuclide', ['element_name','symbol','Z','A','half_life'])\n\n    # fill in the list of containers\n    misses = 0 # counter of nuclides without radius data\n    a_max = 0  # maximum A number with radius data present\n    z_max = 0  # maximum Z number with radius data present\n    t_max = 0\n\n    import pandas as pd\n    \n    # Use the Mendeleev python package (periodic table of elements)\n    from mendeleev import element\n    \n    # if df has duplicates, this loop will take care of it\n    for row in df.itertuples(index=False):\n\n        a = int(row[0])\n        symbol = row[1]\n        z = int(row[2])\n        t = row[4]\n        if pd.isnull(t): # missing half-life\n            misses += 1\n            continue\n        if t/3600/24/365 > 50e+6: # remove anything with more than 50 My\n            continue\n\n        a_max = max(a,a_max)\n        z_max = max(z,z_max)\n        \n        name = symbol+'-'+str(a)\n        t_max = max(t,t_max)\n        nuc = Nuclide( element_name=element(z).name, symbol=symbol, Z=z, A=a, half_life=t )\n        \n        if name in nuclides.keys():\n            half_life = nuclides[name].half_life\n            if half_life == t:\n                continue\n            else:\n                name = name+'m'\n        \n        nuclides[name]=nuc\n\n    print('Number of nuclides with    t_1/2 data = ',len(nuclides))\n    print('Number of nuclides without t_1/2 data = ',misses)\n    print('')\n    print('Max Z number with t_1/2 data = ',z_max)\n    print('Max A number with t_1/2 data = ',a_max)\n    print('t_1/2 [Ma] max = ',t_max/3600/24/365/1000000)\n    \n    return nuclides\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['get_model_params', 'restore_model_params']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef accuracy(X, Y, model):\n    """Calculate the prediction accuracy by using the model to decode each sequence\n    in the input X and comparing the prediction with the true labels in Y.\n    \n    The X should be an array whose first dimension is the number of sentences to test,\n    and each element of the array should be an iterable of the words in the sequence.\n    The arrays X and Y should have the exact same shape.\n    \n    X = [("See", "Spot", "run"), ("Run", "Spot", "run", "fast"), ...]\n    Y = [(), (), ...]\n    """\n    correct = total_predictions = 0\n    for observations, actual_tags in zip(X, Y):\n        \n        # The model.viterbi call in simplify_decoding will return None if the HMM\n        # raises an error (for example, if a test sentence contains a word that\n        # is out of vocabulary for the training set). Any exception counts the\n        # full sentence as an error (which makes this a conservative estimate).\n        try:\n            most_likely_tags = simplify_decoding(observations, model)\n            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n        except:\n            pass\n        total_predictions += len(observations)\n    return correct / total_predictions\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['read_dataset']\n\nUpdated code:\n```python\nCSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\nLABEL_COLUMN = 'weight_pounds'\nKEY_COLUMN = 'key'\nDEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\nTRAIN_STEPS = 1000\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['hypothesis_inlinecounter', 'hypothesis_capswords', 'hypothesis_verbs', 'hypothesis_nouns']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['build_poly']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'train\', \'calc_count_xy_y\', \'calc_transition_count\']\n\nUpdated code:\n```python\nclass Hmm(object):\n    """\n    Stores counts for n-grams and emissions. \n    """\n\n    def __init__(self, n=3):\n        assert n>=2, "Expecting n>=2."\n        self.n = n\n        self.emission_counts = defaultdict(int)\n        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n        self.all_states = set()\n        self.count_y = dict() \n        #[(\'O\', 0), (\'I-MISC\', 0), (\'I-PER\', 0), (\'I-ORG\', 0), (\'I-LOC\', 0), (\'B-MISC\', 0), (\'B-PER\', 0), (\'B-ORG\', 0), (\'B-LOC\', 0)])\n        self.count_xy = dict()\n        self.trigram_counts = dict()\n        self.bigram_counts = dict()\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['coords_of_max']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['VGG16_predict_breed']\n\nUpdated code:\n```python\nfrom extract_bottleneck_features import *\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['outputFeatureMap']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['plot_2d']\n\nUpdated code:\n```python\n#ignore\n# @pysnooper.snoop()\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['buildProfit']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['plot']\n\nUpdated code:\n```python\n#ignore\n# @pysnooper.snoop()\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['calc_sum_product_variable_to_factor_msg']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'sample_withplot\']\n\nUpdated code:\n```python\nimport nestle\nimport io\nimport matplotlib.pyplot as plt\nimport corner\n\ndef sample_withplot(loglike_model, prior_transform_model, datafile, priorRange):\n    data_file = io.get_data_file_path(datafile)\n    data_x, data_xerr, data_y, data_yerr = io.load_data(data_file)\n\n    # n: number of parameters, len(priorRange)\n    n = len(priorRange)\n\n    def new_loglike_model(theta):\n        return loglike_model(theta, (data_x, data_xerr, data_y, data_yerr))\n\n    def new_prior_transform_model(theta):\n        return prior_transform_model(theta, priorRange)\n\n    result = nestle.sample(new_loglike_model, new_prior_transform_model, n)\n\n    print(\'log evidence\')\n    print(result.logz)\n\n    print(\'numerical (sampling) error on logz\')\n    print(result.logzerr)\n\n    print(\'array of sample parameters\')\n    print(result.samples)\n\n    print(\'array of weights associated with each sample\')\n    print(result.weights)\n\n    p_fit, cov_fit = nestle.mean_and_cov(result.samples, result.weights)\n\n    plt.figure()\n    plt.errorbar(data_x, data_y, yerr=data_yerr, fmt=\'*\')\n    plt.xlabel("r (kpc)")\n    plt.ylabel(\'V (km/s)\')\n    plt.title("Results of using the model to fit the DM rotational velocity distribution")\n    xplot = [5 + 5 * i for i in range(40)]\n    yplot = [model.model_NFW(xplot[i], p_fit) for i in range(40)]\n    plt.plot(xplot, yplot)\n    plt.show()\n\n    fig = corner.corner(result.samples, weights=result.weights, labels=[\'a\', \'rho0\'],\n                        range=[0.99999, 0.99999], bins=30)\n    plt.show()\n\n    return result\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['delete_from_bloom_filter_and_time']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['read_s3_csv']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['squared_error']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'clean_election_data\']\n\nUpdated code:\n```python\nimport math\n\n# read in dirty data \ndf = pd.read_csv("2014_election_results.csv")\ndfClean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\nfor i in range(len(dfClean)):\n    row = dfClean.iloc[i]  \n    row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n    if(pd.isnull(row["CANDIDATE NAME"]) or (row["CANDIDATE NAME"] == \'Scattered\')):\n        if(pd.isnull(row["CANDIDATE NAME (Last)"]) or (row["CANDIDATE NAME (Last)"] == \'Scattered\')):\n            row["CANDIDATE NAME"] = "UNKNOWN" \n        else:\n            row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"]\n\ndfClean = dfClean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['face_detector']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['convert_col_type']\n\nUpdated code:\n```python\n# By pasting the above list into a spreadsheet and cross checking with the data dictionary, we can \n# see which category each field should be\n\n# statsmodel requires all fieldsnames to begin with letters, so let's sort this out now.\ntrain = train.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ntest = test.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ndata_full = pd.concat([train, test], keys = ['train', 'test'])\n\n# Makes lists of each type\ncategories = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n              'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'YearBuilt', \n              'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', \n              'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'Heating', \n              'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', \n              'GarageFinish', 'GarageCars', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', \n              'SaleCondition']\nfloats = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n          'FirstFlrSF', 'SecondFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n          'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nints = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \n         'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n\nbools = ['CentralAir']\n\nfeature_names = categories + floats + ints + bools\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['disp_image']\n\nUpdated code:\n```python\nimport matplotlib.image as mpimg\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'get_feature_names\']\n\nUpdated code:\n```python\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {\'a\': [1, 5, 2, 5, 2, 8],\n               \'b\': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key=\'a\')\n    >> data[\'a\'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        # if self.key == \'playlist_pid\': from IPython.core.debugger import set_trace; set_trace()\n        return data_dict[:,[self.key]].astype(np.int64)\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['make_feature_cols']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['dateRange', 'date_to_week', 'week_to_date']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['tokenizer', 'tokenizer_porter']\n\nUpdated code:\n```python\nfrom nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['square']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'buildTraces\']\n\nUpdated code:\n```python\nimport numpy as np\nimport plotly.graph_objs as go\n\ndef buildTraces(bankdata, min_trace=False, mean_trace=False, max_trace=False):\n    data = []\n    min_amount = 0\n    max_amount = 0\n    \n    for accountType in ACCOUNTTYPES: # Account type\n        for bank in bankdata[accountType]: # Bank name\n            dates = np.array(bankdata[accountType][bank][\'date\'])\n            balances = np.array(bankdata[accountType][bank][\'balance\'])\n            \n            bankName = \'(\' + accountType + \') \' + SUPPORTED_BANKS[bank]\n\n            trace_main = go.Scatter(\n                x = dates,\n                y = balances,\n                name = bankName + \': Saldo \' + str(format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".")) + CURRENCY,\n                #line = dict(\n                #    color = \'green\'\n                #),\n                mode = \'lines\'\n            )\n            data.append(trace_main)\n\n            if max_trace:\n                trace_max = go.Scatter(\n                    x = dates,\n                    y = [balances.max() for f in dates],\n                    name = bankName + \': Saldo máximo\',\n                    #visible = \'legendonly\',\n                    #hoverinfo = \'name\',\n                    line = dict(\n                        #color = \'cyan\',\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_max)\n\n            if mean_trace:\n                trace_mean = go.Scatter(\n                    x = dates,\n                    y = [balances.mean() for f in dates],\n                    name = bankName + \': Saldo medio\',\n                    #hoverinfo = \'none\',\n                    line = dict(\n                        #color = \'magenta\',\n                        width = 4,\n                        dash = \'dashdot\'\n                    )\n                )\n                data.append(trace_mean)\n\n            if min_trace:\n                trace_min = go.Scatter(\n                    x = dates,\n                    y = [balances.min() for f in dates],\n                    name = bankName + \': Saldo mínimo\',\n                    line = dict(\n                        #color = \'red\',\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_min)\n                \n            # Extra\n            if balances.max() > max_amount:\n                max_amount = balances.max()\n    \n    max_amount, sum_trace = get_trace_sum_balances(bankdata)\n    data.append(sum_trace)\n    \n    return (data, min_amount, max_amount)\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'get_translation\']\n\nUpdated code:\n```python\n#helper in getting top features and making visual\n#convert relevant dataframe columns to lowercase so we can compare with top feature output\n\ntrack_artist_lower_df = tracks_df["track_artist_uri"].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df["track_album_uri"].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged["track_uri"].apply(lambda x: x.lower())\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'IRLS\']\n\nUpdated code:\n```python\nimport numpy as np\nimport math\n\ndef IRLS(data):\n    length        = len(data)\n    not_converged = True\n    w             = np.array([0, 0, 0])\n    p             = np.empty(length)\n    s             = np.empty(length)\n    z             = np.empty(length)\n\n    ones          = np.ones(200)\n    ones[100:]    = ones[100:]*-1\n    \n    temp_var      = np.hstack((ones[None].T,data))\n    np.random.shuffle(temp_var)\n    \n    y             = temp_var[:,0]\n    data          = temp_var[:,1:]\n    print(data.shape)\n    \n    while not_converged:\n        w_prev = w\n        for i in range(length):\n            p[i] = np.exp(w_prev.dot(data[i])) / (1 + np.exp(w_prev.dot(data[i])))\n            s[i] = p[i]*(1-p[i])\n            \n            if math.isnan(s[i]): s[i]=1\n            \n            z[i] = w_prev.dot(data[i]) + (y[i]-p[i])/(max(s[i],0.00001))\n            \n        diag_s = np.diag(s)\n#         print(\'diag:\', diag_s.shape)\n        t1     = np.linalg.inv(np.dot(np.dot(data.T, diag_s), data))\n        t2     = np.dot(np.dot(data.T, diag_s), z)\n        w      = np.dot(t1, t2)\n#         print("t1, t2",t1.shape, t2.shape)\n        w      = w/np.linalg.norm(w)\n        \n        print(\'Iterations\',w)\n        \n        if abs(sum(w-w_prev)) < 0.000001:\n            \n            print("Converged!!")\n            not_converged = False\n            return w\n            \n        elif sum(abs(w)) > 900 or math.isnan(w[0]):\n            \n            print("Not converging!!!")\n            return w\n            \n    return w\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['pos_vel_filter']\n\nUpdated code:\n```python\nfrom filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['removal_evolution']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['regression_and_rss']\n\nUpdated code:\n```python\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['IsotypicComponent']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['highlight_max']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['kmeans_multiple_runs']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n[]\n\nUpdated code:\n```python\n# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['detect_dog_breed']\n\nUpdated code:\n```python\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as preprocess_inception_input\n\ndef detect_dog_breed(img_path, given_model, use_bottleneck=True, img_H=224, img_W=224):\n    ''' Detect dog breed given image in the img_path,\n        using given model, using either bottleneck features (or not)\n        with given img Height and Width\n        \n        @return: Dog breed (str)\n    '''\n    print('Detecting dog breed...')\n    tensor = path_to_tensor(img_path, img_H, img_W)\n    \n    # using given image, extract its bottleneck features by running thru InceptionV3 n/w first\n    if use_bottleneck: \n        tensor = extract_InceptionV3(tensor)\n    else:\n        tensor = preprocess_inception_input(tensor)\n    \n    # print('  [input tensor shape: {}]'.format(tensor.shape))\n    # make predictions (probabilities)\n    predicted_vector = given_model.predict(tensor)\n    # get max index\n    y_hat = np.argmax(predicted_vector)\n    chance = 100. * predicted_vector[0][y_hat]  # probability of correctness\n    # print('  [y_hat:{}]'.format(y_hat))\n    # print('  prob:{:.2f}%'.format(chance))\n\n    # return dog breed and probability \n    return dog_names[y_hat], chance\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'parser\']\n\nUpdated code:\n```python\nfilename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)\n\n# If the parsed data only contains one column then return a Series\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nclass DiscriminatorNetwork:\n\n    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.adversarial_loss_weight = adversarial_loss_weight\n        self.small_model = small_model\n\n        self.k = 3\n        self.mode = 2\n        self.weights_path = "weights/Discriminator weights.h5"\n\n        self.gan_layers = None\n\n    def append_gan_network(self, true_X_input):\n\n        # Normalize the inputs via custom VGG Normalization layer\n        x = Normalize(type="gan", value=127.5, name="gan_normalize")(true_X_input)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_1\', kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name="gan_lrelu1_1")(x)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_2\', strides=(2, 2), kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu1_2\')(x)\n        x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm1_1\')(x)\n\n        filters = [128, 256] if self.small_model else [128, 256, 512]\n\n        for i, num_filters in enumerate(filters):\n            for j in range(2):\n                strides = (2, 2) if j == 1 else (1, 1)\n            \n                x = Conv2D(num_filters, (self.k, self.k), padding=\'same\', strides=strides,\n                                  name=\'gan_conv%d_%d\' % (i + 2, j + 1), kernel_initializer="glorot_uniform")(x)\n                x = LeakyReLU(0.3, name=\'gan_lrelu_%d_%d\' % (i + 2, j + 1))(x)\n                x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm%d_%d\' % (i + 2, j + 1))(x)\n\n        x = Flatten(name=\'gan_flatten\')(x)\n\n        output_dim = 128 if self.small_model else 1024\n\n        x = Dense(output_dim, name=\'gan_dense1\')(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu5\')(x)\n\n        gan_regulrizer = AdversarialLossRegularizer(weight=self.adversarial_loss_weight)\n        x = Dense(2, activation="softmax", activity_regularizer=gan_regulrizer, name=\'gan_output\')(x)\n\n        return x\n\n    def set_trainable(self, model, value=True):\n        if self.gan_layers is None:\n            disc_model = [layer for layer in model.layers\n                          if \'model\' in layer.name][0] # Only disc model is an inner model\n\n            self.gan_layers = [layer for layer in disc_model.layers\n                               if \'gan_\' in layer.name]\n\n        for layer in self.gan_layers:\n            layer.trainable = value\n\n    def load_gan_weights(self, model):\n        f = h5py.File(self.weights_path)\n\n        layer_names = [name for name in f.attrs[\'layer_names\']]\n        layer_names = layer_names[1:] # First is an input layer. Not needed.\n\n        if self.gan_layers is None:\n            self.gan_layers = [layer for layer in model.layers\n                                if \'gan_\' in layer.name]\n\n        for i, layer in enumerate(self.gan_layers):\n            g = f[layer_names[i]]\n            weights = [g[name] for name in g.attrs[\'weight_names\']]\n            layer.set_weights(weights)\n\n        print("GAN Model weights loaded.")\n        return model\n\n    def save_gan_weights(self, model):\n        print(\'GAN Weights are being saved.\')\n        model.save_weights(self.weights_path, overwrite=True)\n        print(\'GAN Weights saved.\')\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['enable_heatmap', 'disable_heatmap', 'handle_interaction', 'fetch_zipcode', 'add_point', 'add_geojson', 'add_geojsons', 'add_zipcode', 'progressive_iter', 'add_zipcodes_no_check', 'add_zipcodes']\n\nUpdated code:\n```python\nimport ipyleaflet as leaflet\nimport ipywidgets as widgets\n\nclass USMap:\n    def __init__(self):\n        self.us = USZIPCodeRepository()\n\n        self.center = [47.621795, -122.334958]\n        self.zoom = 8\n        self.height = '500px'\n        self.progress_bar_width = '500px'\n        self.area_style = {'color':'#0000ff', 'weight': .5, 'fillColor':'#000077', 'fillOpacity':0.2}\n\n        self.progress_bar = widgets.IntProgress(bar_style='info', layout=widgets.Layout(width=self.progress_bar_width))\n        self.label = widgets.Label()\n        self.progress_label = widgets.Label()\n        self.info_box = widgets.HBox([self.progress_label, self.progress_bar])\n\n        self.basemap = leaflet.basemaps.OpenMapSurfer.Roads\n        self.basemap['name'] = 'basemap'\n        self.heatmap_data = leaflet.basemaps.Strava.All\n        self.heatmap_data['name'] = 'heatmap'\n        self.heatmap = leaflet.basemap_to_tiles(self.heatmap_data)\n        self.layers_control = leaflet.LayersControl()\n        self.map_layout = widgets.Layout(height=self.height)\n\n        self.map = None\n\n\n    def add_zipcodes(self, zipcodes, show_progress=False):\n        zipcodes = set(zipcodes)\n        available_zipcodes = list(zipcodes & self.us.zipcode_set)\n        available_zipcodes.sort()\n\n        return self.add_zipcodes_no_check(available_zipcodes, show_progress)\n\n\n    def display(self):\n        if self.map is None:\n            self.map = leaflet.Map( center=self.center, zoom=self.zoom, basemap=self.basemap, layout=self.map_layout)\n            self.map.on_interaction(self.handle_interaction)\n            self.map.add_control(self.layers_control)\n\n        display(self.map)\n        display(self.label)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['process_image']\n\nUpdated code:\n```python\n# Set up parameters:    \nk_size = 3\nvertex_ratio_h = .45\nvertex_ratio_v = .60\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['is_int']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['sigmoid']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nfrom scipy.misc import imresize\n\ndef prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n    """Zooms and crops the image randomly for data augmentation."""\n\n    # First, let\'s find the largest bounding box with the target size ratio that fits within the image\n    height = image.shape[0]\n    width = image.shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = width if crop_vertically else int(height * target_image_ratio)\n    crop_height = int(width / target_image_ratio) if crop_vertically else height\n        \n    # Now let\'s shrink this bounding box by a random factor (dividing the dimensions by a random number\n    # between 1.0 and 1.0 + `max_zoom`.\n    resize_factor = np.random.rand() * max_zoom + 1.0\n    crop_width = int(crop_width / resize_factor)\n    crop_height = int(crop_height / resize_factor)\n    \n    # Next, we can select a random location on the image for this bounding box.\n    x0 = np.random.randint(0, width - crop_width)\n    y0 = np.random.randint(0, height - crop_height)\n    x1 = x0 + crop_width\n    y1 = y0 + crop_height\n    \n    # Let\'s crop the image using the random bounding box we built.\n    image = image[y0:y1, x0:x1]\n\n    # Let\'s also flip the image horizontally with 50% probability:\n    if np.random.rand() < 0.5:\n        image = np.fliplr(image)\n\n    # Now, let\'s resize the image to the target dimensions.\n    image = imresize(image, (target_width, target_height))\n    \n    # Finally, let\'s ensure that the colors are represented as\n    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n    return image.astype(np.float32) / 255\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nimport numpy as np\n\ndef find_f(K, y, iterations=100):\n    """\n    Finds f using a iterative aproach also finds p(y|f)\n    :param K: np.array(np.array) covariance matrix of data points we have observed\n    :param y: numpy array of the y value of the data points we have observed\n    :iterations: int optional default = 100 number of iterations we will preform to optimize f\n    :return: (numpy array <float> f, numpy array <float> p(y|f)) f is the latent function value for each of the \n    sampled data points, p(y|f) is the probability of y given the latent function we calculated\n    make sure to calculate p(y|f) after having approximated f_hat.\n    """\n    n = len(y)\n    f = np.zeros(n)\n    grad = np.zeros(n)\n    for i in range(iterations):\n        for j in range(n):\n            sigmoid_v = sigmoid(f[j]*y[j])\n            grad[j] = (1-sigmoid_v)*y[j]\n        f = np.array(np.matmul(K, grad)).flatten()\n        \n    y_giv_f = np.zeros(n)\n    for j in range(n):\n        y_giv_f[j] = sigmoid(f[j]*y[j])\n        \n    return f, y_giv_f\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['splitData']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['max_norm_regularizer']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['f']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['calc_sum_product_factor_to_variable_msg']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['calc_sum_product_variable_to_factor_msg']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['mutate']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['max_sum']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['PreProcess']\n\nUpdated code:\n```python\n#Select all variables that are not Categorical\nTables=[table[[v for v in table.keys() if v not in Categorical]]]\n\n#Convert yes/no to 1/0\nAlias_Dict={'SEX':{'F':1},'PRIOR.MAL':{'YES':1},'PRIOR.CHEMO':{'YES':1},'PRIOR.XRT':{'YES':1},\n            'Infection':{'Yes':1},'ITD':{'POS':1,'ND':numpy.nan},'D835':{'POS':1,'ND':numpy.nan},\n            'Ras.Stat':{'POS':1,'NotDone':numpy.nan},'resp.simple':{'CR':1},'Relapse':{'Yes':1},\n            'vital.status':{'A':1}}\n\nTables+=[alias(table,Alias_Dict)]\n\n#Split data that has multiple values\nTables+=[split(table['cyto.cat'],Dream9)]\n\n#Create new data for protein\nTables+=[squared(table[Protein])]\nTables+=[absolute(table[Protein])]\nTables+=[bin_independent(table[Protein],Dream9,2)]\nTables+=[bin_independent(table[Protein],Dream9,3)]\nTables+=[bin_independent(table[Protein],Dream9,4)]\nTables+=[bin_independent(table[Protein],Dream9,5)]\n\n#Make PCA axis\nTables+=[make_pca(table[Protein],Dream9,200,name='PCA')]\nTables+=[make_pca(table[Protein],Dream9,200,name='Whiten_PCA',whiten=True)]\nTables+=[make_pca(squared(table[Protein]),squared(Dream9[Protein]),200,name='PCA_Sq')]\n\n#Bin dependent variables\ntry:\n    Tables+=[cutoff(table[['Overall_Survival','Remission_Duration']],130)]\n    Tables+=[binned(table[['Overall_Survival','Remission_Duration']])]\nexcept KeyError:\n    pass        \n\n#Join everything\nreturn pandas.concat(Tables,axis=1)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['make_pca']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef powers(x):\n    """\n    Return a few powers of x.\n    """\n    return x ** 2, x ** 3, x ** 4\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['get_initial_centroids']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['insert_and_time_filter_bloom_filter']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['delete_from_cuckoo_filter_and_time']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['flip_random_character']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['plotMetrics']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['test']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['check_iambic_pentameter']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['eulerCoupledAdaptive']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['rmse_cv']\n\nUpdated code:\n```python\n# Invoke Ridge regularisation\nmodel_ridge = Ridge()\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['f2']\n\nUpdated code:\n```python\nf1 = lambda x: x**2\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['fit_model']\n\nUpdated code:\n```python\n# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n\n# TODO: Create a decision tree regressor object\nregressor = None\n\n# TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\nparams = {}\n\n# TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \nscoring_fnc = None\n\n# TODO: Create the grid search cv object --> GridSearchCV()\n# Make sure to include the right parameters in the object:\n# (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\ngrid = None\n\n# Fit the grid search object to the data to compute the optimal model\ngrid = grid.fit(X, y)\n\n# Return the optimal model after fitting the data\nreturn grid.best_estimator_\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef fillEmpty(bankdata, maxDate):\n    for accountType in ACCOUNTTYPES:\n        for bank in bankdata[accountType]:\n            if bankdata[accountType][bank][\'date\'][-1] != maxDate:\n                bankdata[accountType][bank]["balance"] = bankdata[accountType][bank]["balance"] + (bankdata[accountType][bank]["balance"][-1],)\n                bankdata[accountType][bank]["date"] = bankdata[accountType][bank]["date"] + (maxDate,)\n                \n    return bankdata\n```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nclass ValueEstimator():\n    """\n    Value Function approximator. \n    """\n    \n    def __init__(self, learning_rate=0.1, scope="value_estimator"):\n        with tf.variable_scope(scope):\n            self.state = tf.placeholder(tf.float32, [400], "state")\n            self.target = tf.placeholder(dtype=tf.float32, name="target")\n\n            # This is just linear classifier\n            self.output_layer = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n\n            self.value_estimate = tf.squeeze(self.output_layer)\n            self.loss = tf.squared_difference(self.value_estimate, self.target)\n\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            self.train_op = self.optimizer.minimize(\n                self.loss, global_step=tf.contrib.framework.get_global_step())        \n    \n    def predict(self, state, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        return sess.run(self.value_estimate, { self.state: state })\n\n    def update(self, state, target, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        feed_dict = { self.state: state, self.target: target }\n        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n        return loss\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['sample']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['generate_line', 'convert_line', 'generate_pair', 'generate_rhyming_and_meter_sonnet', 'generate_10_rhyming_and_meter_sonnets']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['sum_product']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['move_dimension_first']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\nimport unittest\n\nclass TestFeatures(unittest.TestCase):\n\n    def test_features_ground(self):\n        sample = (asl.df.ix[98, 1][features_ground]).tolist()\n        self.assertEqual(sample, [9, 113, -12, 119])\n\n    def test_features_norm(self):\n        sample = (asl.df.ix[98, 1][features_norm]).tolist()\n        np.testing.assert_almost_equal(sample, [ 1.153,  1.663, -0.891,  0.742], 3)\n\n    def test_features_polar(self):\n        sample = (asl.df.ix[98,1][features_polar]).tolist()\n        np.testing.assert_almost_equal(sample, [113.3578, 0.0794, 119.603, -0.1005], 3)\n\n    def test_features_delta(self):\n        sample = (asl.df.ix[98, 0][features_delta]).tolist()\n        self.assertEqual(sample, [0, 0, 0, 0])\n        sample = (asl.df.ix[98, 18][features_delta]).tolist()\n        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], "Sample value found was {}".format(sample))\n                         \nsuite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\nunittest.TextTestRunner().run(suite)\n```'}, {'reason': 'stop', 'result': "Unused functions:\n[]\n\nUpdated code:\n```python\ndef policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4, modified=False):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    policy_stable = False\n    all_k = []\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n\n    while not policy_stable:\n        \n        # 2. POLICY EVALUATION (iterates until V_k converges)\n        k = 0\n        V_kplus1 = copy.deepcopy(V_k)\n        delta = epsilon + 1\n        \n        while delta > epsilon and (k < 5 or not modified):\n\n            delta = 0\n            for i, j in states:\n                \n                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n                a = idx_to_a[PI[i,j]]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Bellman's update rule\n                V_kplus1[i, j] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n                # Keeps biggest difference seen so far\n                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n            # Updates our current estimate\n            V_k = copy.deepcopy(V_kplus1)\n            k += 1\n        all_k.append(k)\n\n        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n        \n        policy_stable = True\n        old_PI = copy.deepcopy(PI)\n        \n        for i, j in states:\n            for a_idx in range(4): # actions\n                    \n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Policy Improvement rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n                    \n            PI[i,j] = np.argmax(Q[i,j,:])\n                    \n            if old_PI[i,j] != PI[i,j]:\n                policy_stable = False\n    \n    return V_k, all_k, PI \n```"}, {'reason': 'stop', 'result': "Unused functions:\n['paths_to_tensor']\n\nUpdated code:\n```python\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['top_resultados', 'bot_resultados', 'correr_y_mostrar']\n\nUpdated code:\n```python\nimport time\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Para usar en caso de tener probabilidades a priori\npriors = [(0.1,0.9),(0.2,0.8),(0.3,0.7),(0.4,0.6),(0.5,0.5),(0.6,0.4),(0.7,0.3),(0.8,0.2),(0.9,0.1)]\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['next_batch']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['call']\n\nUpdated code:\n```python\n# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\nclass EncoderLayer(tf.keras.layers.Layer):\n  # Transformer 論文內預設 dropout rate 為 0.1\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    # 一樣，一個 sub-layer 一個 dropout layer\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n  def call(self, x, training, mask):\n    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n    \n    # sub-layer 1: MHA\n    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n    attn_output, attn = self.mha(x, x, x, mask)  \n    attn_output = self.dropout1(attn_output, training=training) \n    out1 = self.layernorm1(x + attn_output)  \n    \n    # sub-layer 2: FFN\n    ffn_output = self.ffn(out1) \n    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n    out2 = self.layernorm2(out1 + ffn_output)\n    \n    return out2\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['vandermonde']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['sgd_iter']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['LeNet', 'model_arc']\n\nUpdated code:\n```python\nfrom tensorflow.contrib.layers import flatten\n\ndef model_arc(x):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x9.\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 9), mean = mu, stddev = sigma))\n    conv1_b = tf.Variable(tf.zeros(9))\n    conv1   = tf.nn.bias_add(tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID'), conv1_b)\n    \n    # Activation.\n    conv1 = tf.nn.relu(conv1)\n    \n    # Pooling. Input = 28x28x9. Output = 14x14x9.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    \n    # Layer 2: Convolutional. Input = 14x14x9. Output = 12x12x27.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 9, 27), mean = mu, stddev = sigma))\n    conv2_b = tf.Variable(tf.zeros(27))\n    conv2   = tf.nn.bias_add(tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID'), conv2_b)\n    \n    # Activation.\n    conv2 = tf.nn.relu(conv2)\n    \n    # Pooling. Input = 12x12x9. Output = 6x6x9.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    \n    # Layer 3: Convolutional. Input = 6x6x27. Output = 6x6x81.\n    conv3_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 27, 81), mean = mu, stddev = sigma))\n    conv3_b = tf.Variable(tf.zeros(81))\n    conv3   = tf.nn.bias_add(tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='VALID'), conv3_b)\n    \n    # Activation.\n    conv3 = tf.nn.relu(conv3)\n    \n    # Dropout\n    conv3 = tf.nn.dropout(conv3, keep_prob=0.5)\n    \n    # Activation.\n    conv3 = tf.nn.relu(conv3)\n    \n    # Flatten. Input = 6x6x81. Output = 2916.\n    fc0   = flatten(conv3)\n    \n    # Layer 4: Fully Connected. Input = 2916. Output = 972.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(2916, 972), mean = mu, stddev = sigma))\n    fc1_b = tf.Variable(tf.zeros(972))\n    fc1   = tf.nn.xw_plus_b(fc0, fc1_W, fc1_b)\n    \n    # Activation.\n    fc1    = tf.nn.relu(fc1)\n\n    # Layer 5: Fully Connected. Input = 972. Output = 324.\n    fc2_W  = tf.Variable(tf.truncated_normal(shape=(972, 324), mean = mu, stddev = sigma))\n    fc2_b  = tf.Variable(tf.zeros(324))\n    fc2    = tf.nn.xw_plus_b(fc1, fc2_W, fc2_b)\n    \n    # Activation.\n    fc2 = tf.nn.relu(fc2)\n\n    # Layer 6: Fully Connected. Input = 324. Output = 108.\n    fc3_W  = tf.Variable(tf.truncated_normal(shape=(324, 108), mean = mu, stddev = sigma))\n    fc3_b  = tf.Variable(tf.zeros(108))\n    fc3 = tf.nn.xw_plus_b(fc2, fc3_W, fc3_b)\n    \n    # Activation.\n    fc3 = tf.nn.relu(fc3)\n\n    # Layer 7: Fully Connected. Input = 108. Output = 43.\n    fc4_W  = tf.Variable(tf.truncated_normal(shape=(108, 43), mean = mu, stddev = sigma))\n    fc4_b  = tf.Variable(tf.zeros(43))\n    logits = tf.nn.xw_plus_b(fc3, fc4_W, fc4_b)\n    \n    return logits\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['derivative_of']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['process_image']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['broad_less_than_50_meters_starboard']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['whose_a_good_doggy']\n\nUpdated code:\n```python\n### TODO: Write your algorithm.\n### Feel free to use as many code cells as needed.\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['draw_rule_and_reg_id']\n\nUpdated code:\n```python\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['nparray_to_list']\n\nUpdated code:\n```python\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\n# Make sure the input and output formats are the same for X and y\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    X_np = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(X_np, y_np):\n        X_train, X_valid = [x for x in X_np[train_index]], [x for x in X_np[valid_index]]\n        y_train, y_valid = [x for x in y_np[train_index]], [x for x in y_np[valid_index]]\n    train = X_train, y_train\n    valid = X_valid, y_valid\n    return train, valid\n```"}, {'reason': 'stop', 'result': "Unused functions:\n['compute_character']\n\nUpdated code:\n```python\nn = Integer(mu.size())\n# Determinant computation\nv = vandermonde(mu)\n# Span by derivatives\ngenerator = {v.multidegree() : [v]}\nlist_op = partial_derivatives(v.parent())\nV = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n# Projection on isotypic components\nV_iso = IsotypicComponent(V, n, use_antisymmetry=use_antisymmetry)\n# Polarization\nr = n-1\ndeg = v.degree()\nif deg == 0:\n    deg = 1\nop_pol = polarization_operators(r, deg, row_symmetry=row_symmetry)\nV_pol = PolarizedSpace(V_iso, op_pol)\n\n# character\nreturn character(V_pol, row_symmetry=row_symmetry)\n```"}, {'reason': 'stop', 'result': 'Unused functions:\n[\'grayscale\', \'canny\', \'gaussian_blur\', \'region_of_interest\', \'draw_lines\', \'hough_lines\', \'weighted_img\']\n\nUpdated code:\n```python\nimport math\n\nleftline=[(0,0,0,0)]\nrightline=[(0,0,0,0)]\n   \ndef draw_lines_roi(img, lines, vertices, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n\n    global leftline\n    global rightline\n    \n    imshape = img.shape\n    y_min = np.int(imshape[0]*0.61)\n    y_max = imshape[0]\n    left_x1 = []\n    left_x2 = []\n    right_x1 = []\n    right_x2 = []\n    left_count=0\n    right_count=0\n    \n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            m = (y2-y1)/(x2-x1)\n            if (m > 0.3) & (m<7):\n                fit = np.polyfit([x1,x2],[y1,y2],1)\n                right_x1.append(np.int(np.float((y_min - fit[1]))/np.float(fit[0])))\n                right_x2.append(np.int(np.float((y_max - fit[1]))/np.float(fit[0])))\n                right_count+=1\n            if (m < -0.3) & (m>-7):\n                fit = np.polyfit([x1,x2],[y1,y2],1)\n                left_x1.append(np.int(np.float((y_min - fit[1]))/np.float(fit[0])))\n                left_x2.append(np.int(np.float((y_max - fit[1]))/np.float(fit[0])))\n                left_count+=1\n    \n    \n    #check we have some detection for this line and also if the line has not moved too much from\n    #last frame. The first time we dont have history to compare. So that is handled separately\n    #checking if anything has ben stored in the left_line, right_line variables\n   \n    if (left_count>0):\n        left_line_x1 = np.int(np.nanmedian(left_x1))\n        left_line_x2 = np.int(np.nanmedian(left_x2))\n                \n        if leftline[0][2]!=0:\n            if (abs(leftline[0][2]-left_line_x2)<100) & (abs(leftline[0][0]-left_line_x1)<50):\n                leftline=[(left_line_x1,y_min,left_line_x2,y_max)]\n        else:\n            leftline=[(left_line_x1,y_min,left_line_x2,y_max)]\n    \n    if (right_count>0):        \n        right_line_x1 = np.int(np.nanmedian(right_x1))\n        right_line_x2 = np.int(np.nanmedian(right_x2))\n    \n        if rightline[0][2]!=0:\n            if (abs(rightline[0][2]-right_line_x2)<100) & (abs(rightline[0][0]-right_line_x1)<50):\n                rightline=[(right_line_x1,y_min,right_line_x2,y_max)]\n        else:\n            rightline=[(right_line_x1,y_min,right_line_x2,y_max)]\n    \n    #These are the new lines\n    newlines = [leftline,rightline]\n           \n    #line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(img, newlines,color,thickness)\n  \n    return img\n                         \ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap,vertices):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n   \n    #draw_lines(line_img,lines)\n    #return line_img\n    \n    result = draw_lines_roi(line_img, lines,vertices,[255,0,0],14)\n    #draw_lines(result,lines,[255, 255, 0],2)\n    return result\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)\n```'}, {'reason': 'stop', 'result': "Unused functions:\n['convert_invalid_values']\n\nUpdated code:\n```python\n```"}]