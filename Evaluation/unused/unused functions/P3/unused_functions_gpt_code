[{'reason': 'stop', 'result': '```python\n#Example\n#pandas.concat([Trial_data[Protein[:4]]],axis=1).T\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport subprocess \nimport shlex\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nclassifier_activation = {'binary': 'sigmoid', 'categorical': 'softmax'}\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport tensorflow as tf\n```'}, {'reason': 'stop', 'result': '```python\naccuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(np.mean(kf_y_pred == kf_y_test) )\n    accuracy_train.append(np.mean(kf_y_pred_dev == kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, kf_y_pred_dev_proba[:, 1]))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, kf_y_pred_proba[:, 1]))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train      # cambiar por accuracies_training\ndf["Accuracy (validación)"] = accuracy_validation # cambiar por accuracies_validation\ndf["AUC ROC (training)"]    = roc_train           # cambiar por aucs_training\ndf["AUC ROC (validación)"]  = roc_validation      # cambiar por aucs_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nboulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,   \n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n    \n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n    \n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n    \n    # yoga\n    'yoga': boulder_6b / 5,\n    \n    # ARC\n    'arc': boulder_6b_plus / 5,\n    \n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n    \n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%' : boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n    \n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n    \n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n    \n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n    \n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit',]\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+',]\n```"}, {'reason': 'stop', 'result': '```python\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\n\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nloss, mae, mse = models[label].evaluate(normed_test_data, test_labels[label], verbose=0)\n\nprint("Testing set Mean Abs Error: {:5.2f} um".format(mae))\nprint("Testing set RMS: {:5.2f} um".format(np.sqrt(mse)))\n\ntest_predictions = models[label].predict(normed_test_data).flatten()\n\nplt.scatter(test_labels[label], test_labels[label] - test_predictions)\nplt.xlabel(\'True Values [um]\')\nplt.ylabel(\'Residuals [um]\')\nminx, maxx = min(test_labels[label]), max(test_labels[label])\nplt.plot([minx, maxx], [0, 0])\nplt.show()\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nfrom scipy.optimize import fsolve\n\ntsym, dsym, ssym, test_sym = symbols('tsym dsym ssym test_sym')\n\nmodel_test_diag = [\n    tsym - ( ssym + (1 - A - U)*test_sym ),\n    dsym - ( A*ssym*p_true_pos + U*ssym*p_false_pos + (1 - A - U)*test_sym*p_true_pos )\n    ]\n\nsol_test_diag = solve(model_test_diag, tsym, dsym)\ntest_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[tsym])\ndiag_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[dsym])\n\n# set up a function to simulate system dynamics when perturbed from steady state\nfrom scipy.integrate import odeint\n\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n```'}, {'reason': 'stop', 'result': "```python\n#mini_train_data, mini_test_data, mini_train_labels, mini_test_labels = train_test_split(train_data, train_labels,\n#                                    stratify=train_labels, \n#                                    test_size=0.55)\n\n\n#\n# SVM\n#\nclassifier = LinearSVC(penalty='l2')\n\nparams = {'C': [0.01, 0.1, 0.5]}\nsvm = GridSearchCV(classifier, params, cv=4, \n                   scoring='accuracy', return_train_score=True)\n\n# Fit  training data\nsvm.fit(train_data, train_labels)  \n# Show the best C parameter to use and the expected accuracy\nprint('\\nSVM Classifier')\nprint(' Best param:', svm.best_params_)\nprint(' Accuracy:  ', np.round(svm.best_score_, 4) )\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n# robust pca\nfrom __future__ import division, print_function\n\nimport numpy as np\n\ntry:\n    from pylab import plt\nexcept ImportError:\n    print('Unable to import pylab. R_pca.plot_fit() will not work.')\n\ntry:\n    # Python 2: 'xrange' is the iterative version\n    range = xrange\nexcept NameError:\n    # Python 3: 'range' is iterative - no need for 'xrange'\n    pass\n```"}, {'reason': 'stop', 'result': '```python\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport sys\nsys.path.append("..")\n\n#import models\nfrom loss import PSNRLoss, psnr\n\nimport os\nimport time\nimport numpy as np\nfrom imageio import imwrite as imsave\nfrom scipy.misc import imresize\nfrom scipy.ndimage.filters import gaussian_filter\n\nbase_weights_path = "weights/"\nbase_val_images_path = "val_images/"\nbase_test_images = "test_images/"\n\nset5_path = "tests/set5"\nset14_path = "tests/set14"\nbsd100_path = "tests/bsd100"\n\nif not os.path.exists(base_weights_path):\n    os.makedirs(base_weights_path)\n\nif not os.path.exists(base_val_images_path):\n    os.makedirs(base_val_images_path)\n\nif not os.path.exists(base_test_images):\n    os.makedirs(base_test_images)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\n```'}, {'reason': 'stop', 'result': '```python\nfrom astropy import units as u\nfrom astropy.coordinates import SkyCoord\n\nfrom astropy.table import Table\n```'}, {'reason': 'stop', 'result': "```python\nfrom IPython.display import clear_output\n\ndef save_annotations(adict):\n    timestamp = datetime.datetime.today().strftime('%Y%m%d%H%M')\n    filename = 'annotations_' + timestamp + '.csv'\n    print(filename)\n    with open(os.path.join('.', filename), 'w', newline='') as out:\n        writer = csv.writer(out)\n        for key, value in adict.items():\n            line = [key, *value]\n            writer.writerow(line)\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n'''Function to read the blank-spaced column data into a Pandas data frame (table)'''\n\n'''\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    '''\n    import pandas as pd\n\n    # read the data into a data frame (or table)   \n    df = pd.read_csv(file_name, delim_whitespace=True)\n    \n    # to avoid frustrations, set explicitly the data types of each column\n    df = df.astype({'A':int,'Element':str,'Z':int,'N':int,'T1/2-(seconds)':float},copy=False)\n    print(df.dtypes)\n\n    return df\n```"}, {'reason': 'stop', 'result': '```python\nimport numpy as np\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n# Grab another random image\nidx = np.random.randint(0, len(img_list)-1)\nimage = mpimg.imread(img_list[idx])\nwarped, mask = perspect_transform(image, source, destination)\nthreshed = color_thresh(warped)\n\n# Calculate pixel values in rover-centric coords and distance/angle to all pixels\nxpix, ypix = rover_coords(threshed)\ndist, angles = to_polar_coords(xpix, ypix)\nmean_dir = np.mean(angles)\n\n# Do some plotting\nfig = plt.figure(figsize=(12,9))\nplt.subplot(221)\nplt.imshow(image)\nplt.subplot(222)\nplt.imshow(warped)\nplt.subplot(223)\nplt.imshow(threshed, cmap='gray')\nplt.subplot(224)\nplt.plot(xpix, ypix, '.')\nplt.ylim(-160, 160)\nplt.xlim(0, 160)\narrow_length = 100\nx_arrow = arrow_length * np.cos(mean_dir)\ny_arrow = arrow_length * np.sin(mean_dir)\nplt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nclass GenerativeNetwork:\n\n    def __init__(self, img_width=96, img_height=96, batch_size=16, num_upscales=2, small_model=False,\n                 content_weight=1, tv_weight=2e5, gen_channels=64):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.batch_size = batch_size\n        self.small_model = small_model\n        self.num_scales = num_upscales\n\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n\n        self.filters = gen_channels\n        self.mode = 2\n        self.init = \'glorot_uniform\'\n\n        self.sr_res_layers = None\n        self.sr_weights_path = "weights/SRGAN.h5"\n\n        self.output_func = None\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom scipy.integrate import simps\n\ndef calc_spectrum_filter_flux(filter_name, SpecClass):\n    filter_object = pcc.functions.load_filter("/Users/berto/Code/CoCo/data/filters/" + filter_name + ".dat")\n    filter_object.resample_response(new_wavelength = SpecClass.wavelength)\n    filter_area = simps(filter_object.throughput, filter_object.wavelength)\n    \n    transmitted_spec = filter_object.throughput * SpecClass.flux\n\n    integrated_flux = simps(transmitted_spec, SpecClass.wavelength)\n    \n    return  integrated_flux/filter_area\n\ndef calc_specphot(sn, filtername):\n\n    specphot = np.array([])\n    specepoch = np.array([])\n\n    for spec in sn.mangledspec:\n        specphot = np.append(specphot, calc_spectrum_filter_flux(filtername, sn.mangledspec[spec]))\n        specepoch = np.append(specepoch, sn.mangledspec[spec].mjd_obs)\n    \n    return specepoch, specphot\n\ndef compare_phot_specphot(sn, filtername):\n    """"""\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nimport pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv('data/exoplanet.eu_catalog.csv', \n                      usecols=['mass','mass_error_min','mass_error_max',\n                               'semi_major_axis','semi_major_axis_error_min','semi_major_axis_error_max','star_name'])\n```"}, {'reason': 'stop', 'result': '```python\nclass Point:\n    """\n    Simple class for representing a point in a Cartesian coordinate system.\n    """\n    \n    def __init__(self, x, y):\n        """\n        Create a new Point at x, y.\n        """\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        """\n        Translate the point by dx and dy in the x and y direction.\n        """\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return("Point at [%f, %f]" % (self.x, self.y))\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n# voorbeeld data, laad in om werking te zien! Wel even de volgende paragraaf skippen...\n\nimport random\n\ndoc = None\n\nsource = ColumnDataSource({\'x\': [], \'y\': [], \'color\': []})\n\nfig = figure(title=\'Streaming Circle Plot!\', sizing_mode=\'scale_width\',\n             x_range=[0, 1], y_range=[0, 1])\nfig.circle(source=source, x=\'x\', y=\'y\', color=\'color\', size=10)\n\ndoc.title = "Now with live updating!"\ndoc.add_root(fig)\n```'}, {'reason': 'stop', 'result': '```python\nclass FunctionRunner(Runner):\n    def __init__(self, function):\n        """Initialize.  `function` is a function to be executed"""\n        self.function = function\n\n    def __call__(self, inp):\n        return self.function(inp)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nX, y = train[:, 0:-1], train[:, -1]\nX = X.reshape(X.shape[0], 1, X.shape[1])\nmodel = Sequential()\nmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n#define the loss function and optimization algorithm here\nfor i in range(nb_epoch):\n    model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n    model.reset_states()\nreturn model\n```"}, {'reason': 'stop', 'result': '```python\nclass Preprocessor:\n    \n    def __init__(self, train_data_file, train_label_file, train_ids_file,\n                 instr_file, test_data_file=None, test_ids_file=None):\n        """A class to process and reformat data\n        for use in learning models"""\n        \n        # initialize the data the data filenames\n        self.train_data_file = train_data_file\n        self.train_label_file = train_label_file\n        self.train_ids_file = train_ids_file\n        self.instr_file = instr_file\n        \n        # test data is optional\n        self.test_data_file = test_data_file\n        self.test_ids_file = test_ids_file\n        \n    def read_data(self):\n        """Reads in data from the files passed to constructor"""\n        \n        # read in the data\n        train_X_df = pd.read_csv(self.train_data_file)\n        train_y_df = pd.read_csv(self.train_label_file)\n        train_ids_df = pd.read_csv(self.train_ids_file)\n        self.instr_df = pd.read_csv(self.instr_file)\n        \n        self.feature_names = [feature for feature in train_X_df]\n        self.original_feature_names = [feature for feature in train_X_df]\n        self.label_names = [feature for feature in train_y_df]\n        self.id_names = [feature for feature in train_ids_df]\n        \n        # create cross validation data\n        self.cv_X_df = pd.DataFrame(train_X_df)\n        self.cv_y_df = pd.DataFrame(train_y_df)\n        self.cv_ids_df = pd.DataFrame(train_ids_df)\n        \n        # read in the test data if it exists\n        if self.test_data_file != None:\n            self.test_X_df = pd.read_csv(self.test_data_file)\n            self.test_ids_df = pd.read_csv(self.test_ids_file)\n            self.all_X_df = train_X_df.append(self.test_X_df)\n        else:\n            self.test_X_df = None\n            self.test_ids_df = None\n            self.all_X_df = pd.DataFrame(train_X_df)\n        \n        # determine the shape of the input data\n        self.train_X_shape = train_X_df.shape\n        self.train_y_shape = train_y_df.shape\n        self.train_ids_shape = train_ids_df.shape\n        self.instr_shape = self.instr_df.shape\n        self.all_shape = self.all_X_df.shape\n        \n        # get size of test data if it exists\n        if self.test_data_file != None:\n            self.test_X_shape = self.test_X_df.shape\n            self.test_ids_shape = self.test_ids_df.shape\n        else:\n            self.test_X_shape = None\n            self.test_ids_shape = None\n\n        \n    def process(self, shuffle_train_data=False):\n        """Performs the processing on cross validation and train/test data"""\n        \n        # ADD OPTION TO SHUFFLE DATA HERE\n        \n        # processing on all data - remember to include cv_X and all_X for each condition\n        for col in self.original_feature_names:\n            print(col)\n            \n            # determine what to perform at each of the steps\n            col_instr = self.instr_df[col].values\n            col_enc = col_instr[1]\n            col_scl = col_instr[2]\n            col_imp = col_instr[3]\n\n            # impute values\n            # imputed first so that other functions will not use nan values in calculations\n            if col_imp == \'UNIQ\':\n                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n            if col_imp == \'MEAN\':\n                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n                self.all_X_df[col] = MEAN(self.all_X_df[col])\n            if col_imp == \'MODE\':\n                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n                self.all_X_df[col] = MODE(self.all_X_df[col])\n            if col_imp == \'MED\':\n                self.cv_X_df[col] = MED(self.cv_X_df[col])\n                self.all_X_df[col] = MED(self.all_X_df[col])\n            if is_int(col_imp):\n                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n            if col_imp == \'DEL\':\n                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n            \n            \n            # perform encoding of data\n            if col_enc == \'MAP\':\n                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n                self.all_X_df[col] = MAP(self.all_X_df[col])\n            if col_enc == \'OHE\':\n                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n                    feature_names=self.feature_names)\n            if col_enc == \'LOO\':\n                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n                self.all_X_df[col] = LOO(self.all_X_df[col])\n            \n\n            # perform scaling\n            if col_scl == \'NRM1\':\n                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n                self.all_X_df[col] = NRM1(self.all_X_df[col])\n            if col_scl == \'SCL1\':\n                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n                self.all_X_df[col] = SCL1(self.all_X_df[col])\n            if col_scl == \'TRSH\':\n                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n                self.all_X_df[col] = TRSH(self.all_X_df[col])\n\n        \n        # get the values from the dataframes\n        self.cv_X = self.cv_X_df.values\n        self.cv_y = self.cv_y_df.values\n        self.cv_ids = self.cv_ids_df.values\n        \n        all_X = self.all_X_df.values\n        self.train_X = all_X[:self.train_X_shape[0], :]\n        self.train_y = self.cv_y_df.values\n        self.train_ids = self.cv_ids_df.values\n        \n        if self.test_data_file != None:\n            self.test_X = all_X[self.train_X_shape[0]:, :]\n            self.test_ids = self.test_ids_df.values\n        else:\n            self.test_X = None\n            self.test_ids = None\n        \n    def write_data(self, out_dir=\'./processed_data/\'):\n        """Writes all of the data to output files"""\n        \n        # create the output directory if it does not exist\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n            \n        # convert arrays back into DataFrames\n        cv_X_df = pd.DataFrame(self.cv_X,  columns=self.feature_names)\n        cv_y_df = pd.DataFrame(self.cv_y, columns=self.label_names)\n        cv_ids_df = pd.DataFrame(self.cv_ids, columns=self.id_names)\n        train_X_df = pd.DataFrame(self.train_X, columns=self.feature_names)\n        train_y_df = pd.DataFrame(self.train_y, columns=self.label_names)\n        train_ids_df = pd.DataFrame(self.train_ids, columns=self.id_names)\n        if self.test_data_file != None:\n            test_X_df = pd.DataFrame(self.test_X, columns=self.feature_names)\n            test_ids_df = pd.DataFrame(self.test_ids, columns=self.id_names)\n        \n        # write the dataframes to file\n        cv_X_df.to_csv(out_dir+\'cv_X.csv\', index=False)\n        cv_y_df.to_csv(out_dir+\'cv_y.csv\', index=False)\n        cv_ids_df.to_csv(out_dir+\'cv_ids.csv\', index=False)\n        train_X_df.to_csv(out_dir+\'train_X.csv\', index=False)\n        train_y_df.to_csv(out_dir+\'train_y.csv\', index=False)\n        train_ids_df.to_csv(out_dir+\'train_ids.csv\', index=False)\n        if self.test_data_file != None:\n            test_X_df.to_csv(out_dir+\'test_X.csv\', index=False)\n            test_ids_df.to_csv(out_dir+\'test_ids.csv\', index=False)'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n# Configure model presets\nfrom examples.seismic import demo_model\n\n# Enable model presets here:\npreset = 'layers-isotropic'  # A simple but cheap model (recommended)\n# preset = 'marmousi2d-isotropic'  # A larger more realistic model\n\n# Standard preset with a simple two-layer model\nif preset == 'layers-isotropic':\n    filter_sigma = (1, 1)\n    nshots = 21\n    nreceivers = 101\n    t0 = 0.\n    tn = 1000.  # Simulation last 1 second (1000 ms)\n    f0 = 0.010  # Source peak frequency is 10Hz (0.010 kHz)\n\n\n# A more computationally demanding preset based on the 2D Marmousi model\nif preset == 'marmousi2d-isotropic':\n    filter_sigma = (6, 6)\n    nshots = 301  # Need good covergae in shots, one every two grid points\n    nreceivers = 601  # One recevier every grid point\n    t0 = 0.\n    tn = 3500.  # Simulation last 3.5 second (3500 ms)\n    f0 = 0.025  # Source peak frequency is 25Hz (0.025 kHz)\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport math\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nclass EuropeanOption(object):\n    """ Abstract Class for European options. Partially implemented.\n    S0 : float : initial stock/index level\n    strike : float : strike price\n    T : float : time to maturity (in year fractions)\n    r : float : constant risk-free short rate\n    div :    float : dividend yield\n    sigma :  float : volatility factor in diffusion term\n    model: str: name of the model for the pricing"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, option_type, S0, strike, T, r, div, sigma, model):\n        try:\n            self.option_type = option_type\n            assert isinstance(option_type, str)\n            self.S0 = float(S0)\n            self.strike = float(strike)\n            self.T = float(T)\n            self.r = float(r)\n            self.div = float(div)\n            self.sigma = float(sigma)\n            self.model = str(model)\n        except ValueError:\n            print(\'Error passing Options parameters\')\n\n        models = [\'BlackScholes\', \'MonteCarlo\', \n                  \'BinomialTree\', \'TrinomialTree\',\n                  \'FFT\', \'PDE\']\n        \n        if model not in models:\n            raise Exception(\'Error: Model unknown\')\n            \n        option_types = [\'call\', \'put\']\n        \n        if option_type not in option_types:\n            raise ValueError("Error: Option type not valid. Enter \'call\' or \'put\'")\n        if S0 < 0 or strike < 0 or T <= 0 or r < 0 or div < 0 or sigma < 0:\n            raise ValueError(\'Error: Negative inputs not allowed\')\n            \n        self.discount = np.exp(-self.r * self.T)\n\n    def getmodel(self):\n        return self.model\n\n    def __str__(self):\n        return "This European Option is priced using {0}".format(self.getmodel())\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport ipywidgets as widgets\nimport json\nimport time\nfrom threading import Thread\nfrom traitlets import Int, Unicode, Tuple, CInt, Dict, validate, observe\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n#mini_train_data, mini_test_data, mini_train_labels, mini_test_labels = train_test_split(train_data, train_labels,\n#                                    stratify=train_labels, \n#                                    test_size=0.55)\n\n\n#\n# Logistic Regression\n#\nlr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\nparams = {'C': [0.1, 0.25,  0.5,]}\nlogit = GridSearchCV(lr, params, cv=5,\n                     scoring='accuracy', return_train_score=True)\n\n# Fit  training data\nlogit.fit(train_data, train_labels)  \n# Show the best C parameter to use and the expected accuracy\nprint('\\nLogistic Regression Classifier, L2 regularization')\nprint(' Best param:', logit.best_params_)\nprint(' Accuracy:  ', np.round(logit.best_score_, 4) )\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nCSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\nLABEL_COLUMN = 'weight_pounds'\nKEY_COLUMN = 'key'\nDEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\nTRAIN_STEPS = 1000\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom extract_bottleneck_features import *\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n#ignore\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n#ignore\n```'}, {'reason': 'stop', 'result': '```python\ndef get_neighbour_messages(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    if len(neighbour_msg_prod) > 0:\n        return np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        return np.ones(variable.num_states)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n\'\'\'\nFunction to clean election data \n\'\'\'\nimport math\n        \n# read in dirty data \ndf = pd.read_csv("2014_election_results.csv")\ndfClean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\nfor i in range(len(dfClean)):\n    row = dfClean.iloc[i]  \n    row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n    if(pd.isnull(row["CANDIDATE NAME"]) or (row["CANDIDATE NAME"] == \'Scattered\')):\n        if(pd.isnull(row["CANDIDATE NAME (Last)"]) or (row["CANDIDATE NAME (Last)"] == \'Scattered\')):\n            row["CANDIDATE NAME"] = "UNKNOWN" \n        else:\n            row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"]\n\ndfClean = dfClean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\nreturn dfClean\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\n# By pasting the above list into a spreadsheet and cross checking with the data dictionary, we can \n# see which category each field should be\n\n# statsmodel requires all fieldsnames to begin with letters, so let's sort this out now.\ntrain = train.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ntest = test.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ndata_full = pd.concat([train, test], keys = ['train', 'test'])\n\n# Makes lists of each type\ncategories = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n              'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'YearBuilt', \n              'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', \n              'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'Heating', \n              'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', \n              'GarageFinish', 'GarageCars', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', \n              'SaleCondition']\nfloats = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n          'FirstFlrSF', 'SecondFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n          'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nints = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \n         'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n\nbools = ['CentralAir']\n\nfeature_names = categories + floats + ints + bools\n```"}, {'reason': 'stop', 'result': '```python\nimport matplotlib.image as mpimg\n```'}, {'reason': 'stop', 'result': '```python\nclass BaseEstimator:\n    pass\n\nclass TransformerMixin:\n    pass\n\nclass ItemSelector:\n    pass\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n#helper in getting top features and making visual\n#convert relevant dataframe columns to lowercase so we can compare with top feature output\n\ntrack_artist_lower_df = tracks_df["track_artist_uri"].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df["track_album_uri"].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged["track_uri"].apply(lambda x: x.lower())\n\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': "```python\nheterogeneity = {}\n\nmin_heterogeneity_achieved = float('inf')\nbest_seed = None\nfinal_centroids = None\nfinal_cluster_assignment = None\n\nfor i in xrange(num_runs):\n\n    # Use UTC time if no seeds are provided \n    if seed_list is not None: \n        seed = seed_list[i]\n        np.random.seed(seed)\n    else: \n        seed = int(time.time())\n        np.random.seed(seed)\n\n    # Use k-means++ initialization\n    # YOUR CODE HERE\n    initial_centroids = get_initial_centroids(data, k, seed=0)\n\n    # Run k-means\n    # YOUR CODE HERE\n    centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter=400,\n                                   record_heterogeneity=None, verbose=True)\n\n    # To save time, compute heterogeneity only once in the end\n    # YOUR CODE HERE\n    heterogeneity[seed] = compute_heterogeneity(data, k, centroids, cluster_assignment)\n\n    if verbose:\n        print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n        sys.stdout.flush()\n\n    # if current measurement of heterogeneity is lower than previously seen,\n    # update the minimum record of heterogeneity.\n    if heterogeneity[seed] < min_heterogeneity_achieved:\n        min_heterogeneity_achieved = heterogeneity[seed]\n        best_seed = seed\n        final_centroids = centroids\n        final_cluster_assignment\n\n# Return the centroids and cluster assignments that minimize heterogeneity.\nreturn final_centroids, final_cluster_assignment\n```"}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n### TODO: Write a function that takes a path to an image as input\n### and returns the dog breed that is predicted by the model.\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as preprocess_inception_input\n\n```'}, {'reason': 'stop', 'result': '```python\nfilename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)\n\n# If the parsed data only contains one column then return a Series\n```'}, {'reason': 'stop', 'result': '```python\nclass DiscriminatorNetwork:\n\n    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.adversarial_loss_weight = adversarial_loss_weight\n        self.small_model = small_model\n\n        self.k = 3\n        self.mode = 2\n        self.weights_path = "weights/Discriminator weights.h5"\n\n        self.gan_layers = None\n```'}, {'reason': 'stop', 'result': "```python\nimport ipyleaflet as leaflet\nimport ipywidgets as widgets\n\nclass USMap:\n    def __init__(self):\n        self.us = USZIPCodeRepository()\n\n        self.center = [47.621795, -122.334958]\n        self.zoom = 8\n        self.height = '500px'\n        self.progress_bar_width = '500px'\n        self.area_style = {'color':'#0000ff', 'weight': .5, 'fillColor':'#000077', 'fillOpacity':0.2}\n\n        self.basemap = leaflet.basemaps.OpenMapSurfer.Roads\n        self.basemap['name'] = 'basemap'\n        self.heatmap_data = leaflet.basemaps.Strava.All\n        self.heatmap_data['name'] = 'heatmap'\n        self.heatmap = leaflet.basemap_to_tiles(self.heatmap_data)\n        self.layers_control = leaflet.LayersControl()\n        self.map_layout = widgets.Layout(height=self.height)\n\n        self.map = None\n\n\n    def add_zipcodes(self, zipcodes, show_progress=False):\n        zipcodes = set(zipcodes)\n        available_zipcodes = list(zipcodes & self.us.zipcode_set)\n        available_zipcodes.sort()\n\n        return self.add_zipcodes_no_check(available_zipcodes, show_progress)\n\n\n    def display(self):\n        if self.map is None:\n            self.map = leaflet.Map( center=self.center, zoom=self.zoom, basemap=self.basemap, layout=self.map_layout)\n            self.map.add_control(self.layers_control)\n\n        display(self.map)\n```"}, {'reason': 'stop', 'result': '```python\n# Set up parameters:    \nk_size = 3\nvertex_ratio_h = .45\nvertex_ratio_v = .60\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom scipy.misc import imresize\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nneighbour_msg_prod = calc_other_neighbour_msg_prod(factor, variable)\n\nf_neighb_first = move_dimension_first(factor.f, factor.neighbours.index(variable))\n\nreturn marginalize(calculate_factor(f_neighb_first, neighbour_msg_prod), 0)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\ndef flip_random_character(s):\n    """Returns s with a random bit flipped in a random position"""\n    if s == "":\n        return s\n\n    pos = random.randint(0, len(s) - 1)\n    c = s[pos]\n    bit = 1 << random.randint(0, 6)\n    new_c = chr(ord(c) ^ bit)\n    return s[:pos] + new_c + s[pos + 1:]\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n# Invoke Ridge regularisation\nmodel_ridge = Ridge()\n```'}, {'reason': 'stop', 'result': '```python\nf1 = lambda x: x**2\n```'}, {'reason': 'stop', 'result': "```python\n# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n```"}, {'reason': 'skipped', 'result': None}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport numpy as np\nimport poetrytools\n\ndef generate_10_rhyming_and_meter_sonnets():\n    sonnets = \'\'\n    for i in range(10):\n        print(\'Generating Sonnet \' + str(i + 1))\n        sonnets += str(i) + \'\\n\' + generate_rhyming_and_meter_sonnet() + \'\\n\'\n    \n    f = open("project2data/rhyming_and_meter_shakespeare.txt","w")\n    f.write(sonnets)\n    return sonnets\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nimport unittest\n# import numpy as np\n\nclass TestFeatures(unittest.TestCase):\n\n    def test_features_ground(self):\n        pass\n\n    def test_features_norm(self):\n        pass\n\n    def test_features_polar(self):\n        pass\n\n    def test_features_delta(self):\n        pass\n                         \nsuite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\nunittest.TextTestRunner().run(suite)\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\n```'}, {'reason': 'stop', 'result': '```python\nimport time\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Para usar en caso de tener probabilidades a priori\npriors = [(0.1,0.9),(0.2,0.8),(0.3,0.7),(0.4,0.6),(0.5,0.5),(0.6,0.4),(0.7,0.3),(0.8,0.2),(0.9,0.1)]\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom tensorflow.contrib.layers import flatten\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n"""\nReturn a numpy array of randomly generated images of a \npower driven vessel that has one masthead light and one running light\nvisible starboard orientation.\n"""\nwhite = (255, 255, 255)\nblack = (0, 0, 0)\ngreen = (0, 255, 0)\ntotal_gens = np.random.randint(500, 701)\nall_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\nfor i in range(total_gens):\n    new_view = np.zeros((255, 255, 3))\n    masthead_light = np.random.randint(50, 201)\n    mh_horiz = np.random.randint(20, 211)\n    running_light_diff = np.random.randint(10, 31)\n    light_width = np.random.randint(10, 21)\n    masthead_height = masthead_light + light_width\n    masthead_width = mh_horiz + light_width\n    running_light_start = masthead_height + running_light_diff\n    running_light_width = running_light_start + light_width\n    if mh_horiz < 2 * light_width:\n        running_light_loc = np.random.randint(mh_horiz - 20, mh_horiz + 21)\n    else:\n        running_light_loc = np.random.randint(mh_horiz - 20, 211)\n    running_light_area = running_light_loc + light_width\n    new_view[masthead_light:masthead_height, mh_horiz:masthead_width] = white\n    new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n    new_view = new_view.flatten()\n    all_broad_images[i] = new_view\n\nreturn all_broad_images\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\n```'}, {'reason': 'stop', 'result': '```python\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\ndef nparray_to_list(nparray):\n    return [x for x in nparray]\n```'}, {'reason': 'stop', 'result': '```python\nn = Integer(mu.size())\n# Determinant computation\nv = vandermonde(mu)\n# Span by derivatives\ngenerator = {v.multidegree() : [v]}\nlist_op = partial_derivatives(v.parent())\nV = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n# Projection on isotypic components\nV_iso = IsotypicComponent(V, n, use_antisymmetry=use_antisymmetry)\n# Polarization\nr = n-1\ndeg = v.degree()\nif deg == 0:\n    deg = 1\nop_pol = polarization_operators(r, deg, row_symmetry=row_symmetry)\nV_pol = PolarizedSpace(V_iso, op_pol)\n\n# character\nreturn character(V_pol, row_symmetry=row_symmetry)\n```'}, {'reason': 'stop', 'result': '```python\nimport math\n\nleftline=[(0,0,0,0)]\nrightline=[(0,0,0,0)]\n   \ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap,vertices):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n   \n    #draw_lines(line_img,lines)\n    #return line_img\n    \n    result = draw_lines_roi(line_img, lines,vertices,[255,0,0],14)\n    #draw_lines(result,lines,[255, 255, 0],2)\n    return result\n\n# Python 3 has support for cool math symbols.\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)\n```'}, {'reason': 'stop', 'result': '```python\n```'}]