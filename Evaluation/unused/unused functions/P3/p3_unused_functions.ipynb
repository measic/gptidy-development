{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShot and decomposed prompting: decompose into identify then remove\\n\\nIdentify: 3-shot\\nRemove: 4-shot\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Shot and decomposed prompting: decompose into identify then remove\n",
    "\n",
    "Identify: 3-shot\n",
    "Remove: 4-shot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from run_process import get_unused_data\n",
    "sys.path.append('../../../')\n",
    "from utils import print_check_gpt_results, stats_results_unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "NUM_FILES = 209\n",
    "SAMPLES_FOLDER_NAME = '../random_samples_functions'\n",
    "GPT_SAVED_FILE_NAME = 'unused_functions_gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files from folder random_cells\n",
    "random_cells = []\n",
    "\n",
    "for i in range(NUM_FILES):\n",
    "    file_name = f'{SAMPLES_FOLDER_NAME}/{i}.py'\n",
    "    with open(file_name, 'r') as f:\n",
    "        random_cells.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_prompt = \"A function/method is unused if it is defined using 'def' but is not referenced after being defined. Identify unused functions/methods in the code delimited by triple backticks. Output this as a list of function names.\"\n",
    "\n",
    "task1_ex1_input = \"\"\"```python\n",
    "c = 5\n",
    "\n",
    "def calc():\n",
    "    a = 5\n",
    "    b = 4\n",
    "    return a + b\n",
    "```\"\"\"\n",
    "\n",
    "task1_ex1_output = \"['calc']\"\n",
    "\n",
    "task1_ex2_input = \"\"\"```python\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "c = 5\n",
    "\n",
    "def special():\n",
    "    a = 5\n",
    "    b = 4\n",
    "    return multiply(a, b)\n",
    "```\"\"\"\n",
    "\n",
    "task1_ex2_output = \"['special']\"\n",
    "\n",
    "task1_ex3_input = \"\"\"```python\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def update_name(self, name):\n",
    "        self.name = name\n",
    "```\"\"\"\n",
    "\n",
    "task1_ex3_output = \"['update_name']\"\n",
    "\n",
    "task1_ex4_input = \"\"\"```python\n",
    "def seven():\n",
    "    return 7\n",
    "```\"\"\"\n",
    "\n",
    "task1_ex4_output = \"\"\"['seven']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # identify unused functions using vulture (GPT)\n",
    "# import openai\n",
    "# openai.api_key = my_key\n",
    "\n",
    "# # GPT\n",
    "# def identify_unused(cell_src):\n",
    "#     while True:\n",
    "#         try:\n",
    "#             completion = openai.ChatCompletion.create(\n",
    "#                 model=\"gpt-3.5-turbo\",\n",
    "#                 temperature=0,\n",
    "#                 messages = [\n",
    "#                 {\"role\": \"user\", \"content\": task1_prompt},\n",
    "#                 {\"role\": \"user\", \"content\": task1_ex1_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task1_ex1_output},\n",
    "#                 {\"role\": \"user\", \"content\": task1_ex2_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task1_ex2_output},\n",
    "#                 {\"role\": \"user\", \"content\": task1_ex3_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task1_ex3_output},\n",
    "#                 {\"role\": \"user\", \"content\": task1_ex4_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task1_ex4_output},\n",
    "#                 {\"role\": \"user\", \"content\": f\"```python\\n{cell_src}\\n```\"}\n",
    "#             ]\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             if 'maximum context length' in str(e):\n",
    "#                 print('...Error.. too long...' + str(e))\n",
    "#                 return 'length', None\n",
    "#             else:\n",
    "#                 print('...Error.. trying again...' + str(e))\n",
    "#         else:\n",
    "#             break\n",
    "#     return completion.choices[0].finish_reason, completion.choices[0].message[\"content\"]\n",
    "\n",
    "# gpt_results = []\n",
    "# for i, cell_src in enumerate(random_cells):\n",
    "#     print(f'Processing file {i}')\n",
    "#     finish_reason, result = identify_unused(cell_src)\n",
    "#     print(f'File {i} - {finish_reason}')\n",
    "#     gpt_results.append({'reason': finish_reason, 'result': result})\n",
    "\n",
    "# # save the results to a file\n",
    "# with open(GPT_SAVED_FILE_NAME, 'w') as f:\n",
    "#     f.write(str(gpt_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in gpt result from file\n",
    "with open(GPT_SAVED_FILE_NAME, 'r') as f:\n",
    "    gpt_results = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a variable\n",
    "gpt_unused_names = []\n",
    "for var in gpt_results:\n",
    "    if var['reason'] == 'stop':\n",
    "        try:\n",
    "            res = eval(var['result'])\n",
    "        except:\n",
    "            gpt_unused_names.append(None)\n",
    "        else:\n",
    "            if res == []:\n",
    "                gpt_unused_names.append(None)\n",
    "            else:\n",
    "                gpt_unused_names.append(res)\n",
    "    else:\n",
    "        gpt_unused_names.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_prompt = \"Remove the functions specified from the code snippet enclosed by triple backticks. Do not add, modify, or remove anything else. Output the updated code with the specified functions removed.\"\n",
    "\n",
    "task2_ex1_input = \"\"\"Code:\n",
    "```python\n",
    "c = 5\n",
    "\n",
    "def random_adder():\n",
    "    a = 2\n",
    "    b = 3\n",
    "    return a + b + c\n",
    "```\n",
    "\n",
    "Functions to remove:\n",
    "['random_adder']\"\"\"\n",
    "\n",
    "task2_ex1_output = \"\"\"```python\n",
    "c = 5\n",
    "```\"\"\"\n",
    "\n",
    "task2_ex2_input = \"\"\"Code:\n",
    "```python\n",
    "par_val = 23\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "new_val = 1\n",
    "```\n",
    "\n",
    "Functions to remove:\n",
    "['multiply']\"\"\"\n",
    "\n",
    "task2_ex2_output = \"\"\"```python\n",
    "par_val = 23\n",
    "new_val = 1\n",
    "```\"\"\"\n",
    "\n",
    "task2_ex3_input = \"\"\"Code:\n",
    "```python\n",
    "def estimate(earnings):\n",
    "    return 0.1 * earnings\n",
    "```\n",
    "\n",
    "Functions to remove:\n",
    "['earnings']\"\"\"\n",
    "\n",
    "task2_ex3_output = \"\"\"```python\n",
    "```\"\"\"\n",
    "\n",
    "task2_ex4_input = \"\"\"Code:\n",
    "```python\n",
    "def seven():\n",
    "    return 7\n",
    "\n",
    "def validate(val1, val2):\n",
    "    assert val1 and val2\n",
    "    val3 = seven()\n",
    "    return val1 * val2 - val3\n",
    "```\n",
    "\n",
    "Functions to remove:\n",
    "['seven', 'validate']\"\"\"\n",
    "\n",
    "task2_ex4_output = \"\"\"```python\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.470047"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate cost\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "import utils\n",
    "\n",
    "def estimate_tokens():\n",
    "    in_tok = ''\n",
    "    out_tok = ''\n",
    "    for i, cell_src in enumerate(random_cells):\n",
    "        # estimate prompt\n",
    "        in_tok += task1_prompt + task1_ex1_input + task1_ex1_output + task1_ex2_input + task1_ex2_output + task1_ex3_input + task1_ex3_output + task1_ex4_input + task1_ex4_output\n",
    "        in_tok += f\"```python\\n{cell_src}\\n```\"\n",
    "        in_tok += task2_prompt + task2_ex1_input + task2_ex1_output + task2_ex2_input + task2_ex2_output + task2_ex3_input + task2_ex3_output + task2_ex4_input + task2_ex4_output\n",
    "        in_tok += f\"Code:\\n```python\\n{cell_src}\\n```\\n\\nFunctions to remove:\\n['test1', 'test2']\"\n",
    "        # estimate response\n",
    "        out_tok += \"The new name is so much better because it is amazing now and so cool wow yes very beautiful and pretty.\"\n",
    "        out_tok += cell_src\n",
    "    return in_tok, out_tok\n",
    "\n",
    "in_tok, out_tok = estimate_tokens()\n",
    "\n",
    "utils.gpt_35_turbo_token_dollar_cost(in_tok, out_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove unused functions using vulture (GPT)\n",
    "# import openai\n",
    "# openai.api_key = my_key\n",
    "\n",
    "# # GPT\n",
    "# def remove_unused(cell_src, function_names):\n",
    "#     while True:\n",
    "#         try:\n",
    "#             completion = openai.ChatCompletion.create(\n",
    "#                 model=\"gpt-3.5-turbo\",\n",
    "#                 temperature=0,\n",
    "#                 messages = [\n",
    "#                 {\"role\": \"user\", \"content\": task2_prompt},\n",
    "#                 {\"role\": \"user\", \"content\": task2_ex1_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task2_ex1_output},\n",
    "#                 {\"role\": \"user\", \"content\": task2_ex2_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task2_ex2_output},\n",
    "#                 {\"role\": \"user\", \"content\": task2_ex3_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task2_ex3_output},\n",
    "#                 {\"role\": \"user\", \"content\": task2_ex4_input},\n",
    "#                 {\"role\": \"assistant\", \"content\": task2_ex4_output},\n",
    "#                 {\"role\": \"user\", \"content\": f\"Code:\\n```python\\n{cell_src}\\n```\\n\\nFunctions to remove:\\n{function_names}\"}\n",
    "#             ]\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             if 'maximum context length' in str(e):\n",
    "#                 print('...Error.. too long...' + str(e))\n",
    "#                 return 'length', None\n",
    "#             else:\n",
    "#                 print('...Error.. trying again...' + str(e))\n",
    "#         else:\n",
    "#             break\n",
    "#     return completion.choices[0].finish_reason, completion.choices[0].message[\"content\"]\n",
    "\n",
    "# gpt_results_code = []\n",
    "# for i, cell_src in enumerate(random_cells):\n",
    "#     print(f'Processing file {i}')\n",
    "#     if gpt_unused_names[i] is None:\n",
    "#         finish_reason = 'skipped'\n",
    "#         result = None\n",
    "#         print('...skipping due to failed identified...')\n",
    "#     else:\n",
    "#         finish_reason, result = remove_unused(cell_src, gpt_unused_names[i])\n",
    "#     print(f'File {i} - {finish_reason}')\n",
    "#     gpt_results_code.append({'reason': finish_reason, 'result': result})\n",
    "\n",
    "# # save the results to a file\n",
    "# with open(GPT_SAVED_FILE_NAME + \"_code\", 'w') as f:\n",
    "#     f.write(str(gpt_results_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in gpt result from file\n",
    "with open(GPT_SAVED_FILE_NAME + \"_code\", 'r') as f:\n",
    "    gpt_results_code = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated code to files\n",
    "gpt_code = []\n",
    "\n",
    "# get all the code from the results\n",
    "for i, result in enumerate(gpt_results_code):\n",
    "    if result['reason'] == 'stop':\n",
    "        new = result['result'].split(\"```\")\n",
    "        if len(new) == 1:\n",
    "            new = None\n",
    "        else:\n",
    "            new = new[1]\n",
    "            if new.startswith('python'):\n",
    "                new = new[6:].strip(\"\\n\")\n",
    "        gpt_code.append(new)\n",
    "    else:\n",
    "        gpt_code.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated code to files\n",
    "import os\n",
    "if not os.path.exists('gpt_code'):\n",
    "    os.makedirs('gpt_code')\n",
    "for i, code in enumerate(gpt_code):\n",
    "    with open(f'gpt_code/{i}.py', 'w') as f:\n",
    "        if gpt_unused_names[i] is None or gpt_code[i] is None:\n",
    "            f.write(random_cells[i])\n",
    "        else:\n",
    "            f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total before: 302\n"
     ]
    }
   ],
   "source": [
    "before = get_unused_data(NUM_FILES, SAMPLES_FOLDER_NAME, 'function')\n",
    "\n",
    "total_before = sum(len(item) for item in before)\n",
    "print(f'Total before: {total_before}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total after: 20\n"
     ]
    }
   ],
   "source": [
    "after = get_unused_data(NUM_FILES, 'gpt_code', 'function')\n",
    "\n",
    "total_after = sum(len(item) for item in after)\n",
    "print(f'Total after: {total_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percentage difference: -93.37748344370861%\n"
     ]
    }
   ],
   "source": [
    "# List percentage difference between before and after for total\n",
    "print(f'Total percentage difference: {(total_after - total_before) / total_before * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT before count: 357\n",
      "Vulture before count: 302\n",
      "------------\n",
      "True positives: 289\n",
      "False positives: 68\n",
      "False negatives: 12\n",
      "------------\n",
      "Files with at least one false positive (and no false negatives)\n",
      "12: 2 false positives\n",
      "17: 4 false positives\n",
      "18: 2 false positives\n",
      "24: 1 false positives\n",
      "29: 1 false positives\n",
      "30: 1 false positives\n",
      "32: 3 false positives\n",
      "38: 1 false positives\n",
      "41: 4 false positives\n",
      "42: 1 false positives\n",
      "53: 1 false positives\n",
      "67: 4 false positives\n",
      "69: 2 false positives\n",
      "71: 2 false positives\n",
      "74: 1 false positives\n",
      "75: 1 false positives\n",
      "76: 4 false positives\n",
      "81: 1 false positives\n",
      "82: 1 false positives\n",
      "83: 1 false positives\n",
      "88: 3 false positives\n",
      "92: 1 false positives\n",
      "103: 1 false positives\n",
      "124: 1 false positives\n",
      "142: 1 false positives\n",
      "158: 7 false positives\n",
      "165: 1 false positives\n",
      "186: 1 false positives\n",
      "188: 4 false positives\n",
      "193: 1 false positives\n",
      "194: 2 false positives\n",
      "207: 2 false positives\n",
      "------------\n",
      "Files with at least one false negative (and no false positives)\n",
      "77: 1 false negatives\n",
      "85: 3 false negatives\n",
      "91: 1 false negatives\n",
      "143: 1 false negatives\n",
      "185: 1 false negatives\n"
     ]
    }
   ],
   "source": [
    "stats_results_unused(gpt_unused_names, before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def daterange(d1, d2):\n",
      "    return (d1 + dt.timedelta(days=i) for i in range((d2 - d1).days + 1))\n",
      "\n",
      "def get_trace_sum_balances(bankdata):\n",
      "    sum_balances = []\n",
      "    for bank in bankdata[CURRENT]:\n",
      "        dates = bankdata[CURRENT][bank]['date']\n",
      "        balances = bankdata[CURRENT][bank]['balance']\n",
      "        sum_account = {}\n",
      "        for date, balance in zip(dates, balances):\n",
      "            sum_account[date] = balance\n",
      "                \n",
      "        sum_balances.append(sum_account)\n",
      "    \n",
      "    total = {}\n",
      "    (ini, fin) = getIntervalDates(bankdata)\n",
      "    last = 0\n",
      "    max_amount = 0\n",
      "    for b in sum_balances:\n",
      "        for d in daterange(ini, fin):\n",
      "            if d in b:\n",
      "                last = b[d]                    \n",
      "                if d in total:\n",
      "                    total[d] += b[d]\n",
      "                    if total[d] > max_amount:\n",
      "                        max_amount = total[d]\n",
      "                else:\n",
      "                    total[d] = b[d]\n",
      "            else:\n",
      "                if d in total:\n",
      "                    total[d] += last\n",
      "                else:\n",
      "                    total[d] = last\n",
      "                \n",
      "    \n",
      "    dates = total.keys()\n",
      "    balances = total.values()\n",
      "    \n",
      "    (dates, balances) = zip(*sorted(zip(dates, balances)))\n",
      "    \n",
      "    trace = go.Scatter(\n",
      "        x = dates,\n",
      "        y = balances,\n",
      "        name = \"All Accounts - Amount: \" + format(balances[-1], ',.2f').replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\") + CURRENCY,\n",
      "        mode = 'lines',\n",
      "        line = dict ( width = 4 )\n",
      "    )\n",
      "\n",
      "    return (max_amount, trace)\n"
     ]
    }
   ],
   "source": [
    "print(random_cells[85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Preprocessor:\n",
      "    \n",
      "    def __init__(self, train_data_file, train_label_file, train_ids_file,\n",
      "                 instr_file, test_data_file=None, test_ids_file=None):\n",
      "        \"\"\"A class to process and reformat data\n",
      "        for use in learning models\"\"\"\n",
      "        \n",
      "        # initialize the data the data filenames\n",
      "        self.train_data_file = train_data_file\n",
      "        self.train_label_file = train_label_file\n",
      "        self.train_ids_file = train_ids_file\n",
      "        self.instr_file = instr_file\n",
      "        \n",
      "        # test data is optional\n",
      "        self.test_data_file = test_data_file\n",
      "        self.test_ids_file = test_ids_file\n",
      "        \n",
      "    def read_data(self):\n",
      "        \"\"\"Reads in data from the files passed to constructor\"\"\"\n",
      "        \n",
      "        # read in the data\n",
      "        train_X_df = pd.read_csv(self.train_data_file)\n",
      "        train_y_df = pd.read_csv(self.train_label_file)\n",
      "        train_ids_df = pd.read_csv(self.train_ids_file)\n",
      "        self.instr_df = pd.read_csv(self.instr_file)\n",
      "        \n",
      "        self.feature_names = [feature for feature in train_X_df]\n",
      "        self.original_feature_names = [feature for feature in train_X_df]\n",
      "        self.label_names = [feature for feature in train_y_df]\n",
      "        self.id_names = [feature for feature in train_ids_df]\n",
      "        \n",
      "        # create cross validation data\n",
      "        self.cv_X_df = pd.DataFrame(train_X_df)\n",
      "        self.cv_y_df = pd.DataFrame(train_y_df)\n",
      "        self.cv_ids_df = pd.DataFrame(train_ids_df)\n",
      "        \n",
      "        # read in the test data if it exists\n",
      "        if self.test_data_file != None:\n",
      "            self.test_X_df = pd.read_csv(self.test_data_file)\n",
      "            self.test_ids_df = pd.read_csv(self.test_ids_file)\n",
      "            self.all_X_df = train_X_df.append(self.test_X_df)\n",
      "        else:\n",
      "            self.test_X_df = None\n",
      "            self.test_ids_df = None\n",
      "            self.all_X_df = pd.DataFrame(train_X_df)\n",
      "        \n",
      "        # determine the shape of the input data\n",
      "        self.train_X_shape = train_X_df.shape\n",
      "        self.train_y_shape = train_y_df.shape\n",
      "        self.train_ids_shape = train_ids_df.shape\n",
      "        self.instr_shape = self.instr_df.shape\n",
      "        self.all_shape = self.all_X_df.shape\n",
      "        \n",
      "        # get size of test data if it exists\n",
      "        if self.test_data_file != None:\n",
      "            self.test_X_shape = self.test_X_df.shape\n",
      "            self.test_ids_shape = self.test_ids_df.shape\n",
      "        else:\n",
      "            self.test_X_shape = None\n",
      "            self.test_ids_shape = None\n",
      "\n",
      "        \n",
      "    def process(self, shuffle_train_data=False):\n",
      "        \"\"\"Performs the processing on cross validation and train/test data\"\"\"\n",
      "        \n",
      "        # ADD OPTION TO SHUFFLE DATA HERE\n",
      "        \n",
      "        # processing on all data - remember to include cv_X and all_X for each condition\n",
      "        for col in self.original_feature_names:\n",
      "            print(col)\n",
      "            \n",
      "            # determine what to perform at each of the steps\n",
      "            col_instr = self.instr_df[col].values\n",
      "            col_enc = col_instr[1]\n",
      "            col_scl = col_instr[2]\n",
      "            col_imp = col_instr[3]\n",
      "\n",
      "            # impute values\n",
      "            # imputed first so that other functions will not use nan values in calculations\n",
      "            if col_imp == 'UNIQ':\n",
      "                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n",
      "                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n",
      "            if col_imp == 'MEAN':\n",
      "                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = MEAN(self.all_X_df[col])\n",
      "            if col_imp == 'MODE':\n",
      "                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = MODE(self.all_X_df[col])\n",
      "            if col_imp == 'MED':\n",
      "                self.cv_X_df[col] = MED(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = MED(self.all_X_df[col])\n",
      "            if is_int(col_imp):\n",
      "                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n",
      "                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n",
      "            if col_imp == 'DEL':\n",
      "                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n",
      "                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n",
      "            \n",
      "            \n",
      "            # perform encoding of data\n",
      "            if col_enc == 'MAP':\n",
      "                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = MAP(self.all_X_df[col])\n",
      "            if col_enc == 'OHE':\n",
      "                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n",
      "                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n",
      "                    feature_names=self.feature_names)\n",
      "            if col_enc == 'LOO':\n",
      "                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = LOO(self.all_X_df[col])\n",
      "            \n",
      "\n",
      "            # perform scaling\n",
      "            if col_scl == 'NRM1':\n",
      "                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = NRM1(self.all_X_df[col])\n",
      "            if col_scl == 'SCL1':\n",
      "                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = SCL1(self.all_X_df[col])\n",
      "            if col_scl == 'TRSH':\n",
      "                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n",
      "                self.all_X_df[col] = TRSH(self.all_X_df[col])\n",
      "\n",
      "        \n",
      "        # get the values from the dataframes\n",
      "        self.cv_X = self.cv_X_df.values\n",
      "        self.cv_y = self.cv_y_df.values\n",
      "        self.cv_ids = self.cv_ids_df.values\n",
      "        \n",
      "        all_X = self.all_X_df.values\n",
      "        self.train_X = all_X[:self.train_X_shape[0], :]\n",
      "        self.train_y = self.cv_y_df.values\n",
      "        self.train_ids = self.cv_ids_df.values\n",
      "        \n",
      "        if self.test_data_file != None:\n",
      "            self.test_X = all_X[self.train_X_shape[0]:, :]\n",
      "            self.test_ids = self.test_ids_df.values\n",
      "        else:\n",
      "            self.test_X = None\n",
      "            self.test_ids = None\n",
      "        \n",
      "    def write_data(self, out_dir='./processed_data/'):\n",
      "        \"\"\"Writes all of the data to output files\"\"\"\n",
      "        \n",
      "        # create the output directory if it does not exist\n",
      "        if not os.path.exists(out_dir):\n",
      "            os.makedirs(out_dir)\n",
      "            \n",
      "        # convert arrays back into DataFrames\n",
      "        cv_X_df = pd.DataFrame(self.cv_X,  columns=self.feature_names)\n",
      "        cv_y_df = pd.DataFrame(self.cv_y, columns=self.label_names)\n",
      "        cv_ids_df = pd.DataFrame(self.cv_ids, columns=self.id_names)\n",
      "        train_X_df = pd.DataFrame(self.train_X, columns=self.feature_names)\n",
      "        train_y_df = pd.DataFrame(self.train_y, columns=self.label_names)\n",
      "        train_ids_df = pd.DataFrame(self.train_ids, columns=self.id_names)\n",
      "        if self.test_data_file != None:\n",
      "            test_X_df = pd.DataFrame(self.test_X, columns=self.feature_names)\n",
      "            test_ids_df = pd.DataFrame(self.test_ids, columns=self.id_names)\n",
      "        \n",
      "        # write the dataframes to file\n",
      "        cv_X_df.to_csv(out_dir+'cv_X.csv', index=False)\n",
      "        cv_y_df.to_csv(out_dir+'cv_y.csv', index=False)\n",
      "        cv_ids_df.to_csv(out_dir+'cv_ids.csv', index=False)\n",
      "        train_X_df.to_csv(out_dir+'train_X.csv', index=False)\n",
      "        train_y_df.to_csv(out_dir+'train_y.csv', index=False)\n",
      "        train_ids_df.to_csv(out_dir+'train_ids.csv', index=False)\n",
      "        if self.test_data_file != None:\n",
      "            test_X_df.to_csv(out_dir+'test_X.csv', index=False)\n",
      "            test_ids_df.to_csv(out_dir+'test_ids.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "print(gpt_code[85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read_data', 'process', 'write_data', 'select_features']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read_data', 'process', 'write_data']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select_features']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_unused_names[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'gpt_code' folder\n",
    "!rm -rf gpt_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
