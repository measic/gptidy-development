[{'reason': 'stop', 'result': 'Unused functions:\n[\'fit\']\n\nUpdated code:\n```python\nclass two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef precision(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n    tp = tf.diag_part(conf_matrix)\n    precision_scores = tp/(tp_and_fp)\n    if weights:\n        precision_score = tf.multiply(precision_scores, weights)/tf.reduce_sum(weights)\n    else:\n        precision_score = tf.reduce_mean(precision_scores)        \n    return precision_score```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef recall(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n    tp = tf.diag_part(conf_matrix)\n    recall_scores = tp/(tp_and_fn)\n    if weights:\n        recall_score = tf.multiply(recall_scores, weights)/tf.reduce_sum(weights)\n    else:\n        recall_score = tf.reduce_mean(recall_scores)        \n    return recall_score```'}, {'reason': 'stop', 'result': 'Unused functions:\n[]\n\nUpdated code:\n```python\ndef roc_auc(labels, predictions, thresholds, get_fpr_tpr=True):\n    tpr = []\n    fpr = []\n    for th in thresholds:    \n        # Compute number of true positives\n        tp_cases = tf.where((tf.greater_equal(predictions, th)) & \n                            (tf.equal(labels, 1)))\n        tp = tf.size(tp_cases)\n        \n        # Compute number of true negatives\n        tn_cases = tf.where((tf.less(predictions, th)) & \n                            (tf.equal(labels, 0)))\n        tn = tf.size(tn_cases)\n        \n        # Compute number of false positives\n        fp_cases = tf.where((tf.greater_equal(predictions, th)) & \n                            (tf.equal(labels,0)))\n        fp = tf.size(fp_cases)\n        \n        # Compute number of false negatives\n        fn_cases = tf.where((tf.less(predictions, th)) & \n                            (tf.equal(labels,1)))\n        fn = tf.size(fn_cases)\n        \n        # Compute True Positive Rate for this threshold\n        tpr_th = tp/(tp + fn)\n        \n        # Compute the False Positive Rate for this threshold\n        fpr_th = fp/(fp + tn)\n        \n        # Append to the entire True Positive Rate list\n        tpr.append(tpr_th)\n        \n        # Append to the entire False Positive Rate list\n        fpr.append(fpr_th)\n        \n    # Approximate area under the curve using Riemann sums and the trapezoidal rule\n    auc_score = 0\n    for i in range(0, len(thresholds)-1):\n        height_step = tf.abs(fpr[i+1]-fpr[i])\n        b1 = tpr[i]\n        b2 = tpr[i+1]\n        step_area = height_step*(b1+b2)/2\n        auc_score += step_area\n    return auc_score, fpr, tpr```'}, {'reason': 'stop', 'result': 'Unused functions:\n[\'read_s3_csv\']\n\nUpdated code:\n```python\nimport boto3\nimport pandas as pd\n\ndef read_s3_csv(dates):\n    s3 = boto3.resource(\'s3\')\n    deutsche_boerse_bucket = \'deutsche-boerse-xetra-pds\'\n    \n    bucket = s3.Bucket(deutsche_boerse_bucket)\n    \n    dataframes = []\n    \n    for date in dates:\n        objs_count = 0\n        csv_objects = bucket.objects.filter(Prefix=date)\n        for csv_obj in csv_objects:\n            csv_key = csv_obj.key\n            if csv_key[-4:] == \'.csv\':\n                objs_count += 1\n                csv_body = csv_obj.get()[\'Body\']\n                df = pd.read_csv(csv_body)\n                dataframes.append(df)\n        \n        print ("Loaded {} data objects for {}".format (objs_count, date))\n    return pd.concat(dataframes)```'}]