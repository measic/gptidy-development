[{'reason': 'stop', 'result': 'class two_layer_nn(tf.keras.Model):\n    def __init__(self, output_size=2, loss_type=\'cross-entropy\'):\n        super(two_layer_nn, self).__init__()\n        """ Define here the layers used during the forward-pass \n            of the neural network.     \n            Args:\n                output_size: int (default=2). \n                loss_type: string, \'cross-entropy\' or \'regression\' (default=\'cross-entropy\')\n        """   \n        # First hidden layer\n        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n        # Second hidden layer\n        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n        # Output layer. Unscaled log probabilities\n        self.dense_out = tf.layers.Dense(output_size, activation=None)     \n        # Initialize loss type\n        self.loss_type = loss_type\n    \n    def predict(self, input_data):\n        """ Runs a forward-pass through the network.     \n            Args:\n                input_data: 2D tensor of shape (n_samples, n_features).   \n            Returns:\n                logits: unnormalized predictions.\n        """\n        layer_1 = self.dense_1(input_data)\n        layer_2 = self.dense_2(layer_1)\n        logits = self.dense_out(layer_2)\n        return logits\n    \n    def loss_fn(self, input_data, target):\n        """ Defines the loss function used during \n            training.         \n        """\n        preds = self.predict(input_data)\n        if self.loss_type==\'cross-entropy\':\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n        else:\n            loss = tf.losses.mean_squared_error(target, preds)\n        return loss\n    \n    def grads_fn(self, input_data, target):\n        """ Dynamically computes the gradients of the loss value\n            with respect to the parameters of the model, in each\n            forward pass.\n        """\n        with tfe.GradientTape() as tape:\n            loss = self.loss_fn(input_data, target)\n        return tape.gradient(loss, self.variables)\n    \n    def fit(self, input_data, target, optimizer, num_epochs=500, \n            verbose=50, track_accuracy=True):\n        """ Function to train the model, using the selected optimizer and\n            for the desired number of epochs. It also stores the accuracy\n            of the model after each epoch.\n        """   \n        \n        if track_accuracy:\n            # Initialize list to store the accuracy of the model\n            self.hist_accuracy = []     \n            # Initialize class to compute the accuracy metric\n            accuracy = tfe.metrics.Accuracy()\n\n        for i in range(num_epochs):\n            # Take a step of gradient descent\n            grads = self.grads_fn(input_data, target)\n            optimizer.apply_gradients(zip(grads, self.variables))\n            if track_accuracy:\n                # Predict targets after taking a step of gradient descent\n                logits = self.predict(X)\n                preds = tf.argmax(logits, axis=1)\n                # Compute the accuracy\n                accuracy(preds, target)\n                # Get the actual result and add it to our list\n                self.hist_accuracy.append(accuracy.result())\n                # Reset accuracy value (we don\'t want to track the running mean accuracy)\n                accuracy.init_variables()\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def precision(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n    tp = tf.diag_part(conf_matrix)\n    precision_scores = tp/(tp_and_fp)\n    if weights:\n        precision_score = tf.multiply(precision_scores, weights)/tf.reduce_sum(weights)\n    else:\n        precision_score = tf.reduce_mean(precision_scores)        \n    return precision_score\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def recall(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n    tp = tf.diag_part(conf_matrix)\n    recall_scores = tp/(tp_and_fn)\n    if weights:\n        recall_score = tf.multiply(recall_scores, weights)/tf.reduce_sum(weights)\n    else:\n        recall_score = tf.reduce_mean(recall_scores)        \n    return recall_score\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def roc_auc(labels, predictions, thresholds, get_fpr_tpr=True):\n    tpr = []\n    fpr = []\n    for th in thresholds:    \n        # Compute number of true positives\n        tp_cases = tf.where((tf.greater_equal(predictions, th)) & \n                            (tf.equal(labels, 1)))\n        tp = tf.size(tp_cases)\n        \n        # Compute number of true negatives\n        tn_cases = tf.where((tf.less(predictions, th)) & \n                            (tf.equal(labels, 0)))\n        tn = tf.size(tn_cases)\n        \n        # Compute number of false positives\n        fp_cases = tf.where((tf.greater_equal(predictions, th)) & \n                            (tf.equal(labels,0)))\n        fp = tf.size(fp_cases)\n        \n        # Compute number of false negatives\n        fn_cases = tf.where((tf.less(predictions, th)) & \n                            (tf.equal(labels,1)))\n        fn = tf.size(fn_cases)\n        \n        # Compute True Positive Rate for this threshold\n        tpr_th = tp/(tp + fn)\n        \n        # Compute the False Positive Rate for this threshold\n        fpr_th = fp/(fp + tn)\n        \n        # Append to the entire True Positive Rate list\n        tpr.append(tpr_th)\n        \n        # Append to the entire False Positive Rate list\n        fpr.append(fpr_th)\n        \n    # Approximate area under the curve using Riemann sums and the trapezoidal rule\n    auc_score = 0\n    for i in range(0, len(thresholds)-1):\n        height_step = tf.abs(fpr[i+1]-fpr[i])\n        b1 = tpr[i]\n        b2 = tpr[i+1]\n        step_area = height_step*(b1+b2)/2\n        auc_score += step_area\n    return auc_score, fpr, tpr\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def read_s3_csv(dates):\n    s3 = boto3.resource(\'s3\')\n    deutsche_boerse_bucket = \'deutsche-boerse-xetra-pds\'\n    \n    bucket = s3.Bucket(deutsche_boerse_bucket)\n    \n    dataframes = []\n    \n    for date in dates:\n        objs_count = 0\n        csv_objects = bucket.objects.filter(Prefix=date)\n        for csv_obj in csv_objects:\n            csv_key = csv_obj.key\n            if csv_key[-4:] == \'.csv\':\n                objs_count += 1\n                csv_body = csv_obj.get()[\'Body\']\n                df = pd.read_csv(csv_body)\n                dataframes.append(df)\n        \n        print("Loaded {} data objects for {}".format(objs_count, date))\n    return pd.concat(dataframes)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def build_index(non_empty_days, from_time, to_time):\n    date_ranges = []\n    for date in non_empty_days:\n        yyyy, mm, dd = date.split(\'-\')\n        from_hour, from_min = from_time.split(\':\')\n        to_hour, to_min = to_time.split(\':\')    \n        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n        date_ranges.append(pd.DataFrame({"OrganizedDateTime": pd.date_range(t1, t2, freq=\'1Min\').values}))\n    agg = pd.concat(date_ranges, axis=0) \n    agg.index = agg["OrganizedDateTime"]\n    return agg\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def basic_stock_features(input_df, mnemonic, new_time_index, inplace=False):\n    stock = input_df.loc[mnemonic]\n    if not inplace:\n        stock = input_df.loc[mnemonic].copy()\n    \n    stock = stock.reindex(new_time_index)\n    \n    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n    for f in features:\n        stock[f] = stock[f].fillna(method='ffill')   \n    \n    features = ['TradedVolume', 'NumberOfTrades']\n    for f in features:\n        stock[f] = stock[f].fillna(0.0)\n        \n    stock['HourOfDay'] = stock.index.hour\n    stock['MinOfHour'] = stock.index.minute\n    stock['MinOfDay'] = stock.index.hour*60 + stock.index.minute\n\n    stock['DayOfWeek'] = stock.index.dayofweek\n    stock['DayOfYear'] = stock.index.dayofyear\n    stock['MonthOfYear'] = stock.index.month\n    stock['WeekOfYear'] = stock.index.weekofyear\n    \n    stock['Mnemonic'] = mnemonic\n    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n    return stock.drop(unwanted_features, axis=1)\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def clean_data(df, inplace=False):\n    column_filter = [\'ISIN\', \'Mnemonic\', \'SecurityDesc\', \'SecurityType\', \'Currency\', \'SecurityID\', \'Date\', \'Time\', \'StartPrice\', \'MaxPrice\', \'MinPrice\', \'EndPrice\', \'TradedVolume\', \'NumberOfTrades\']\n    n_df = df[column_filter]\n    if not inplace:\n        n_df = df.copy()\n        \n    n_df.drop(n_df.Time == \'Time\', inplace=True)\n    # we want the dates to be comparable to datetime.strptime()\n    n_df["CalcTime"] = pd.to_datetime("1900-01-01 " + n_df["Time"], errors=\'coerce\')\n    n_df["CalcDateTime"] = pd.to_datetime(n_df["Date"] + " " + n_df["Time"], errors=\'coerce\')\n\n    # Filter common stock\n    # Filter between trading hours 08:00 and 20:00\n    # Exclude auctions (those are with TradeVolume == 0)\n    only_common_stock = n_df[n_df.SecurityType == \'Common stock\']\n    time_fmt = "%H:%M"\n    opening_hours_str = "08:00"\n    closing_hours_str = "20:00"\n    opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n    closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n\n    cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n                      (only_common_stock.CalcTime >= opening_hours) & \\\n                      (only_common_stock.CalcTime <= closing_hours)]\n    \n    bymnemonic = cleaned_common_stock[[\'Mnemonic\', \'TradedVolume\']].groupby([\'Mnemonic\']).sum()\n    number_of_stocks = 100\n    top = bymnemonic.sort_values([\'TradedVolume\'], ascending=[0]).head(number_of_stocks)\n    top_k_stocks = list(top.index.values)\n    cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n    sorted_by_index = cleaned_common_stock.set_index([\'Mnemonic\', \'CalcDateTime\']).sort_index()\n    non_empty_days = sorted(list(cleaned_common_stock[\'Date\'].unique()))\n    new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)["OrganizedDateTime"].values\n    \n    stocks = []\n    for stock in top_k_stocks:\n        stock = basic_stock_features(sorted_by_index, stock, new_datetime_index, inplace=True)\n        stocks.append(stock)\n    # prepared should contain the numeric features for all top k stocks,\n    # for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n    # for all minutes from 08:00 until 20:00\n    # in minutes without trades the prices from the last available minute are carried forward\n    # trades are filled with zero for such minutes\n    # a new column called HasTrade is introduced to denote the presence of trades\n    prepared = pd.concat(stocks, axis=0).dropna(how=\'any\')\n    prepared.Mnemonic = prepared.Mnemonic.astype(\'category\')\n    return prepared\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def create_xgb_target(df):\n    return df.MaxPrice.shift(-1).fillna(method='ffill')```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def create_xgb_features(df, horizon, inplace=False):\n    n_df = df\n    if not inplace:\n        n_df = df.copy()\n\n    for offset in range(1, horizon+1):\n        min_price = n_df[\'MinPrice\'].shift(offset).fillna(method=\'bfill\')\n        max_price = n_df[\'MaxPrice\'].shift(offset).fillna(method=\'bfill\')\n        start_price = n_df[\'StartPrice\'].shift(offset).fillna(method=\'bfill\')\n        end_price = n_df[\'EndPrice\'].shift(offset).fillna(method=\'bfill\')\n        trade_vol = n_df[\'TradedVolume\'].shift(offset).fillna(method=\'bfill\')\n        num_trades = n_df[\'NumberOfTrades\'].shift(offset).fillna(method=\'bfill\')\n\n        n_df["h{}_MinPrice".format(offset)] = min_price\n        n_df["h{}_MaxPrice".format(offset)] = max_price\n        n_df["h{}_StartPrice".format(offset)] = start_price\n        n_df["h{}_EndPrice".format(offset)] = end_price\n        n_df["h{}_TradeVolume".format(offset)] = trade_vol\n        n_df["h{}_NumberOfTrades".format(offset)] = num_trades\n\n    return n_df\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def engineer_date_range (dates):\n    unprocessed_df = read_s3_csv (dates)\n    print ("Loaded CSV data set from S3")\n    \n    cleaned_df = clean_data (unprocessed_df, inplace = True)\n    print ("Cleaned CSV data set")\n     \n    xgb_data = create_xgb_features (cleaned_df, 5, inplace=True)\n    xgb_data[\'NextMaxPrice\'] = create_xgb_target (xgb_data)\n    print ("Engineered CSV data set")\n    \n    train_data, validate_data = train_test_split (xgb_data, train_size=0.8, test_size=0.2, shuffle=True)\n\n    cols = list(train_data.columns.values)\n    cols.remove (\'NextMaxPrice\')\n    cols = [\'NextMaxPrice\'] + cols\n\n    train_data = pd.get_dummies (train_data[cols])\n    validate_data = pd.get_dummies (validate_data[cols])\n    print ("Data split for training purposes")\n    \n    return (train_data, validate_data)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def plot_image(image):\n    plt.imshow(image, cmap="gray", interpolation="nearest")\n    plt.axis("off")```\n\nUnused functions:\n- plot_color_image'}, {'reason': 'stop', 'result': 'def restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + "/Assign")\n                  for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)```\n\nUnused functions:\n- get_model_params'}, {'reason': 'stop', 'result': 'import sys\nimport tarfile\nfrom six.moves import urllib\n\nTF_MODELS_URL = "http://download.tensorflow.org/models"\nINCEPTION_V3_URL = TF_MODELS_URL + "/inception_v3_2016_08_28.tar.gz"\nINCEPTION_PATH = os.path.join("datasets", "inception")\nINCEPTION_V3_CHECKPOINT_PATH = os.path.join(INCEPTION_PATH, "inception_v3.ckpt")\n\ndef download_progress(count, block_size, total_size):\n    percent = count * block_size * 100 // total_size\n    sys.stdout.write("\\rDownloading: {}%".format(percent))\n    sys.stdout.flush()\n\ndef fetch_pretrained_inception_v3(url=INCEPTION_V3_URL, path=INCEPTION_PATH):\n    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\n        return\n    os.makedirs(path, exist_ok=True)\n    tgz_path = os.path.join(path, "inception_v3.tgz")\n    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n    inception_tgz = tarfile.open(tgz_path)\n    inception_tgz.extractall(path=path)\n    inception_tgz.close()\n    os.remove(tgz_path)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'import re\n\nCLASS_NAME_REGEX = re.compile(r"^n\\d+\\s+(.*)\\s*$", re.M | re.U)\n\ndef load_class_names():\n    with open(os.path.join("datasets", "inception", "imagenet_class_names.txt"), "rb") as f:\n        content = f.read().decode("utf-8")\n        return CLASS_NAME_REGEX.findall(content)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import sys\nimport tarfile\nfrom six.moves import urllib\n\nFLOWERS_URL = "http://download.tensorflow.org/example_images/flower_photos.tgz"\nFLOWERS_PATH = os.path.join("datasets", "flowers")\n\ndef fetch_flowers(url=FLOWERS_URL, path=FLOWERS_PATH):\n    if os.path.exists(FLOWERS_PATH):\n        return\n    os.makedirs(path, exist_ok=True)\n    tgz_path = os.path.join(path, "flower_photos.tgz")\n    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n    flowers_tgz = tarfile.open(tgz_path)\n    flowers_tgz.extractall(path=path)\n    flowers_tgz.close()\n    os.remove(tgz_path)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'from scipy.misc import imresize\n\ndef prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n    """Zooms and crops the image randomly for data augmentation."""\n\n    # First, let\'s find the largest bounding box with the target size ratio that fits within the image\n    height = image.shape[0]\n    width = image.shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = width if crop_vertically else int(height * target_image_ratio)\n    crop_height = int(width / target_image_ratio) if crop_vertically else height\n        \n    # Now let\'s shrink this bounding box by a random factor (dividing the dimensions by a random number\n    # between 1.0 and 1.0 + `max_zoom`.\n    resize_factor = np.random.rand() * max_zoom + 1.0\n    crop_width = int(crop_width / resize_factor)\n    crop_height = int(crop_height / resize_factor)\n    \n    # Next, we can select a random location on the image for this bounding box.\n    x0 = np.random.randint(0, width - crop_width)\n    y0 = np.random.randint(0, height - crop_height)\n    x1 = x0 + crop_width\n    y1 = y0 + crop_height\n    \n    # Let\'s crop the image using the random bounding box we built.\n    image = image[y0:y1, x0:x1]\n\n    # Let\'s also flip the image horizontally with 50% probability:\n    if np.random.rand() < 0.5:\n        image = np.fliplr(image)\n\n    # Now, let\'s resize the image to the target dimensions.\n    image = imresize(image, (target_width, target_height))\n    \n    # Finally, let\'s ensure that the colors are represented as\n    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n    return image.astype(np.float32) / 255\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def prepare_image_with_tensorflow(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n    """Zooms and crops the image randomly for data augmentation."""\n\n    # First, let\'s find the largest bounding box with the target size ratio that fits within the image\n    image_shape = tf.cast(tf.shape(image), tf.float32)\n    height = image_shape[0]\n    width = image_shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = tf.cond(crop_vertically,\n                         lambda: width,\n                         lambda: height * target_image_ratio)\n    crop_height = tf.cond(crop_vertically,\n                          lambda: width / target_image_ratio,\n                          lambda: height)\n\n    # Now let\'s shrink this bounding box by a random factor (dividing the dimensions by a random number\n    # between 1.0 and 1.0 + `max_zoom`.\n    resize_factor = tf.random_uniform(shape=[], minval=1.0, maxval=1.0 + max_zoom)\n    crop_width = tf.cast(crop_width / resize_factor, tf.int32)\n    crop_height = tf.cast(crop_height / resize_factor, tf.int32)\n    box_size = tf.stack([crop_height, crop_width, 3])   # 3 = number of channels\n\n    # Let\'s crop the image using a random bounding box of the size we computed\n    image = tf.random_crop(image, box_size)\n\n    # Let\'s also flip the image horizontally with 50% probability:\n    image = tf.image.random_flip_left_right(image)\n\n    # The resize_bilinear function requires a 4D tensor (a batch of images)\n    # so we need to expand the number of dimensions first:\n    image_batch = tf.expand_dims(image, 0)\n\n    # Finally, let\'s resize the image to the target dimensions. Note that this function\n    # returns a float32 tensor.\n    image_batch = tf.image.resize_bilinear(image_batch, [target_height, target_width])\n    image = image_batch[0] / 255  # back to a single image, and scale the colors from 0.0 to 1.0\n    return image\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'from random import sample\n\ndef prepare_batch(flower_paths_and_classes, batch_size):\n    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n    prepared_images = [prepare_image(image) for image in images]\n    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n    return X_batch, y_batch\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '#apply style function to highlight profit losses\ndef color_negative_red(val):\n    """\n    Takes a scalar and returns a string with\n    the css property `\'color: red\'` for negative\n    strings, black otherwise.\n    """\n    color = \'red\' if val < 0 else \'black\'\n    return \'color: %s\' % color\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "#apply style function to highlight max values\ndef highlight_max(s):\n    '''\n    highlight the maximum in a Series yellow.\n    '''\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "#apply style function to highlight max values\ndef highlight_min(s):\n    '''\n    highlight the minimum in a Series pink.\n    '''\n    is_min = s == s.min()\n    return ['background-color: pink' if v else '' for v in is_min]```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def accuracy(predictions, labels):\n    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n            / predictions.shape[0])```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import datetime\n\ndef eta(start, n, total):\n    now = datetime.datetime.now()\n    diff = now - start\n    secs = (total-n) * 1.0 * diff.seconds / (n+1) # +1 to avoid zero division.\n    ends = now + datetime.timedelta(seconds=secs)\n    return ends.strftime("%H:%M:%S")\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def is_int(x):\n    try: \n        int(x)\n        return True\n    except ValueError:\n        return False\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def NONE(X):\n    """Return the values - placeholder function for other operations"""\n    return X\n```\n\nUnused functions:\n- NONE'}, {'reason': 'stop', 'result': '# Encoding Functions\n\ndef MAP(X):\n    """Map all values to integer numbers."""\n    """NaN values are treated as a unique value."""\n    \n    # create an encoding for categorical vars\n    unique_elems = set(X)\n    mapping = {label:idx for idx, label in enumerate(unique_elems)}\n    return X.map(mapping).astype(int)\n\ndef OHE(df_cv, df_all, col_name, feature_names, feature_threshold=0.02):\n    """Map categorical values to a one hot encoding scheme."""\n    \n    X_cv = MAP(df_cv[col_name])\n    X_all = MAP(df_all[col_name])\n    \n    X_cv = X_cv.values.reshape(-1, 1)\n    X_all = X_all.values.reshape(-1, 1)\n    OHE = OneHotEncoder(sparse=False).fit(X_all)\n    X_cv_ohe = OHE.transform(X_cv)\n    X_all_ohe = OHE.transform(X_all)\n    \n    low_freq_features = []\n    for i in range(X_all_ohe.shape[1]):\n        new_feature = col_name + str(i)\n        \n        # determine the frequency of the categorical data value\n        freq = np.sum(X_all_ohe[:, i]) / X_all_ohe.shape[0]\n        if freq > feature_threshold:\n            df_cv[new_feature] = X_cv_ohe[:, i]\n            df_all[new_feature] = X_all_ohe[:, i]\n            feature_names.append(new_feature)\n        else:\n            low_freq_features.append(i)\n    \n    # aggregate low frequency features\n    if len(low_freq_features) > 0:\n        extra_label = col_name + str(X_all_ohe.shape[1])\n        feature_names.append(extra_label)\n        \n        X_all_extra = np.array([0 for x in range(X_all.shape[0])])\n        X_cv_extra = np.array([0 for x in range(X_cv.shape[0])])\n        \n        for i in low_freq_features:\n            for idx, val in enumerate(X_all_ohe[:, i]):\n                if val == 1:\n                    X_all_extra[idx] = 1\n            for idx, val in enumerate(X_cv_ohe[:, i]):\n                if val == 1:\n                    X_cv_extra[idx] = 1\n        \n        df_cv[extra_label] = X_cv_extra\n        df_all[extra_label] = X_all_extra                    \n            \n    feature_names.remove(col_name)\n    df_cv = df_cv.drop(col_name, axis=1)\n    df_all = df_all.drop(col_name, axis=1)\n    \n    return df_cv, df_all, feature_names\n```\n\nUnused functions:\n- LOO'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef SCL1(X):\n    """Scale between (-1, 1)"""\n    mean = X.mean()\n    maximum = X.max()\n    minimum = X.min()\n    return (X - mean) / (maximum - minimum)\n\ndef TRSH(X, threshold_value=20):\n    X = [0 if val < threshold_value else 1 for val in X]\n    return X\n```\n\nUnused functions:\n- NRM1'}, {'reason': 'stop', 'result': '```python\n# Imputing Functions\n\ndef MEAN(X):\n    """Replace missing values with the mean of the others"""\n    \n    mean = np.mean(X)\n    X.fillna(value=mean, inplace=True)\n    return X\n\ndef MED(X):\n    """Replace missing values with median of data"""\n    \n    median = np.nanmedian(X)\n    X.fillna(value=median, inplace=True)\n    return X\n\ndef CONST(X, value=0):\n    """Replace missing values with a constant."""\n    \n    X.fillna(value=int(value), inplace=True)\n    return X\n\ndef MODE(X):\n    """Replace missing values with the mode."""\n    \n    mode = stats.mode(X)[0][0]\n    X.fillna(value=mode, inplace=True)\n    return X\n\ndef DEL(df_cv, df_all, col_name, feature_names):\n    df_cv = df_cv.drop(col_name, axis=1)\n    df_all = df_all.drop(col_name, axis=1)\n    feature_names.remove(col_name)\n    \n    return df_cv, df_all, feature_names\n```\n\nUnused functions:\n- UNIQ'}, {'reason': 'stop', 'result': 'class Preprocessor:\n    \n    def __init__(self, train_data_file, train_label_file, train_ids_file,\n                 instr_file, test_data_file=None, test_ids_file=None):\n        """A class to process and reformat data\n        for use in learning models"""\n        \n        # initialize the data the data filenames\n        self.train_data_file = train_data_file\n        self.train_label_file = train_label_file\n        self.train_ids_file = train_ids_file\n        self.instr_file = instr_file\n        \n        # test data is optional\n        self.test_data_file = test_data_file\n        self.test_ids_file = test_ids_file\n        \n    def read_data(self):\n        """Reads in data from the files passed to constructor"""\n        \n        # read in the data\n        train_X_df = pd.read_csv(self.train_data_file)\n        train_y_df = pd.read_csv(self.train_label_file)\n        train_ids_df = pd.read_csv(self.train_ids_file)\n        self.instr_df = pd.read_csv(self.instr_file)\n        \n        self.feature_names = [feature for feature in train_X_df]\n        self.original_feature_names = [feature for feature in train_X_df]\n        self.label_names = [feature for feature in train_y_df]\n        self.id_names = [feature for feature in train_ids_df]\n        \n        # create cross validation data\n        self.cv_X_df = pd.DataFrame(train_X_df)\n        self.cv_y_df = pd.DataFrame(train_y_df)\n        self.cv_ids_df = pd.DataFrame(train_ids_df)\n        \n        # read in the test data if it exists\n        if self.test_data_file != None:\n            self.test_X_df = pd.read_csv(self.test_data_file)\n            self.test_ids_df = pd.read_csv(self.test_ids_file)\n            self.all_X_df = train_X_df.append(self.test_X_df)\n        else:\n            self.test_X_df = None\n            self.test_ids_df = None\n            self.all_X_df = pd.DataFrame(train_X_df)\n        \n        # determine the shape of the input data\n        self.train_X_shape = train_X_df.shape\n        self.train_y_shape = train_y_df.shape\n        self.train_ids_shape = train_ids_df.shape\n        self.instr_shape = self.instr_df.shape\n        self.all_shape = self.all_X_df.shape\n        \n        # get size of test data if it exists\n        if self.test_data_file != None:\n            self.test_X_shape = self.test_X_df.shape\n            self.test_ids_shape = self.test_ids_df.shape\n        else:\n            self.test_X_shape = None\n            self.test_ids_shape = None\n\n        \n    def process(self, shuffle_train_data=False):\n        """Performs the processing on cross validation and train/test data"""\n        \n        # ADD OPTION TO SHUFFLE DATA HERE\n        \n        # processing on all data - remember to include cv_X and all_X for each condition\n        for col in self.original_feature_names:\n            print(col)\n            \n            # determine what to perform at each of the steps\n            col_instr = self.instr_df[col].values\n            col_enc = col_instr[1]\n            col_scl = col_instr[2]\n            col_imp = col_instr[3]\n\n            # impute values\n            # imputed first so that other functions will not use nan values in calculations\n            if col_imp == \'UNIQ\':\n                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n            if col_imp == \'MEAN\':\n                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n                self.all_X_df[col] = MEAN(self.all_X_df[col])\n            if col_imp == \'MODE\':\n                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n                self.all_X_df[col] = MODE(self.all_X_df[col])\n            if col_imp == \'MED\':\n                self.cv_X_df[col] = MED(self.cv_X_df[col])\n                self.all_X_df[col] = MED(self.all_X_df[col])\n            if is_int(col_imp):\n                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n            if col_imp == \'DEL\':\n                self.cv_X_df, self.all_X_df, self.feature_names = DEL(\n                    self.cv_X_df, self.all_X_df, col, self.feature_names)\n            \n            \n            # perform encoding of data\n            if col_enc == \'MAP\':\n                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n                self.all_X_df[col] = MAP(self.all_X_df[col])\n            if col_enc == \'OHE\':\n                self.cv_X_df, self.all_X_df, self.feature_names = OHE(\n                    df_cv=self.cv_X_df, df_all=self.all_X_df, col_name=col, \n                    feature_names=self.feature_names)\n            if col_enc == \'LOO\':\n                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n                self.all_X_df[col] = LOO(self.all_X_df[col])\n            \n\n            # perform scaling\n            if col_scl == \'NRM1\':\n                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n                self.all_X_df[col] = NRM1(self.all_X_df[col])\n            if col_scl == \'SCL1\':\n                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n                self.all_X_df[col] = SCL1(self.all_X_df[col])\n            if col_scl == \'TRSH\':\n                self.cv_X_df[col] = TRSH(self.cv_X_df[col])\n                self.all_X_df[col] = TRSH(self.all_X_df[col])\n\n        \n        # get the values from the dataframes\n        self.cv_X = self.cv_X_df.values\n        self.cv_y = self.cv_y_df.values\n        self.cv_ids = self.cv_ids_df.values\n        \n        all_X = self.all_X_df.values\n        self.train_X = all_X[:self.train_X_shape[0], :]\n        self.train_y = self.cv_y_df.values\n        self.train_ids = self.cv_ids_df.values\n        \n        if self.test_data_file != None:\n            self.test_X = all_X[self.train_X_shape[0]:, :]\n            self.test_ids = self.test_ids_df.values\n        else:\n            self.test_X = None\n            self.test_ids = None\n        \n    def write_data(self, out_dir=\'./processed_data/\'):\n        """Writes all of the data to output files"""\n        \n        # create the output directory if it does not exist\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n            \n        # convert arrays back into DataFrames\n        cv_X_df = pd.DataFrame(self.cv_X,  columns=self.feature_names)\n        cv_y_df = pd.DataFrame(self.cv_y, columns=self.label_names)\n        cv_ids_df = pd.DataFrame(self.cv_ids, columns=self.id_names)\n        train_X_df = pd.DataFrame(self.train_X, columns=self.feature_names)\n        train_y_df = pd.DataFrame(self.train_y, columns=self.label_names)\n        train_ids_df = pd.DataFrame(self.train_ids, columns=self.id_names)\n        if self.test_data_file != None:\n            test_X_df = pd.DataFrame(self.test_X, columns=self.feature_names)\n            test_ids_df = pd.DataFrame(self.test_ids, columns=self.id_names)\n        \n        # write the dataframes to file\n        cv_X_df.to_csv(out_dir+\'cv_X.csv\', index=False)\n        cv_y_df.to_csv(out_dir+\'cv_y.csv\', index=False)\n        cv_ids_df.to_csv(out_dir+\'cv_ids.csv\', index=False)\n        train_X_df.to_csv(out_dir+\'train_X.csv\', index=False)\n        train_y_df.to_csv(out_dir+\'train_y.csv\', index=False)\n        train_ids_df.to_csv(out_dir+\'train_ids.csv\', index=False)\n        if self.test_data_file != None:\n            test_X_df.to_csv(out_dir+\'test_X.csv\', index=False)\n            test_ids_df.to_csv(out_dir+\'test_ids.csv\', index=False)\n        \n    def select_features(self):\n        """Perform features selection / compression algs like PCA."""\n        """These will be implemented once more has been done."""\n        self.feature_names = self.feature_names\n```\n\nUnused functions:\n- select_features'}, {'reason': 'stop', 'result': 'def Hart(x,H):\n    F = 1.0;L = 1.0;Bo = 1.0;rhoo = 1.0\n    eta = 1.0; nu =1.0\n    u = (F*L)/(np.sqrt(rhoo*Bo))*np.sqrt(eta/nu)*(1/np.tanh(H))*(1-((np.cosh(H*x/L))/(np.cosh(H)))) \n    b = ((F*L)/(Bo))*(((np.sinh(H*x/L))/(np.sinh(H)))-x/L)\n    return [u,b]\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def Lectura(ux,uy,uz):\n    Ux = np.loadtxt(ux, unpack = True)\n    Uy = np.loadtxt(uy, unpack = True)\n    Uz = np.loadtxt(uz, unpack = True)\n    return Ux,Uy,Uz\n\ndef Grafica(udat,vdat,xdat,ydat,linea):\n    uesc = 1.0#dx/dt\n    u = np.loadtxt(udat, unpack = True)\n    v = np.loadtxt(vdat, unpack = True)\n    x = np.loadtxt(xdat, unpack = True)\n    y = np.loadtxt(ydat, unpack = True)\n    X = np.linspace(0,1,50)\n\n    f, axarr = plt.subplots(1,2, figsize=(24,6))\n    st = f.suptitle("$\\\\tau = 0.6$", fontsize=25)\n    st.set_y(1.0)\n    pasos = 12\n    M= np.hypot(u, v)\n    im=axarr[0].quiver(x,y,u,v,M , cmap=plt.cm.jet,width=0.022,scale=1/0.1)\n    axarr[0].set_title("Campos",fontsize = 20)\n    axarr[0].set_xlim(-0.01,1)\n    axarr[0].set_xlabel("$x[m]$",fontsize = 20)\n    axarr[0].set_ylabel("$y[m]$",fontsize = 20)\n    axarr[0].tick_params(axis="x", labelsize=20)\n    axarr[0].tick_params(axis="y", labelsize=20)\n    \n    axarr[1].plot(x,v[linea,:],"b", label = "Simulacion")\n    axarr[1].legend()\n    axarr[1].grid(True)\n    axarr[1].set_title(\'Perfil de Velocidad\',fontsize = 20,y=1.0)\n    axarr[1].set_xlabel("$x[m]$",fontsize = 20)\n    axarr[1].set_ylabel("$v[m/s]$",fontsize =20)\n    axarr[1].tick_params(axis="x", labelsize=20)\n    axarr[1].tick_params(axis="y", labelsize=20)\n    \n    cbar = f.colorbar(im, ax=axarr, shrink = 1.0)\n    cbar.set_label(\'$v[m/s]$\',fontsize =20)\n    cbar.ax.tick_params(labelsize=20)\n```\n\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to scikit-learn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {\'a\': [1, 5, 2, 5, 2, 8],\n               \'b\': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key=\'a\')\n    >> data[\'a\'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[:,[self.key]].astype(np.int64)\n\n    def get_feature_names(self):\n        return [dataset.columns[self.key]]'}, {'reason': 'stop', 'result': '#helper in getting top features and making visual\n#convert relevant dataframe columns to lowercase so we can compare with top feature output\n\ntrack_artist_lower_df = tracks_df["track_artist_uri"].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df["track_album_uri"].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged["track_uri"].apply(lambda x: x.lower())\n\n#Take a uri and return album, artist or song title in plain english\ndef get_translation(uri_type, uri):\n    \n    track = False  #if a single track/song is input as uri, I need to handle it differently\n    if uri_type == "track_artist_uri":\n        df = track_artist_lower_df\n        col = "track_artist_name"\n    elif uri_type == "track_album_uri":\n        df = track_album_lower_df\n        col = "track_album_name"\n    elif uri_type == "track_uri":\n        df = merged_track_uri_lower_df\n        col = "track_name"\n    for i in range(len(tracks_df)):\n        if df[i] == uri:\n            if track == True:\n                return merged.iloc[i][col]\n            return tracks_df.iloc[i][col]\n            break\n```\n\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def hypothesis_inlinecounter(text):\n    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split(\'\\n\')])[:-1]\n    return hyp\n\ndef hypothesis_inside_one(text, single):\n    hyp = re.sub(\'\\{}.*?\\{}\'.format(single, single), lambda m: single+\'#\'*(len(m.group())-2)+single, text)\n    return np.array([1 if x == \'#\' else -1 for x in hyp])\n\ndef hypothesis_inside_two(text, left, right):\n    hyp = np.full(len(text), -1)\n    inside = False\n    for i in range(len(text) - 1):\n        if text[i] == left:\n            inside = True\n        elif text[i] == right:\n            inside = False\n        if inside:\n            hyp[i+1] = 1\n    return hyp\n\nhypothesis_inside_quotation = lambda x: hypothesis_inside_one(x, \'"\')\nhypothesis_inside_parantheses = lambda x: hypothesis_inside_two(x, \'(\', \')\')\n\ndef hypothesis_comments(text):\n    hyp = np.full(len(text), -1)\n    in_brac_comment = False\n    in_line_comment = False\n    for i in range(len(text)):\n        if text[i:i+2] == \'//\':\n            in_line_comment = True\n        elif text[i] == \'\\n\':\n            in_line_comment = False\n        elif text[i:i+2] == \'/*\':\n            in_brac_comment = True\n        elif text[i:i+2] == \'*/\':\n            in_brac_comment = False\n        if in_brac_comment:\n            hyp[i:i+3] = 1\n        if in_line_comment:\n            hyp[i:i+1] = 1\n    return hyp\n\ndef hypothesis_indentation(text, level):\n    hyp = np.full(len(text), -1)\n    cur_level = 0\n    for i, char in enumerate(text):\n        if char == \'\\n\':\n            cur_level = 0\n        elif char == \'\\t\':\n            cur_level += 1\n        if cur_level >= level:\n            hyp[i] = 1\n    return hyp\n```\n\n\nUnused functions:\n- hypothesis_inlinecounter\n- hypothesis_inside_quotation\n- hypothesis_inside_parantheses\n- hypothesis_comments\n- hypothesis_indentation'}, {'reason': 'stop', 'result': 'def full_test(model, hypothesis, name, plot=False, train_len=95, test_len=10,\n              ex_name=\'test\'):\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(), hypothesis,\n                                         train_len=train_len, test_len=train_len,\n                                         save_hyp=\'plots/hyp_{}.png\'.format(ex_name),\n                                         save_diag=\'plots/diag_{}.png\'.format(ex_name),\n                                         save_resp=\'plots/resp_{}.png\'.format(ex_name))\n    metric_pearsonr = lambda a, b: stats.pearsonr(a, b)[0]\n    \n    print("Hypothesis: {} (normal)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))\n    y_true, y_pred = validate_hypothesis(model, LogisticRegression(class_weight=\'balanced\'),\n                                         hypothesis, train_len=train_len, test_len=test_len,\n                                         save_hyp=\'plots/hyp_{}_balanced.png\'.format(ex_name),\n                                         save_diag=\'plots/diag_{}_balanced.png\'.format(ex_name),\n                                         save_resp=\'plots/resp_{}_balanced.png\'.format(ex_name))\n    print("Hypothesis: {} (balanced)".format(name))\n    print(\'acc:      \', metrics.accuracy_score(y_true, y_pred))\n    print(\'prec:     \', metrics.precision_score(y_true, y_pred))\n    print(\'recall:   \', metrics.recall_score(y_true, y_pred))\n    print(\'f1-score: \', metrics.f1_score(y_true, y_pred))\n    print(\'pearsonr: \', metric_pearsonr(y_true, y_pred))```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def validate_hypothesis(model, diag_classifier, hypothesis, train_len=50,\n                        test_len=1, text_len=500, temperature=0.8,\n                        save_hyp=None, save_diag=None, save_resp=None):\n\n    # Generate train and test data\n    _, train_hyps, train_hiddens = gen_hyp_data(model, train_len)\n    test_texts, test_hyps, test_hiddens = gen_hyp_data(model, test_len)\n    print(pearsonr(train_hiddens, train_hyps))\n    print(pearsonr(test_hiddens, test_hyps))\n\n    # Train Diagnostic Classifier\n    diag_classifier.fit(train_hiddens, train_hyps)\n    \n    # Predict with Diagnostic Classifier\n    pred_hyps = diag_classifier.predict(test_hiddens)\n    \n    # Find responsible neuron\n    resp_neuron = np.argmax(np.abs(diag_classifier.coef_))\n    print(resp_neuron)\n    \n    # Plot results\n    if save_hyp:\n        plot_colored_text(test_texts[:text_len], test_hyps[:text_len],\n                          title='Formed Hypothesis',\n                          save_file=save_hyp)\n    if save_diag:\n        plot_colored_text(test_texts[:text_len], pred_hyps[:text_len],\n                          title='Diagnostic Classifier Prediction',\n                          save_file=save_diag)\n    if save_resp:\n        plot_colored_text(test_texts[:text_len], test_hiddens[:text_len, resp_neuron],\n                          title='Most Responsible Neuron {}'.format(resp_neuron),\n                          save_file=save_resp)\n        \n    del(train_hyps)\n    del(train_hiddens)\n    del(test_texts)\n    del(test_hiddens)\n    gc.collect()\n    \n    return test_hyps, pred_hyps\n```\n\nUnused functions: `gen_hyp_data`"}, {'reason': 'stop', 'result': "def hypothesis_pos(text, pos_tag):\n    hyp = text.replace('1', '0')\n    for word, tag in pynlpir.segment(text):\n        if tag == pos_tag:\n            hyp = hyp.replace(word, '1'*len(word), 1)\n        else:\n            hyp = hyp.replace(word, '0'*len(word), 1)\n    hyp = [1 if x=='1' else -1 for x in re.sub('[^1]', '0', hyp)]\n    \n    return hyp\n\ndef hypothesis_verbs(text):\n    return hypothesis_pos(text, 'verb')\n\ndef hypothesis_nouns(text):\n    return hypothesis_pos(text, 'noun')```\n\nUnused functions:\n- hypothesis_inlinecounter\n- hypothesis_capswords"}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef formfaktor(winkel, a, amp=1):\n    q = 4 * np.pi * n / wavelen * np.sin(winkel * gamma / 2)\n    return 9 * (np.sin(q * a) - (q * a) * np.cos(q * a)) ** 2 / (q * a) ** 6 * amp\n```\n\nUnused functions:\n- `formfaktorQ`'}, {'reason': 'stop', 'result': '```python\ndef FDM(f, a, b, N, u_bc):\n    # Solving -u\'\'=f in (a, b) by Finite Difference Method \n    # Dirichlet boundary conditions at two ending points\n    # Van-Dang Nguyen 2019  \n    xd = np.linspace(a, b, num=N)\n    # Grid size\n    h = xd[1] - xd[0]\n    ones = np.ones(N)\n\n    # Create left-hand side digonal matrix [1 -2 1]\n    data = -1 / (h * h) * np.array([ones, -2 * ones, ones])\n    diags = np.array([-1, 0, 1])\n    A = spdiags(data, diags, N, N, format="csr").toarray()\n\n    # Boundary conditions for the left-hand side \n    A[0, :] = 0  # zero out row 0 corresponding to x=a.\n    A[0, 0] = 1\n    A[N - 1, :] = 0  # zero out row N-1 corresponding to x=b.\n    A[N - 1, N - 1] = 1\n\n    # Boundary conditions for the right-hand side  \n    lam_f = sp.lambdify(x, f, modules=[\'numpy\'])\n    rhs = lam_f(xd)  # rhs = source \n    rhs[0], rhs[N - 1] = u_bc(xd[0]), u_bc(xd[N - 1])  # Dirichlet values\n\n    ## solving the linear system\n    u = spsolve(A, rhs, permc_spec=None, use_umfpack=True)\n    return xd, u\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def FEM(f, a, b, N, u_bc):\n    # Solving -u''=f in (a, b) by Linear Finite Element Method\n    # Dirichlet boundary conditions at two ending points\n    # Van-Dang Nguyen 2019\n\n    xd = np.linspace(a, b, num=N)\n    A = np.zeros((N, N))\n    rhs = [0] * N\n    for ele in range(0, N - 1):\n        node0, node1 = ele, ele + 1\n        x0, x1 = xd[node0], xd[node1]\n        phi = basis_func(x0, x1)\n        dphi = diff_basis_func(x0, x1)\n        b0 = sp.integrate(phi[0] * f, (x, x0, x1))\n        b1 = sp.integrate(phi[1] * f, (x, x0, x1))\n\n        A00 = sp.integrate(dphi[0] * dphi[0], (x, x0, x1))\n        A01 = sp.integrate(dphi[0] * dphi[1], (x, x0, x1))\n        A10 = sp.integrate(dphi[1] * dphi[0], (x, x0, x1))\n        A11 = sp.integrate(dphi[1] * dphi[1], (x, x0, x1))\n\n        A[node0, node0] += A00\n        A[node0, node1] += A01\n        A[node1, node0] += A10\n        A[node1, node1] += A11\n\n        rhs[node0] += float(b0)\n        rhs[node1] += float(b1)\n\n    # Boundary conditions for the left-hand side\n    A[0, :] = 0  # zero out row 0 corresponding to x=a.\n    A[0, 0] = 1\n    A[N - 1, :] = 0  # zero out row N-1 corresponding to x=b.\n    A[N - 1, N - 1] = 1\n\n    # Boundary conditions for the right-hand side\n    rhs[0], rhs[N - 1] = float(u_bc(xd[0])), float(u_bc(xd[N - 1]))  # Dirichlet values\n\n    ## solving the linear system\n    u = spsolve(A, rhs, permc_spec=None, use_umfpack=True)\n    return xd, u\n\ndef basis_func(x0, x1):\n    L = x1 - x0\n    phi = [0] * 2\n    phi[0] = -(x - x1) / L\n    phi[1] = (x - x0) / L\n    return phi\n\ndef diff_basis_func(x0, x1):\n    L = x1 - x0\n    dphi = [0] * 2\n    dphi[0] = -1. / L\n    dphi[1] = 1. / L\n    return dphi\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def friction(vi, vf, F0):\n    """\n    Returns the friction of the bottom plate against blocks moving at a specific velocity\n    \n    Arguments:  vi - initial velocity of the block\n                vf - final velocity of the block\n                F0 - the static friction force\n    \n    Returned: The force due to friction\n    \n    Examples:\n    \n    >>> friction (0, 1, 20)\n    -20.0\n    \n    >>> friction (1, 1, 20)\n    -10.0\n    """\n    # Calculates sign of vi\n    if vi == 0:\n        sign = 1\n        \n    else:\n        sign = vi / abs(vi)\n        \n    force = -((F0) * sign / (1 + abs(vi/vf)))\n    return force\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def blockMotion (t, blockPositions, vBlock, i, blockNum, kp, kc, mass, F0, v0, vf):\n    """\n    Returns the differential equation that models the motion of the blocks\n    \n    Arguments:  t - time\n                blockPositions - the positions of the blocks\n                vBlock - the velocity of the block\n                i - the index of the current block\n                blockNum - the number of blocks\n                kp - spring constant of leaf springs\n                kc - spring constant of springs between blocks\n                mass - mass of individual block\n                F0 - the static friction force\n                v0 - initial velocity of top plate\n                vf - the friction coefficient\n                \n    Returned: The differential equation modeling the motion of the individual blocks\n    \n    Examples:\n    \n    >>> blockMotion (0, (0, 1, 2, 3, 4), 0, 2, 5, 0, 0, 1, 0, 1, 20)\n    array([ 0.,  0.])\n    \n    """\n    # Sets position and velocity of the block\n    xi = blockPositions[i] - i\n    vi = vBlock\n    \n    # Block motion for the first block, connected to a block one distance unit away\n    if i == 0:\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc*(xiP - xi) + kp * (v0 * t - xi)\n    \n    # Block motion for the last block, connected to a block one distance unit away\n    elif i == blockNum - 1:\n        xiM = blockPositions[i - 1] - (i - 1)\n        springForce = kc*(xiM - xi) + kp * (v0 * t - xi)\n   \n    # Block motion for all the middle blocks, connected to their neighbors\n    else:\n        xiM = blockPositions[i - 1] - (i - 1)\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc*(xiP + xiM - 2 * xi) + kp * (v0 * t - xi)\n    \n    frictionForce = friction (vi, vf, F0)\n    \n    # If spring force is large enough to overpower friction, change velocity\n    if abs(springForce) <= abs(frictionForce):\n        dv = -vi\n        vi = 0\n        dx = vi\n    \n    else: \n        totalForce = (springForce + frictionForce) / mass\n        dx = vi\n        dv = totalForce\n        \n    return np.array([dx, dv], float)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def blockMotionVaryingMasses(t, blockPositions, vBlock, i, blockNum, kp, kc, mass, F0, v0, vf):\n    """\n    Returns the differential equation that models the motion of the blocks\n    \n    Arguments:  t - time\n                blockPositions - the positions of the blocks\n                vBlock - the velocity of the block\n                i - the index of the current block\n                blockNum - the number of blocks\n                kp - spring constant of leaf springs\n                kc - spring constant of springs between blocks\n                mass - masses of individual blocks\n                F0 - the static friction force\n                v0 - initial velocity of top plate\n                vf - the friction coefficient\n                \n    Returned: The differential equation modeling the motion of the individual blocks\n    \n    Examples:\n    \n    >>> blockMotion (0, (0, 1, 2, 3, 4), 0, 2, 5, 0, 0, 1, 0, 1, 20)\n    array([ 0.,  0.])\n    \n    """\n    # Sets position and velocity of the block\n    xi = blockPositions[i] - i\n    vi = vBlock\n    mi = mass[i]\n    \n    # Block motion for the first block, connected to a block one distance unit away\n    if i == 0:\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc*(xiP - xi) + kp * (v0 * t - xi)\n    \n    # Block motion for the last block, connected to a block one distance unit away\n    elif i == blockNum - 1:\n        xiM = blockPositions[i - 1] - (i - 1)\n        springForce = kc*(xiM - xi) + kp * (v0 * t - xi)\n   \n    # Block motion for all the middle blocks, connected to their neighbors\n    else:\n        xiM = blockPositions[i - 1] - (i - 1)\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc*(xiP + xiM - 2 * xi) + kp * (v0 * t - xi)\n    \n    frictionForce = friction(vi, vf, F0)\n    \n    # If spring force is large enough to overpower friction, change velocity\n    if abs(springForce) <= abs(frictionForce):\n        dv = -vi\n        vi = 0\n        dx = vi\n    \n    else: \n        totalForce = (springForce + frictionForce) / mi\n        dx = vi\n        dv = totalForce\n        \n    return np.array([dx, dv], float)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def eulerCoupledAdaptive(f, n, blockPositions, vBlock, blockNum, h1, h2, maxPoints, minBlockV, *args, verbose=False):\n    """ \n    Solve a coupled system of ODEs by Euler method with fixed number of steps.\n\n    Arguements: f - function giving ODE as y\'=f(x,y)\n                n - the number of ODEs\n                blockPositions - the array containging the initial block positions\n                vBlock - initial block velocity\n                blockNum - the number of blocks\n                interval - tuple region (a,b) on which to solve ODE\n                steps - number of steps\n    \n    Returned: An array containing the positions and velocies of the blocks over time\n    """\n    \n    points = np.zeros((maxPoints, blockNum*2 + 1)) # array to contain all the block positions and velocities over time\n\n    # set up initial point\n    t = 0\n    points[0,0] = t\n    count = 0\n    \n    # Sets up first row of poitns\n    for l in range(1, blockNum + 1):\n        points[0,l] = blockPositions[count]   \n        count += 1\n    for m in range(blockNum + 1, blockNum * 2 + 1):\n        points[0,m] = vBlock\n    \n    # initializes count, dv and r\n    count = 1\n    dv = 0\n    r = (0,0)\n    \n    while points[maxPoints - 1, 0] == 0: # Repeats until points array is filled up                   \n        \n        # Proceeds if none of the blocks are moving\n        if dv < minBlockV:\n            h = h1 # Long timestep\n            \n            # Repeats Euler calculation until one of the blocks moves or points is full\n            while dv < minBlockV and points[maxPoints - 1, 0] == 0:\n                \n                if verbose == True:\n                    print("h1:", t)\n                \n                # Saves block positions and updates timestep\n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n                \n                # Repeats Euler calculation for each block\n                for i in range(0, blockNum): \n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n                        \n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n                \n        # Proceeds if any of the blocks are moving        \n        elif points[maxPoints - 1, 0] == 0:\n            # Resets to previous timestep\n            h = h2 # Short timestep\n            t -= h1\n            count -= 1\n            blockPositions = oldBlockPositions\n            dv = 0\n\n            # Repeats Euler calculation until one of the blocks moves or points is full\n            # This is necessary, because the timestep was reset, so dv will still be zero\n            while dv < minBlockV and points[maxPoints - 1, 0] == 0: \n                \n                if verbose == True:\n                    print("h2:", t)\n                \n                # Saves block positions and updates timestep                \n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n\n                # Repeats Euler calculation for each block                \n                for i in range(0, blockNum):\n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new            \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n\n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n                \n            # Repeats Euler calculation until all of the blocks are still or points is full                \n            while dv >= minBlockV and points[maxPoints - 1, 0] == 0: \n                \n                if verbose == True:\n                    print("h2:", t)\n                \n                # Saves block positions and updates timestep\n                oldBlockPositions = blockPositions\n                t = t + h\n                points[count,0] = t\n                dv = 0\n\n                # Repeats Euler calculation for each block  \n                for i in range(0, blockNum):\n\n                    r = np.array([points[count-1, i + 1], points[count-1, i + 1 + blockNum]]) # Updates r from previous timestep\n\n                    r_new = r + h * f(t, blockPositions, r[1], i, blockNum, *args) # Calculates new r\n\n                    r = r_new            \n                    blockPositions[i] = r[0]\n                    \n                    # Updates dv\n                    if r[1] > dv:\n                        dv = r[1]\n\n                    if verbose == True:\n                        print(i, blockPositions[i], r[1], dv)\n                        \n                    # Saves values to points\n                    points[count,i + 1] = r[0]\n                    points[count,i + 1 + blockNum] = r[1]\n\n                count += 1\n\n    return points\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def countQuakes (earthquakes, blockNum, timeStep):\n    """\n    Produces a list of each earthquake and its magnitude\n    \n    Arguments:  earthquakes - the array containing all earthquake data\n                blockNum - the number of blocks\n                i - the block index to be examined\n                \n    Returned: a list containing the magnitude of each earthquake\n    """    \n    quakes = []\n    time = []\n    count = 0\n    quakeNum = -1\n    dvOld = 0\n    \n    # Repeats for each row of earthquakes\n    while count < len(earthquakes):\n        \n        # Determines if any of the blocks are moving\n        dv = 0\n        for i in range (0, blockNum):\n            if dv < earthquakes[count, blockNum + i + 1]:\n                dv = earthquakes[count, blockNum + i + 1]\n        \n        # If any of the blocks are accelerating add an earthquake\n        if dv > dvOld:\n            quakeNum += 1\n            quakes.append(0)\n            time.append(earthquakes[count, 0])\n            \n            # Add the motion from all the blocks until the earthquake stops\n            earthquakeLength = 0\n            while dv > 0.0001 and count < len(earthquakes) - 1 and earthquakeLength < 200:\n\n                for i in range (0, blockNum):\n                    quakes[quakeNum] += earthquakes[count, blockNum + i + 1] * timeStep\n\n                # Incriments count and recalculates dv\n                count += 1            \n                dv = 0\n                for i in range (0, blockNum):\n                    if dv < earthquakes[count, blockNum + i + 1]:\n                        dv = earthquakes[count, blockNum + i + 1]\n                earthquakeLength += 1\n    \n        else:\n            count += 1\n        dvOld = dv\n        \n    return time, quakes\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef countMagnitudes(magnitudes):\n    """\n    Produces a list of the number of earthquakes that fall within a certain range of magnitudes\n    \n    Arguments: magnitudes - a list of magnitudes\n                \n    Returned: a list containing the number of occurrences of each magnitude\n    """\n    magValues = np.linspace(-7, 7, 29)\n    occurences = np.zeros(29, int)\n    \n    for i in range(0, len(magnitudes)):\n        count = 0\n        for n in magValues:\n            if magnitudes[i] >= n and magnitudes[i] < n + 0.25:\n                occurences[count] += 1\n            count += 1\n                \n    return magValues, occurences\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'class BucketDataIterator():\n    """ Iterator for feeding seq2seq model during training """\n    def __init__(self,\n                 images,\n                 targets,\n                 num_buckets=5,\n                 train=True):\n        \n        self.train = train\n        length = [len(seq) for seq in images]  \n\n        # Create pandas dataFrame and sort it by images seq lenght (length) \n        # in_length == out_length\n        self.dataFrame = pd.DataFrame({\'length\': length,\n                                       \'images\': images,\n                                       \'targets\': targets\n                                      }).sort_values(\'length\').reset_index(drop=True)\n\n        bsize = int(len(images) / num_buckets)\n        self.num_buckets = num_buckets\n        \n        # Create buckets by slicing parts by indexes\n        self.buckets = []\n        for bucket in range(num_buckets-1):\n            self.buckets.append(self.dataFrame.iloc[bucket * bsize: (bucket+1) * bsize])\n        self.buckets.append(self.dataFrame.iloc[(num_buckets-1) * bsize:])        \n        \n        self.buckets_size = [len(bucket) for bucket in self.buckets]\n\n        # cursor[i] will be the cursor for the ith bucket\n        self.cursor = np.array([0] * num_buckets)\n        self.bucket_order = np.random.permutation(num_buckets)\n        self.bucket_cursor = 0\n        self.shuffle()\n        print("Iterator created.")\n\n\n    def shuffle(self, idx=None):\n        """ Shuffle idx bucket or each bucket separately """\n        for i in [idx] if idx is not None else range(self.num_buckets):\n            self.buckets[i] = self.buckets[i].sample(frac=1).reset_index(drop=True)\n            self.cursor[i] = 0\n\n\n    def next_batch(self, batch_size):\n        """\n        Creates next training batch of size: batch_size\n        Retruns: image seq, letter seq, seq lengths\n        """\n        i_bucket = self.bucket_order[self.bucket_cursor]\n        # Increment cursor and shuffle in case of new round\n        self.bucket_cursor = (self.bucket_cursor + 1) % self.num_buckets\n        if self.bucket_cursor == 0:\n            self.bucket_order = np.random.permutation(self.num_buckets)\n            \n        if self.cursor[i_bucket] + batch_size > self.buckets_size[i_bucket]:\n            self.shuffle(i_bucket)\n\n        # Handle too big batch sizes\n        if (batch_size > self.buckets_size[i_bucket]):\n            batch_size = self.buckets_size[i_bucket]\n\n        res = self.buckets[i_bucket].iloc[self.cursor[i_bucket]:\n                                          self.cursor[i_bucket]+batch_size]\n        self.cursor[i_bucket] += batch_size\n\n        # PAD input sequence and output\n        # Pad sequences with <PAD> to same length\n        max_length = max(res[\'length\'])\n        \n        input_seq = np.zeros((batch_size, max_length, N_INPUT), dtype=np.float32)\n        for i, img in enumerate(res[\'images\']):\n            input_seq[i][:res[\'length\'].values[i]] = img\n        \n        # Need to pad according to the maximum length output sequence\n        targets = np.ones([batch_size, max_length], dtype=np.float32) * PAD\n        for i, target in enumerate(targets):\n            target[:res[\'length\'].values[i]] = res[\'targets\'].values[i]\n        \n        return input_seq, targets, res[\'length\'].values\n\n\n    def next_feed(self, size):\n        """ Create feed directly for model training """\n        (inputs_,\n         targets_,\n         length_) = self.next_batch(size)\n        return {\n            inputs: inputs_,            \n            targets: targets_,\n            length: length_,\n            keep_prob: (1.0 - dropout) if self.train else 1.0\n        }```'}, {'reason': 'stop', 'result': 'def next_batch(size, i):\n    return train_data[i*size:(i+1)*size]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def calc_loss(y, y_):\n    # Calcula cross-entropy usando y como resposta real e y_ como previsão\n    return np.mean(-y * np.log(y_ + 1e-8) - (1-y) * np.log(1-y_ + 1e-8))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def forward_pass(x, W, b):\n    # Calcula a classe baseado em um vetor de pesos e um escalar de víes\n    # Note que nessa função aceitam-se matrizes para W e vetores para b,\n    # afinal estamos trabalhando com batches e não com um exemplo de\n    # cada vez.\n    z = x.dot(W) + b\n    y_ = sigmoid(z)\n    return y_, z\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def backward_pass(x, y, W, b):\n    # Responsável por calcular as derivadas parciais para W e b.\n    # Funciona com batches também, nesse caso retornando a média\n    # dos gradientes.\n    y_, z = forward_pass(x, W, b)\n    dLdb = (-y/(y_ + 1e-8) + (1-y)/(1-y_ + 1e-8)) * sigmoid(z, True)\n    dLdW = np.zeros((dLdb.shape[0], 31))\n    for i, el in enumerate(dLdb):\n        dLdW[i] = x[i] * el\n    dLdW = np.mean(dLdW, axis=0)\n    dLdb = np.mean(dLdb, axis=0)\n    return dLdW, dLdb\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def next_batch(size, user):\n    # Retorna um batch retirado do conjunto de treino.\n    # A cada vez que a função é chamada, um novo batch\n    # é retornado até todo o dataset ser coberto.\n    for i in range(train_data.shape[0] // size):\n        l = (train_labels[i*size:(i+1)*size] == user).astype(np.float64)\n        k = train_data[i*size:(i+1)*size]\n        yield k,l\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def test_accuracy(user, W, b):\n    # Função que mede a acurácia do modelo em um conjunto de teste.\n    predictions = forward_pass(test_data, W, b)[0]\n    accuracy = np.mean((predictions > 0.5) == (test_labels == user))\n    return accuracy, predictions\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def next_batch(size, i):\n    # Recorta o bloco i de tamanho size de labels do\n    # conjunto de treinamento.\n    l = train_labels[i*size:(i+1)*size]\n    \n    # Prepara os vetores que viram a ser one-hot.\n    # Um para cada exemplo do batch.\n    onehot = np.zeros((len(l),51))\n    \n    # Itera sobre as labels, marcando 1 na posição\n    # correta de cada vetor one-hot.\n    for j, idx in enumerate(l):\n        onehot[j][idx] = 1\n        \n    # Recorta os vetore de entrada do conjunto de\n    # treinamento.\n    k = train_data[i*size:(i+1)*size]\n    \n    # Retorna os vetores e labels correspondentes do\n    # batch.\n    return k, onehot.astype(float)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def test_accuracy():\n    with tf.device("/device:GPU:0"):\n        hits_and_misses = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n        accuracy = tf.reduce_mean(tf.cast(hits_and_misses, tf.float32))\n        return accuracy.eval(feed_dict={x: test_data, y: test_labels_one_hot})```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef forward_sub(A, b):\n    """\n    Implements the forward-substitution algorithm to solve a lower triangular system of equations\n    """\n    n, m = A.shape\n    \n    assert n == m, "A must be a square matrix"\n    \n    x = np.zeros(n)\n    for i in range(n):\n        \n        summ = 0\n        for j in range(i):\n            summ += A[i, j] * x[j]\n        \n        x[i] = (b[i] - summ) / A[i, i]   \n    \n    return x\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef vm(n):\n    ## define matrix\n    A = np.array([i**j for i in range(1,n+1) for j in range(0, n)] )\n    A.shape = (n, n)\n    ## determine the solution vector\n    x_target = np.ones(n)\n    b = A @ x_target\n    ## solve SLE\n    x = np.linalg.solve(A, b)\n    \n    return x, A\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def gauss_seidel(A, b, x0):\n    """ \n    Implements the Gauss-Seidel method with a over-relaxation parameter\n    """\n    ## tolerance level for stopping rule\n    tol = 1e-8\n    eps = 1\n    ## iteration counter and max number of iterations\n    it = 0\n    maxit = 100\n    \n    ## initialize x\n    x = x0\n    Q = np.triu(A)\n    Q_inv = np.linalg.inv(Q)\n    \n    while eps > tol and it < maxit:\n        it += 1 \n        x_new = Q_inv @ b + ( np.eye(len(b)) - Q_inv @ A) @ x\n    \n        eps = np.linalg.norm(x_new - x)\n        \n        x = x_new\n        \n    return x\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'import pandas as pd \nimport numpy as np\n\ndata_ep = pd.read_csv(\'data/exoplanet.eu_catalog.csv\', \n                      usecols=[\'mass\',\'mass_error_min\',\'mass_error_max\',\n                               \'semi_major_axis\',\'semi_major_axis_error_min\',\'semi_major_axis_error_max\',\'star_name\'])\n\nclass System:\n    def __init__(self, data):\n        self.data=data\n        self.system = list(self.data.groupby("star_name").groups.keys())\n        self.Number()\n        self.Mass()\n        self.CenterOfMass()\n        \n    def Number(self):\n        sys = self.data.groupby("star_name")\n        self.N_total = len(sys["mass"])\n        \n    def Mass(self):\n        sys = self.data.groupby("star_name")\n        self.M_total = sys["mass"].sum()\n    \n    def CenterOfMass(self):\n        self.rm_i = self.data["mass"].multiply(self.data["semi_major_axis"])\n        self.data_i = self.data.assign(CM_i = self.rm_i.values) \n        p_system = self.data_i.groupby("star_name")\n        sum_rm = p_system[\'CM_i\'].sum()\n        self.CM = sum_rm.divide(self.M_total)'}, {'reason': 'stop', 'result': '```python\nimport random\n\ndef make_document(doc):\n    source = ColumnDataSource({\'x\': [], \'y\': [], \'color\': []})\n\n    def update():\n        new = {\'x\': [random.random()],\n               \'y\': [random.random()],\n               \'color\': [random.choice([\'red\', \'blue\', \'green\'])]}\n        source.stream(new)\n\n    doc.add_periodic_callback(update, 100)\n\n    fig = figure(title=\'Streaming Circle Plot!\', sizing_mode=\'scale_width\',\n                 x_range=[0, 1], y_range=[0, 1])\n    fig.circle(source=source, x=\'x\', y=\'y\', color=\'color\', size=10)\n\n    doc.title = "Now with live updating!"\n    doc.add_root(fig)\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# maak document, met daarin grafiek. Hier zullen (uiteindelijk) meerdere grafieken moeten worden \n# weergegeven met meerdere opties.\n\ndef make_document(doc):\n    #Kolommen zijn de kolommen uit de df_totaalSBB dataframe. Dit gaat dan om 2013 AANT etc. Dit is voor de eerste groepering. \n    kolommen = list(df_totaalSBB)\n\n    #Sectoren haalt alle unieke waardes uit het dataframe voor de kolom SECTORUNIT SBB.\n    sectoren = list(df[\'SECTORUNIT SBB\'].unique())\n\n    #Voeg per kolom alle sectoren toe. Dit wordt dan de data voor de x-as. \n    #Kolom is bijvoorbeeld 2013 AANT en hier worden vervolgens alle sectoren aan toegevoegd.\n    x = [ (kolom, sector) for kolom in kolommen for sector in sectoren ]\n\n    #Data voor de y-as\n    #Data doorlopen om de totalen op de juiste volgorde in een array te plaatsen.\n    #Zelfde volgorde zoals hierboven staat voor de gegevens voor de x-as\n\n    counts = []\n\n    for kolom in kolommen:\n        for sector in sectoren:\n            counts.append(df_totaalSBB.loc[sector][kolom])\n\n    #Teken grafiek\n    source = ColumnDataSource(data=dict(x=x, counts=counts))\n\n    p = figure(x_range=FactorRange(*x), plot_height=400, title="Totalen per SBB sector per jaar")\n\n    p.vbar(x=\'x\', top=\'counts\', width=0.8, source=source)\n\n    p.width=900\n    p.y_range.start = 0\n    p.x_range.range_padding = 0.1\n    p.xaxis.major_label_orientation = 1\n    p.xgrid.grid_line_color = None\n\n\n    #update functie om nieuwe data(selecties) weer te geven\n    def updateSBB(attr, old, new):\n        kolommen = [jaren.labels[i] for i in jaren.active]\n        x = [ (kolom, sector) for kolom in kolommen for sector in sectoren ]\n\n        counts = []\n\n        for kolom in kolommen:\n            for sector in sectoren:\n                counts.append(df_totaalSBB.loc[sector][kolom])\n\n        newSource = ColumnDataSource(data=dict(x=x, counts=counts))\n        source.data.update(newSource.data)\n    \n    \n    jaren = CheckboxGroup(labels=kolommen, active = [0, 1, 2, 3 ,4])\n    jaren.on_change(\'active\', updateSBB)\n\n    layout = row(p, jaren)\n\n    #callback om updates elke 100ms op te halen\n    #doc.add_periodic_callback(update, 100)\n\n    doc.title = "Sectorunit SBB chart..."\n    doc.add_root(layout)'}, {'reason': 'stop', 'result': 'def updateSBB(attr, old, new):\n    kolommen = [jaren.labels[i] for i in jaren.active]\n    x = [(kolom, sector) for kolom in kolommen for sector in sectoren]\n\n    counts = []\n\n    for kolom in kolommen:\n        for sector in sectoren:\n            counts.append(df_totaalSBB.loc[sector][kolom])\n\n    newSource = ColumnDataSource(data=dict(x=x, counts=counts))\n    source.data.update(newSource.data)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def test():\n    print("test")```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'filename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)'}, {'reason': 'stop', 'result': '# Helper function that takes numpy array of the raw time series and a lag or number of shifted series to create \n# and use as inputs\ndef timeseries_to_supervised(data,lag=1):\n    df=pd.DataFrame(data)\n    columns=[df.shift(i) for i in range(1,lag+1)] # depends on how many lags are required\n    columns.append(df) # Appending list to each other \n    df=pd.concat(columns,axis=1)\n    df.fillna(0,inplace=True) #takes care of the Nan value\n    return df```'}, {'reason': 'stop', 'result': 'def difference(dataset,interval=1):\n    diff=list()\n    for i in range(interval, len(dataset)):\n        value=dataset[i]-dataset[i-interval] # calcualting row vs row(-1) differnce\n        diff.append(value)\n    return pd.Series(diff)'}, {'reason': 'stop', 'result': '# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def fit_lstm(train, batch_size, nb_epoch, neurons):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def forecast(model, batch_size,row):\n    X=row[0:-1]\n    X=X.reshape(1,1,len(x))\n    yhat=model.predict(X,batch_size=batch_size)\n    return yhat[0,0]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from pandas import DataFrame\nfrom pandas import Series\nfrom pandas import concat\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nfrom matplotlib import pyplot\nimport numpy\n\nfilename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\n# datetime parsing function for loading the dataset\ndef parser(x):\n    return datetime.strptime(\'190\'+x, \'%Y-%b\') # String manpuation\n\n# create a differenced series\ndef difference(dataset,interval=1):\n    diff=list()\n    for i in range(interval, len(dataset)):\n        value=dataset[i]-dataset[i-interval]\n        diff.append(value)\n    return Series(diff)\n```\n\nUnused functions:\n- timeseries_to_supervised'}, {'reason': 'stop', 'result': '# scale train and test data to [-1,1]\ndef scale(train,test):\n    # fit scaler\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler=scaler.fit(train)\n    # transform train\n    train = train.reshape(train.shape[0],train.shape[1])\n    train_scaled=scaler.transform(train)\n    # transform test\n    test = test.reshape(test.shape[0],test.shape[1])\n    test_scaled=scaler.transform(test)\n    return scaler, train_scaled, test_scaled\n\n# fit an LSTM network to training data\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n    X,y=train[:,0:-1],train[:,-1]\n    X=X.reshape(X.shape[0],1,X.shape[1])\n    model=Sequential()\n    model.add(LSTM(neurons,batch_input_shape=(batch_size,X.shape[1],X.shape[2]),stateful=True))\n    model.add(Dense(1))\n    model.compile(loss="mean_squared_error",optimizer="adam")\n    for i in range(nb_epoch):\n        model.fit(X,y,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)\n        model.reset_states()\n    return model\n```\n\nUnused functions:\n- inverse_difference\n- invert_scale'}, {'reason': 'stop', 'result': '# load the dataset\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)\n\n# transform data to be staionary\nraw_values=series.values\ndiff_values=difference(raw_values,1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values,1)\nsupervised_values=supervised.values\n\n# split data into train and test-sets\ntrain,test=supervised_values[0:-12],supervised_values[-12:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)\n```\n\nUnused functions:\n- forecast_lstm'}, {'reason': 'stop', 'result': 'from pandas import DataFrame\nfrom pandas import Series\nfrom pandas import concat\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nfrom matplotlib import pyplot\nimport numpy\n\nfilename="/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv"\n\n# frame a sequence as a supervised learning problem\ndef timeseries_to_supervised(data, lag=1):\n\tdf = DataFrame(data)\n\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n\tcolumns.append(df)\n\tdf = concat(columns, axis=1)\n\tdf.fillna(0, inplace=True)\n\treturn df\n\n# create a differenced series\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn Series(diff)\n\n# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n\treturn yhat + history[-interval]\n\n# scale train and test data to [-1, 1]\ndef scale(train, test):\n\t# fit scaler\n\tscaler = MinMaxScaler(feature_range=(-1, 1))\n\tscaler = scaler.fit(train)\n\t# transform train\n\ttrain = train.reshape(train.shape[0], train.shape[1])\n\ttrain_scaled = scaler.transform(train)\n\t# transform test\n\ttest = test.reshape(test.shape[0], test.shape[1])\n\ttest_scaled = scaler.transform(test)\n\treturn scaler, train_scaled, test_scaled\n\n# inverse scaling for a forecasted value\ndef invert_scale(scaler, X, value):\n\tnew_row = [x for x in X] + [value]\n\tarray = numpy.array(new_row)\n\tarray = array.reshape(1, len(array))\n\tinverted = scaler.inverse_transform(array)\n\treturn inverted[0, -1]\n\n# fit an LSTM network to training data\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n\tX, y = train[:, 0:-1], train[:, -1]\n\tX = X.reshape(X.shape[0], 1, X.shape[1])\n\tmodel = Sequential()\n\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n\tmodel.add(Dense(1))\n\tmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\tfor i in range(nb_epoch):\n\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n\t\tmodel.reset_states()\n\treturn model\n\n# make a one-step forecast\ndef forecast_lstm(model, batch_size, X):\n\tX = X.reshape(1, 1, len(X))\n\tyhat = model.predict(X, batch_size=batch_size)\n\treturn yhat[0,0]\n\n# load dataset\nseries=read_csv(filename, header=0,parse_dates=[0],index_col=0,squeeze=True)\n\n# transform data to be stationary\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\ntrain, test = supervised_values[0:-12], supervised_values[-12:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)\n\n# fit the model\nlstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n# forecast the entire training dataset to build up state for forecasting\ntrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\nlstm_model.predict(train_reshaped, batch_size=1)\n\n# walk-forward validation on the test data\npredictions = list()\nfor i in range(len(test_scaled)):\n\t# make one-step forecast\n\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n\tyhat = forecast_lstm(lstm_model, 1, X)\n\t# invert scaling\n\tyhat = invert_scale(scaler, X, yhat)\n\t# invert differencing\n\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n\t# store forecast\n\tpredictions.append(yhat)\n\texpected = raw_values[len(train) + i + 1]\n\tprint(\'Month=%d, Predicted=%f, Expected=%f\' % (i+1, yhat, expected))\n\n# report performance\nrmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\nprint(\'Test RMSE: %.3f\' % rmse)\n# line plot of observed vs predicted\npyplot.plot(raw_values[-12:])\npyplot.plot(predictions)\npyplot.show()'}, {'reason': 'stop', 'result': "def get_missed_predictions(tourney_comp_ratings, model_features, numeric_feature_to_scale,\n                           prediction_probabilities, X, y, y_pred):\n    \n    pred_probs = pd.Series(prediction_probabilities[:,1], index=X.index)\n    predictions = pd.Series(y_pred, index=y.index)\n    test_games = tourney_comp_ratings[tourney_comp_ratings.index.isin(X.index)].copy()\n    test_games[numeric_feature_to_scale] = scaler.inverse_transform(test_games[numeric_feature_to_scale])\n    test_games['predicted_result'] = predictions\n    test_games['pred_win_prob'] = pred_probs\n    missed_predictions = test_games[test_games['game_result'] != \n                                test_games['predicted_result']].sort_values(by='pred_win_prob', ascending=False)\n   \n    missed_predictions.apply(lambda x: feature_dictionary.print_game_info(test_games,x['season_t'], x['round'], x['team_t'] ), axis=1)\n    \n    return missed_predictions\n```\n\nUnused functions:\n- `supporting_features`\n- `supporting_model_features`"}, {'reason': 'stop', 'result': 'def eliminate_features_logistic_regression(classifier, X, y ):\n    iteration = 0\n    print("Iteration= ", iteration)\n    iteration += 1\n    model_stats = {}\n    drop_list = []\n    # get baseline by identifying sorted important features using all of the provided features\n    model_stats = utils.save_model_stats(classifier,X,y,model_stats)\n    important_features = utils.display_important_features(classifier.coef_[0], X,0)\n    # least important feature\n    least_important_label = important_features[-1][0]\n    print("least_important label= ", least_important_label)\n    \n    drop_list.append(least_important_label)\n    del important_features[-1]\n    \n    # drop list contains all of the feature labels except for the feature label identified as being most important\n    list_count = len(important_features)\n    while list_count > 0:\n        print("Iteration= ", iteration)\n        iteration += 1\n        model_stats = utils.save_model_stats(classifier,X.drop(columns=drop_list),y,model_stats)\n        least_important_label = important_features[-1][0]\n        print("least_important label= ", least_important_label)\n        drop_list.append(least_important_label)\n        del important_features[-1]\n        list_count-=1\n    return model_stats\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class EuropeanOption(object):\n    """ Abstract Class for European options. Partially implemented.\n    S0 : float : initial stock/index level\n    strike : float : strike price\n    T : float : time to maturity (in year fractions)\n    r : float : constant risk-free short rate\n    div :    float : dividend yield\n    sigma :  float : volatility factor in diffusion term\n    model: str: name of the model for the pricing"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, option_type, S0, strike, T, r, div, sigma, model):\n        try:\n            self.option_type = option_type\n            assert isinstance(option_type, str)\n            self.S0 = float(S0)\n            self.strike = float(strike)\n            self.T = float(T)\n            self.r = float(r)\n            self.div = float(div)\n            self.sigma = float(sigma)\n            self.model = str(model)\n        except ValueError:\n            print(\'Error passing Options parameters\')\n\n        models = [\'BlackScholes\', \'MonteCarlo\', \n                  \'BinomialTree\', \'TrinomialTree\',\n                  \'FFT\', \'PDE\']\n        \n        if model not in models:\n            raise Exception(\'Error: Model unknown\')\n            \n        option_types = [\'call\', \'put\']\n        \n        if option_type not in option_types:\n            raise ValueError("Error: Option type not valid. Enter \'call\' or \'put\'")\n        if S0 < 0 or strike < 0 or T <= 0 or r < 0 or div < 0 or sigma < 0:\n            raise ValueError(\'Error: Negative inputs not allowed\')\n            \n        self.discount = np.exp(-self.r * self.T)\n\n    def getmodel(self):\n        return self.model\n\n    def __str__(self):\n        return "This European Option is priced using {0}".format(self.getmodel())\n\n    @abstractmethod\n    def value(self):\n        pass\n    \n    @abstractmethod\n    def delta(self):\n        pass\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def value_in_pool(simulations):\n    if isinstance(simulations, int):\n        simulations = [simulations] \n    arguments = ['call', 100., 100., .5, 0.01, 0., .35]\n    reduce(lambda x,y : x.extend(y), [simulations, arguments])\n    return MonteCarlo(*simulations).value\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def delta_in_pool(simulations):\n    if isinstance(simulations, int):\n        simulations = [simulations] \n    arguments = ['call', 100., 100., .5, 0.01, 0., .35]\n    reduce(lambda x,y : x.extend(y), [simulations, arguments])\n    return MonteCarlo(*simulations).delta\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef kernel(x0, x1, params, sigma_n):\n    """\n    Runs the exponential kernal function and returns output\n    :param x0: numpy array with values of x\n    :param x1: numpy array with values of x\'\n    :param params: [float simga_f, float l]\n    :param sigma_n: float sigma_n\n    :return: np.array(np.array) nested np.arrays where output[i,j] == k(x0[i],x1[j])\n    """\n    diff = np.subtract.outer(x0, x1)\n    value = params[0]**2 * np.exp( -0.5 * (1.0/params[1]**2) *diff**2)\n    value[np.where(diff == 0.0)] += sigma_n**2\n    return value\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef periodic_kernel(x0, x1, params, sigma_n):\n    """\n    Runs the Periodic kernal described above\n    :param x0: numpy array with values of x\n    :param x1: numpy array with values of x\'\n    :param params: [float simga_f1, float l, float simga_f1, float nu]\n    :param sigma_n: float sigma_n\n    :return: np.array(np.array) nested np.arrays where output[i,j] == k(x0[i] ,x1[j])\n    """\n    diff = np.subtract.outer(x0, x1) # USE THIS FOR CALCULATING (x-x\') #+\n    value = params[0]**2 * np.exp( -0.5 * (1.0/params[1]**2) * diff**2) #-\n    value += params[2]**2 * np.exp(-2 * np.sin(params[3]*diff)**2) #-\n    value[np.where(diff == 0.0)] += sigma_n**2 #-\n    return value\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def get_Ks(x_new, x, kernel, theta):\n    """\n    Generates K, KS, and KSS by using the given kernel and theta\n    :param x_new: numpy array of new data\n    :param x: numpy array of data where we have sampled\n    :param theta: list (params + [sigma_n]), this is a list that has the params (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    :return: np.array(np.array) K, np.array(np.array) KS, np.array(np.array) KSS  all as described above.\n    """\n    K = kernel(x, x, theta[:-1], theta[-1]) # K\n    KS = kernel(x_new, x, theta[:-1], theta[-1]) # K*\n    KSS = kernel(x_new, x_new, theta[:-1], theta[-1]) # K**\n    return K, KS, KSS\n```\n\n\nUnused functions:\nNone'}, {'reason': 'stop', 'result': 'def regression_GP(x_new, x, y, kernel, theta):\n    """\n    runs regression and returns y_new_bar and variance\n    :param x_new: numpy array of new data\n    :param x: numpy array of data where we have sampled\n    :param y: numpy array of y values for data where we have sampled\n    :param kernal:  the kernel function which we will be using\n    :param theta: list (params + [sigma_n]), this is a list that has the params (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    :return: (np.array Y_new_bar, np.array var), the mean and variance (only diagonal of variance)\n    of the gaussian that describes p(b|a)\n    """\n    K, KS, KSS = get_Ks(x_new, x, kernel, theta)\n\n    y = np.matmul(np.matmul(KS, np.linalg.inv(K)),y)\n    var = KSS - KS.dot(np.linalg.inv(K).dot(KS.T))\n    var = np.diagonal(var)\n    return(y.squeeze(), var.squeeze())'}, {'reason': 'stop', 'result': 'def regression_optimize_theta(x, y, sigma_n, kernel, params_0=[0.1, 0.1]):\n\n    """\n    Optimizes parameters for the data given by maximizing logp(data|parameters)\n    :param x: numpy array of data where we have sampled\n    :param y: numpy array of y values for data where we have sampled\n    :sigma_n: float sigma_n\n    :param kernal: the kernel function which we will be using\n    :param params_0: list params_0 this is a list that has the initial params (must be length\n    of params which your kernel is expecting) from this point the optimizer will run.\n    :return: list (optimal_params + [sigma_n]), this is a list that has the optimal parameterss (must be length\n    of params which your kernel is expecting), but also has sigma_n in the last index.\n    """\n\n    def log_pY(theta):\n        """\n        Calculates the - log(p(y|parameters))\n        :param theta: list params this is a list that has the params (must be length\n        of params which your kernel is expecting)\n        :return: float - log(p(y|parameters)) (using negative because our optimizer is a minimizer)\n        """\n        K = kernel(x, x, theta, sigma_n) #+\n        log_k = np.linalg.slogdet(K)[1] #+\n        output = 0.5 * np.matmul(np.matmul(y.T, np.linalg.inv(K)),y) #-\n        output += 0.5 * log_k #-\n        return output #-\n\n    res = minimize(log_pY, params_0, method=\'nelder-mead\', options={\'xtol\': 1e-8, \'disp\': False}) #+\n    return list(res.x) + [sigma_n] # +```'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef sigmoid(x):\n    """\n    Calculates the Sigmoid(x)\n    :param x: numpy array <dtype=float> points we want to find sigmoid of\n    :return: numpy array <dtype=float> the sigmoid of those points\n    """\n    return 1./(1+np.exp(-x))\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def find_f(K, y, iterations=100):\n    """\n    Finds f using a iterative aproach also finds p(y|f)\n    :param K: np.array(np.array) covariance matrix of data points we have observed\n    :param y: numpy array of the y value of the data points we have observed\n    :iterations: int optional default = 100 number of iterations we will preform to optimize f\n    :return: (numpy array <float> f, numpy array <float> p(y|f)) f is the latent function value for each of the \n    sampled data points, p(y|f) is the probability of y given the latent function we calculated\n    make sure to calculate p(y|f) after having approximated f_hat.\n    """\n    n = len(y)\n    f = np.zeros(n)\n    grad = np.zeros(n)\n    for i in range(iterations):\n        for j in range(n):\n            sigmoid_v = sigmoid(f[j]*y[j])\n            grad[j] = (1-sigmoid_v)*y[j]\n        f = np.array(np.matmul(K, grad)).flatten()\n        \n    y_giv_f = np.zeros(n)\n    for j in range(n):\n        y_giv_f[j] = sigmoid(f[j]*y[j])\n        \n    return f, y_giv_f\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def GPC(x_new, x, y, kernel, theta):\n    K, KS, KSS = get_Ks(x_new, x, kernel, theta)\n    \n    f, y_giv_f = find_f(K, y)\n    W = util.calculate_W(f, y)\n    KP = util.calculate_KP(K, W)\n    \n    f_bar = np.matmul(np.matmul(KS, np.linalg.inv(K)), f)\n    var = KSS - KS.dot(np.linalg.inv(KP).dot(KS.T))\n    var = np.diagonal(var)\n    return(f_bar.squeeze(), var.squeeze())'}, {'reason': 'stop', 'result': "def optimize_theta(x, y, kernel, params_0=[0.1, 0.1], sigma_n=0.1):\n    res = minimize(log_pY, params_0, method='nelder-mead', options={'xtol': 1e-8, 'disp': False})\n    return list(res.x) + [sigma_n]"}, {'reason': 'stop', 'result': 'def get_mean_band(data):\n    \n    final_variances = []\n    for d in data:\n        variances = []\n        skewnesses = []\n        kurtosises = []\n        for channel in d:\n            var = channel.var()\n            variances.append(var)\n            \n            skewness = skew(channel, axis=0)\n            skewnesses.append(skewness)\n\n            kurt = kurtosis(channel)\n            kurtosises.append(kurt)\n        \n        final_variances.append([variances, skewnesses, kurtosises])\n    \n    return np.array(final_variances)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '#Performs integration.\ndef integrate(f,lower,upper,N,C):\n    H=C\n    w=findw(f,H,lower,upper,True)    \n    #Generate uniform random inputs.\n    inputs=np.random.rand(N)    \n    a=w[0]/2  \n    b=w[1]\n    c=-(a*lower**2+b*lower)\n    \n    SUM=0\n    SUM2=0\n    \n    inverse_inputs=[]\n    for i in inputs:\n        p=[(-b-np.sqrt(b**2-4*a*(c-i)))/(2*a),(-b+np.sqrt(b**2-4*a*(c-i)))/(2*a)]\n        if p[0]>=lower and p[0]<=upper:\n            inverse_inputs.append(p[0])\n        else :\n            inverse_inputs.append(p[1])\n\n    inverse_inputs=np.array(inverse_inputs)\n    #Calculate f(inverse(x))/w(inverse(x)).\n    outputsF=f(inverse_inputs,H)\n    outputsW=w[0]*(inverse_inputs)+w[1]\n    outputs=outputsF/outputsW\n    SUM=outputs.sum()\n    SUM2=(outputs*outputs).sum()\n    var=SUM2/N-(SUM/N)**2\n    var=var/N\n    #Store generated points for variance calculation.\n    Vsum=outputs.sum()\n    return Vsum/N-H*(upper-lower),(upper-lower)**2*var\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def alias(Table,aliases):\n    Aliased=[]\n    for key in Alias_Dict:\n        if key in Table.keys():\n            Series=Table[key]\n            new_Series=pandas.Series()\n            new_Series.name=Series.name\n            for key2,data in zip(Series.keys(),Series):\n                new_Series[key2]=0.0\n                for val in aliases[key]:\n                    try:\n                        if numpy.isnan(data):\n                            new_Series[key2]=numpy.nan\n                    except:\n                        pass             \n                    if data==val:\n                        new_Series[key2]=aliases[key][val]\n                        break\n            Aliased+=[new_Series]\n    return pandas.concat(Aliased,axis=1)\n    \n#Example\n#Alias_Dict={'SEX':{'F':1},'PRIOR.MAL':{'YES':1},'PRIOR.CHEMO':{'YES':1},'PRIOR.XRT':{'YES':1},\n#                'Infection':{'Yes':1},'ITD':{'POS':1,'ND':numpy.nan},'D835':{'POS':1,'ND':numpy.nan},\n#                'Ras.Stat':{'POS':1,'NotDone':numpy.nan},'resp.simple':{'CR':1},'Relapse':{'Yes':1},\n#                'vital.status':{'A':1}}\n#pandas.concat([Trial_data[Categorical[:4]+['cyto.cat']],alias(Trial_data,Alias_Dict),split(Trial_data['cyto.cat'],Dream9)],axis=1).T```"}, {'reason': 'stop', 'result': "import pandas\n\ndef squared(Table):\n    #This function squares all the values on a table\n    D=[]\n    for i,var in enumerate(Table.keys()):\n        D+=[Table[var]**2]\n        D[i].name='%s_Squared'%var\n    return pandas.concat(D,axis=1)\n\n#Example\n#pandas.concat([Trial_data[Protein[:4]],squared(Trial_data[Protein[:4]])],axis=1).T\n```\n\nUnused functions:\n- absolute"}, {'reason': 'stop', 'result': "def cutoff(Table,cutoff):\n    #This function makes values above a threeshold equal to the threeshold\n    Tab=[]\n    for key0 in Table.keys():\n        Series=Table[key0]\n        new_Series=Table[key0].copy()\n        for key,data in zip(Series.keys(),Series):\n            if data>cutoff:\n                new_Series[key]=cutoff\n            else:\n                new_Series[key]=data\n        new_Series.name='%s_cut'%Series.name\n        Tab+=[new_Series]\n    return pandas.concat(Tab,axis=1)"}, {'reason': 'stop', 'result': "def make_pca(Table,All_Data,n,name='PCA_test',whiten=False):\n    pca = PCA(n_components=n,whiten=whiten)\n    pca.fit(All_Data[Table.keys()])\n    trans_PCA=pca.transform(Table)\n    #Saving the components of the PCA\n    components=pca.components_\n    PCA_table=pandas.DataFrame(components,columns=Table.keys(),index=['%s_%i'%(name,i+1) for i in range(n)])\n    PCA_table.to_csv('%s_components.csv'%name)\n    #pca.explained_variance_ratio_##pca.get_params()\n    #Needs to return the PCA transformation\n    return pandas.DataFrame(trans_PCA,columns=['%s_%i'%(name,i+1) for i in range(n)],index=Table.index)"}, {'reason': 'stop', 'result': "def bin_independent(Table,All_Data,n,name='Binned'):\n    Tab=[]\n    Thre=[]\n    for category in Table:\n        #Read the values from all the data\n        A=All_Data[category].values.copy()\n        #Calculate the threesholds\n        A=A[~numpy.isnan(A)]\n        A.sort()\n        Ts=[]\n        for i in range(n):\n            Ts+=[A[int(len(A)/float(n)*(i+1))-1]]\n        #Make the new Series\n        Ser=pandas.Series(Table[category].values.copy()*0,index=Table.index)\n        for t in Ts:\n            Ser=Ser+(Table[category]<=t)\n        Ser.name='%s%i_%s'%(name,n,category)\n        Tab+=[Ser]\n        Thre+=[pandas.Series(Ts,index=['Threeshold %i'%(i+1) for i in range(len(Ts))],name=category)]\n    Threesholds=pandas.concat(Thre,axis=1)\n    Threesholds.to_csv('%s%i_Threesholds.csv'%(name,n))\n    \n    return pandas.concat(Tab,axis=1)"}, {'reason': 'stop', 'result': "def PreProcess(table,Dream9):\n    #Select all variables that are not Categorical\n    Tables=[table[[v for v in table.keys() if v not in Categorical]]]\n    \n    #Convert yes/no to 1/0\n    Alias_Dict={'SEX':{'F':1},'PRIOR.MAL':{'YES':1},'PRIOR.CHEMO':{'YES':1},'PRIOR.XRT':{'YES':1},\n                'Infection':{'Yes':1},'ITD':{'POS':1,'ND':numpy.nan},'D835':{'POS':1,'ND':numpy.nan},\n                'Ras.Stat':{'POS':1,'NotDone':numpy.nan},'resp.simple':{'CR':1},'Relapse':{'Yes':1},\n                'vital.status':{'A':1}}\n    \n    Tables+=[alias(table,Alias_Dict)]\n    \n    #Split data that has multiple values\n    Tables+=[split(table['cyto.cat'],Dream9)]\n    \n    #Create new data for protein\n    Tables+=[squared(table[Protein])]\n    Tables+=[absolute(table[Protein])]\n    Tables+=[bin_independent(table[Protein],Dream9,2)]\n    Tables+=[bin_independent(table[Protein],Dream9,3)]\n    Tables+=[bin_independent(table[Protein],Dream9,4)]\n    Tables+=[bin_independent(table[Protein],Dream9,5)]\n    \n    #Make PCA axis\n    Tables+=[make_pca(table[Protein],Dream9,200,name='PCA')]\n    Tables+=[make_pca(table[Protein],Dream9,200,name='Whiten_PCA',whiten=True)]\n    Tables+=[make_pca(squared(table[Protein]),squared(Dream9[Protein]),200,name='PCA_Sq')]\n    \n    #Bin dependent variables\n    try:\n        Tables+=[cutoff(table[['Overall_Survival','Remission_Duration']],130)]\n        Tables+=[binned(table[['Overall_Survival','Remission_Duration']])]\n    except KeyError:\n        pass        \n    \n    #Join everything\n    return pandas.concat(Tables,axis=1)```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def dateRange(start, end):\n    days = (datetime.datetime.strptime(end, "%Y-%m-%d") - datetime.datetime.strptime(start, "%Y-%m-%d")).days + 1\n    return [datetime.datetime.strftime(datetime.datetime.strptime(start, "%Y-%m-%d") + datetime.timedelta(i), "%Y-%m-%d") for i in xrange(days)]\n\ndef date_to_week(date):\n    if type(date) == str:\n        date = pd.to_datetime(date).date()\n    return (date - datetime.date(2015,7,7)).days  / 7\n\ndef week_to_date(week_number,return_str=True):\n    if week_to_date:\n        return [(datetime.date(2015,7,7)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d"),(datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)).strftime("%Y-%m-%d")]\n    return [datetime.date(2015,7,7)+ datetime.timedelta(week_number*7),datetime.date(2015,7,13)+ datetime.timedelta(week_number*7)]\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def draw_ids(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):\n    if by_week:\n        xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n        shop_info.loc[ids,start_date:end_date].T.plot(figsize=figsize,ax=ax)\n    else: \n        shop_info.loc[ids,start_date:end_date].T.plot(figsize=figsize)\n    plt.show()\n\ndef draw_ids_avg(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=True,figsize=(70,10)):\n    if by_week:\n        xmajorLocator   = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[ids,start_date:end_date].mean(axis=0).plot(figsize=figsize)\n    plt.show()\n\ndef draw_ids_diff(ids,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):  \n    if by_week:\n        xmajorLocator   = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    (shop_info.loc[ids[0],start_date:end_date]-shop_info.loc[ids[1],start_date:end_date]).plot(figsize=figsize)\n\ndef draw_id(id,start_date='2015-07-01',end_date='2016-10-31',by_week=False,figsize=(16,9)):\n    if by_week:\n        xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n        ax = plt.subplot(111) \n        ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id,start_date:end_date].plot(legend=True,figsize=figsize)\n```\n\n\nUnused functions:\n- draw_week_id\n- draw_week_ids"}, {'reason': 'stop', 'result': "def draw_rule_and_reg_id(id):\n    plt.title(str(id))\n    list_result = dateRange('2016-06-06','2016-10-31')\n    for i in range(1,15):\n        list_result.append('predict_day_'+str(i))\n    xmajorLocator = MultipleLocator(7) #将x轴次刻度标签设置为7的倍数\n    ax = plt.subplot(111) \n    ax.xaxis.set_major_locator(xmajorLocator)\n    shop_info.loc[id ,list_result].T.plot(figsize=(16,9),ax=ax)\n    \n    plt.show()\n```"}, {'reason': 'stop', 'result': 'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sys import exit\nimport math\n\nplt.close()\n\n#-----------Part 1----------------\n\n# Generating Training and Testing Data\n# Creating a class for the dataset\nclass Data_set:\n    def __init__(self, mean, cov):\n        self.mean = mean\n        self.cov  = cov\n    \n    def split_data(self):\n        np.random.shuffle(self.data)\n        self.train = self.data[:len(self.data)//2]\n        self.test  = self.data[len(self.data)//2:]\n        \n        return self\n    \n    def multivariate_normal(self, num):\n        # asself.data.shape = num *  2\n        self.data = np.random.multivariate_normal(self.mean,self.cov,size=num)\n    \n        return self\n    \ndef accuracy(c1, c2, w, b, var):\n    # Calculating accuracy of learned model\n    # Using generated data\n    acc_c1 = []\n    acc_c2 = []\n    miss = []\n    \n    for item in c1:\n        bool_c1 = point_loc(item, c1, w, b, var)\n        acc_c1.append(bool_c1)\n        if not bool_c1:\n            miss = np.append(miss, item)\n    \n    for item in c2:\n        bool_c2 = point_loc(item, c2, w, b, var)\n        acc_c2.append(bool_c2)\n        if bool_c2:\n            miss = np.append(miss, item)\n        \n    acc = (sum(acc_c1)+(100-sum(acc_c2)))/200\n    miss = miss.reshape(len(miss)//2, 2)\n        \n    return acc, miss\n    \ndef gen_line_vec(data_set, w, b, var):\n    # This function is for generating decision boundary\n    \n    # var = W.dot(data_set) + b\n    # var = 0 when plotting line for LDA\n    # var = 0.5 when plotting line for LR\n    \n    # Find the min and max value on x axis\n    mini = min(data_set[:,0])\n    maxi = max(data_set[:,0])\n    \n    # generating x axis vector (step size 0.1)\n    x_vec = np.arange(mini, maxi, 0.1)\n    \n    # calculating y vector\n    y_vec = (-var-b-w[0]*x_vec)/w[1]\n    \n    return x_vec, y_vec\n\ndef point_loc(point, data, w, b, var):\n    # var = W.dot(data_set) + b\n    # var = 0 when plotting line for LDA\n    # var = 0.5 when plotting line for LR\n    \n    _x, _y = gen_line_vec(data, w, b, var)\n    point_a = np.array([_x[0],_y[0]])\n    point_b = np.array([_x[-1],_y[-1]])\n    \n    cross_prod = np.cross(point-point_a, point_b-point_a)\n    \n    result = (cross_prod < 0) # less than 0 when a point is above the divider\n    \n    return result\n```\n\nUnused functions:\n- lda_accuracy'}, {'reason': 'stop', 'result': 'def IRLS(data):\n    length        = len(data)\n    not_converged = True\n    w             = np.array([0, 0, 0])\n    p             = np.empty(length)\n    s             = np.empty(length)\n    z             = np.empty(length)\n\n    ones          = np.ones(200)\n    ones[100:]    = ones[100:]*-1\n    \n    temp_var      = np.hstack((ones[None].T,data))\n    np.random.shuffle(temp_var)\n    \n    y             = temp_var[:,0]\n    data          = temp_var[:,1:]\n    print(data.shape)\n    \n    while not_converged:\n        w_prev = w\n        for i in range(length):\n            p[i] = np.exp(w_prev.dot(data[i])) / (1 + np.exp(w_prev.dot(data[i])))\n            s[i] = p[i]*(1-p[i])\n            \n            if math.isnan(s[i]): s[i]=1\n            \n            z[i] = w_prev.dot(data[i]) + (y[i]-p[i])/(max(s[i],0.00001))\n            \n        diag_s = np.diag(s)\n        t1     = np.linalg.inv(np.dot(np.dot(data.T, diag_s), data))\n        t2     = np.dot(np.dot(data.T, diag_s), z)\n        w      = np.dot(t1, t2)\n        w      = w/np.linalg.norm(w)\n        \n        print(\'Iterations\',w)\n        \n        if abs(sum(w-w_prev)) < 0.000001:\n            \n            print("Converged!!")\n            not_converged = False\n            return w\n            \n        elif sum(abs(w)) > 900 or math.isnan(w[0]):\n            \n            print("Not converging!!!")\n            return w\n            \n    return w\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def qda(data):\n    term1 = 0.5*np.dot((data-sample_mean_c1).T, np.linalg.inv(sample_cov_c1)).dot(data-sample_mean_c1)\n    term2 = 0.5*np.dot((data-sample_mean_c2).T, np.linalg.inv(sample_cov_c2)).dot(data-sample_mean_c2)\n    term3 = 0.5*(np.log(np.linalg.norm(sample_cov_c1)))\n    term4 = 0.5*(np.log(np.linalg.norm(sample_cov_c2)))\n    term5 = np.log(pi_2/pi_1)\n    \n    result = term1 - term2 + term3 - term4 + term5\n    \n    return result\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def qda_accracy():\n    c1_result = [] \n    c2_result = []\n    miss = []\n    \n    for i in c1.test:\n        c1_result.append(qda(i)<0)\n        if qda(i)>0:\n            miss.append(i)\n    \n    for i in c2.test:\n        c2_result.append(qda(i)>0)\n        if qda(i)<0:\n            miss.append(i)\n    \n    miss = np.array(miss)\n    acc = (sum(c1_result) + sum(c2_result))/200\n    \n    return acc, miss\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import subprocess \nimport shlex\n\ndef systemcall ( cmdstr ):\n    \'\'\' System call to execute command string in a shell. \'\'\'\n    try:\n        retcode = subprocess.call( cmdstr, shell=True)\n        if retcode != 0:\n            print ("Error code:", retcode)\n        return retcode\n    except OSError as e:\n        print ("Execution failed:", e )\n        \ndef systemcall_pipe( cmdstr, allow=None, disp=True ):\n    \'\'\' System call to execute command string, to get stderr and stdout output in variable proc. \'\'\'\n    # this function is superior to systemcall for use with Spyder where otherwise stdout/stderr are not visible.\n    # it is also needed if your main program needs to capture this output instead of only print it to terminal.\n    args = shlex.split(cmdstr)\n    try:\n        proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        #stdout and stderr from your process\n        out, err = proc.communicate()\n        retcode = proc.returncode\n        if err:\n            #decode the standard errors to readable form\n            str_err = err.decode("utf-8")\n            #Exclude error messages in allow list which are expected.\n            bShow = True\n            if allow:\n                for allowstr in allow:\n                    if allowstr in str_err:\n                        bShow = False\n            if bShow:\n                print ("System command \'{0}\' produced stderr message:\\n{1}".format(cmdstr, str_err))\n\n        if disp:\n            str_out = out.decode("utf-8")\n            if str_out:\n                print ("System command \'{0}\' produced stdout message:\\n{1}".format(cmdstr, str_out))\n\n        return retcode, out\n    except OSError as e:\n        print ("Execution failed:", e )```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# transform character-based input/output into equivalent numerical versions\ndef encode_io_pairs(text,window_size,step_size):\n    # number of unique chars\n    chars = sorted(list(set(text)))\n    num_chars = len(chars)\n    \n    # cut up text into character input/output pairs\n    inputs, outputs = window_transform_text(text,window_size,step_size)\n    \n    # create empty vessels for one-hot encoded input/output\n    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n    \n    # loop over inputs/outputs and transform and store in X/y\n    for i, sentence in enumerate(inputs):\n        for t, char in enumerate(sentence):\n            X[i, t, chars_to_indices[char]] = 1\n        y[i, chars_to_indices[outputs[i]]] = 1\n        \n    return X,y\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def window_bounds(window, affine):\n    (row_start, row_stop), (col_start, col_stop) = window\n    w, s = (col_start, row_stop) * affine\n    e, n = (col_stop, row_start) * affine\n    return w, s, e, n\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def window_bounds(window, affine):\n    (row_start, row_stop), (col_start, col_stop) = window\n    w, s = (col_start, row_stop) * affine\n    e, n = (col_stop, row_start) * affine\n    return w, s, e, n\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '#Plots edge (x1,y1)(x2,y2) on the road\n#"nyc" is map objet we are plotting on\n#default color is red; default width of road is 3\ndef plotRoad(x1, x2, y1, y2, nyc, color = \'red\', width = 3):\n    nyc.add_child(PolyLine(locations=[(x1,y1),(x2,y2)],color=color,weight=width))\n\n#Given list of linkIDs in ROADS plots them onto the map\n#data should be pandas dataframe of links\n#nyc - map object; roads - list of linkids that need to be plotted\n#data - pandas object should just be link\ndef plotRoads(nyc, roads, data, color = \'red\', width = 3):\n    print(len(roads))\n    count = 0\n    for ID in roads:\n        road = data[data[\'link_id\'] == ID]\n        plotRoad(road[\'startY\'].values[0], road[\'endY\'].values[0],road[\'startX\'].values[0], road[\'endX\'].values[0], nyc, color, width)\n        count = count + 1\n        if(count % 100 == 0):\n            print(count)\n\n#initialize Map object centered in NYC\ndef initializeMap():\n    NY_COORDINATES = (40.7566,-73.9815)\n    m = folium.Map(location=NY_COORDINATES, zoom_start=15)\n    folium.TileLayer(\'cartodbpositron\').add_to(m)\n    folium.LayerControl().add_to(m)\n    #LatLngPopup enables us to know the latitude and longtitude of any position with one click\n    m.add_child(LatLngPopup())\n    print(\'initialized map\')\n    return m\n\n#reads txt file into a list\ndef readIntTxtToList(path):\n        f = open(path)\n        arr = []\n        for i in f:\n            arr.append(i)\n        arr = [int(x.strip()) for x in arr]\n        return arr\n\n#writes txt file into a list\ndef writeIntArrayToTxt(path, arr):\n    print(len(arr))\n    with open(path, \'wb\') as csvfile:\n        spamwriter = csv.writer(csvfile, delimiter=\',\', quoting=csv.QUOTE_MINIMAL)\n        spamwriter.writerow(arr)\n\n\n#saves map\ndef saveMap(map_name, str_, ave_):\n    citymap = initializeMap()\n    if(str_ != None):\n        plotRoads(citymap, str_, links, \'blue\', 3)\n        print(\'plotted streets\')\n    if(ave_ != None):\n        plotRoads(citymap, ave_, links, \'red\', 3)\n        print(\'plotted avenues\')\n\n    citymap.save(map_name)\n    print(\'{0} saved\'.format(map_name))'}, {'reason': 'stop', 'result': '#Given kml file path, extracts boundary points in regions\ndef extractPolyFromKML(kmlpath):\n    root = parser.fromstring(open(kmlpath, \'r\').read())\n    regions = []\n    placemark = root.Document.Placemark\n    for i in range(len(placemark)):\n        block = []\n        r = str(root.Document.Placemark[i].Polygon.outerBoundaryIs.LinearRing.coordinates).split()\n        if r is not None:\n            for point in r:\n                loc = point.split(\',\')[0:2]\n                block.append(tuple([float(loc[0]),float(loc[1])]))\n            regions.append(block)\n    return regions\n\n#visualizes each of the regions with a list of colors onto map m\ndef visualizePoly(regions, colors, m):\n    for region in range(len(regions)):\n        for i in range(len(regions[region])):\n            if( i < len(regions[region]) -1):\n                point1 = regions[region][i]\n                point2 = regions[region][i+1]\n                plotRoad(point1[1], point2[1], point1[0], point2[0], m, color = colors[region], width = 3)\n    return m\n\n#visualizes each of the nodes in inter with a list of colors onto a map m\n#nodes should be passed into nodes\ndef visualizeNodes(inter, colors, nodes, m):\n    for region in range(len(inter)):\n        c = colors[region]\n        for idd in inter[region]:\n            p = nodes[nodes[\'node_id\'] == idd]\n            x = p[\'xcoord\'].values[0]\n            y = p[\'ycoord\'].values[0]\n            plotNode(y,x,m,sides = 4, radius = 2, color = c)\n    return m\n\n#plots a list of lists containing linkIds with different colors\ndef visualizeLinks(_regionNodes, _regionLinks, colors, links, m):\n    for i in range(len(_regionNodes)):\n        plotRoads(m, _regionLinks[i], links, color = colors[i], width = 3)\n    return m\n\ndef nodesPerRegion(regions,nodes):\n    regionNodes = []\n    for i in range(len(regions)):\n        print(i)\n        regionNodes.append(getNodes(regions[i],nodes))\n    return regionNodes\n\ndef linksPerRegion(regionNodes, links):\n    regionLinks = [[] for i in range(len(regionNodes))]\n    for i in range(len(links)):\n        link = links.iloc[i]\n        beginNode = link[\'begin_node_id\']\n        endNode = link[\'end_node_id\']\n        for i in range(len(regionLinks)):\n            if beginNode in regionNodes[i] and endNode in regionNodes[i]:\n                regionLinks[i].append(link[\'link_id\'])\n    return regionLinks\n   \n\n\ndef getNodes(region, nodes):\n    inter = []\n    verts = region\n    codes = [mplPath.Path.LINETO] * len(verts)\n    codes[0] = mplPath.Path.MOVETO\n    codes[-1] = mplPath.Path.CLOSEPOLY\n    poly = mplPath.Path(verts,codes)\n    for i in range(len(nodes)):\n        p = nodes.iloc[i]\n        x = p[\'xcoord\']\n        y = p[\'ycoord\']\n        if(poly.contains_point((x,y))):\n            inter.append(p[\'node_id\'])\n    inter = [int(i) for i in inter]\n    return inter\n\ndef writeToCSV(data, filename):\n    with open(filename, "wb") as f:\n        writer = csv.writer(f)\n        writer.writerows(data)'}, {'reason': 'stop', 'result': 'def run_episode_neural_network(show_renderer=False):    \n    with tf.Session() as sess:\n        init.run()\n        obs = env.reset()\n        obs_pretty = obs.reshape(1, nb_neurons_input)\n        done = False\n        final_score = 0\n        while not done:\n            if show_renderer:\n                env.render()\n            proba_move_to_left = output.eval(feed_dict={X: obs_pretty})        \n            if random.uniform(0, 1) < proba_move_to_left:\n                action = 0  # move to left\n            else:\n                action = 1       \n            obs, reward, done, info = env.step(action) \n            if done:\n                break\n            final_score += reward\n    env.reset()\n    env.close()\n    return final_score\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "boulder_4 = 4\nboulder_5a = 8\nboulder_5b = 9\nboulder_5c = 10\n\nboulder_6a = 20\nboulder_6a_plus = 21\nboulder_6b = 30\n\nboulder_6b_plus = 40\nboulder_6c = 60\nboulder_6c_plus = 70\n\nboulder_7a = 100\nboulder_7a_plus = 130\n\nscores_translator = {\n    # climbing\n    '4': boulder_4,   \n    '5a': boulder_5a,\n    '5b': boulder_5b,\n    '5c': boulder_5c,\n    '6a': boulder_6a,\n    '6a+': boulder_6a_plus,\n    '6b': boulder_6b,\n    '6b+': boulder_6b_plus,\n    '6c': boulder_6c,\n    '6c+': boulder_6c_plus,\n    '7a': boulder_7a,\n    '7a+': boulder_7a_plus,\n    \n    # gymnastics\n    'block': boulder_6a_plus / 5,\n    'block+': boulder_6c / 4,\n    'core': boulder_6a / 5,\n    'core+': boulder_6c / 4,\n    'anta': boulder_5c / 5,\n    'legs': boulder_5c / 5,\n    'L-Sit': boulder_6b,\n    \n    # power phase\n    'power': boulder_6b,\n    'speed': boulder_6a_plus,\n    'speed+': boulder_7a / 4,\n    \n    # yoga\n    'yoga': boulder_6b / 5,\n    \n    # ARC\n    'arc': boulder_6b_plus / 5,\n    \n    # technique\n    'tech': boulder_6a_plus / 5,\n    'visual': boulder_6b / 5,\n    \n    # Hangboard for each 10 seconds\n    '4F2G': boulder_5c / 10,\n    '3F2G': boulder_6a / 10,\n    '3F2G+10%': boulder_6a / 10,\n    '3F2G+15%' : boulder_6a_plus / 10,\n    '3F2G+20%': boulder_6b / 10,\n    '3F2G+25%': boulder_6b_plus / 10,\n    '2F2G': boulder_6b / 10,\n    \n    # crimp in mm\n    '16': boulder_6a_plus / 10,\n    '16-3F': boulder_6b_plus / 10,\n    \n    '12': boulder_6b_plus / 10,\n    '12-3F': boulder_6c / 10,\n    \n    # slopers\n    'sloper': boulder_6a / 10,\n    '15°': boulder_6a / 10,\n    '35°': boulder_6b_plus / 10,\n    '45°': boulder_7a / 10,\n    \n    'pinch': boulder_6b_plus / 10,\n}\n\nhangboard = [\n    '4F2G',\n    '3F2G',\n    '3F2G+10%',\n    '3F2G+15%',\n    '3F2G+20%',\n    '3F2G+25%',\n    '2F2G',\n    '16',\n    '16-3F',\n    '12',\n    '12-3F',\n    'sloper',\n    '15°',\n    '35°',\n    '45°',\n    'pinch'\n]\n\ngymnastics = ['block', 'block+', 'core', 'core+', 'anta', 'legs', 'L-Sit',]\nclimbing = ['4', '5a', '5b', '5c', '6a', '6a+', '6b', '6b+', '6c', '6c+', '7a', '7a+',]\n\n\ndef calc_score(row):\n    s = [row[key] * scores_translator[key] for key in scores_translator]\n    return sum(s)\n\n\ndef calc_hangboard_scores(row):\n    s = [row[key] * scores_translator[key] for key in hangboard]\n    return sum(s)\n        \n    \ndef calc_gym_scores(row):\n    s = [row[key] * scores_translator[key] for key in gymnastics]\n    return sum(s)\n\ndef calc_climbing_scores(row):\n    s = [row[key] * scores_translator[key] for key in climbing]\n    return sum(s)"}, {'reason': 'stop', 'result': "def plot_stacked_scores(athlete):\n    plt.figure(figsize=(30,10))\n    \n    x = np.arange(len(athlete))\n    \n    athlete['additional_index'] = x\n    scores = [\n        athlete.climbing_scores,\n        athlete.yoga_scores,\n\n        athlete.tech_scores,\n        athlete.power_scores,\n\n        athlete.gym_scores,\n        athlete.arc_scores,\n\n        athlete.hang_scores,\n    ]\n    \n    labels = ['Climbing', 'Yoga', 'Technique', 'Power', 'Gymnastics', 'ARC', 'Hangboarding',  ]\n    colors = [ 'cornflowerblue', 'darkturquoise', 'mediumorchid', 'red', 'gold', 'gray', 'lightgreen', ]\n\n    y = np.vstack(scores)\n    plt.stackplot(x, y, labels=labels, colors=colors)\n    plt.plot(x, athlete.scores, marker='o', color='lightgray')\n\n    for index, row in athlete.iterrows():\n        if row.notes:\n            plt.annotate(row.notes, xy=(row.additional_index, row.scores), fontsize=14, fontweight='bold')\n            \n    plt.legend(loc=2)\n    plt.show()"}, {'reason': 'stop', 'result': "def plot_categorical_count(df, column, title='', limit=2, xtick_rotation='horizontal'):\n    column_count = df[column].value_counts()[:limit]\n    \n    fig = plt.figure(figsize=(14, 8))\n    sns.barplot(x=column_count.index, y=column_count.values, palette='Paired')\n    sns.despine(left=True)\n    plt.title(title, fontsize=16)\n    plt.xticks(rotation=xtick_rotation)```\n\nUnused functions:\n- plot_heatmap"}, {'reason': 'stop', 'result': "# Helper Functions\n\ndef make_graph(corr_mat, node_labels, graph_type):\n\n    G = nx.Graph()\n    G.add_nodes_from(node_labels)\n    dim = corr_mat.shape[0]\n\n    if not dim == len(node_labels):\n        raise ValueError('number node labels not = corr matrix dimensions')\n\n    if graph_type=='signed':\n        for i in range(dim):\n            for j in range(i+1, dim):\n                if corr_mat[i,j] < 0:\n                    G.add_edge(node_labels[i], node_labels[j], sign=-1)\n                elif corr_mat[i,j] > 0:\n                    G.add_edge(node_labels[i], node_labels[j], sign=1)\n    \n    if graph_type=='corr':\n        for i in range(dim):\n            for j in range(i+1, dim):\n                if corr_mat[i,j] != 0.0:\n                    G.add_edge(node_labels[i], node_labels[j])\n    \n    density = (2*G.number_of_edges())/(G.number_of_nodes()*(G.number_of_nodes() - 1))\n                \n    return G, density\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '# Spase PCA\nimport pywt\n\nclass SPC(object):\n    \n    def __init__(self, number_of_components,max_iter=10, threshold_val=1.5 ):\n        \n        \n        """Initialize the SPC object\n        \n        Positional arguments:\n        number_of_components -- the number of sparse principal components \n        to compute, must be between 1 and p (total number of features)\n        \n        Keyword argument:\n        max_iter -- the number of iterations to perform (default=10)\n        threshold_val -- value of the lambda regularisation \n        parameter (default=10)\n        """\n        self.number_of_components=number_of_components\n        self.max_iter=max_iter\n        self.threshold_val=threshold_val\n    \n    def fit(self, X_):\n        """learn the sparse pc of a data matrix, return sparse estimates\n        of the left and right singular vectors (U and V respectively) \n        as well as the standard principal components loading matrix W\n    \n        Positional arguments:\n        X_ -- training data matrix, as numpy ndarray\n        \n        """ \n        print("computing sparse principal components...")\n        print("computing SVD of data matrix...")\n        U, s, V = np.linalg.svd(X_, full_matrices=True)  \n        cnt = 0\n        self.U = U\n        self.W = V.T\n        def normalize(vector):\n            norm=np.linalg.norm(vector)\n            if norm>0:\n                return vector/norm\n            else:\n                return vector\n        print("starting iterations...")\n        while True:\n           \n            self.V = pywt.threshold(np.dot(U[:self.number_of_components],X_), self.threshold_val)\n            self.U = np.dot(self.V,X_.T)\n            self.U = np.array([normalize(u_i) for u_i in self.U])\n            if cnt%2==0:\n                print("{} out of {} iterations".format(cnt,self.max_iter))\n            cnt += 1\n            if cnt == self.max_iter:\n                self.V = np.array([normalize(v_i) for v_i in self.V])\n                break\n        print("...finish")\n        return self.U, self.V, self.W\n    \n    def transform(self, X_, k=2):\n        X_reduced_spca     = np.dot(X_, np.dot(self.V[:k].T, self.V[:k]))\n        return X_reduced_spca\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "# robust pca\nfrom __future__ import division, print_function\n\nimport numpy as np\n\ntry:\n    from pylab import plt\nexcept ImportError:\n    print('Unable to import pylab. R_pca.plot_fit() will not work.')\n\ntry:\n    # Python 2: 'xrange' is the iterative version\n    range = xrange\nexcept NameError:\n    # Python 3: 'range' is iterative - no need for 'xrange'\n    pass\n\n\nclass R_pca:\n\n    def __init__(self, D, mu=None, lmbda=None):\n        self.D = D\n        self.S = np.zeros(self.D.shape)\n        self.Y = np.zeros(self.D.shape)\n\n        if mu:\n            self.mu = mu\n        else:\n            self.mu = np.prod(self.D.shape) / (4 * self.norm_p(self.D, 2))\n\n        self.mu_inv = 1 / self.mu\n\n        if lmbda:\n            self.lmbda = lmbda\n        else:\n            self.lmbda = 1 / np.sqrt(np.max(self.D.shape))\n\n    @staticmethod\n    def norm_p(M, p):\n        return np.sum(np.power(M, p))\n\n    @staticmethod\n    def shrink(M, tau):\n        return np.sign(M) * np.maximum((np.abs(M) - tau), np.zeros(M.shape))\n\n    def svd_threshold(self, M, tau):\n        U, S, V = np.linalg.svd(M, full_matrices=False)\n        return np.dot(U, np.dot(np.diag(self.shrink(S, tau)), V))\n\n    def fit(self, tol=None, max_iter=1000, iter_print=100):\n        iter = 0\n        err = np.Inf\n        Sk = self.S\n        Yk = self.Y\n        Lk = np.zeros(self.D.shape)\n\n        if tol:\n            _tol = tol\n        else:\n            _tol = 1E-7 * self.norm_p(np.abs(self.D), 2)\n\n        while (err > _tol) and iter < max_iter:\n            Lk = self.svd_threshold(\n                self.D - Sk + self.mu_inv * Yk, self.mu_inv)\n            Sk = self.shrink(\n                self.D - Lk + (self.mu_inv * Yk), self.mu_inv * self.lmbda)\n            Yk = Yk + self.mu * (self.D - Lk - Sk)\n            err = self.norm_p(np.abs(self.D - Lk - Sk), 2)\n            iter += 1\n            if (iter % iter_print) == 0 or iter == 1 or iter > max_iter or err <= _tol:\n                print('iteration: {0}, error: {1}'.format(iter, err))\n\n        self.L = Lk\n        self.S = Sk\n        return Lk, Sk\n\n    def plot_fit(self, size=None, tol=0.1, axis_on=True):\n\n        n, d = self.D.shape\n\n        if size:\n            nrows, ncols = size\n        else:\n            sq = np.ceil(np.sqrt(n))\n            nrows = int(sq)\n            ncols = int(sq)\n\n        ymin = np.nanmin(self.D)\n        ymax = np.nanmax(self.D)\n        print('ymin: {0}, ymax: {1}'.format(ymin, ymax))\n\n        numplots = np.min([n, nrows * ncols])\n        plt.figure()\n\n        for n in range(numplots):\n            plt.subplot(nrows, ncols, n + 1)\n            plt.ylim((ymin - tol, ymax + tol))\n            plt.plot(self.L[n, :] + self.S[n, :], 'r')\n            plt.plot(self.L[n, :], 'b')\n            if not axis_on:\n                plt.axis('off')```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def create_2statesworld():\n\n    P = np.zeros((2,2,3)) # P(s'|s,a) ... our model of the environment\n    P[0,0,0] = 0.5\n    P[1,0,0] = 0.5\n    P[1,0,1] = 1.\n    P[1,1,2] = 1.\n\n    R = np.zeros((2,3))  # R(s,a) ... the reward funciton \n    R[0,0] = 5\n    R[0,1] = 10\n    R[1,2] = -1\n\n    states = [0, 1]\n    actions = [[0, 1], [2]]\n    next_states = [0, 1]\n    gamma = 0.95\n    \n    return P, R, states, actions, next_states, gamma\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def policy_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4, modified=False):\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n    policy_stable = False\n    all_k = []\n    \n    while not policy_stable:\n        \n        # 2. POLICY EVALUATION (iterates until V_k converges) \n        k = 0\n        V_kplus1 = copy.deepcopy(V_k)\n        delta = epsilon + 1\n        while delta > epsilon and (k < 5 or not modified):\n\n            delta = 0\n            for s in states:\n                v = 0\n                for n in next_states:\n\n                    # Bellman's update rule\n                    a = int(PI[s])\n                    v += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n\n                # Keeps biggest difference seen so far\n                V_kplus1[s] = v\n                delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n\n            # Updates our current estimate\n            V_k = copy.deepcopy(V_kplus1)\n            k += 1\n        all_k.append(k)\n\n        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n        Q = {0: {0: 0,   # state0, action0\n                 1: 0},  # state0, action1\n             1: {2: 0}}  # state1, action2\n        \n        policy_stable = True\n        old_PI = copy.deepcopy(PI)\n        \n        for s in states: \n            for a in actions[s]:\n                for n in next_states:\n                    \n                    # Policy Improvement rule\n                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n                    \n            PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n                    \n            if old_PI[s] != PI[s]:\n                policy_stable = False\n    \n    return V_k, all_k, PI\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4):\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    \n    while delta > epsilon:\n\n        delta = 0\n        \n        Q = {0: {0: 0,   # state0, action0\n                 1: 0},  # state0, action1\n             1: {2: 0}}  # state1, action2\n        for s in states:\n            v = 0\n            for a in actions[s]:\n                for n in next_states:\n                \n                    # Bellman's optimality update rule\n                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n\n            # This step replaces the poilicy improvement step (gets the maximal value)\n            V_kplus1[s] = max(Q[s].items(), key=operator.itemgetter(1))[1]\n            \n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n    \n    # Updates the policy to be greedy with respect to the value function\n    for s in states:\n        PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n    \n    return V_k, k, PI\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '#######################################################################\n# Copyright (C)                                                       #\n# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\ndef create_gridworld(world_size, terminal_states):\n    """\n    world_size: height and width of the squared-shape gridworld\n    return\n        actions: list of str, possible actions\n        states: list of coordinate tuples representing all non-terminal states\n        nextState: list of list of dict, index 3 times to return the next state coordinate tuple\n    """\n\n    # left, up, right, down\n    actions = [\'L\', \'U\', \'R\', \'D\']\n\n    # Next\n    nextState = []\n    for i in range(0, world_size):\n        nextState.append([])\n        for j in range(0, world_size):\n            # Creates a dictionnary that\n            next = dict()\n            if i == 0:\n                next[\'U\'] = (i, j)\n            else:\n                next[\'U\'] = (i - 1, j)\n\n            if i == world_size - 1:\n                next[\'D\'] = (i, j)\n            else:\n                next[\'D\'] = (i + 1, j)\n\n            if j == 0:\n                next[\'L\'] = (i, j)\n            else:\n                next[\'L\'] = (i, j - 1)\n\n            if j == world_size - 1:\n                next[\'R\'] = (i, j)\n            else:\n                next[\'R\'] = (i, j + 1)\n\n            nextState[i].append(next)\n            \n    states = []\n    for i in range(0, world_size):\n        for j in range(0, world_size):\n            if (i,j) in terminal_states:\n                continue\n            else:\n                states.append((i, j))\n                \n    return actions, states, nextState'}, {'reason': 'stop', 'result': "def policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4, modified=False):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    policy_stable = False\n    all_k = []\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n\n    while not policy_stable:\n        \n        # 2. POLICY EVALUATION (iterates until V_k converges)\n        k = 0\n        V_kplus1 = copy.deepcopy(V_k)\n        delta = epsilon + 1\n        \n        while delta > epsilon and (k < 5 or not modified):\n\n            delta = 0\n            for i, j in states:\n                \n                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n                a = idx_to_a[PI[i,j]]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Bellman's update rule\n                V_kplus1[i, j] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n                # Keeps biggest difference seen so far\n                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n            # Updates our current estimate\n            V_k = copy.deepcopy(V_kplus1)\n            k += 1\n        all_k.append(k)\n\n        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n        \n        policy_stable = True\n        old_PI = copy.deepcopy(PI)\n        \n        for i, j in states:\n            for a_idx in range(4): # actions\n                    \n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Policy Improvement rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n                    \n            PI[i,j] = np.argmax(Q[i,j,:])\n                    \n            if old_PI[i,j] != PI[i,j]:\n                policy_stable = False\n    \n    return V_k, all_k, PI\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4):\n\n    # The reward is always -1\n    R = -1\n    \n    #1. INITIALIZATION\n    V_k = copy.deepcopy(V_init)\n    PI = copy.deepcopy(PI_init)\n    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n        \n    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n    k = 0\n    V_kplus1 = copy.deepcopy(V_k)\n    delta = epsilon + 1\n    Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n    while delta > epsilon:\n\n        # Only one sweep of evaluation before taking the max\n        delta = 0\n        for i, j in states:\n            # Now evaluates the value function for each state for every possible action (not just with respect to current policy)\n            for a_idx in range(4): # actions\n\n                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n                a = idx_to_a[a_idx]\n                newPosition = nextState[i][j][a]\n                P = 1.\n\n                # Update rule\n                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n\n            # This step replaces the poilicy improvement step\n            V_kplus1[i,j] = np.max(Q[i,j,:])\n\n            # Keeps biggest difference seen so far\n            delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n\n        # Updates our current estimate\n        V_k = copy.deepcopy(V_kplus1)\n        k += 1\n        \n    # Updates the policy to be greedy with respect to the value function\n    for i, j in states:\n        PI[i,j] = np.argmax(Q[i,j,:])\n    \n    return V_k, k, PI\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def print_policy(policy, terminal_states):\n    \n    idx_to_symbol = {0:\'\\u2190\', 1:\'\\u2191\', 2:\'\\u2192\', 3:\'\\u2193\'}\n    \n    border_str = "\\u00B7 "\n    for i in range(policy.shape[0]):\n        border_str += "\\u2015 "\n    border_str += "\\u00B7 "\n    \n    for i in range(policy.shape[0]):\n        \n        string = ""\n        for j in range(policy.shape[1]):\n            \n            if (i,j) in terminal_states:\n                string += \'\\u25A0 \'\n            else:\n                string += idx_to_symbol[policy[i, j]]+" "\n        \n        print(string)\n    \n    return'}, {'reason': 'stop', 'result': 'def mean(array):\n    return np.sum(array)/len(array)'}, {'reason': 'stop', 'result': 'def poisson_approx(data, mean):\n    pdist = np.array(np.exp((data*np.log(mean))-(data*np.log(data))+data-mean))\n    return pdist\n```\n\nUnused functions:\n- gaussian'}, {'reason': 'stop', 'result': 'def readData(folder):\n    """\n    Read all frames from a given directory into one matrix.\n    input:  directory to folder containing frames\n    output: array3d (dimension: xpixel x ypixel x # frames)\n            array2D (dimension: 1D flattened img x # frames)\n    """\n    \n    files = os.listdir(folder)\n\n    array3D, array2D = [], []\n    for ff in files:\n        arr = fits.getdata(folder + ff)\n        array3D.append(arr)\n        array2D.append(arr.flatten())\n    \n    return np.array(array3D)\n\ndef combineFrame(data_array):\n    """\n    Combine image frames and return 1D array.\n    input:  data array (dimension: # 1D detector counts x # frames)\n    output: 1D combined array of detector counts (ADU)\n    """\n    \n    return np.median(data_array, axis=0)'}, {'reason': 'stop', 'result': 'def plotImg(arr, **kwargs):\n    \n    avg = mean(arr.flatten())\n    std = stdev(arr.flatten())\n    \n    fig,ax = plt.subplots(1)\n    pl = plt.imshow(arr, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', \\\n                    vmin=np.percentile(arr, 10), vmax=np.percentile(arr, 90))\n    if \'window\' in kwargs:\n        window = kwargs.get(\'window\')\n        rect = patches.Rectangle(window[0], window[1][1], window[1][0], edgecolor=\'r\', facecolor=\'none\')\n        ax.add_patch(rect)\n    plt.colorbar().set_label(\'Detector Value (ADU)\')\n    plt.xlabel(\'pixels(x)\')\n    plt.ylabel(\'pixels(y)\')\n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'selection_window.png\')\n    plt.show()\n\ndef plotAll(array2D, **kwargs):\n    """\n    Input:  \'array2D\': 2D image array\n    Output: show plot of histogram and image\n    """\n    \n    arr = array2D.flatten()\n    \n    avg = mean(arr)\n    std = stdev(arr)\n    med = np.median(arr)\n    Npix = len(arr)    \n    \n    sigma = kwargs.get(\'sigma\', 2)\n    low = int(np.round((avg-sigma*std)))\n    high = int(np.round((avg+sigma*std)))\n    rng = kwargs.get(\'rng\', [low, high])\n    exp = kwargs.get(\'exp\')\n    if \'nbins\' in kwargs:\n        nbins = kwargs.get(\'nbins\')\n        bin_size = (rng[1]-rng[0])/nbins\n    else:\n        bin_size = kwargs.get(\'bin_size\', 1)\n    \n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=[18,6])\n    \n    # Histogram\n    #===========\n    hr = np.arange(rng[0], rng[1]+1, bin_size)\n    hist = []\n    for i in range(len(hr)):\n        try:\n            counts = len(np.where((arr >= hr[i]) & (arr < hr[i+1]))[0])\n        except:\n            counts = 0\n        hist.append(counts)\n    ax1.step(hr, hist, color=\'k\')\n\n    #mean and median lines\n    ax1.axvline(avg, color=\'b\', label=r\'$\\bar{x}=%s$\'%(np.round(avg,2)))\n#     ax1.axvline(med, color=\'b\', label=r\'$\\tilde{x}=%s$\'%(np.round(med,2)), linestyle=\'dashed\')\n    \n    #sigma levels\n    if kwargs.get(\'show_level\', True) == True:\n        for i in np.arange(1,sigma+1):\n            if i == 1:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05, label=r\'$s=\\pm %s$\'%(np.round(std,2)))\n            else:\n                ax1.axvspan(avg-i*std, avg+i*std, facecolor=\'g\', alpha=0.05)\n                \n                \n    #poisson distribution\n    xarray = np.arange(rng[0]-10, rng[1]+10, 1)\n    pdist = poisson_approx(xarray, avg)\n    pdist = max(hist)/max(pdist)*pdist\n    ax1.plot(xarray, pdist, color=\'r\', label=r\'$P_{Poisson}(\\bar{x})$\')\n    std_expected = math.sqrt(avg)\n    ax1.axvspan(avg - std_expected, avg + std_expected, facecolor=\'r\', alpha=0.05, \\\n                label=r\'$\\sigma=\\pm %s$\'%(np.round(std_expected,2)))\n    \n    #gaussian distribution\n    gdist = gaussian(xarray, avg, std)\n    gdist = max(hist)/max(gdist)*gdist\n    ax1.plot(xarray, gdist, color=\'c\', label=r\'$P_{Gaussian}(\\bar{x}, s)$\')\n    \n    ax1.legend(loc=\'upper left\')\n    ax1.set_xlabel(\'Counts (ADU)\')\n    ax1.set_ylabel(\'Frequency\')\n    \n    if \'exp\' in kwargs:\n        ax1.set_title(\'Combined Histogram (Exposure Time: %s sec)\'%(exp))\n    ax1.set_xlim(rng)\n    \n    # Image\n    #===========\n    hrng = kwargs.get(\'hrng\', [np.percentile(arr, 10), np.percentile(arr, 90)])\n    pl = ax2.imshow(array2D, origin=\'lower\', interpolation=\'nearest\', cmap=\'gray\', vmin=hrng[0], vmax=hrng[1])\n    fig.colorbar(pl, ax=ax2, fraction=0.046, pad=0.04).set_label(\'Detector Value (ADU)\')\n    \n    ax2.set_xlabel(\'pixels(x)\')\n    ax2.set_ylabel(\'pixels(y)\')\n    ax2.set_title(\'Combined Image\')\n    \n    if \'save_dir\' in kwargs:\n        save_dir = kwargs.get(\'save_dir\')\n        plt.savefig(save_dir + \'exposure%s.png\'%(exp))\n    plt.show()'}, {'reason': 'stop', 'result': 'import unittest\n\nclass TestFeatures(unittest.TestCase):\n\n    def test_features_ground(self):\n        sample = (asl.df.ix[98, 1][features_ground]).tolist()\n        self.assertEqual(sample, [9, 113, -12, 119])\n\n    def test_features_delta(self):\n        sample = (asl.df.ix[98, 0][features_delta]).tolist()\n        self.assertEqual(sample, [0, 0, 0, 0])\n        sample = (asl.df.ix[98, 18][features_delta]).tolist()\n        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], "Sample value found was {}".format(sample))\n                         \nsuite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\nunittest.TextTestRunner().run(suite)```\n\nUnused functions:\n- test_features_norm\n- test_features_polar'}, {'reason': 'stop', 'result': 'def datinfo(X, name=\'Dataset\'):\n    r"""Print dataset size and dimensionality"""\n    print(\'{}:\\n\'\n          \'  size: N={:,} x n={} -> {:,} floats\\n\'\n          \'  dim: {:,} features per clip\\n\'\n          \'  shape: {}\'\n          .format(name, np.prod(X.shape[:-1]), X.shape[-1],\n                  np.prod(X.shape), np.prod(X.shape[2:]), X.shape))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def counting_bloom_and_cuckoo_filter_fpr(capacity, finger_print_size=0.5, bucket_size=4,\\\n                                         number_of_bytes=100000, alpha=0.75):\n    alpha = number_of_bytes / (capacity * finger_print_size * bucket_size)\n    \n    analytic_bloom_error = exp(-(((number_of_bytes-1)*log(2)**2)/capacity) ) \n    \n    analytic_cuckoo_error = 1 / (2**( finger_print_size*alpha*8 - log(2*bucket_size, 2)))#this is estimated from the paper\n     \n    return analytic_bloom_error, analytic_cuckoo_error\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def return_cuckoo_filter_with_specified_load_factor(capacity, finger_print_size=2, load_factor=0.2):\n    c_filter = CuckooFilter(capacity, finger_print_size, bucket_size=2)\n    for i in range(int(capacity*6)):\n        try:\n            item = "".join(random.sample(string.ascii_lowercase, 12))\n            c_filter.add(item)\n        except Exception(\'CuckooFilter has filled up!\'):\n            break\n            \n        if round(c_filter.get_load_factor(), 2) == round(load_factor, 2):\n            return c_filter, c_filter.get_load_factor()\n    raise ValueError\n```\n\nUnused functions:\n- return_bloom_filter_with_specified_load_factor'}, {'reason': 'stop', 'result': 'def insert_and_time_filter_cuckoo_filter(capacity, percent_fill=0.9):\n    num_inserted = 0\n    c_filter = CuckooFilter(capacity, 2)\n    now = time.time()\n    for i in range(int(percent_fill*capacity)):\n        try:\n            c_filter.insert(str(i))\n            num_inserted += 1\n        except Exception("CuckooFilter has filled up!"):\n            break\n    elapsed_insertion = time.time() - now\n    return c_filter, elapsed_insertion, num_inserted\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def insert_and_time_filter_bloom_filter(capacity, percent_fill=0.9):\n    num_inserted = 0\n    b_filter = CountingBloomFilter(capacity)\n    now = time.time()\n    for i in range(int(percent_fill*capacity)):\n        b_filter.add(str(i))\n    elapsed_insertion = time.time() - now\n    return b_filter, elapsed_insertion\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def delete_from_cuckoo_filter_and_time(capacity, percent_to_fill=0.9, percent_delete=1.0):\n    c_filter, _, _ = insert_and_time_filter_cuckoo_filter(capacity, percent_fill=percent_to_fill)\n    \n    total_size = c_filter.get_size()\n    number_of_items_to_delete = total_size*percent_delete\n    \n    now = time.time()\n    for i in range(int(number_of_items_to_delete)):\n        c_filter.remove(str(i))\n    elapsed_time = time.time() - now\n        \n    return c_filter, elapsed_time\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def delete_from_bloom_filter_and_time(capacity, percent_to_fill=0.9, percent_delete=1.0):\n    b_filter, _ = insert_and_time_filter_bloom_filter(capacity, percent_fill=percent_to_fill)\n    \n    total_size = b_filter.get_size()\n    number_of_items_to_delete = total_size*percent_delete\n    \n    now = time.time()\n    for i in range(int(number_of_items_to_delete)):\n        b_filter.remove(str(i))\n    elapsed_time = time.time() - now\n        \n    return b_filter, elapsed_time\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def get_distinct_values(column_name):\n    sql = """\nSELECT\n  {0},\n  COUNT(1) AS num_babies,\n  AVG(weight_pounds) AS avg_wt\nFROM\n  publicdata.samples.natality\nWHERE\n  year > 2000\nGROUP BY\n  {0}\n    """.format(column_name)\n    return bq.Query(sql).execute().result().to_dataframe()```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\nLABEL_COLUMN = 'weight_pounds'\nKEY_COLUMN = 'key'\nDEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\nTRAIN_STEPS = 1000\n\ndef read_dataset(prefix, pattern, batch_size=512):\n    # use prefix to create filename\n    filename = 'gs://{}/babyweight/preproc/{}*{}*'.format(BUCKET, prefix, pattern)\n    if prefix == 'train':\n        mode = tf.estimator.ModeKeys.TRAIN\n        num_epochs = None # indefinitely\n    else:\n        mode = tf.estimator.ModeKeys.EVAL\n        num_epochs = 1 # end-of-input after this\n    \n    # the actual input function passed to TensorFlow\n    def _input_fn():\n        # could be a path to one file or a file pattern.\n        input_file_names = tf.train.match_filenames_once(filename)\n        filename_queue = tf.train.string_input_producer(\n            input_file_names, shuffle=True, num_epochs=num_epochs)\n \n        # read CSV\n        reader = tf.TextLineReader()\n        _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n        if mode == tf.estimator.ModeKeys.TRAIN:\n          value = tf.train.shuffle_batch([value], batch_size, capacity=10*batch_size, \n                                         min_after_dequeue=batch_size, enqueue_many=True, \n                                         allow_smaller_final_batch=False)\n        value_column = tf.expand_dims(value, -1)\n        columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n        features = dict(zip(CSV_COLUMNS, columns))\n        features.pop(KEY_COLUMN)\n        label = features.pop(LABEL_COLUMN)\n        return features, label\n  \n    return _input_fn\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def get_wide_deep():\n    # define column types\n    is_male = tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n                    ['True', 'False', 'Unknown'])\n    mother_age = tf.feature_column.numeric_column('mother_age')\n    plurality = tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n                    ['Single(1)', 'Twins(2)', 'Triplets(3)',\n                     'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)'])\n    gestation_weeks = tf.feature_column.numeric_column('gestation_weeks')\n\n    # discretize\n    age_buckets = tf.feature_column.bucketized_column(mother_age, \n                    boundaries=np.arange(15,45,1).tolist())\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n                    boundaries=np.arange(17,47,1).tolist())\n  \n    # sparse columns are wide \n    wide = [is_male,\n            plurality,\n            age_buckets,\n            gestation_buckets]\n  \n    # continuous columns are deep\n    deep = [mother_age,\n            gestation_weeks]\n    return wide, deep\n```\n\nUnused functions:\n- `crossed_column`\n- `embedding_column`"}, {'reason': 'stop', 'result': "def serving_input_fn():\n    feature_placeholders = {\n        'is_male': tf.placeholder(tf.string, [None]),\n        'mother_age': tf.placeholder(tf.float32, [None]),\n        'plurality': tf.placeholder(tf.string, [None]),\n        'gestation_weeks': tf.placeholder(tf.float32, [None])\n    }\n    features = {\n        key: tf.expand_dims(tensor, -1)\n        for key, tensor in feature_placeholders.items()\n    }\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "# Define a function to convert from image coords to rover coords\ndef rover_coords(binary_img):\n    # Identify nonzero pixels\n    ypos, xpos = binary_img.nonzero()\n    # Calculate pixel positions with reference to the rover position being at the \n    # center bottom of the image.  \n    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n    return x_pixel, y_pixel\n\n# Define a function to convert to radial coords in rover space\ndef to_polar_coords(x_pixel, y_pixel):\n    # Convert (x_pixel, y_pixel) to (distance, angle) \n    # in polar coordinates in rover space\n    # Calculate distance to each pixel\n    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n    # Calculate angle away from vertical for each pixel\n    angles = np.arctan2(y_pixel, x_pixel)\n    return dist, angles\n\n# Define a function to map rover space pixels to world space\ndef rotate_pix(xpix, ypix, yaw):\n    # Convert yaw to radians\n    yaw_rad = yaw * np.pi / 180\n    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n                            \n    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n    # Return the result  \n    return xpix_rotated, ypix_rotated\n\ndef translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n    # Apply a scaling and a translation\n    xpix_translated = (xpix_rot / scale) + xpos\n    ypix_translated = (ypix_rot / scale) + ypos\n    # Return the result  \n    return xpix_translated, ypix_translated\n\n\n# Define a function to apply rotation and translation (and clipping)\n# Once you define the two functions above this function should work\ndef pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n    # Apply rotation\n    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n    # Apply translation\n    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n    # Perform rotation, translation and clipping all at once\n    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n    # Return the result\n    return x_pix_world, y_pix_world\n\n# Grab another random image\nidx = np.random.randint(0, len(img_list)-1)\nimage = mpimg.imread(img_list[idx])\nwarped, mask = perspect_transform(image, source, destination)\nthreshed = color_thresh(warped)\n\n# Calculate pixel values in rover-centric coords and distance/angle to all pixels\nxpix, ypix = rover_coords(threshed)\ndist, angles = to_polar_coords(xpix, ypix)\nmean_dir = np.mean(angles)\n\n# Do some plotting\nfig = plt.figure(figsize=(12,9))\nplt.subplot(221)\nplt.imshow(image)\nplt.subplot(222)\nplt.imshow(warped)\nplt.subplot(223)\nplt.imshow(threshed, cmap='gray')\nplt.subplot(224)\nplt.plot(xpix, ypix, '.')\nplt.ylim(-160, 160)\nplt.xlim(0, 160)\narrow_length = 100\nx_arrow = arrow_length * np.cos(mean_dir)\ny_arrow = arrow_length * np.sin(mean_dir)\nplt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': '# Define a function to pass stored images to\n# reading rover position and yaw angle from csv file\n# This function will be used by moviepy to create an output video\ndef process_image(img):\n    # Example of how to use the Databucket() object defined above\n    # to print the current x, y and yaw values \n    # print(data.xpos[data.count], data.ypos[data.count], data.yaw[data.count])\n\n    # TODO: \n    # 1) Define source and destination points for perspective transform\n    # 2) Apply perspective transform\n    # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n    # 4) Convert thresholded image pixel values to rover-centric coords\n    # 5) Convert rover-centric pixel values to world coords\n    # 6) Update worldmap (to be displayed on right side of screen)\n        # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n        #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n        #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n        \n    # Perspective Transform and mask\n    warped, mask = perspect_transform(img, source, destination)\n    # 2) Apply color threshold to identify navigable terrain/obstacles/rock samples\n    threshed = color_thresh(warped)\n    obs_map = np.absolute(np.float32(threshed) - 1) * mask\n    xpix, ypix = rover_coords(threshed)\n    # 5) Convert rover-centric pixel values to world coords\n    world_size = data.worldmap.shape[0]\n    scale = 2 * dst_size\n    xpos = data.xpos[data.count]\n    ypos = data.ypos[data.count]\n    yaw = data.yaw[data.count]\n    x_world, y_world = pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale)\n    obsxpix, obsypix = rover_coords(obs_map)\n    obs_x_world, obs_y_world = pix_to_world(obsxpix, obsypix, xpos, ypos, yaw, world_size, scale)\n    # 6) Update worldmap (to be displayed on right side of screen)\n        # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n        #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n        #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n    data.worldmap[y_world, x_world, 2] = 255\n    data.worldmap[obs_y_world, obs_x_world, 0] = 255\n    nav_pix = data.worldmap[:,:,2] > 0\n    \n    data.worldmap[nav_pix, 0] = 0\n    # See if we can find some rocks\n    rock_map = find_rocks(warped, levels=(110, 110, 50))\n    if rock_map.any():\n        rock_x, rock_y = rover_coords(rock_map)\n        rock_x_world, rock_y_world = pix_to_world(rock_x, rock_y, xpos, ypos, yaw, world_size, scale)\n        data.worldmap[rock_y_world, rock_x_world, :] = 255\n\n    # 7) Make a mosaic image, below is some example code\n        # First create a blank image (can be whatever shape you like)\n    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n        # Next you can populate regions of the image with various output\n        # Here I\'m putting the original image in the upper left hand corner\n    output_image[0:img.shape[0], 0:img.shape[1]] = img\n\n        # Let\'s create more images to add to the mosaic, first a warped image\n    # warped, mask = perspect_transform(img, source, destination)\n        # Add the warped image in the upper right hand corner\n    output_image[0:img.shape[0], img.shape[1]:] = warped\n\n        # Overlay worldmap with ground truth map\n    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n        # Flip map overlay so y-axis points upward and add to output_image \n    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n\n\n        # Then putting some text over the image\n    cv2.putText(output_image,"Populate this image with your analyses to make a video!", (20, 20), \n                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n    if data.count < len(data.images) - 1:\n        data.count += 1 # Keep track of the index in the Databucket()\n    \n    return output_image\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "import re\n\ndef preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = (re.sub('[\\W]+', ' ', text.lower()) +\n            ' '.join(emoticons).replace('-', ''))\n    return text\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'from nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]```\n\nUnused functions:\n- tokenizer'}, {'reason': 'stop', 'result': "import re\nfrom nltk.corpus import stopwords\n\n\n# The `stop` is defined as earlier in this chapter\n# Added it here for convenience, so that this section\n# can be run as standalone without executing prior code\n# in the directory\nstop = stopwords.words('english')\n\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\n\ndef stream_docs(path):\n    with open(path, 'r', encoding='utf-8') as csv:\n        next(csv)  # skip header\n        for line in csv:\n            text, label = line[:-3], int(line[-2])\n            yield text, label\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def get_minibatch(doc_stream, size):\n    docs, y = [], []\n    try:\n        for _ in range(size):\n            text, label = next(doc_stream)\n            docs.append(text)\n            y.append(label)\n    except StopIteration:\n        return None, None\n    return docs, y\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def vandermonde(gamma, r=0):\n    n = gamma.size()\n    if r == 0:\n        r = 1\n    P = DiagonalPolynomialRing(QQ, n, r, inert=1) \n    X = P.variables()\n    Theta = P.inert_variables()\n    return matrix([[x**i[1]*theta**i[0] for i in gamma.cells()] \n                   for x,theta in zip(X[0],Theta[0])]).determinant()```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def partial_derivatives(P):\n    """\n    Return the partial derivative functions in all the variables of `P`.\n    \n    INPUT:\n    - `P` -- a diagonal polynomial ring\n    \n    """\n    n = P.ncols()\n    r = P.nrows()\n    D = P.grading_set()\n    X = P.derivative_variables()\n    op = {}\n    for i in range(r):\n        op[D((-1 if j==i else 0 for j in range(r)))] = [attrcall("derivative", X[i,k]) for k in range(n)]\n    return op\n\ndef polarization_operators(r, max_deg=1, side=None, row_symmetry=None):\n    D = cartesian_product([ZZ for i in range(r)])\n    return {D([-d if i==i1 else 1 if i==i2 else 0 for i in range(r)]):\n            [attrcall("polarization", i1=i1, i2=i2, d=d, row_symmetry=row_symmetry)]\n            for d in range(1, max_deg+1)\n            for i1 in range(0, r)\n            for i2 in range(0, r)\n            if (i1<i2 if side == \'down\' else i1!=i2)\n           }'}, {'reason': 'stop', 'result': 'def merge(dict1, dict2):\n    result = dict1\n    for key, value in dict2.items():\n        if key in result:\n            result[key] += value\n        else:\n            result[key] = value\n    return result\n```\n\nUnused functions:\n- antisymmetries'}, {'reason': 'stop', 'result': 'def IsotypicComponent(S, arg, use_antisymmetry=False):\n    if isinstance(arg, Partition):\n        list_partitions = [arg]\n    elif isinstance(arg, Integer):\n        list_partitions = Partitions(arg)\n    else : \n        print("Error: arg should be a partition or an integer.")\n    \n    basis = S.basis()\n    result = {}\n    P1 = basis.values().pop()[0].parent()\n    for nu in list_partitions:\n        result_nu = {}\n        if use_antisymmetry == True:\n            antisymmetries = antisymmetries_of_tableau(nu.initial_tableau())\n            P2 = DiagonalAntisymmetricPolynomialRing(P1._R, P1.ncols(), P1.nrows(), \n                                                 P1.ninert(), antisymmetries)\n        for deg, value in basis.iteritems():\n            if use_antisymmetry:\n                gen = []\n                for p in value:\n                    temp = apply_young_idempotent(P2(p), nu)\n                    if temp != 0: \n                        gen += [temp]\n            else:\n                gen = []\n                for p in value:\n                    temp = apply_young_idempotent(p, nu)\n                    if temp != 0:\n                        gen += [temp]\n            if gen != []:\n                result_nu[(deg, tuple(nu))] = Subspace(gen, {}).basis()[0]\n        if result_nu != {}:\n            result[nu] = Subspace(result_nu, operators={})\n                \n    if len(result.keys()) == 1:\n        key = result.keys()[0]\n        return result[key]\n    else:\n        return result\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def add_degrees_symmetric(gen_deg,op_deg):\n    D = cartesian_product([ZZ for i in range(len(gen_deg[0]))])\n    d = D(gen_deg[0])+D(op_deg)\n    return D(sorted(d, reverse=True)), gen_deg[1]```\n\nUnused functions:\n- add_degrees_isotypic\n- add_degrees_test'}, {'reason': 'stop', 'result': "def character_quotient(M, N, n, r, left_basis=s, right_basis=s):\n    b_tot = M.basis()\n    b_ideal = N.basis()\n    charac = 0\n    q = PolynomialRing(QQ,'q',r).gens()\n    \n    for nu in Partitions(n):\n        basis_nu_tot = {}\n        basis_nu_ideal = {}\n        charac_nu = 0\n        # Get the nu_isotypic part of the bases\n        for key, value in b_tot.iteritems():\n            if Partition(key[1]) == nu:\n                basis_nu_tot[key[0]] = value\n        for key, value in b_ideal.iteritems():\n            if Partition(key[1]) == nu:\n                basis_nu_ideal[key[0]] = value\n                \n        # Use the degrees to compute the character\n        for deg, b in basis_nu_tot.iteritems():\n            charac_nu += sum(prod(q[i]**deg[i] for i in range(0,len(deg))) for p in b)\n        for deg, b in basis_nu_ideal.iteritems():\n            charac_nu -= sum(prod(q[i]**deg[i] for i in range(0,len(deg))) for p in b)\n        if charac_nu != 0 :\n            if left_basis == s :\n                charac_nu = s.from_polynomial(charac_nu).restrict_partition_lengths(r,exact=False)           \n            else:\n                charac_nu = left_basis.from_polynomial(charac_nu)      \n            # Make the tensor product with s[nu]\n            charac += tensor([charac_nu, right_basis(s(nu))])\n            \n    return charac\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def compute_character(mu, use_antisymmetry=True, row_symmetry="permutation"):\n    n = Integer(mu.size())\n    # Determinant computation\n    v = vandermonde(mu)\n    # Span by derivatives\n    generator = {v.multidegree() : [v]}\n    list_op = partial_derivatives(v.parent())\n    V = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n    # Projection on isotypic components\n    V_iso = IsotypicComponent(V, n, use_antisymmetry=use_antisymmetry)\n    # Polarization\n    r = n-1\n    deg = v.degree()\n    if deg == 0:\n        deg = 1\n    op_pol = polarization_operators(r, deg, row_symmetry=row_symmetry)\n    V_pol = PolarizedSpace(V_iso, op_pol)\n    \n    # character\n    return character(V_pol, row_symmetry=row_symmetry)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def f(gamma):\n    v = vandermonde(gamma)\n    n = gamma.size()\n    deg_v = v.degree()\n    generator = {v.multidegree() : [v]}\n    list_op = partial_derivatives(v.parent())\n    W1 = Subspace(generators=generator, operators=list_op, add_degrees=add_degree)\n\n    r = 1\n    op_pol = polarization_operators(r, deg_v, row_symmetry="permutation")\n    show(IsotypicComponent(W1, Partition([1 for i in range(n)])).basis())\n    show(IsotypicComponent(W1, gamma).basis())\n    W1 = PolarizedSpace(IsotypicComponent(W1, n, use_antisymmetry=True), op_pol)\n    return character(W1, row_symmetry="permutation")```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def f(x): return -(1/2)*x**2 + 4*x - 6\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def sframe_to_scipy(x, column_name):\n    '''\n    Convert a dictionary column of an SFrame into a sparse matrix format where\n    each (row_id, column_id, value) triple corresponds to the value of\n    x[row_id][column_id], where column_id is a key in the dictionary.\n       \n    Example\n    >>> sparse_matrix, map_key_to_index = sframe_to_scipy(sframe, column_name)\n    '''\n    assert x[column_name].dtype() == dict, \\\n        'The chosen column must be dict type, representing sparse data.'\n        \n    # Create triples of (row_id, feature_id, count).\n    # 1. Add a row number.\n    x = x.add_row_number()\n    # 2. Stack will transform x to have a row for each unique (row, key) pair.\n    x = x.stack(column_name, ['feature', 'value'])\n\n    # Map words into integers using a OneHotEncoder feature transformation.\n    f = graphlab.feature_engineering.OneHotEncoder(features=['feature'])\n    # 1. Fit the transformer using the above data.\n    f.fit(x)\n    # 2. The transform takes 'feature' column and adds a new column 'feature_encoding'.\n    x = f.transform(x)\n    # 3. Get the feature mapping.\n    mapping = f['feature_encoding']\n    # 4. Get the feature id to use for each key.\n    x['feature_id'] = x['encoded_features'].dict_keys().apply(lambda x: x[0])\n\n    # Create numpy arrays that contain the data for the sparse matrix.\n    i = np.array(x['id'])\n    j = np.array(x['feature_id'])\n    v = np.array(x['value'])\n    width = x['id'].max() + 1\n    height = x['feature_id'].max() + 1\n\n    # Create a sparse matrix.\n    mat = csr_matrix((v, (i, j)), shape=(width, height))\n\n    return mat, mapping\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def get_initial_centroids(data, k, seed=None):\n    '''Randomly choose k data points as initial centroids'''\n    if seed is not None: # useful for obtaining consistent results\n        np.random.seed(seed)\n    n = data.shape[0] # number of data points\n    # Pick K indices from range [0, N).\n    rand_indices = np.random.randint(0, n, k)\n    \n    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n    # As long as at least one document in a cluster contains a word,\n    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n    centroids = data[rand_indices,:].toarray()\n    \n    return centroids\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def assign_clusters(data, centroids):\n    \n    # Compute distances between each data point and the set of centroids:\n    distances_from_centroids = pairwise_distances(data, centroids, metric='euclidean')\n    \n    # Compute cluster assignments for each data point:\n    cluster_assignment = np.argmin(distances_from_centroids, axis=1)\n    \n    return cluster_assignment\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': '```python\ndef revise_centroids(data, k, cluster_assignment):\n    new_centroids = []\n    for i in range(k):\n        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n        member_data_points = data[cluster_assignment==i]\n        # Compute the mean of the data points. Fill in the blank (RHS only)\n        centroid = member_data_points.mean(axis=0)\n        \n        # Convert numpy.matrix type to numpy.ndarray type\n        centroid = centroid.A1\n        new_centroids.append(centroid)\n    new_centroids = np.array(new_centroids)\n    \n    return new_centroids\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def compute_heterogeneity(data, k, centroids, cluster_assignment):\n    \n    heterogeneity = 0.0\n    for i in xrange(k):\n        \n        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n        member_data_points = data[cluster_assignment==i, :]\n        \n        if member_data_points.shape[0] > 0: # check if i-th cluster is non-empty\n            # Compute distances from centroid to data points (RHS only)\n            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')\n            squared_distances = distances**2\n            heterogeneity += np.sum(squared_distances)\n        \n    return heterogeneity\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "```python\ndef kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):\n    '''This function runs k-means on given data and initial set of centroids.\n       maxiter: maximum number of iterations to run.\n       record_heterogeneity: (optional) a list, to store the history of heterogeneity as function of iterations\n                             if None, do not store the history.\n       verbose: if True, print how many data points changed their cluster labels in each iteration'''\n    centroids = initial_centroids[:]\n    prev_cluster_assignment = None\n    \n    for itr in xrange(maxiter):        \n        if verbose:\n            print(itr)\n        \n        # 1. Make cluster assignments using nearest centroids\n        # YOUR CODE HERE\n        cluster_assignment = assign_clusters(data, centroids)\n            \n        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\n        # YOUR CODE HERE\n        centroids = revise_centroids(data, k, cluster_assignment)\n            \n        # Check for convergence: if none of the assignments changed, stop\n        if prev_cluster_assignment is not None and \\\n          (prev_cluster_assignment==cluster_assignment).all():\n            break\n        \n        # Print number of new assignments \n        if prev_cluster_assignment is not None:\n            num_changed = np.sum(prev_cluster_assignment!=cluster_assignment)\n            if verbose:\n                print('    {0:5d} elements changed their cluster assignment.'.format(num_changed))   \n        \n        # Record heterogeneity convergence metric\n        if record_heterogeneity is not None:\n            # YOUR CODE HERE\n            score = compute_heterogeneity(data, k, centroids, cluster_assignment)\n            record_heterogeneity.append(score)\n        \n        prev_cluster_assignment = cluster_assignment[:]\n        \n    return centroids, cluster_assignment\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "```python\ndef plot_heterogeneity(heterogeneity, k):\n    plt.figure(figsize=(7,4))\n    plt.plot(heterogeneity, linewidth=4)\n    plt.xlabel('# Iterations')\n    plt.ylabel('Heterogeneity')\n    plt.title('Heterogeneity of clustering over time, K={0:d}'.format(k))\n    plt.rcParams.update({'font.size': 16})\n    plt.tight_layout()\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def smart_initialize(data, k, seed=None):\n    '''Use k-means++ to initialize a good set of centroids'''\n    if seed is not None: # useful for obtaining consistent results\n        np.random.seed(seed)\n    centroids = np.zeros((k, data.shape[1]))\n    \n    # Randomly choose the first centroid.\n    # Since we have no prior knowledge, choose uniformly at random\n    idx = np.random.randint(data.shape[0])\n    centroids[0] = data[idx,:].toarray()\n    # Compute distances from the first centroid chosen to all the other data points\n    squared_distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten()**2\n    \n    for i in xrange(1, k):\n        # Choose the next centroid randomly, so that the probability for each data point to be chosen\n        # is directly proportional to its squared distance from the nearest centroid.\n        # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.\n        idx = np.random.choice(data.shape[0], 1, p=squared_distances/sum(squared_distances))\n        centroids[i] = data[idx,:].toarray()\n        # Now compute distances from the centroids to all data points\n        squared_distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean')**2,axis=1)\n    \n    return centroids\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):\n    heterogeneity = {}\n    \n    min_heterogeneity_achieved = float('inf')\n    best_seed = None\n    final_centroids = None\n    final_cluster_assignment = None\n    \n    for i in xrange(num_runs):\n        \n        # Use UTC time if no seeds are provided \n        if seed_list is not None: \n            seed = seed_list[i]\n            np.random.seed(seed)\n        else: \n            seed = int(time.time())\n            np.random.seed(seed)\n        \n        # Use k-means++ initialization\n        initial_centroids = get_initial_centroids(data, k, seed=0)\n        \n        # Run k-means\n        centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter=400,\n                                       record_heterogeneity=None, verbose=True)\n        \n        # To save time, compute heterogeneity only once in the end\n        heterogeneity[seed] = compute_heterogeneity(data, k, centroids, cluster_assignment)\n        \n        if verbose:\n            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n            sys.stdout.flush()\n        \n        # if current measurement of heterogeneity is lower than previously seen,\n        # update the minimum record of heterogeneity.\n        if heterogeneity[seed] < min_heterogeneity_achieved:\n            min_heterogeneity_achieved = heterogeneity[seed]\n            best_seed = seed\n            final_centroids = centroids\n            final_cluster_assignment = cluster_assignment\n    \n    # Return the centroids and cluster assignments that minimize heterogeneity.\n    return final_centroids, final_cluster_assignment\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def visualize_document_clusters(wiki, tf_idf, centroids, cluster_assignment, k, map_index_to_word, display_content=True):\n    '''wiki: original dataframe\n       tf_idf: data matrix, sparse matrix format\n       map_index_to_word: SFrame specifying the mapping betweeen words and column indices\n       display_content: if True, display 8 nearest neighbors of each centroid'''\n    \n    print('==========================================================')\n\n    # Visualize each cluster c\n    for c in xrange(k):\n        # Cluster heading\n        print('Cluster {0:d}    '.format(c)),\n        # Print top 5 words with largest TF-IDF weights in the cluster\n        idx = centroids[c].argsort()[::-1]\n        for i in xrange(5): # Print each word along with the TF-IDF weight\n            print('{0:s}:{1:.3f}'.format(map_index_to_word['category'][idx[i]], centroids[c,idx[i]])),\n        print('')\n        \n        if display_content:\n            # Compute distances from the centroid to all data points in the cluster,\n            # and compute nearest neighbors of the centroids within the cluster.\n            distances = pairwise_distances(tf_idf, centroids[c].reshape(1, -1), metric='euclidean').flatten()\n            distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n            nearest_neighbors = distances.argsort()\n            # For 8 nearest neighbors, print the title as well as first 180 characters of text.\n            # Wrap the text at 80-character mark.\n            for i in xrange(8):\n                text = ' '.join(wiki[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n                print('\\n* {0:50s} {1:.5f}\\n  {2:s}\\n  {3:s}'.format(wiki[nearest_neighbors[i]]['name'],\n                    distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) > 90 else ''))\n        print('==========================================================')"}, {'reason': 'stop', 'result': '```#@title "display utilities [RUN ME]"\ndef display_9_images_from_dataset(dataset):\n  plt.figure(figsize=(13,13))\n  subplot=331\n  for i, (image, label) in enumerate(dataset):\n    plt.subplot(subplot)\n    plt.axis(\'off\')\n    plt.imshow(image.numpy().astype(np.uint8))\n    plt.title(label.numpy().decode("utf-8"), fontsize=16)\n    subplot += 1\n    if i==8:\n      break\n  plt.tight_layout()\n  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n  plt.show()```'}, {'reason': 'stop', 'result': "def decode_jpeg_and_label(filename):\n  bits = tf.io.read_file(filename)\n  image = tf.image.decode_jpeg(bits)\n  # parse flower name from containing directory\n  label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')\n  label = label.values[-2]\n  return image, label\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.fc1 = nn.Linear(400, 340)\n        self.fc2 = nn.Linear(340, 19)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 400)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return F.log_softmax(x, dim=0)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def test():\n    net.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = net(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.data.view_as(pred)).sum()\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(test_loader.dataset),\n    100. * correct / len(test_loader.dataset)))```\n\nUnused functions:\nNone"}, {'reason': 'stop', 'result': 'def stick_breaking(beta):\n    portion_remaining = tt.concatenate([[1], tt.extra_ops.cumprod(1 - beta)[:-1]])\n\n    return beta * portion_remaining\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '@print_log("Awesome function")\ndef do_something(arg, **other_args):\n    print("Doing something")\n    return\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def return_itself(arg):\n    return arg\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def get_data_from_tsv(feature_file, x_fields, y_field, x_filter=None, seed=0, as_np_array=False, scale=False, shuffle=False, train_portion=0.6, test_portion=0.2):\n\n    # Load the dataset\n    my_data = pandas.read_csv(feature_file)\n\n\n    # Drop missing values\n    print(my_data.shape)\n    my_data = my_data.dropna()\n    my_data = my_data.reset_index(drop=True)\n    print(my_data.shape)\n    \n    # Filter responses if len(x_filter) > 0\n    if not x_filter == None:\n        for ff in x_filter:\n            my_data = my_data[my_data[ff[0]].isin(ff[1:])]\n            print("filtering %s by " % ff[0], ff[1:], "num datapoints left: ", len(my_data))\n        my_data = my_data.reset_index(drop=True)\n    labels = my_data[\'word\']\n        \n    # turn frequencies into log frequencies\n    if \'freq\' in x_fields or my_data.columns.get_loc(\'freq\') in x_fields:\n        idx = my_data.columns.get_loc(\'freq\')\n        my_data.iloc[:,idx] = np.log(my_data.iloc[:,idx])\n        my_data=my_data.rename(columns = {\'freq\':\'log_freq\'})\n    \n    # turn lengths into log lengths\n    if \'length\' in x_fields or my_data.columns.get_loc(\'length\') in x_fields:\n        idx = my_data.columns.get_loc(\'length\')\n        my_data.iloc[:,idx] = np.log(my_data.iloc[:,idx])\n        my_data=my_data.rename(columns = {\'length\':\'log_length\'})\n \n    # Choose the coefficients\n    if type(x_fields[0]) == str:\n        x_fields.append(y_field)\n        my_data = my_data[x_fields]\n    else:\n        x_fields.append(my_data.columns.get_loc(y_field))\n        my_data = my_data.iloc[:,x_fields]\n    del x_fields[-1]\n    \n    \n    # Split the data into training/testing sets\n    x_train, y_train, x_dev, y_dev, x_test, y_test = train_validate_test_split(my_data, \n                                                                               y_field,\n                                                                               train_percent=train_portion,\n                                                                               validate_percent=test_portion,\n                                                                               seed=seed,\n                                                                               shuffle=shuffle)\n\n    if scale:\n        x_train = pandas.DataFrame(preprocessing.scale(x_train), columns=x_train.columns, index=x_train.index)\n        x_dev   = pandas.DataFrame(preprocessing.scale(x_dev)  , columns=x_dev.columns, index=x_dev.index)\n        x_test  = pandas.DataFrame(preprocessing.scale(x_test) , columns=x_test.columns, index=x_test.index)\n        y_train = pandas.DataFrame(preprocessing.scale(y_train), columns=y_train.columns, index=y_train.index)\n        y_dev   = pandas.DataFrame(preprocessing.scale(y_dev)  , columns=y_dev.columns, index=y_dev.index)\n        y_test  = pandas.DataFrame(preprocessing.scale(y_test) , columns=y_test.columns, index=y_test.index)\n\n    \n    if as_np_array:\n        x_train = np.array(x_train).astype(np.float)\n        y_train = np.array(y_train).astype(np.float)\n        x_dev   = np.array(x_dev).astype(np.float)\n        y_dev   = np.array(y_dev).astype(np.float)\n        x_test  = np.array(x_test).astype(np.float)\n        y_test  = np.array(y_test).astype(np.float)\n        \n    return x_train, y_train, x_dev, y_dev, x_test, y_test, labels\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def train_validate_test_split(df, y, train_percent=.6, validate_percent=.2, seed=0, shuffle=True):\n    np.random.seed(seed)\n    \n    if shuffle:\n        perm = np.random.permutation(df.index)\n    else:\n        perm = df.index\n\n    train_end    = int(train_percent * len(df))\n    validate_end = int(validate_percent * len(df)) + train_end\n\n    \n    x_train        = df.iloc[perm[:train_end]]\n    x_validate     = df.iloc[perm[train_end:validate_end]]\n    x_test         = df.iloc[perm[validate_end:]]\n        \n    y_train    = x_train[y]\n    y_validate = x_validate[y]\n    y_test     = x_test[y]\n    \n    x_train = x_train.drop(y, axis=1)\n    x_validate = x_validate.drop(y, axis=1)\n    x_test = x_test.drop(y, axis=1)\n\n    return x_train, y_train, x_validate, y_validate, x_test, y_test\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'prod = lambda x: reduce(mul, x)\n\ndef get_f(x, params, backend, lnK):\n    init_concs = params[:n]\n    eq_constants = params[n:]\n    le = linear_exprs(preserv, x, linear_exprs(preserv, init_concs), rref=True)\n    if lnK:\n        return le + [\n            sum(backend.log(xi)*p for xi, p in zip(x, coeffs)) - backend.log(K) \n            for coeffs, K in zip(stoichs, eq_constants)\n        ]\n    else:\n        return le + [\n            prod(xi**p for xi, p in zip(x, coeffs)) - K for coeffs, K in zip(stoichs, eq_constants)\n        ]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "# Configure model presets\nfrom examples.seismic import demo_model\n\n# Enable model presets here:\npreset = 'layers-isotropic'  # A simple but cheap model (recommended)\n# preset = 'marmousi2d-isotropic'  # A larger more realistic model\n\n# Standard preset with a simple two-layer model\nif preset == 'layers-isotropic':\n    filter_sigma = (1, 1)\n    nshots = 21\n    nreceivers = 101\n    t0 = 0.\n    tn = 1000.  # Simulation last 1 second (1000 ms)\n    f0 = 0.010  # Source peak frequency is 10Hz (0.010 kHz)\n\n\n# A more computationally demanding preset based on the 2D Marmousi model\nif preset == 'marmousi2d-isotropic':\n    filter_sigma = (6, 6)\n    nshots = 301  # Need good covergae in shots, one every two grid points\n    nreceivers = 601  # One recevier every grid point\n    t0 = 0.\n    tn = 3500.  # Simulation last 3.5 second (3500 ms)\n    f0 = 0.025  # Source peak frequency is 25Hz (0.025 kHz)```\n\nUnused functions:\n- create_model"}, {'reason': 'stop', 'result': "# Define gradient operator for imaging\nfrom devito import TimeFunction, Operator, Eq, solve\nfrom examples.seismic import PointSource\n\ndef ImagingOperator(model, image):\n    # Define the wavefield with the size of the model and the time dimension\n    v = TimeFunction(name='v', grid=model.grid, time_order=2, space_order=4)\n\n    u = TimeFunction(name='u', grid=model.grid, time_order=2, space_order=4,\n                     save=geometry.nt)\n    \n    # Define the wave equation, but with a negated damping term\n    eqn = model.m * v.dt2 - v.laplace + model.damp * v.dt.T\n\n    # Use `solve` to rearrange the equation into a stencil expression\n    stencil = Eq(v.backward, solve(eqn, v.backward))\n    \n    # Define residual injection at the location of the forward receivers\n    dt = model.critical_dt\n    residual = PointSource(name='residual', grid=model.grid,\n                           time_range=geometry.time_axis,\n                           coordinates=geometry.rec_positions)    \n    res_term = residual.inject(field=v.backward, expr=residual * dt**2 / model.m)\n\n    # Correlate u and v for the current time step and add it to the image\n    image_update = Eq(image, image - u * v)\n\n    return Operator([stencil] + res_term + [image_update],\n                    subs=model.spacing_map)```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def processCSV(file, splitToken, bank): #TODO Move to pandas\n    fecha = []\n    saldo = []\n    movimientos = []\n    first = True\n    \n    if bank == 'cajamar':\n        with open(file) as f:\n            for line in f:\n                if first is False:\n                    tokens = line.split(splitToken)\n                    fecha.append(dt.datetime.strptime(tokens[0], '%d/%m/%Y').date())\n                    saldo.append(float(tokens[len(tokens)-1][:-1].replace('.', '').replace(',', '.')))\n                    movimientos.append(float(tokens[len(tokens)-2].replace('.','').replace(',','.').replace(' ', '')))\n                first = False\n\n    return (saldo, fecha, movimientos)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def processXLS(file, bank):   \n    balance = []\n    date = []\n    movements = []\n    \n    if bank == 'unicaja':\n        book = xlrd.open_workbook(file)\n        first_sheet = book.sheet_by_index(0)\n        date_int = first_sheet.col_values(0)[5:]\n        balance = first_sheet.col_values(5)[5:]\n        movements = first_sheet.col_values(3)[5:]\n        date = [xlrd.xldate_as_datetime(date_int[i], book.datemode).date() for i in xrange(0, len(date_int))]\n    \n    if bank == 'openbank':\n        df = pd.read_html(file)\n        df = df[0].dropna(axis=0, thresh=4)\n        date_str = df.iloc[1:,1]\n        date = [dt.datetime.strptime(x, '%d/%m/%Y').date() for x in date_str]\n        balance_str = df.iloc[1:,9]\n        balance = [float((x[:-2] + ',' + x[-2:]).replace('.','').replace(',','.')) for x in balance_str]\n        movements_str = df.iloc[1:,7]\n        movements = [float((x[:-2] + ',' + x[-2:]).replace('.','').replace(',','.')) for x in movements_str]\n\n    return (balance, date, movements)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def sortDataFiles(bankdata):\n    for accountType in ACCOUNTTYPES:\n        for bankname in bankdata[accountType]:\n            if "movements" in bankdata[accountType][bankname]:\n                (bankdata[accountType][bankname]["date"], bankdata[accountType][bankname]["balance"], bankdata[accountType][bankname]["movements"]) = zip(*sorted(zip(bankdata[accountType][bankname]["date"], bankdata[accountType][bankname]["balance"], bankdata[accountType][bankname]["movements"])))\n            else:\n                (bankdata[accountType][bankname]["date"], bankdata[accountType][bankname]["balance"]) = zip(*sorted(zip(bankdata[accountType][bankname]["date"], bankdata[accountType][bankname]["balance"])))\n            \n    return bankdata\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def loadDataFiles():    \n    print("Data files found:")\n    bankdata = {}\n    for accountType in ACCOUNTTYPES:\n        path = "accounts/{}/".format(accountType)\n        directories = [f for f in os.listdir(path) if os.path.isdir(path+f)]\n        bankdata[accountType] = {}\n        for bankname in directories:\n            bankdata[accountType][bankname] = {"date":[], "balance":[], "movements":[]}\n            files = [os.path.join(path+bankname,f) for f in os.listdir(path+bankname)]\n            for datafile in files:\n                extension = os.path.splitext(datafile)[1]\n                if extension == ".xls":\n                    print("{} - {}".format(bankname, datafile))\n                    (balance, date, movements) = processXLS(datafile, bankname)\n                    bankdata[accountType][bankname]["balance"].extend(balance)\n                    bankdata[accountType][bankname]["date"].extend(date)\n                    bankdata[accountType][bankname]["movements"].extend(movements)\n                if extension == ".csv":\n                    print("{} - {}".format(bankname, datafile))\n                    (balance, date, movements) = processCSV(datafile, \'\\t\', bankname)\n                    bankdata[accountType][bankname]["balance"].extend(balance)\n                    bankdata[accountType][bankname]["date"].extend(date)\n                    bankdata[accountType][bankname]["movements"].extend(movements)\n\n    return sortDataFiles(bankdata)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def getIntervalDates(bankdata):\n    minDate = ''\n    maxDate = ''\n    first = True\n    \n    for accountType in ACCOUNTTYPES:\n        for bank in bankdata[accountType]:\n            dates = np.array(bankdata[accountType][bank]['date'])\n            \n            if first:\n                minDate = dates.min()\n                maxDate = dates.max()\n                \n                first = False\n            else:\n                if minDate > dates.min():\n                    minDate = dates.min()\n\n                if maxDate < dates.max():\n                    maxDate = dates.max()\n\n    return (minDate, maxDate)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def fillEmpty(bankdata, maxDate):\n    for accountType in ACCOUNTTYPES:\n        for bank in bankdata[accountType]:\n            if bankdata[accountType][bank][\'date\'][-1] != maxDate:\n                bankdata[accountType][bank]["balance"] = bankdata[accountType][bank]["balance"] + (bankdata[accountType][bank]["balance"][-1],)\n                bankdata[accountType][bank]["date"] = bankdata[accountType][bank]["date"] + (maxDate,)\n                \n    return bankdata\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def get_trace_sum_balances(bankdata):\n    sum_balances = []\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        balances = bankdata[CURRENT][bank][\'balance\']\n        sum_account = {}\n        for date, balance in zip(dates, balances):\n            sum_account[date] = balance\n                \n        sum_balances.append(sum_account)\n    \n    total = {}\n    (ini, fin) = getIntervalDates(bankdata)\n    last = 0\n    max_amount = 0\n    for b in sum_balances:\n        for d in daterange(ini, fin):\n            if d in b:\n                last = b[d]                    \n                if d in total:\n                    total[d] += b[d]\n                    if total[d] > max_amount:\n                        max_amount = total[d]\n                else:\n                    total[d] = b[d]\n            else:\n                if d in total:\n                    total[d] += last\n                else:\n                    total[d] = last\n                \n    \n    dates = total.keys()\n    balances = total.values()\n    \n    (dates, balances) = zip(*sorted(zip(dates, balances)))\n    \n    trace = go.Scatter(\n        x = dates,\n        y = balances,\n        name = "All Accounts - Amount: " + format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".") + CURRENCY,\n        mode = \'lines\',\n        line = dict ( width = 4 )\n    )\n\n    return (max_amount, trace)```\n\nUnused functions:\n- daterange'}, {'reason': 'stop', 'result': 'def buildTraces(bankdata, min_trace=False, mean_trace=False, max_trace=False):\n    data = []\n    min_amount = 0\n    max_amount = 0\n    \n    for accountType in ACCOUNTTYPES: # Account type\n        for bank in bankdata[accountType]: # Bank name\n            dates = np.array(bankdata[accountType][bank][\'date\'])\n            balances = np.array(bankdata[accountType][bank][\'balance\'])\n            \n            bankName = \'(\' + accountType + \') \' + SUPPORTED_BANKS[bank]\n\n            trace_main = go.Scatter(\n                x = dates,\n                y = balances,\n                name = bankName + \': Saldo \' + str(format(balances[-1], \',.2f\').replace(",", "X").replace(".", ",").replace("X", ".")) + CURRENCY,\n                mode = \'lines\'\n            )\n            data.append(trace_main)\n\n            if max_trace:\n                trace_max = go.Scatter(\n                    x = dates,\n                    y = [balances.max() for f in dates],\n                    name = bankName + \': Saldo máximo\',\n                    line = dict(\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_max)\n\n            if mean_trace:\n                trace_mean = go.Scatter(\n                    x = dates,\n                    y = [balances.mean() for f in dates],\n                    name = bankName + \': Saldo medio\',\n                    line = dict(\n                        width = 4,\n                        dash = \'dashdot\'\n                    )\n                )\n                data.append(trace_mean)\n\n            if min_trace:\n                trace_min = go.Scatter(\n                    x = dates,\n                    y = [balances.min() for f in dates],\n                    name = bankName + \': Saldo mínimo\',\n                    line = dict(\n                        width = 4,\n                        dash = \'dot\'\n                    )\n                )\n                data.append(trace_min)\n                \n            if balances.max() > max_amount:\n                max_amount = balances.max()\n    \n    max_amount, sum_trace = get_trace_sum_balances(bankdata)\n    data.append(sum_trace)\n    \n    return (data, min_amount, max_amount)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def buildPiggy(bankdata):\n    traces = []\n    \n    for bank in bankdata[SAVING]: # Bank name\n        balances = np.array(bankdata[SAVING][bank]['balance'])\n        \n        trace = go.Bar(\n            x = ['Piggy'],\n            y = [balances[-1]],\n            name = SUPPORTED_BANKS[bank]\n        )\n        \n        traces.append(trace)\n        \n    return traces\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def buildProfit(bankdata):\n    data = []\n\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        movements = bankdata[CURRENT][bank][\'movements\']\n\n        profit = {}\n        for date, movement in zip(dates, movements):\n            key = str(date.month) + \'/\' + str(date.year)\n\n            if key in profit:\n                profit[key] += float(movement)\n            else:\n                profit[key] = float(movement)\n            \n        months = []\n        profits = []\n        for key, value in profit.items():\n            months.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            profits.append(value)\n        \n        trace = go.Bar(\n            x = months,\n            y = profits,\n            name = "Profit for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n    return data\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def buildIncomesExpenses(bankdata):\n    data = []\n\n    for bank in bankdata[CURRENT]:\n        dates = bankdata[CURRENT][bank][\'date\']\n        movements = bankdata[CURRENT][bank][\'movements\']\n\n        incomes = {}\n        expenses = {}\n        for date, movement in zip(dates, movements):\n            key = str(date.month) + \'/\' + str(date.year)\n\n            if float(movement) > 0:\n                if key in incomes:\n                    incomes[key] += float(movement)\n                else:\n                    incomes[key] = float(movement)\n            else:\n                if key in expenses:\n                    expenses[key] += float(movement)\n                else:\n                    expenses[key] = float(movement)\n            \n        months_x = []\n        incomes_y = []\n        for key, value in incomes.items():\n            months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            incomes_y.append(value)\n        \n        trace = go.Bar(\n            x = months_x,\n            y = incomes_y,\n            name = "Incomes for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n        months_x = []\n        expenses_y = []\n        for key, value in expenses.items():\n            months_x.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n            expenses_y.append(value)\n        \n        trace = go.Bar(\n            x = months_x,\n            y = expenses_y,\n            name = "Expenses for {}".format(SUPPORTED_BANKS[bank])\n        )\n        data.append(trace)\n        \n    return data\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def plot_general(bankdata, minDate, maxDate):\n    (data, _, _) = buildTraces(bankdata)\n    layout = go.Layout(title = \'Saldo \' + minDate.strftime("%m/%d/%Y") + \' - \' + maxDate.strftime("%m/%d/%Y"),\n                  xaxis = dict(title = \'Fecha\'),\n                  yaxis = dict(title = \'Saldo (\' + CURRENCY + \')\'),\n                  showlegend = True\n    )\n\n    fig = dict(data=data, layout=layout)\n    py.offline.iplot(fig, filename=\'styled-line\')```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "```python\ndef plot_piggy(bankdata):\n    data = buildPiggy(bankdata)\n    layout = go.Layout(\n        title = 'Saving',\n        barmode='stack'\n    )\n\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename='stacked-bar')\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def plot_super_view(bankdata, minDate, maxDate):\n    (data, min_amount, max_amount) = buildTraces(bankdata)\n    piggyData = buildPiggy(bankdata)\n    for piggyTrace in piggyData:\n        newTrace = go.Bar(\n            x = piggyTrace.x,\n            y = piggyTrace.y,\n            name = piggyTrace.name,\n            xaxis = \'x2\',\n            yaxis = \'y2\'\n        )\n        data.append(newTrace)\n    layout = go.Layout(\n        xaxis=dict(\n            domain=[0, 0.9],\n            title = \'Date\',\n            rangeselector=dict(\n                buttons=list([\n                    dict(count=1,\n                         label=\'1 month\',\n                         step=\'month\',\n                         stepmode=\'backward\'),\n                    dict(count=3,\n                         label=\'3 months\',\n                         step=\'month\',\n                         stepmode=\'backward\'),\n                    dict(count=6,\n                         label=\'6 months\',\n                         step=\'month\',\n                         stepmode=\'backward\'),\n                    dict(count=1,\n                        label=\'1 year\',\n                        step=\'year\',\n                        stepmode=\'backward\'),\n                    dict(step=\'all\')\n                ])\n            ),\n            rangeslider=dict(\n                visible = True\n            ),\n            type=\'date\'\n        ),\n        yaxis=dict(\n            range=[min_amount, max_amount],\n            title = \'Amount (\' + CURRENCY + \')\'\n        ),\n        xaxis2=dict(\n            domain=[0.9, 1]\n        ),\n        yaxis2=dict(\n            anchor=\'x2\',\n            range=[min_amount, max_amount],\n            showticklabels=False\n        ),\n        title = \'Super view \' + minDate.strftime("%m/%d/%Y") + \' - \' + maxDate.strftime("%m/%d/%Y"),\n        barmode=\'stack\'\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename=\'side-by-side-subplot\')```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def plot_profit(bankdata):\n    data = buildProfit(bankdata)\n    layout = go.Layout(\n        title = 'Monthly profit',\n        barmode ='group',\n        xaxis = dict(title = 'Date'),\n        yaxis = dict(title = 'Amount (' + CURRENCY + ')')\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename='grouped-bar')```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def plot_incomesExpenses(bankdata):\n    data = buildIncomesExpenses(bankdata)\n    layout = go.Layout(\n      xaxis = dict(title = 'Date'),\n      yaxis = dict(title = 'Amount (' + CURRENCY + ')'),\n      barmode = 'relative',\n      title = 'Incomes and expenses'\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename='barmode-relative')```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def plot_incomesExpensesProfits(bankdata):\n    data = buildIncomesExpenses(bankdata)\n    \n    dataProfit = buildProfit(bankdata)\n    \n    # Profits per bank\n    for barProfit in dataProfit:\n        trace_profit = go.Scatter(\n            x = barProfit.x,\n            y = barProfit.y,\n            name = barProfit.name,\n            mode = \'markers\'\n        )\n        data.append(trace_profit)\n    \n    # Total profits\n    totalProfits = {}\n    for barProfit in dataProfit:\n        for date, value in zip(barProfit.x, barProfit.y):\n            key = date.strftime("%m/%Y")\n            if key in totalProfits:\n                totalProfits[key] += value\n            else:\n                totalProfits[key] = value\n    \n    xs = []\n    ys = []\n    for key, value in totalProfits.items():\n        xs.append(dt.datetime.strptime(key, \'%m/%Y\').date())\n        ys.append(value)\n    \n    trace_profit = go.Scatter(\n        x = xs,\n        y = ys,\n        name = \'Profit\',\n        mode = \'markers\',\n        marker = dict(\n            size = 10,\n            line = dict(\n                width = 2,\n            )\n        )\n    )\n    data.append(trace_profit)\n    \n    layout = go.Layout(\n      xaxis = dict(\n          title = \'Date\',\n          rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\'1 month\',\n                     step=\'month\',\n                     stepmode=\'backward\'),\n                dict(count=3,\n                     label=\'3 months\',\n                     step=\'month\',\n                     stepmode=\'backward\'),\n                dict(count=6,\n                     label=\'6 months\',\n                     step=\'month\',\n                     stepmode=\'backward\'),\n                dict(count=1,\n                    label=\'1 year\',\n                    step=\'year\',\n                    stepmode=\'backward\'),\n                dict(step=\'all\')\n            ])\n        ),\n        rangeslider=dict(\n            visible = True\n        ),\n        type=\'date\'\n      ),\n      yaxis = dict(title = \'Amount (\' + CURRENCY + \')\'),\n      barmode = \'relative\',\n      title = \'Incomes, expenses and profit\'\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.offline.iplot(fig, filename=\'barmode-relative\')'}, {'reason': 'stop', 'result': '# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\nfrom sklearn.metrics import fbeta_score, accuracy_score\n\nbeta = 0.5\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    \'\'\'\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    \'\'\'\n    \n    results = {}\n    \n    # TODO: Fit the learner to the training data using slicing with \'sample_size\' using .fit(training_features[:], training_labels[:])\n    start = time() # Get start time\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # Get end time\n    \n    # TODO: Calculate the training time\n    results[\'train_time\'] = end - start\n        \n    # TODO: Get the predictions on the test set(X_test),\n    #       then get predictions on the first 300 training samples(X_train) using .predict()\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    end = time() # Get end time\n    \n    # TODO: Calculate the total prediction time\n    results[\'pred_time\'] = end - start\n            \n    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n    results[\'acc_train\'] = accuracy_score(y_train[:300], predictions_train)\n        \n    # TODO: Compute accuracy on test set using accuracy_score()\n    results[\'acc_test\'] = accuracy_score(y_test, predictions_test)\n    \n    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n    results[\'f_train\'] = fbeta_score(y_train[:300], predictions_train, beta)\n        \n    # TODO: Compute F-score on the test set which is y_test\n    results[\'f_test\'] = fbeta_score(y_test, predictions_test, beta)\n       \n    # Success\n    print("{} trained on {} samples.".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n    \n    return fig, axes\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def to_appropriate_column_name(s):\n    s = s.lower().replace(' ', '_')\n    s = ''.join([ch for ch in s if ch.isalnum() or ch == '_'])\n    return s\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'import numpy as np\nimport poetrytools\n\ndef get_rhymes(words):\n    num_words = len(words)\n    rhymes = np.empty((num_words, num_words))\n    \n    for i in range(num_words):\n        for j in (k for k in range(num_words) if not k == i):\n            if poetrytools.rhymes(words[i], words[j]):\n                rhymes[i][j] = 1\n    return rhymes\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def generate_line(model, words, seed=None, rhymes=None):\n    syllables_left = 10\n    last_stress = 0\n    \n    line= []\n    start_probs = model.startprob_\n    emission_probs = model.emissionprob_\n    transition_probs = model.transmat_\n    \n    start_state = np.random.choice(len(start_probs), p = start_probs)\n    \n    if seed is not None:\n        possible_start_emissions = np.where(rhymes[seed] == 1)\n        probs = np.array(emission_probs[start_state][possible_start_emissions])\n\n        scaled_probs = probs / sum(probs)\n        while True:\n            start_emission = np.random.choice(possible_start_emissions[0], p=scaled_probs)\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1 :\n                break\n    else:\n        while True:\n            start_emission = np.random.choice(len(emission_probs[start_state]), p=emission_probs[start_state])\n            start_stress = poetrytools.stress(words[start_emission])\n            if len(start_stress) == 1 or int(start_stress[-1]) == 1:\n                break\n    \n    line.append(start_emission)\n    start_stress = poetrytools.stress(words[start_emission])\n    syllables_left -= len(start_stress)\n    \n    if len(start_stress) == 1:\n        prev_starting_stress = 1\n    else:\n        prev_starting_stress = int(start_stress[0])\n\n    curr_state = start_state\n    while syllables_left > 0:\n        possible_transitions = transition_probs[curr_state]\n        curr_state = np.random.choice(len(possible_transitions), p=possible_transitions)\n        possible_emissions = emission_probs[curr_state]\n        while True:\n            curr_emission = np.random.choice(len(possible_emissions), p=possible_emissions)\n            curr_stress = poetrytools.stress(words[curr_emission])\n            if len(curr_stress) == 1:\n                prev_starting_stress = 1 - prev_starting_stress\n                syllables_left -= 1\n                break\n            elif len(curr_stress) > syllables_left or int(curr_stress[-1]) == prev_starting_stress:\n                continue\n            else:\n                prev_starting_stress = int(curr_stress[0])\n                syllables_left -= len(curr_stress)\n                break\n        line.append(curr_emission)\n\n    return line\n\ndef convert_line(sample, words):\n    ret = \'\'\n    i = 0\n    for word in reversed(sample):\n        curr_word = words[word]\n        if i == 0 or (curr_word == \'i\'):\n            ret += curr_word.title() + \' \'\n        else:\n            ret += curr_word + \' \'\n        i += 1\n    return ret\n\ndef generate_pair(model, words, rhymes):\n    while True:\n        a_line = generate_line(model, words)\n        seed = a_line[0]\n        if len(np.where(rhymes[seed] == 1)[0]) > 0:\n            b_line = generate_line(model, words, seed, rhymes)\n            return a_line, b_line\n        \ndef generate_rhyming_and_meter_sonnet():\n    sonnet = \'\'\n    a_lines = []\n    b_lines = []\n    \n    for _ in range(4):\n        a_line, b_line = generate_pair(reversed_quatrain_model, quatrain_words, quatrain_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    for i in range(2):\n        sonnet += convert_line(a_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(a_lines[2 * i + 1], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i], quatrain_words) + \'\\n\'\n        sonnet += convert_line(b_lines[2 * i + 1], quatrain_words) + \'\\n\'\n    \n    a_lines = []\n    b_lines = []\n    \n    for _ in range(2):\n        a_line, b_line = generate_pair(reversed_volta_model, volta_words, volta_rhymes)\n        a_lines.append(a_line)\n        b_lines.append(b_line)\n    \n    sonnet += convert_line(a_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(a_lines[1], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[0], volta_words) + \'\\n\'\n    sonnet += convert_line(b_lines[1], volta_words) + \'\\n\'\n    \n    a_line, b_line = generate_pair(reversed_couplet_model, couplet_words, couplet_rhymes)\n    sonnet += convert_line(a_line, couplet_words) + \'\\n\'\n    sonnet += convert_line(b_line, couplet_words) + \'\\n\'\n    \n    return sonnet\n\ndef generate_10_rhyming_and_meter_sonnets():\n    sonnets = \'\'\n    for i in range(10):\n        print(\'Generating Sonnet \' + str(i + 1))\n        sonnets += str(i) + \'\\n\' + generate_rhyming_and_meter_sonnet() + \'\\n\'\n    \n    f = open("project2data/rhyming_and_meter_shakespeare.txt","w")\n    f.write(sonnets)\n    return sonnets\n```\n\n\nUnused functions:\n- generate_10_rhyming_and_meter_sonnets'}, {'reason': 'stop', 'result': "# checks if line is in iambic pentameter (i.e. 0101010101 stress pattern)\ndef check_iambic_pentameter(line):\n    # get the stresses from cmu dict \n    # if word is 1 syllable, then have the option for it to be stressed or unstressed\n    stresses = []\n    for i in line.split(' '):\n        stress = poetrytools.stress(i)\n        if len(stress) == 1:\n            stresses.append(['0','1'])\n        else:\n            stresses.append([stress])\n    \n    # make combination of all possible stress patterns\n    result = [[]]\n    final = []\n    for pool in stresses:\n        result = [x+[y] for x in result for y in pool]\n    final = [''.join(i) for i in result]\n    \n    # return if any pattern fits iambic pentameter \n    return ('0101010101' in final)```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def importInflunet(path):\n    \'\'\'\n    Reads the Influnet data and creates a unique multiindex dataframe of the format\n    \n    (year,week) - incidence\n    \n    :param path: location of the influnet folder\n    :return: compacted version of \n    \'\'\'\n    \n    df = pd.concat([pd.read_csv(path+t, names=["time", "incidence"], sep=" ", header=1, usecols=[0,4], decimal=",") for t in listdir(path)], ignore_index=True)\n    df[["year","week"]] = df["time"].str.split("-", expand=True).astype(int)\n    df.drop(["time"], axis=1, inplace=True)\n    df = df.set_index(["year","week"])\n    df.sortlevel(inplace=True)\n    df = df.astype(float)\n    df = df.loc[2008:]\n    return df\n\ndef padInflunet(aux, year):\n    \'\'\'\n    The influnet dataset lacks information about the weeks that do not belog to the flu season (usally, but not necessarly, from week 17 to 40).\n    This functions fills the dataset with empty position in order to match the wikipedia format.\n    \n    :param aux: Influnet dataframe from a specific year\n    :param year: year of the previous Influnet dataframe\n    :return: padded version of the original dataframe\n    \'\'\'\n    year_weeks = aux.index.values[-1]\n    week_range = range(1,year_weeks+1)\n    aux = aux.reindex(week_range, fill_value=0)\n    aux["year"] = year\n    aux["week"] = week_range\n    \n    aux.set_index([\'year\', \'week\'], append=False, inplace=True)\n    return aux\n\n\ndef getInflunet(path = "/home/aalto/Desktop/DE/hw2/influnet/data/"):\n    \'\'\'\n    import and reformat the original Influnet dataset\n    \n    :param path: \n    :return: clean and padded version of the Influnet dataset\n    \'\'\'\n    \n    df = importInflunet();\n    previous = None\n    for x,y in df.index.values:\n        if previous == None:\n            df2 = reindexDF(df.loc[x], x)\n        elif x != previous:\n            df2 = df2.append(reindexDF(df.loc[x], x))\n        previous = x\n    return df\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '```python\ndef getWiki(wikiPages, path = "/home/aalto/PycharmProjects/digitalepidemiology/data/"):\n    \'\'\'\n    \n    :param wikiPages: list of the wikipages that we want to analyze\n    :param path: location of the downloaded wikipedia pages\n    :return: \n    \'\'\'\n    df = pd.DataFrame()\n    for wikiPage in wikiPages:\n        wiki2 = pd.read_csv(path+wikiPage+".csv", usecols=[0,1], parse_dates=[0], index_col=[0], header=None)\n        wiki2 = wiki2.resample("W-Sun").sum()\n        df[wikipage] = wiki2\n    return df\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def comparePlots(elems, years):\n    '''\n    ta\n    :param elems: list of dataframes that we  want to compare\n    :param years: list of years that we want to anlayze\n    :return: None (prompts the plot)\n    '''\n    for year in years:\n        for elem in elems:\n            plt.plot(x, y2/y2.max())\n            plt.plot(x,y/y.max())```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'class USZIPCodeRepository:\n    CACHE = {}\n\n    def __init__(self, data_url_prefix = \'https://raw.githubusercontent.com/yyu/GeoJSON-US/master\'):\n        self.data_url_prefix = data_url_prefix\n        self.geojson_url_prefix = f\'{data_url_prefix}/perZIPgeojson\'\n\n        self.refresh_zipcode_latlons(f\'{data_url_prefix}/ZIPCodesGazetteer.tsv\')\n        self.refresh_available_zipcodes(f\'{data_url_prefix}/perZIPgeojson/all_zipcodes.txt\')\n\n\n    def refresh_zipcode_latlons(self, url):\n        lines = [ line.decode(\'UTF8\').strip() for line in urllib.request.urlopen(url).readlines() ]\n        tsv = csv.DictReader(lines, delimiter=\'\\t\')\n        self.gazetteer = dict((d[\'GEOID\'], {\'lat\': float(d[\'INTPTLAT\']), \'lon\': float(d[\'INTPTLONG\'])}) for d in tsv)\n\n\n    def refresh_available_zipcodes(self, url):\n        lines = [ zipcode.decode(\'UTF8\').strip() for zipcode in urllib.request.urlopen(url).readlines() ]\n        self.zipcode_list = lines[1:] # ignore the first line\n        self.zipcode_set = set(self.zipcode_list)\n\n\n    def make_url(self, zipcode):\n        return f\'{self.data_url_prefix}/perZIPgeojson/{zipcode[0]}/{zipcode[1]}/{zipcode[2]}/{zipcode}.json\'\n\n\n    def fetch_zipcode(self, zipcode):\n        \'\'\'returns a (dict, err) tuple where err could be a string for error message or None\'\'\'\n\n        url = self.make_url(zipcode)\n\n        if url in USZIPCodeRepository.CACHE:\n            return (USZIPCodeRepository.CACHE[url], None)\n\n        try:\n            s = urllib.request.urlopen(url).read()\n        except urllib.error.URLError as e:\n            return (None, \'failed to get \' + url, \':\', e.reason)\n\n        j = json.loads(s)\n\n        USZIPCodeRepository.CACHE[url] = j\n\n        return (j, None)\n\n\n    def fetch_zipcodes(self, *zipcodes):\n        d = {"type": "FeatureCollection", "features": []}\n\n        available_zipcodes = set(zipcodes) & self.zipcode_set\n\n        for z in available_zipcodes:\n            j, err = self.fetch_zipcode(z)\n\n            if j is not None:\n                d[\'features\'].append(j)\n\n        return d\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import ipyleaflet as leaflet\nimport ipywidgets as widgets\n\nclass USMap:\n    def __init__(self):\n        self.us = USZIPCodeRepository()\n\n        self.center = [47.621795, -122.334958]\n        self.zoom = 8\n        self.height = \'500px\'\n        self.progress_bar_width = \'500px\'\n        self.area_style = {\'color\':\'#0000ff\', \'weight\': .5, \'fillColor\':\'#000077\', \'fillOpacity\':0.2}\n\n        self.progress_bar = widgets.IntProgress(bar_style=\'info\', layout=widgets.Layout(width=self.progress_bar_width))\n        self.label = widgets.Label()\n        self.progress_label = widgets.Label()\n        self.info_box = widgets.HBox([self.progress_label, self.progress_bar])\n\n        self.basemap = leaflet.basemaps.OpenMapSurfer.Roads\n        self.basemap[\'name\'] = \'basemap\'\n        self.heatmap_data = leaflet.basemaps.Strava.All\n        self.heatmap_data[\'name\'] = \'heatmap\'\n        self.heatmap = leaflet.basemap_to_tiles(self.heatmap_data)\n        self.layers_control = leaflet.LayersControl()\n        self.map_layout = widgets.Layout(height=self.height)\n\n        self.map = None\n\n\n    def enable_heatmap(self):\n        self.map.add_layer(self.heatmap)\n\n\n    def disable_heatmap(self):\n        self.map.remove_layer(self.heatmap)\n\n\n    def handle_interaction(self, **kwargs):\n        \'\'\'mouse interaction handling\'\'\'\n        if kwargs.get(\'type\') == \'mousemove\':\n            self.label.value = str(kwargs.get(\'coordinates\'))\n\n    def fetch_zipcode(self, zipcode):\n        d, err = self.us.fetch_zipcode(zipcode)\n        if err is not None:\n            print(err)\n        return d\n\n\n    def add_point(self, lat, lng, name=\'\', popup=None):\n        feature = {"type": "Feature", "properties": {}, "geometry": {"type": "Point", "coordinates": [lng, lat]}}\n        self.add_geojson(feature, name, popup)\n\n\n    def add_geojson(self, geojson, name=\'\', popup=None):\n        g = leaflet.GeoJSON(data=geojson, hover_style={\'fillColor\': \'#00aaff\'}, name=name)\n\n        if popup is not None:\n            g.popup = popup\n\n        self.map += g\n\n\n    def add_geojsons(self, geojsons, name=\'\'):\n        d = {"type": "FeatureCollection", "features": list(geojsons)}\n\n        self.add_geojson(d, name)\n\n\n    def add_zipcode(self, zipcode):\n        d = self.fetch_zipcode(zipcode)\n        if d is None:\n            print(\'failed to add \' + zipcode + \'.\')\n            return\n\n        d[\'properties\'][\'style\'] = self.area_style\n\n        text_template = Template(\'\'\'<div>ZIP Code\n                                        <ul class=\'list-group\'>\n                                            <li class=\'list-group-item\'>$zipcode</li>\n                                        </ul>\n                                    </div>\'\'\')\n        popup_text = text_template.substitute(zipcode=zipcode)\n        popup = widgets.HTML(value=popup_text, placeholder=\'\', description=\'\')\n\n        self.add_geojson(d, name=zipcode, popup=popup)\n\n\n    def progressive_iter(self, iterable, n=None, label_on_finish=\'\'):\n        display(self.info_box)\n        \n        if n is None:\n            n = len(iterable)\n\n        self.progress_bar.value = self.progress_bar.min\n        self.progress_bar.max = n\n\n        for v in iterable:\n            yield v\n            self.progress_label.value = v\n            self.progress_bar.value += 1\n\n        self.progress_label.value = label_on_finish\n\n\n    def add_zipcodes_no_check(self, zipcodes, show_progress=False):\n        zipcode_gen = self.progressive_iter(zipcodes) if show_progress else zipcodes\n\n        for z in zipcode_gen:\n            self.add_zipcode(z)\n\n        return zipcodes\n\n\n    def add_zipcodes(self, zipcodes, show_progress=False):\n        zipcodes = set(zipcodes)\n        available_zipcodes = list(zipcodes & self.us.zipcode_set)\n        available_zipcodes.sort()\n\n        return self.add_zipcodes_no_check(available_zipcodes, show_progress)\n\n\n    def display(self):\n        if self.map is None:\n            self.map = leaflet.Map( center=self.center, zoom=self.zoom, basemap=self.basemap, layout=self.map_layout)\n            self.map.on_interaction(self.handle_interaction)\n            self.map.add_control(self.layers_control)\n\n        display(self.map)\n        display(self.label)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def generate_graphs(n=10, prog='neato', multi=True):\n    d = {}\n    for i in range(n):\n        max_nodes = np.random.randint(3, 8)\n        max_iter = np.random.randint(10, 100)\n        \n        if multi is True:\n            g, p = generate_multi_bbn(max_nodes, max_iter=max_iter) \n        else: \n            g, p = generate_singly_bbn(max_nodes, max_iter=max_iter)\n            \n        bbn = convert_for_exact_inference(g, p)\n        pos = nx.nx_agraph.graphviz_layout(g, prog=prog)\n        \n        d[i] = {\n            'g': g,\n            'p': p,\n            'bbn': bbn,\n            'pos': pos\n        }\n    return d\n\ndef draw_graphs(graphs, prefix):\n    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n    for i, ax in enumerate(np.ravel(axes)):\n        graph = graphs[i]\n        nx.draw(graph['g'], pos=graph['pos'], with_labels=True, ax=ax)\n        ax.set_title('{} Graph {}'.format(prefix, i + 1))\n\nmulti_graphs = generate_graphs(multi=True)\nsingly_graphs = generate_graphs(multi=False)```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': '# Define a function to replace nulls for many columns: \ndef fill_nulls(df, col_list, na_val):\n    for col in col_list:\n        df[col].fillna(value = na_val, inplace = True)\n    return df```'}, {'reason': 'stop', 'result': '# Print out data types\ndef data_types(df):\n    for col in df:\n        print(col, type(df[col][1]))   ```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "# By pasting the above list into a spreadsheet and cross checking with the data dictionary, we can \n# see which category each field should be\n\n# statsmodel requires all fieldsnames to begin with letters, so let's sort this out now.\ntrain = train.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ntest = test.rename(columns = {'1stFlrSF': 'FirstFlrSF','2ndFlrSF': 'SecondFlrSF','3SsnPorch': 'ThreeSsnPorch'})\ndata_full = pd.concat([train, test], keys = ['train', 'test'])\n\n# Makes lists of each type\ncategories = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n              'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'YearBuilt', \n              'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', \n              'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'Heating', \n              'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', \n              'GarageFinish', 'GarageCars', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', \n              'SaleCondition']\nfloats = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n          'FirstFlrSF', 'SecondFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n          'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nints = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \n         'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n\nbools = ['CentralAir']\n\nfeature_names = categories + floats + ints + bools\n\n# Define a function for converting a list of columns to a particular type: \ndef convert_col_type(df, cols, type):\n    for col in cols:\n        df[col] = df[col].astype(type)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '```python\n# Invoke Ridge regularisation\nmodel_ridge = Ridge()\n```\n\nUnused functions:\n- `rmse_cv`'}, {'reason': 'stop', 'result': '#let\'s look at the residuals as well:\ndef plot_residuals(model, X_train, y):\n    matplotlib.rcParams[\'figure.figsize\'] = (6.0, 6.0)\n    preds = pd.DataFrame({"preds":model.predict(X_train), "true":y})\n    preds["residuals"] = preds["true"] - preds["preds"]\n    preds.plot(x = "preds", y = "residuals",kind = "scatter")```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def make_export_table(model):\n    kaggle_export = pd.DataFrame({\n        'id': test['Id'],\n        'SalePrice': preds[model]\n    },\n    columns = ['id', 'SalePrice'])\n    return kaggle_export\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'from arcgis import geometry\nfrom arcgis import features\n\ndef create_feature(map1, g):\n    try:\n        oid = 1\n        pt = geometry.Point(g)\n        feat = features.Feature(geometry=pt, attributes={\'OBJECTID\': 1,\n                                                        \'name\': \'name\',\n                                                        \'type\': \'park\',\n                                                        \'surface\': \'dirt\'})\n        feature_layer.edit_features(adds=[feat])\n        print(str(g))\n        map1.draw(g)\n    except:\n        print("Couldn\'t create the feature. Try again, please...")```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def least_squares(y, tx):\n    """calculate the least squares solution."""\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # least squares: TODO\n    # returns mse, and optimal weights\n    # ***************************************************\n    raise NotImplementedError\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from helpers import *\n\ndef test_your_least_squares():\n    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n    x, mean_x, std_x = standardize(height)\n    y, tx = build_model_data(x, weight)\n    raise NotImplementedError\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def build_poly(x, degree):\n    """polynomial basis functions for input data x, for j=0 up to j=degree."""\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # polynomial basis function: TODO\n    # this function should return the matrix formed\n    # by applying the polynomial basis to the input data\n    # ***************************************************\n    raise NotImplementedError\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from plots import *\n\ndef polynomial_regression():\n    """Constructing the polynomial basis function expansion of the data,\n       and then running least squares regression."""\n    # define parameters\n    degrees = [1, 3, 7, 12]\n    \n    # define the structure of the figure\n    num_row = 2\n    num_col = 2\n    f, axs = plt.subplots(num_row, num_col)\n\n    for ind, degree in enumerate(degrees):\n        print("Processing {i}th experiment, degree={d}, rmse={loss}".format(\n              i=ind + 1, d=degree, loss=rmse))\n        # plot fit\n        plot_fitted_curve(\n            y, x, weights, degree, axs[ind // num_col][ind % num_col])\n    plt.tight_layout()\n    plt.savefig("visualize_polynomial_regression")\n    plt.show()```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef split_data(x, y, ratio, seed=1):\n    """\n    split the dataset based on the split ratio. If ratio is 0.8 \n    you will have 80% of your data set dedicated to training \n    and the rest dedicated to testing\n    """\n    # set seed\n    np.random.seed(seed)\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # split the data based on the given ratio: TODO\n    # ***************************************************\n    raise NotImplementedError\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def train_test_split_demo(x, y, degree, ratio, seed):\n    """polynomial regression with different split ratios and different degrees."""\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # split the data, and return train and test data: TODO\n    # ***************************************************\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # form train and test data with polynomial basis function: TODO\n    # ***************************************************\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # calcualte weight through least square.: TODO\n    # ***************************************************\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # calculate RMSE for train and test data,\n    # and store them in rmse_tr and rmse_te respectively: TODO\n    # ***************************************************\n    print("proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}".format(\n          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))'}, {'reason': 'stop', 'result': 'def ridge_regression(y, tx, lambda_):\n    """implement ridge regression."""\n    # ***************************************************\n    # INSERT YOUR CODE HERE\n    # ridge regression: TODO\n    # ***************************************************\n    raise NotImplementedError\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def ridge_regression_demo(x, y, degree, ratio, seed):\n    """ridge regression demo."""\n    # define parameter\n    lambdas = np.logspace(-5, 0, 15)\n\n    rmse_tr = []\n    rmse_te = []\n    for ind, lambda_ in enumerate(lambdas):\n        # ***************************************************\n        # INSERT YOUR CODE HERE\n        # ridge regression with a given lambda\n        # ***************************************************\n        print("proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}".format(\n               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n        \n    # Plot the obtained results\n    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n\n    raise NotImplementedError\n```\n\nUnused functions:\n- split the data, and return train and test data\n- form train and test data with polynomial basis function'}, {'reason': 'stop', 'result': "def get_description(row):\n    fields = row[['place', 'city', 'state', 'country']].dropna().drop_duplicates()\n    if len(fields) == 1:\n        # if there's only 1 field, just return it\n        return fields.iloc[0]\n    elif len(fields) == 2:\n        # if there are 2, return them with a line break between\n        return fields.iloc[0] + '<br />' + fields.iloc[1]\n    elif len(fields) == 3:\n        # if there are 3, return the city/state comma-separated, then country after a line break\n        return fields.iloc[0] + ', ' + fields.iloc[1] + '<br />' + fields.iloc[2]\n    elif len(fields) == 4:\n        # if there are 4, return place then line break, then city/state, line break, then country\n        return fields.iloc[0] + '<br />' + fields.iloc[1] + ', ' + fields.iloc[2] + '<br />' + fields.iloc[3]```"}, {'reason': 'stop', 'result': "def df_to_geojson(df, properties=[], lat='lat', lon='lon'):\n    geojson = {'type':'FeatureCollection', 'features':[]}\n    for _, row in df.iterrows():\n        feature = {'type':'Feature',\n                   'properties':{},\n                   'geometry':{'type':'Point',\n                               'coordinates':[]}}\n        feature['geometry']['coordinates'] = [row[lon],row[lat]]\n        for prop in properties:\n            feature['properties'][prop] = row[prop] if prop in row else None\n        geojson['features'].append(feature)\n    return geojson\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'from IPython.display import clear_output, Image, display, HTML\n\ndef strip_consts(graph_def, max_const_size=32):\n    """Strip large constant values from graph_def."""\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add() \n        n.MergeFrom(n0)\n        if n.op == \'Const\':\n            tensor = n.attr[\'value\'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = "<stripped %d bytes>"%size\n    return strip_def\n\ndef show_graph(graph_def, max_const_size=32):\n    """Visualize TensorFlow graph."""\n    if hasattr(graph_def, \'as_graph_def\'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    code = """\n        <script>\n          function load() {{\n            document.getElementById("{id}").pbtxt = {data};\n          }}\n        </script>\n        <link rel="import" href="https://tensorboard.appspot.com/tf-graph-basic.build.html" onload=load()>\n        <div style="height:600px">\n          <tf-graph-basic id="{id}"></tf-graph-basic>\n        </div>\n    """.format(data=repr(str(strip_def)), id=\'graph\'+str(np.random.rand()))\n\n    iframe = """\n        <iframe seamless style="width:1600px;height:750px;border:0" srcdoc="{}"></iframe>\n    """.format(code.replace(\'"\', \'&quot;\'))\n    display(HTML(iframe))'}, {'reason': 'stop', 'result': "def eval_grads(new_weights):\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    variables = TensorFlowVariables(loss, sess)\n    variables.set_weights(new_weights)\n    grads = {}\n    for g, v in grads_and_vars:\n        grads[v.name] = sess.run(g, feed_dict={input_layer : x0.T})\n    sess.close()\n    return grads\n\ndef process_weights(w):\n    nw = dict()\n    nw['fc1/kernel'] = w['fc1/weights']\n    nw['fc1/bias'] = w['fc1/biases']\n    nw['fc_out/kernel'] = w['fc_out/weights']\n    nw['fc_out/bias'] = w['fc_out/biases']\n    return nw        \n\ndef flatten_grads(g):\n    arr = np.array([])\n    arr = np.append(arr, g['fc1/kernel:0'])  # weights\n    arr = np.append(arr, g['fc1/bias:0'])\n    arr = np.append(arr, g['fc_out/kernel:0'])  # weights\n    arr = np.append(arr, g['fc_out/bias:0'])\n    return arr\n\ndef full(iteration):\n    true_grad = flatten_grads(eval_grads(process_weights(iteration['weights'])))\n    return np.corrcoef(true_grad, iteration['grad'])```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def getDataAndLabels(cases):\n    labels_string = cases.cancer_type\n    le = preprocessing.LabelEncoder()\n    labels = le.fit_transform(labels_string)\n\n    # Get rid of the cancer type and patient_id columns \n    data = cases[cases.columns[3:]]\n    return {'data': data, 'labels': labels , 'label_encoder': le }\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def foldData(data, labels):\n    skf = StratifiedKFold(n_splits=10)\n    folds = []\n    for train_index, dev_index in skf.split(data, labels):\n        train_data, dev_data     = data.values[train_index], data.values[dev_index]\n        train_labels, dev_labels = labels[train_index], labels[dev_index]        \n        folds.append( {'train_data': train_data, 'train_labels': train_labels, \n                        'dev_data':   dev_data,   'dev_labels': dev_labels })\n    return folds\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def splitData(data, labels):\n    train_data_all, test_data, train_labels_all, test_labels = train_test_split(data, labels,\n                                                               stratify=labels, \n                                                               test_size=0.25)\n\n    train_data, dev_data, train_labels, dev_labels = train_test_split(train_data_all, train_labels_all,\n                                                                     stratify=train_labels_all, \n                                                                     test_size=0.20)\n\n    print("training data:", train_data.shape)\n    print("dev data     :", dev_data.shape)\n    print("test data    :",  test_data.shape)\n    return {\'train_data\': train_data, \'train_labels\': train_labels, \n            \'dev_data\':   dev_data,   \'dev_labels\': dev_labels,\n            \'test_data\':  test_data,  \'test_labels\': test_labels}\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def getBestParams(train_data, train_labels):\n    #\n    # Logistic Regression\n    #\n    lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n    params = {'C': [0.1, 0.25,  0.5,]}\n    logit = GridSearchCV(lr, params, cv=5,\n                         scoring='accuracy', return_train_score=True)\n\n    # Fit  training data\n    logit.fit(train_data, train_labels)  \n    # Show the best C parameter to use and the expected accuracy\n    print('\\nLogistic Regression Classifier, L2 regularization')\n    print(' Best param:', logit.best_params_)\n    print(' Accuracy:  ', np.round(logit.best_score_, 4) )\n    \n    return logit.best_params_```\n\nUnused functions:\n- getBestParams"}, {'reason': 'stop', 'result': "def getBestParamsSVM(train_data, train_labels):\n    classifier = LinearSVC(penalty='l2')\n\n    params = {'C': [0.01, 0.1, 0.5]}\n    svm = GridSearchCV(classifier, params, cv=4, \n                       scoring='accuracy', return_train_score=True)\n\n    svm.fit(train_data, train_labels)  \n    print('\\nSVM Classifier')\n    print(' Best param:', svm.best_params_)\n    print(' Accuracy:  ', np.round(svm.best_score_, 4) )\n    \n    return svm.best_params_```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def coords_of_max(theArray, n):\n    # Flatten the 2D array\n    flat = theArray.flatten()\n    # Partition so that the we know the sort order for\n    # the cells with the highest values.  We just\n    # care about the top n highest values.  So for example,\n    # if n = 3, get return 3 indices.  \n    indices = np.argpartition(flat, -n)[-n:]\n    # Reverse so that we show index of highest value first\n    # (descending)\n    indices = indices[np.argsort(-flat[indices])]\n    # Now return the coordinates for these indices\n    # for a 2D array.  This will return 2 arrays,\n    # the first for the row index, the second for the\n    # column index.  The row index represents the\n    # actual digit, the column index represents\n    # the confused digit\n    return np.unravel_index(indices, theArray.shape)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def plotMetrics(precision, recall, precision_by_label, recall_by_label, confusion, label_encoder):\n    labels = [ \'\\n\'.join(wrap(l, 8)) for l in feature_size ]       \n    \n    plt.rcParams["figure.figsize"] = (20,6)\n\n    plt.plot(labels, precision, color=\'olivedrab\', \n             linewidth=3, label=\'Precision (Logistic regression) \', marker=\'o\' )\n    plt.plot(labels, recall, color=\'olivedrab\', linestyle=\'dashed\',\n             linewidth=3, label=\'Recall (Logistic regression)\', marker=\'o\' )\n\n    plt.plot(labels, svm_precision, color=\'slateblue\', \n             linewidth=3, label=\'Precision (SVM)\', marker=\'o\' )\n    plt.plot(labels, svm_recall, color=\'slateblue\', linestyle=\'dashed\',\n             linewidth=3, label=\'Recall (SVM)\', marker=\'o\' )\n\n    plt.yticks(np.arange(.42, .65, .01))\n    plt.ylabel(\'Precision, Recall\', fontsize=20)\n    plt.xlabel(\'Feature size with L1 regularization at different C parameters\', fontsize=20, labelpad=20)\n    plt.legend()\n    plt.grid()\n    plt.show()\n    \n    # find optimal f1\n    best_idx = np.argmax(precision)\n    \n    # Show precision and recall across different labels\n    showPrecisionRecallPairByLabel(precision_by_label[best_idx], recall_by_label[best_idx], label_encoder,\n                                  \'Logistic Regression\', [\'olivedrab\', \'darkolivegreen\'])\n    showPrecisionRecallPairByLabel(svm_precision_by_label[best_idx], svm_recall_by_label[best_idx], label_encoder,\n                                  \'SVM\', [\'slateblue\', \'darkslateblue\'])\n    \n    \n    # Get the confusion matrix for the optimal precision\n    # Show the labels that have the highest error rate\n    conf_mx = confusion[best_idx]\n    showTopConfused(conf_mx, label_encoder)\n```\n\nUnused functions:\n- `plotMetrics`'}, {'reason': 'stop', 'result': "def showTopConfused(conf_mx, label_encoder):\n    # Determine the error rates for each misclassification pair\n    row_sums = conf_mx.sum(axis=1, keepdims=True)\n    norm_conf_mx = conf_mx / row_sums\n    # Set the error rates for correctly classified pairs (the diagonal) to zero\n    np.fill_diagonal(norm_conf_mx, 0)\n    \n    max_coords = coords_of_max(norm_conf_mx, 20)\n    confusion_rows = []\n    for i in range(len(max_coords[0])):\n\n        # This is the actual label\n        actual_label_idx  = max_coords[0][i]\n        actual_label      = label_encoder.inverse_transform([actual_label_idx])[0]\n\n        # This is the predicted label\n        predicted_label_idx = max_coords[1][i]\n        predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]\n        \n        # This is the error rate\n        error_rate  = norm_conf_mx[max_coords[0][i], max_coords[1][i]]\n        error_count = conf_mx[max_coords[0][i], max_coords[1][i]]\n\n        row = list([ actual_label,                     \n                     predicted_label,\n                     code_to_disease[actual_label][0], \n                     code_to_disease[predicted_label][0], \n                     error_rate, \n                     error_count ])\n        confusion_rows.append(row)\n    \n    df = pd.DataFrame(confusion_rows, columns=['actual', 'predicted',  'actual_name', 'predicted_name', 'error_rate', 'error_count'])\n    display(df)"}, {'reason': 'stop', 'result': 'def showPrecisionRecallPairByLabel(precision_by_label, recall_by_label, label_encoder, classifier_name, colors):\n    labels = []\n    for i in range(len(precision_by_label)):\n        label = label_encoder.inverse_transform([i])[0]\n        labels.append(label)\n    \n    y_pos = np.arange(len(labels))    \n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n\n    ax1.invert_xaxis()\n    ax1.yaxis.tick_right()\n    \n    ax1.set_yticks(y_pos)\n    ax1.set_yticklabels(labels)\n    \n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels(labels)\n        \n    ax1.barh(y_pos, precision_by_label, color=colors[0] , label="precision")\n    ax2.barh(y_pos, recall_by_label,    color=colors[1],  label=\'recall\')\n\n    ax1.set_title(\'Precision( \' + classifier_name + \')\')\n    ax2.set_title(\'Recall (\' + classifier_name + \')\')\n    \n    plt.grid()\n    plt.show()\n```\n\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n    l1 = LogisticRegression(penalty=\'l1\', tol=.01, \n                            solver="liblinear", multi_class="ovr",\n                            max_iter=500, C=c_param)\n    # Fit model\n    l1.fit(train_data, train_labels) \n    # Predict\n    predict = l1.predict(dev_data)\n    # Get precision, recall, f1 scores\n    scores = precision_recall_fscore_support(dev_labels, predict, average=\'weighted\')  \n    \n    # Get the features with non-zero coefficients.  We will use\n    # this list to reduce the features for the\n    # following logistic regression with L2 regularization\n    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n    names = np.array(list(train_data.columns))\n    non_zero_names = names[non_zero_sums] \n    \n    return {\'scores\': scores, \'non_zero_genes\': non_zero_names}```\n\nUnused functions:\nNone'}, {'reason': 'stop', 'result': 'def eliminateFeatures(train_data, train_labels, dev_data, dev_labels, \n                      logit_C_param, svm_C_param, label_encoder):\n\n    params = {\'C\':  [1000, 100, 10, 1, .5, .3, .1, .05]}\n\n    precision_l1        = []\n    recall_l1           = []\n    \n    accuracy            = []\n    precision           = []\n    recall              = []\n    precision_by_label  = []\n    recall_by_label     = []\n    \n    svm_accuracy            = []\n    svm_precision           = []\n    svm_recall              = []\n    svm_precision_by_label  = []\n    svm_recall_by_label     = []\n\n    feature_size        = []\n    confusion           = []\n\n    for c_param in reversed(params[\'C\']):\n        np.random.seed(0)\n\n        l1_info = runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param)    \n        non_zero_genes = l1_info[\'non_zero_genes\']\n        feature_size.append(str(len(non_zero_genes)) + \' (L1 C=\' + str(c_param) + ")")\n        precision_l1.append(l1_info[\'scores\'][0])\n        recall_l1.append(l1_info[\'scores\'][1])\n\n        min_train_data = train_data[non_zero_genes]\n        min_dev_data   = dev_data[non_zero_genes]\n\n        lr = LogisticRegression(penalty=\'l2\', tol=.01, max_iter=150, \n                                C=logit_C_param, solver="liblinear", multi_class="ovr")\n        lr.fit(min_train_data, train_labels) \n        predict = lr.predict(min_dev_data)\n\n        the_accuracy = accuracy_score(dev_labels, predict)\n        scores = precision_recall_fscore_support(dev_labels, predict, average=\'weighted\')\n        scores_by_label = precision_recall_fscore_support(dev_labels, predict, average=None)\n\n        confusion_mx = confusion_matrix(dev_labels, predict)\n\n        accuracy.append(the_accuracy)\n        precision.append(scores[0])\n        recall.append(scores[1])\n        precision_by_label.append(scores_by_label[0])\n        recall_by_label.append(scores_by_label[1])\n        confusion.append(confusion_mx)\n        \n        svm = LinearSVC(penalty=\'l2\', C=svm_C_param)\n        svm.fit(min_train_data, train_labels,) \n        predict = svm.predict(min_dev_data)\n\n        svm_accuracy_score = accuracy_score(dev_labels, predict)\n        svm_scores = precision_recall_fscore_support(dev_labels, predict, average=\'weighted\')\n        svm_scores_by_label = precision_recall_fscore_support(dev_labels, predict, average=None)\n\n        svm_accuracy.append(svm_accuracy_score)\n        svm_precision.append(svm_scores[0])\n        svm_recall.append(svm_scores[1])\n        svm_precision_by_label.append(svm_scores_by_label[0])\n        svm_recall_by_label.append(svm_scores_by_label[1])\n    \n    best_idx = np.argmax(precision)\n    print("\\nBest precision", feature_size[best_idx])\n    print("  precision:", np.round(precision[best_idx], 4))  \n    print("  recall:   ", np.round(recall[best_idx], 4))  \n\n    best_idx = np.argmax(recall)\n    print("\\nBest recall", feature_size[best_idx])\n    print("  precision:", np.round(precision[best_idx], 4))  \n    print("  recall:   ", np.round(recall[best_idx], 4))  \n\n    plotMetrics(precision_l1, recall_l1, \n                accuracy, precision, recall, \n                precision_by_label, recall_by_label,\n                svm_accuracy, svm_precision, svm_recall, \n                svm_precision_by_label, svm_recall_by_label,\n                confusion, feature_size, label_encoder)\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "```#\n#  Run classifier on each binary matrix, that has different\n#  number of columns (genes).  Iterate through different\n#  C values, using Logistic Regression L1 regularization \n#  to eliminate features.  Now run Logistic Regression, L2\n#  regularization and keep track of precision, recall,\n#  and confusion matrix.  Plot these metrics per feature\n#  size and show the confusion matrix for the best performing\n#  feature size.\n#\ndef runClassifier(name, logit_c_param=.1, svm_c_param=.01):\n    warnings.filterwarnings('ignore')\n    data_object    = all_data[name]\n    data          = data_object['data']\n    labels        = data_object['labels']\n    label_encoder = data_object['label_encoder']\n    splits = splitData(data, labels)\n    eliminateFeatures(splits['train_data'], splits['train_labels'],\n                      splits['dev_data'], splits['dev_labels'], \n                      logit_c_param, svm_c_param, label_encoder)```"}, {'reason': 'stop', 'result': 'def highlight_column_matches(data, column=\'\', color=\'yellow\'):\n    \'\'\'\n    highlight the maximum in a Series or DataFrame\n    \'\'\'\n    attr = \'background-color: {}\'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_mixed = data == data[column]\n        return [attr if v else \'\' for v in is_mixed]\n    else:  # from .apply(axis=None)\n        is_mixed = data == data[column]\n        return pd.DataFrame(np.where(is_mixed, attr, \'\'), index=data.index, columns=data.columns)\n\ndef plot_stats(csv_filename, columns=[\'total_reward\'], **kwargs):\n    """Plot specified columns from CSV file."""\n    df_stats = pd.read_csv(csv_filename)\n    df_stats[columns].plot(**kwargs)\n\ndef save_rnn_layers(hidden_layers, output_layers):\n    for i, layer in hidden_layers.items():\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_output\'), hidden_layers[i][\'output\'])\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_state\'), hidden_layers[i][\'state\'])\n    np.save(os.path.join(vsig.out_dir, \'valid_output_layer\'), output_layers)\n\ndef save_mlp_layers(hidden_layers, output_layers):\n    for i, layer in hidden_layers.items():\n        np.save(os.path.join(vsig.out_dir, \'valid_hidden_layer_\' + i + \'_output\'), layer)\n    np.save(os.path.join(vsig.out_dir, \'valid_output_layer\'), output_layers)\n\nclassifier_activation = {\'binary\': \'sigmoid\', \'categorical\': \'softmax\'}```\n\nUnused functions:\n- glance_at_tensor'}, {'reason': 'stop', 'result': 'def make_input_fn(df, num_epochs):\n  return tf.estimator.inputs.pandas_input_fn(\n    x = df,\n    y = df[LABEL],\n    batch_size = 128,\n    num_epochs = num_epochs,\n    shuffle = True,\n    queue_capacity = 1000,\n    num_threads = 1\n  )```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def make_feature_cols():\n  input_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n  return input_columns\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def sample_withplot (loglike_model, prior_transform_model, datafile,priorRange):\n    data_file = io.get_data_file_path(datafile)\n    data_x,data_xerr,data_y,data_yerr = io.load_data(data_file)\n    \n    #n: number of parameters, len(priorRange)\n    n=len(priorRange) \n\n    def new_loglike_model(theta):\n        return loglike_model(theta, (data_x,data_xerr,data_y,data_yerr))\n        \n    def new_prior_transform_model(theta):\n        return prior_transform_model(theta,priorRange)\n    \n    result = nestle.sample(new_loglike_model, new_prior_transform_model, n)\n    \n    \n    print (\'log evidence\')\n    print (result.logz)\n\n    print (\'numerical (sampling) error on logz\')\n    print (result.logzerr)   \n       \n    print (\'array of sample parameters\')\n    print (result.samples)  \n       \n    print (\'array of weights associated with each sample\')\n    print (result.weights)\n    \n\n    \n    import matplotlib.pyplot as plt\n    import corner\n\n\n    p_fit, cov_fit = nestle.mean_and_cov(result.samples, result.weights)\n\n\n    plt.figure()\n    plt.errorbar(data_x,data_y,yerr=data_yerr,fmt=\'*\')\n    plt.xlabel("r (kpc)")\n    plt.ylabel(\'V (km/s)\')\n    plt.title("Results of using the model to fit the DM rotational velocity distribution")\n    xplot = [5+5*i for i in range(40)]\n    yplot = [model.model_NFW(xplot[i], p_fit) for i in range(40)]\n    plt.plot(xplot,yplot)\n    plt.show()\n\n    fig = corner.corner(result.samples, weights=result.weights, labels=[\'a\', \'rho0\'],\n                            range=[0.99999, 0.99999], bins=30)\n    plt.show()\n\n    \n    return result \n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def stern_light():\n    """\n    Makes a numpy array of a lot of different arrays meant to behave as an image of a stern light. \n    The color of stern lights is white, so this is fairly straight forward with a bunch of zeros.\n    """\n    min_pixel_bound = 50\n    max_pixel_bound = 200\n    total_gens = np.random.randint(500, 701)\n    all_stern_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        light_width = np.random.randint(10, 51)\n        start_x = np.random.randint(min_pixel_bound, max_pixel_bound)\n        start_y = np.random.randint(min_pixel_bound, max_pixel_bound)\n        new_array = np.zeros((255, 255, 3))\n        aggregate_horiz = start_x + light_width\n        aggregate_vert = start_y + light_width\n        new_array[start_x:aggregate_horiz, start_y:aggregate_vert] = (255, 255, 255)\n        new_array = new_array.flatten()\n        all_stern_images[i] = new_array\n\n    return all_stern_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def broad_greater_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has two masthead lights and one running light\n    visible for a starboard orientation.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 126)\n        shorter_masthead_light = np.random.randint(130, 186)\n        left_endpoint = np.random.randint(20, 126)\n        right_endpoint = np.random.randint(125, 211)\n        running_light_height_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 16)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = left_endpoint + light_width\n        short_masthead_height = shorter_masthead_light + light_width\n        short_masthead_width = right_endpoint + light_width\n        running_light_start = shorter_masthead_light + running_light_height_diff\n        running_light_width = running_light_start + light_width\n        if right_endpoint - left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(left_endpoint - 20, left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(left_endpoint, right_endpoint)\n        running_light_area = running_light_loc + light_width\n        new_view[taller_masthead_light:tall_masthead_height, left_endpoint:tall_masthead_width] = white\n        new_view[shorter_masthead_light:short_masthead_height, right_endpoint:short_masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def broad_greater_than_50_meters_port():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has two masthead lights and one running light\n    visible for the port orientation.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 126)\n        shorter_masthead_light = np.random.randint(130, 186)\n        left_endpoint = np.random.randint(20, 126)\n        right_endpoint = np.random.randint(125, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 16)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = right_endpoint + light_width\n        short_masthead_height = shorter_masthead_light + light_width\n        short_masthead_width = left_endpoint + light_width\n        running_light_start = shorter_masthead_light + running_light_diff\n        running_light_width = running_light_start + light_width\n        if right_endpoint - left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(right_endpoint - 20, right_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(left_endpoint, right_endpoint)\n        running_light_area = running_light_loc + light_width\n        new_view[taller_masthead_light:tall_masthead_height, right_endpoint:tall_masthead_width] = white\n        new_view[shorter_masthead_light:short_masthead_height, left_endpoint:short_masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = red\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def bow_lights_gt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    \n    Ships greater than 50m in length.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 126)\n        tall_mh_height = taller_masthead_light + light_width\n        shorter_masthead_light = np.random.randint(tall_mh_height + 2, tall_mh_height + 16)\n        short_mast_head_height = shorter_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(15, 56)\n        running_light_dist_vert = np.random.randint(short_mast_head_height, short_mast_head_height + 51)\n        new_view[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        new_view[shorter_masthead_light:short_mast_head_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_view[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_view = new_view.flatten()\n        all_bow_images[i] = new_view\n    \n    return all_bow_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def broad_less_than_50_meters_starboard():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible starboard orientation.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_view = np.zeros((255, 255, 3))\n        masthead_light = np.random.randint(50, 201)\n        mh_horiz = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        masthead_height = masthead_light + light_width\n        masthead_width = mh_horiz + light_width\n        running_light_start = masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if mh_horiz < 2 * light_width:\n            running_light_loc = np.random.randint(mh_horiz - 20, mh_horiz + 21)\n        else:\n            running_light_loc = np.random.randint(mh_horiz - 20, 211)\n        running_light_area = running_light_loc + light_width\n        new_view[masthead_light:masthead_height, mh_horiz:masthead_width] = white\n        new_view[running_light_start:running_light_width, running_light_loc: running_light_area] = green\n        new_view = new_view.flatten()\n        all_broad_images[i] = new_view\n\n    return all_broad_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def broad_less_than_50_meters_port():\n    """\n    Return a numpy array of randomly generated images of a \n    power driven vessel that has one masthead light and one running light\n    visible.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    total_gens = np.random.randint(500, 701)\n    all_broad_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        taller_masthead_light = np.random.randint(50, 201)\n        distance_bw_left_endpoint = np.random.randint(20, 211)\n        running_light_diff = np.random.randint(10, 31)\n        light_width = np.random.randint(10, 21)\n        tall_masthead_height = taller_masthead_light + light_width\n        tall_masthead_width = distance_bw_left_endpoint + light_width\n        running_light_start = tall_masthead_height + running_light_diff\n        running_light_width = running_light_start + light_width\n        if distance_bw_left_endpoint < 2 * light_width:\n            running_light_loc = np.random.randint(distance_bw_left_endpoint - 20, distance_bw_left_endpoint + 21)\n        else:\n            running_light_loc = np.random.randint(25, distance_bw_left_endpoint + 20)\n        running_light_area = running_light_loc + light_width\n        new_array[taller_masthead_light:tall_masthead_height, distance_bw_left_endpoint:tall_masthead_width] = white\n        new_array[running_light_start:running_light_width, running_light_loc: running_light_area] = red\n        new_array = new_array.flatten()\n        all_broad_images[i] = new_array\n\n    return all_broad_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def bow_lights_lt_50m():\n    """\n    Generate light configuration as if you were looking at a ship\'s bow.\n    """\n    white = (255, 255, 255)\n    black = (0, 0, 0)\n    red = (255, 0, 0)\n    green = (0, 255, 0)\n    total_gens = np.random.randint(500, 701)\n    all_bow_images = np.empty([total_gens, 195075], dtype=np.uint8)\n    for i in range(total_gens):\n        new_array = np.zeros((255, 255, 3))\n        light_width = np.random.randint(10, 16)\n        center_horiz = np.random.randint(75, 176)\n        taller_masthead_light = np.random.randint(25, 201)\n        tall_mh_height = taller_masthead_light + light_width\n        center_for_runs = light_width // 2\n        running_light_dist_horiz = np.random.randint(56)\n        running_light_dist_vert = np.random.randint(tall_mh_height, tall_mh_height + 51)\n        new_array[taller_masthead_light:tall_mh_height, center_horiz: center_horiz + light_width] = white\n        left_running_light = center_horiz + center_for_runs - running_light_dist_horiz - light_width\n        new_array[running_light_dist_vert: running_light_dist_vert + light_width, left_running_light: left_running_light + light_width] = green\n        right_running_light = center_horiz + center_for_runs + running_light_dist_horiz\n        new_array[running_light_dist_vert: running_light_dist_vert + light_width, right_running_light: right_running_light + light_width] = red\n        new_array = new_array.flatten()\n        all_bow_images[i] = new_array\n    \n    return all_bow_images\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def make_target_label_array():\n    """\n    Makes an array of labels for the lights_data.\n    """\n    lights_data_lengths = [len(stern_images), len(bow_lights_lt_50_images), len(bow_images_gt_50m),\n                           len(port_broad_images_gt_50m), len(port_broad_lt_50m_images), len(stbd_broad_images_gt_50m),\n                           len(stbd_broad_lt_50m_images)]\n    labels = np.array([], dtype=np.uint8)\n    for i, length in enumerate(lights_data_lengths):\n        new_labels = np.full(length, i)\n        labels = np.append(labels, new_labels)\n    \n    return labels\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "'''Function to read the blank-spaced column data into a Pandas data frame (table)'''\n\ndef read_table(file_name):\n    '''\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    '''\n    import pandas as pd\n    \n    df = pd.read_csv( file_name,  \n                      skiprows=6,\n                      delim_whitespace=True)\n    \n    # to avoid frustrations, set explicitly the data types of each column\n    for c in df.columns: \n        if c == 'Y(NO-DIM)':\n            continue\n        df = df.astype({c:float},copy=False)\n\n    #print(df.dtypes)\n    return df\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "'''Function for creating a FP yield(A,Z) list container'''\n\ndef get_fpy_az( df ):\n    '''\n    Create a list of named tuple nuclides\n    Parameters\n    ----------\n    df: pandas data frame, required\n        Table of data for nuclides.\n    \n    Returns\n    -------\n    nuclides: list(namedtuple)\n        List of namedtuples. Names: name, element_name, Z, A, yield_percent.\n\n    Examples\n    --------\n    '''\n    \n    nuclides = list()\n\n    # design a container data structure\n    from collections import namedtuple\n    FPY = namedtuple('FPY', ['name','element_name','Z','A','yield_percent'])\n\n    import pandas as pd\n\n    # Use the Mendeleev python package (periodic table of elements)\n    from mendeleev import element\n    \n    total_yield = 0.0 # sum total yield\n    for row in df.itertuples(index=False):\n        z = int(row[0])\n        for j in range(1,len(row)-1):\n            if row[j] < 1.e-10: # this will eliminate many zeros\n                continue\n            a_str = df.columns[j] # index column is not part of the columns\n            symbol = element(z).symbol\n            name = name=symbol+'-'+a_str\n            element_name = element(z).name\n            yield_value = row[j]\n            total_yield += yield_value\n            nuc = FPY( name=name, element_name=element_name, Z=z, A=int(a_str), yield_percent=yield_value )\n        \n            nuclides.append(nuc)\n            \n    print('Sum of yield values in data file = ',round(total_yield,2))\n    return nuclides```"}, {'reason': 'stop', 'result': "'''Plot function for nuclides yield'''\n\ndef plot_nuclides(fpy):\n    \n    from matplotlib import pyplot as plt # import the pyplot function of the matplotlib package\n    (fig, ax) = plt.subplots(figsize=(18,7))\n\n    ax.plot(range(len(fpy)), [nc.yield_percent for nc in fpy], \n            '-.',color='black', marker='o',markersize=10)\n    \n    ax.set_xlabel('Nuclide',fontsize=18)\n    ax.set_ylabel(r'Yield [%]',fontsize=18)\n    \n    plt.xticks(range(0,len(fpy),2),[nc.name for nc in fpy][::2],rotation=70,fontsize=12)\n\n    ax.set_xlim((-1,len(fpy)))\n    \n    # create a twin y axis to reconfigure the top x axis\n    ay1 = ax.twiny()\n    ay1.set_xlim(ax.get_xlim())\n    #ay1.xaxis.tick_top()\n    ay1.set_xticks([])\n    ay1.set_xticks(range(1,len(fpy),2),[nc.name for nc in fpy][1::2])\n    ay1.set_xticklabels([nc.name for nc in fpy][1::2],minor=True,fontsize=12,rotation=70)\n\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    \n    plt.title('Cumulative (Decayed) Fission Products (>1% Yield)',fontsize=22)\n    ax.grid(True)\n    #plt.yscale('log')\n    plt.show()\n\n    return\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "'''Function to read the blank-spaced column data into a Pandas data frame (table)'''\n\ndef read_table(file_name):\n    '''\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    '''\n    import pandas as pd\n\n    # read the data into a data frame (or table)   \n    df = pd.read_csv(file_name, delim_whitespace=True)\n    \n    # to avoid frustrations, set explicitly the data types of each column\n    df = df.astype({'A':int,'Element':str,'Z':int,'N':int,'T1/2-(seconds)':float},copy=False)\n    print(df.dtypes)\n\n    return df\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "'''Function for creating a nuclide container'''\n\ndef get_nuclides( df ):\n    '''\n    Create a dictionary of nuclide name key and named tuple value\n    Parameters\n    ----------\n    df: pandas data frame, required\n        Table of data for nuclides.\n    \n    Returns\n    -------\n    nuclides: dict(name:namedtuple)\n        Dictionary of name key and namedtuple values. Tuple names: element_name, symbol, Z, A, half_life.\n\n    Examples\n    --------\n    '''\n    \n    nuclides = dict()\n\n    # design a container data structure\n    from collections import namedtuple\n    Nuclide = namedtuple('Nuclide', ['element_name','symbol','Z','A','half_life'])\n\n    # fill in the list of containers\n    misses = 0 # counter of nuclides without radius data\n    a_max = 0  # maximum A number with radius data present\n    z_max = 0  # maximum Z number with radius data present\n    t_max = 0\n\n    import pandas as pd\n    \n    # Use the Mendeleev python package (periodic table of elements)\n    from mendeleev import element\n    \n    # if df has duplicates, this loop will take care of it\n    for row in df.itertuples(index=False):\n\n        a = int(row[0])\n        symbol = row[1]\n        z = int(row[2])\n        t = row[4]\n        if pd.isnull(t): # missing half-life\n            misses += 1\n            continue\n        if t/3600/24/365 > 50e+6: # remove anything with more than 50 My\n            continue\n\n        a_max = max(a,a_max)\n        z_max = max(z,z_max)\n        \n        name = symbol+'-'+str(a)\n        t_max = max(t,t_max)\n        nuc = Nuclide( element_name=element(z).name, symbol=symbol, Z=z, A=a, half_life=t )\n        \n        if name in nuclides.keys():\n            half_life = nuclides[name].half_life\n            if half_life == t:\n                continue\n            else:\n                name = name+'m'\n        \n        nuclides[name]=nuc\n\n    print('Number of nuclides with    t_1/2 data = ',len(nuclides))\n    print('Number of nuclides without t_1/2 data = ',misses)\n    print('')\n    print('Max Z number with t_1/2 data = ',z_max)\n    print('Max A number with t_1/2 data = ',a_max)\n    print('t_1/2 [Ma] max = ',t_max/3600/24/365/1000000)\n    \n    return nuclides```"}, {'reason': 'stop', 'result': "'''Plot function for nuclides half-life'''\n\ndef plot_nuclides(nuclides):\n    \n    from matplotlib import pyplot as plt # import the pyplot function of the matplotlib package\n    (fig, ax) = plt.subplots(figsize=(18,7))\n\n    ax.plot([nc.Z for nc in nuclides.values()], [nc.half_life/3600/24/365 for nc in nuclides.values()], \n            ' ',color='black', marker='x',markersize=10)\n    \n    min_z = min([nc.Z for nc in nuclides.values()])\n    max_z = max([nc.Z for nc in nuclides.values()])\n\n    ax.xaxis.set_ticks(range(min_z, max_z+1,2))\n    ax.set_xlim((min_z-1,max_z+1))\n\n    plt.xlabel(r'Nuclide Z Number',fontsize=18)\n    plt.ylabel(r'$T_{1/2} [a]$',fontsize=18)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=16)\n    \n    ay1 = ax.twiny()\n    ay1.set_xlim(ax.get_xlim())\n    ay1.set_xticks([])\n    from mendeleev import element\n    ay1.set_xticks(range(min_z,max_z+1), [element(z).symbol for z in range(min_z,max_z+1)])\n    ay1.set_xticklabels([element(z).symbol for z in range(min_z,max_z+1)],minor=True,fontsize=12)\n\n    min_a = min([nc.A for nc in nuclides.values()])\n    max_a = max([nc.A for nc in nuclides.values()])\n    \n    plt.title(r'%i Nuclides: $%i \\leq A \\leq %i$ '%(len(nuclides),min_a,max_a),fontsize=22)\n    ax.grid(True)\n    plt.yscale('log')\n    plt.show()\n\n    return\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "'''Function to compute half-life times of given nuclides'''\n\ndef half_life_times(fp_list, nuclides):\n\n    ordered_fp_list = sorted( fp_list, key=lambda entry: entry.Z, reverse=False )\n    \n    half_life_1_10       = list()\n    half_life_10_100     = list()\n    half_life_100_1000   = list()\n    half_life_1000_1M    = list()\n    half_life_1M_10M     = list()\n    half_life_10M_100M   = list()\n    half_life_100M_infty = list()\n    \n    for fp in ordered_fp_list:\n        name = fp.name \n        if name in nuclides.keys():\n            nc    = nuclides[name]\n            element_name = nc.element_name\n            h_f_y = nc.half_life/3600/24/365\n        else:\n            element_name = fp.element_name\n            h_f_y = 100e+6 + 1/365       \n    \n        save = (name,element_name)\n        if h_f_y >= 1 and h_f_y < 10:\n            half_life_1_10.append(save)\n        elif h_f_y >= 10 and h_f_y < 100:\n            half_life_10_100.append(save)\n        elif h_f_y >= 100 and h_f_y < 1000:\n            half_life_100_1000.append(save)\n        elif h_f_y >= 1000 and h_f_y < 1e+6:\n            half_life_1000_1M.append(save)\n        elif h_f_y >= 1e+6 and h_f_y < 10e+6:\n            half_life_1M_10M.append(save)\n        elif h_f_y >= 10e+6 and h_f_y < 100e+6:\n            half_life_10M_100M.append(save)\n        else:\n            half_life_100M_infty.append(save)\n            \n            \n    print('|    Half-Life     |  FP Nuclides   |')\n    print('|:-----------------|:------------|')\n    nuclides = str()\n    for ele in half_life_1_10:\n        nuclides += ele[0]+', '\n    print('|1 to 10 y         |', nuclides[:-2],'|')\n    nuclides = str()\n    for ele in half_life_10_100:\n        nuclides += ele[0]+', '\n    print('|10 to 100 y       |', nuclides[:-2],'|')\n    nuclides = str()\n    for ele in half_life_100_1000:\n        nuclides += ele[0]+', '\n    print('|100 to 1 ky       |', nuclides[:-2],'|')\n    nuclides = str()\n    for ele in half_life_1000_1M:\n        nuclides += ele[0]+', '\n    print('|1 ky to 1 My      |', nuclides[:-2],'|')\n    nuclides = str()\n    for ele in half_life_1M_10M:\n        nuclides += ele[0]+', '\n    print('|1 My to 10 My     |', nuclides[:-2],'|')\n    nuclides = str()\n    for ele in half_life_10M_100M:\n        nuclides += ele[0]+', '\n    print('|10 My to 100 My    |', nuclides[:-2],'|')\n    nuclides = str()\n    for ele in half_life_100M_infty:\n        nuclides += ele[0]+', '\n    \n    return"}, {'reason': 'stop', 'result': "'''Function to read the blank-spaced column data into a Pandas data frame (table)'''\n\ndef read_table(file_name):\n    '''\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    '''\n    import pandas as pd\n    \n    df = pd.read_csv( file_name,  \n                      skiprows=6,\n                      delim_whitespace=True)\n    \n    # to avoid frustrations, set explicitly the data types of each column\n    for c in df.columns: \n        if c == 'Y(NO-DIM)':\n            continue\n        df = df.astype({c:float},copy=False)\n\n    #print(df.dtypes)\n    return df\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def neuron(x, w):\n    '''Simple McCulloch-Pitts Neuron model, x and w and are numpy arrays of the same shape'''\n    return np.dot(x, w) > 0\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def update_nueron(w, x, label):\n    '''\n    Update McCulloch + Pitts Nueron using Rosenblatt's Perceptron Algorithms\n    w = neuron weights\n    x = (n dimensional numpy array example)\n    label = binary label\n    '''\n    if neuron(x, w) == 0 and label == 1:\n        w = w + x\n        print('incorrect!, New weights = ' + str(w))\n\n    elif neuron(x, w) == 1 and label == 0:\n        w = w - x\n        print('incorrect!, New weights = ' + str(w))\n\n    else:\n        print('correct!')  \n    \n    return w\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def plot_decision_boundary(X, y, w, i):\n    x1s = np.arange(7)\n    x2s = (-w[2]-w[0]*x1s)/w[1]\n\n    fig = figure(0, (8,6))\n    #Scatter data\n    scatter(X[:,0], X[:,1], c = y, s = 75)\n    scatter(X[i,0], X[i,1], c = 'r', s = 75)\n    #Plot decision boundary\n    plot(x1s, x2s)\n\n    xlabel('$x_1$', fontsize = 20)\n    ylabel('$x_2$', fontsize = 20)\n    grid(1)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'from IPython.display import clear_output\n\ndef save_annotations(adict):\n    timestamp = datetime.datetime.today().strftime(\'%Y%m%d%H%M\')\n    filename = \'annotations_\' + timestamp + \'.csv\'\n    print(filename)\n    with open(os.path.join(\'.\', filename), \'w\', newline=\'\') as out:\n        writer = csv.writer(out)\n        for key, value in adict.items():\n            line = [key, *value]\n            writer.writerow(line)\n\ndef create_anottations(lista, save=True):\n    """Use dumb walk heuristic to create anottations\n    Args: \n    \n        lista: list of images\n        save: if true, save on current directory a csv <annottations_timestamp.csv>\n    \n    Returns: \n        \n        a dict with name of image: (xleft, ytop, xright, ytop) coordinates\n    """\n    cont = 0\n    result = {}\n    for img in lista:\n        try:\n            result[img] = find_conteiner(img)\n        except ValueError:\n            pass\n        cont += 1\n        if cont % 100 == 0:\n            clear_output()\n            print(\'...\', cont, \'...\')\n    if save:\n        save_annotations(result)\n    return result\n```\n\nUnused functions:\n- draw_anottation'}, {'reason': 'stop', 'result': '# 8 Pipeline \ndef estimate_anottation_correct(img, coords, line_width=2, threshold=0.8):\n    """Make histograms of boundaries to estimate annotation error\n    (if boundaries are blank, high chance of dumb walk failure)"""\n    total = 0\n    cont = 0\n    xleft, ytop, xright, ybottom = [int(c) for c in coords]\n    leftside = img[ytop:ybottom, xleft:xleft + line_width - 1]\n    rightside = img[ytop:ybottom, xright - line_width:xright]\n    total = leftside.sum() + rightside.sum()\n    cont = leftside.size + rightside.size\n    percent_black =  total / cont \n    return int(percent_black)\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def http_program(url):\n    supported_schemes = ["http", "https"]\n    result = urlparse(url)\n    if result.scheme not in supported_schemes:\n        raise ValueError("Scheme must be one of " + repr(supported_schemes))\n    if result.netloc == \'\':\n        raise ValueError("Host must be non-empty")\n\n    # Do something with the URL\n    return True\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def delete_random_character(s):\n    """Returns s with a random character deleted"""\n    if s == "":\n        return s\n\n    pos = random.randint(0, len(s) - 1)\n    return s[:pos] + s[pos + 1:]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def insert_random_character(s):\n    """Returns s with a random character inserted"""\n    pos = random.randint(0, len(s))\n    random_character = chr(random.randrange(32, 127))\n    return s[:pos] + random_character + s[pos:]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def flip_random_character(s):\n    """Returns s with a random bit flipped in a random position"""\n    if s == "":\n        return s\n\n    pos = random.randint(0, len(s) - 1)\n    c = s[pos]\n    bit = 1 << random.randint(0, 6)\n    new_c = chr(ord(c) ^ bit)\n    return s[:pos] + new_c + s[pos + 1:]\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def mutate(s):\n    """Return s with a random mutation applied"""\n    mutators = [\n        delete_random_character,\n        insert_random_character,\n        flip_random_character\n    ]\n    mutator = random.choice(mutators)\n    return mutator(s)```\n\nUnused functions:\n- delete_random_character\n- insert_random_character\n- flip_random_character'}, {'reason': 'stop', 'result': 'def is_valid_url(url):\n    try:\n        result = http_program(url)\n        return True\n    except ValueError:\n        return False\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class MutationFuzzer(MutationFuzzer):\n    def create_candidate(self):\n        candidate = random.choice(self.population)\n        trials = random.randint(self.min_mutations, self.max_mutations)\n        for i in range(trials):\n            candidate = self.mutate(candidate)\n        return candidate\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class MutationFuzzer(MutationFuzzer):\n    def fuzz(self):\n        if self.seed_index < len(self.seed):\n            # Still seeding\n            self.inp = self.seed[self.seed_index]\n            self.seed_index += 1\n        else:\n            # Mutating\n            self.inp = self.create_candidate()\n        return self.inp\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'class FunctionRunner(Runner):\n    def __init__(self, function):\n        """Initialize.  `function` is a function to be executed"""\n        self.function = function\n\n    def run_function(self, inp):\n        return self.function(inp)\n\n    def run(self, inp):\n        try:\n            result = self.run_function(inp)\n            outcome = self.PASS\n        except Exception:\n            result = None\n            outcome = self.FAIL\n\n        return result, outcome\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def build_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n        layers.Dense(64, activation=tf.nn.relu),\n        layers.Dense(1)\n    ])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n    model.compile(\n        loss='mean_squared_error',\n        optimizer=optimizer,\n        metrics=['mean_absolute_error', 'mean_squared_error']\n    )\n    return model\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '# Display training progress by printing a single dot for each completed epoch\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: \n            print(\'\')\n        print(\'.\', end=\'\')\n\nEPOCHS = 8000\n\nhistories = {}\n\nfor l in labels:\n    print(f"Training {l} model....")\n    histories[l] = models[l].fit(\n        normed_train_data, train_labels[l],\n        epochs=EPOCHS, validation_split = 0.2, verbose=0,\n        callbacks=[early_stop, PrintDot()]\n    )\n    print("\\n")```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def plot_history(history, label=None):\n    hist = pd.DataFrame(history.history)\n    hist[\'epoch\'] = history.epoch\n\n    plt.figure()\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Mean Abs Error\')\n    plt.plot(hist[\'epoch\'], hist[\'mean_absolute_error\'],\n           label=\'Train Error\')\n    plt.plot(hist[\'epoch\'], hist[\'val_mean_absolute_error\'],\n           label = \'Val Error\')\n    plt.legend()\n    if label is not None:\n        plt.title(label)\n\n    plt.figure()\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Mean Square Error\')\n    plt.plot(hist[\'epoch\'], hist[\'mean_squared_error\'],\n           label=\'Train Error\')\n    plt.plot(hist[\'epoch\'], hist[\'val_mean_squared_error\'],\n           label = \'Val Error\')\n    plt.legend()\n    if label is not None:\n        plt.title(label)\n    plt.savefig(f"{label}_train.pdf")\n    plt.show()```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '```python\ndef show_results(label):\n    loss, mae, mse = models[label].evaluate(normed_test_data, test_labels[label], verbose=0)\n\n    print("Testing set Mean Abs Error: {:5.2f} um".format(mae))\n    print("Testing set RMS: {:5.2f} um".format(np.sqrt(mse)))\n\n    test_predictions = models[label].predict(normed_test_data).flatten()\n\n    plt.scatter(test_labels[label], test_labels[label] - test_predictions)\n    plt.xlabel(\'True Values [um]\')\n    plt.ylabel(\'Residuals [um]\')\n    minx, maxx = min(test_labels[label]), max(test_labels[label])\n    plt.plot([minx, maxx], [0, 0])\n    plt.show()\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def func1(s):\n    """\n    Print a string \'s\' and tell how many characters it has    \n    """\n    \n    print(s + " has " + str(len(s)) + " characters")```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def square(x):\n    """\n    Return the square of x.\n    """\n    return x ** 2\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def powers(x):\n    """\n    Return a few powers of x.\n    """\n    return x ** 2, x ** 3, x ** 4\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def myfunc(x, p=2, debug=False):\n    if debug:\n        print("evaluating myfunc for x = " + str(x) + " using exponent p = " + str(p))\n    return x**p```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'f2 = lambda x: x**2'}, {'reason': 'stop', 'result': 'class Point:\n    """\n    Simple class for representing a point in a Cartesian coordinate system.\n    """\n    \n    def __init__(self, x, y):\n        """\n        Create a new Point at x, y.\n        """\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        """\n        Translate the point by dx and dy in the x and y direction.\n        """\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return("Point at [%f, %f]" % (self.x, self.y))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '```python\ndef plot_2d(value, ax=None, group=None, mask=None, matrix_id=0, mat_as_group=False, \n            group_id=None, linewidths=0, is_string=False, fmt="d", square=False):\n  \n  if hasattr(value, "numpy"):\n    value = value.numpy()\n  if group is not None and hasattr(group, "numpy"):\n    group = group.numpy()\n  if mask is not None and hasattr(mask, "numpy"):\n    mask = tf.squeeze(mask)\n    mask = tf.ones_like(value) * mask\n    mask = mask.numpy()\n    \n\n  cmaps = [\'PuOr\', \'tab20b\', \'RdBu\']\n  group_id = int(group[0][0])\n  cmap = cmaps[group_id]\n  \n  if is_string:\n    fmt = \'\'\n  \n  sns.heatmap(group, \n              fmt=fmt,\n              cmap=cmap,\n              annot=value, \n              cbar=False, \n              xticklabels=False, \n              yticklabels=False, \n              square=square,\n              mask=mask,\n              linewidths=linewidths,\n              ax=ax)\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '#ignore\ndef plot(value, group=None, group_dim=0, mask=None, labels=[], square=False, shape_desc=\'\', width_prec=None, bottom_prec=None, \n         is_string=False, name=\'\', fmt="d", single_plot_size=4, h_dist_ratio=0.7, w_dist_ratio=0.7, linewidths=None):\n\n  shape = value.shape\n  num_groups = shape[0]\n  \n  if hasattr(value, \'numpy\'):\n    if value.dtype in [tf.int32, tf.int64]:\n      value = tf.cast(value, tf.int32)\n    elif value.dtype in [tf.float32, tf.float64] and fmt == "d":\n      fmt = ".2g"\n    \n    value = value.numpy()\n\n  if hasattr(value, \'ndim\'):\n    ndim = value.ndim\n  else:\n    ndim = len(value)\n    \n    \n  if ndim == 2:\n    value = value[np.newaxis, np.newaxis, :, :]\n  if ndim == 3:\n    value = value[np.newaxis, :, :, :]\n  if ndim == 4:\n    pass\n  \n  # decide how to group sub-tensors smartly\n  if not group_dim:\n    group_dim = ndim - 1\n    \n  # generate group identifier tensor for differentiating between\n  # different bactch / sentence\n  if group is None:\n    group_ids = tf.range(num_groups, dtype=tf.int64).numpy()\n    if group_dim == 1:\n      group = group_ids[:, tf.newaxis]\n    elif group_dim == 2:\n      group = group_ids[:, tf.newaxis, tf.newaxis]\n    elif group_dim == 3:\n      group = group_ids[:, tf.newaxis, tf.newaxis, tf.newaxis]\n\n    # broadcast to all groups    \n    group = tf.ones(shape=value.shape) * group\n\n  d0, d1, d2, d3 = value.shape\n\n  # set figure size based on tensor dimensions\n  fig_width = (d3 * 1.0 / 4) * d0 * single_plot_size\n  fig_height = (d2 * 1.0 / 4) * single_plot_size\n  figsize = (fig_width, fig_height)\n  \n  if width_prec is None:\n    width_prec = 1.0 / d0\n  \n  if bottom_prec is None:\n    bottom_prec = 1.0 / d1\n\n  fig = plt.figure(figsize=figsize)\n  fig_title = f\'name: {name}, shape: {shape}\' if name else f\'shape: {shape}\'\n  \n  if shape_desc:\n    fig_title = fig_title + \' = \' + shape_desc\n    \n  for e0 in range(d0):\n\n    # plot 2d array in reverse order since the earlier plot will be covered\n    for e1 in reversed(range(d1)):\n      annot = value[e0, e1]\n\n      # select corresponding matplotlib axis      \n      cur_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3) * w_dist_ratio, \n                             e1 / d2 * h_dist_ratio, \n                             width_prec, \n                             bottom_prec]) \n\n      matrix_id = e0 + e1 * 2\n      \n      if mask is not None:\n        if ndim == 2:\n          mask_idx = e0\n        elif ndim == 3:\n          mask_idx = e1\n        elif ndim ==4:\n          mask_idx = e0\n          \n        # mimic broadcasting\n        if mask.shape[0] == 1:\n          mask_idx = 0\n          \n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, \n                is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n      else:\n        plot_2d(annot, group=group[e0, e1], ax=cur_ax, matrix_id=matrix_id, is_string=is_string, fmt=fmt, square=square)\n      \n      # minic shadowing for each 2d matrix\n      width_delta_prec = 0.0005\n      height_delta_prec = width_delta_prec * d2 / d3\n      \n      for k in range(1, 3):\n        shadow_ax = fig.add_axes([(0.7) * e0 + (e1 / d0 / d3)  * w_dist_ratio - width_delta_prec * k, \n                                  e1 / d2 * h_dist_ratio - height_delta_prec * k, \n                                  width_prec, \n                                  bottom_prec])  \n        \n        if k == 2:\n          linewidths = 1\n        else:\n          linewidths = 0\n          \n        if mask is not None:\n          if ndim == 2:\n            mask_idx = e0\n          elif ndim == 3:\n            mask_idx = e1\n          elif ndim ==4:\n            mask_idx = e0\n            \n          # mimic broadcasting\n          if mask.shape[0] == 1:\n            mask_idx = 0  \n            \n            \n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, mask=mask[mask_idx], square=square)\n        else:\n          plot_2d(annot, group=group[e0, e1], ax=shadow_ax, matrix_id=matrix_id, \n                  linewidths=linewidths, is_string=is_string, fmt=fmt, square=square)\n\n      if e0 == 0 and e1 == 0:\n        ax1 = cur_ax\n        \n        if labels:\n            plt.ylabel(labels[-2])\n            plt.xlabel(labels[-1] + \'\\n\' + fig_title)\n        else:\n          plt.xlabel(fig_title)\n\n        # 4D 中的 axis1 說明 label\n#           if len(labels) >= 3:\n#             plt.text(d3 + 2, 1 + 0.5, labels[-3],\n#                      rotation=0, rotation_mode=\'anchor\')\n\n      if e1 == d0 - 1:\n        ax2 = cur_ax\n        \n        \n#       transFigure = fig.transFigure.inverted()\n#       coord1 = transFigure.transform(ax1.transData.transform([d3 + 2 + 0.5, 0]))\n#       coord2 = transFigure.transform(ax2.transData.transform([d3 + 0.5, d2]))\n\n\n#       line = mpl.lines.Line2D((coord1[0],coord2[0]),(coord1[1],coord2[1]), \n#                               transform=fig.transFigure, \n#                               linestyle=\'--\',\n#                               color=\'black\')\n#       fig.lines.append(line)\n```'}, {'reason': 'stop', 'result': 'def scaled_dot_product_attention(q, k, v, mask):\n  """Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n  \n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n    \n  Returns:\n    output, attention_weights\n  """\n  # 將 `q`、 `k` 做點積再 scale\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n\n  # 將遮罩「加」到被丟入 softmax 前的 logits\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n  \n  # 以注意權重對 v 做加權平均（weighted average）\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '#ignore\ndef scaled_dot_product_attention_demo(q, k, v, mask, draw=False):\n  """Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n  \n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n    \n  Returns:\n    output, attention_weights\n  """\n  labels= [\'subwords in q\', \'subwords in k\']\n  shape_desc = \'\\n(batch_size, seq_len_q, seq_len_k)\'\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  if draw:\n    plot(matmul_qk, name=\'matmul_qk\', labels=labels, shape_desc=shape_desc)\n  \n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n  if draw:\n    plot(scaled_attention_logits, name="scaled_attention_logits", \n         labels=labels, shape_desc=shape_desc)\n\n  # add the mask to the scaled tensor.\n  mask_name = \'\'\n  if mask is not None:\n    mask_name = \'masked_\'\n    orig_mask = mask\n    # demo usage for q, k, v less than ndim=4\n    if q.ndim == 4:\n      pass\n    elif q.ndim == 3 and mask.ndim == 4:\n      orig_mask = mask\n      mask = tf.squeeze(mask, axis=[1])\n    \n    if draw:\n      plot(mask, name="mask", \n         labels=[\'sentence\', \'subwords in seq_q\'], shape_desc=\'\\n(batch_size, 1, seq_len_q)\')\n      \n      temp_mask = mask * -1e9\n      plot(temp_mask, name="mask", \n         labels=[\'sentence\', \'subwords in seq_q\'], shape_desc=\'\\n(batch_size, 1, seq_len_q)\')\n    \n    scaled_attention_logits += (mask * -1e9)\n    if draw:\n      plot(scaled_attention_logits, name="scaled_attention_logits", \n           labels=labels, shape_desc=shape_desc)\n    \n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n  if draw:\n    plot(attention_weights, name=\'attention_weights\', labels=labels, \n         shape_desc=shape_desc)\n  \n  if draw:\n    plot(v, name=\'v\', labels=[\'subwords in q\', \'depth_v\'], \n       shape_desc=\'\\n(batch_size, seq_len_v, depth_v)\')\n  \n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n  if draw:\n    plot(output, name="output", labels=[\'subwords in q\', \'depth_v\'], \n         shape_desc=\'\\n(batch_size, seq_len_q, depth_v)\')\n\n  return output, attention_weights\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# 實作一個執行多頭注意力機制的 keras layer\n# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n# output.shape            == (batch_size, seq_len_q, d_model)\n# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  # 在初始的時候建立一些必要參數\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n    self.d_model = d_model # 在 split_heads 之前的基底維度\n    \n    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n    \n    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n    \n    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n  \n  # 這跟我們前面看過的函式有 87% 相似\n  def split_heads(self, x, batch_size):\n    """Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    """\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n  \n  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n    \n    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    \n    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n    # 輸出會多一個 head 維度\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    \n    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n    # (batch_size, seq_len_q, num_heads, depth)\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model)) \n    # (batch_size, seq_len_q, d_model)\n\n    # 通過最後一個線性轉換\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n    return output, attention_weights\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '#ignore\nclass MultiHeadAttentionDemo(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttentionDemo, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n    \n    assert d_model % self.num_heads == 0\n    \n    self.depth = d_model // self.num_heads\n    \n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)\n  \n  def split_heads(self, x, batch_size, draw=False, only_draw_reshape=False):\n    """Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    """\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    if draw:\n      plot(x, name=\'tf.reshape in split_heads\', shape_desc=\'\\n(batch_size, seq_len, num_heads, depth)\',\n           single_plot_size=8, h_dist_ratio=0.3, w_dist_ratio=0.3, width_prec=1.0 / 2 / 2, bottom_prec=1.0 / 8 * 2)\n      \n    x = tf.transpose(x, perm=[0, 2, 1, 3])\n    if draw and not only_draw_reshape:\n      plot(x, name=\'tf.transpose in split_heads\', shape_desc=\'\\n(batch-size, num_heads, seq_len, depth)\')\n    return x\n  \n  def call(self, v, k, q, mask, draw=False, only_draw_reshape=False):\n    batch_size = tf.shape(q)[0]\n    shape_desc = \'\\n(batch_size, seq_len, d_model)\'\n    \n    if draw and not only_draw_reshape:\n      plot(q, name="q", shape_desc=shape_desc)\n      plot(k, name="k", shape_desc=shape_desc)\n      plot(v, name="v", shape_desc=shape_desc)\n    \n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    if draw and not only_draw_reshape:\n      plot(q, name="q", shape_desc=shape_desc)\n      plot(k, name="k", shape_desc=shape_desc)\n      plot(v, name="v", shape_desc=shape_desc)\n    \n    q = self.split_heads(q, batch_size, draw)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size, draw)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size, draw)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    if only_draw_reshape:\n      scaled_attention, attention_weights = scaled_dot_product_attention(\n          q, k, v, mask)\n    else:\n      scaled_attention, attention_weights = scaled_dot_product_attention_demo(\n          q, k, v, mask)\n      \n    if draw and not only_draw_reshape:\n      plot(scaled_attention, name=\'scaled_attention\', shape_desc=\'\\n(batch_size, num_heads, seq_len, depth)\')\n      plot(attention_weights, name=\'attention_weights\', shape_desc=\'\\n(batch_size, num_heads, seq_len_q, seq_len_k)\')\n    \n    \n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n    if draw:\n      plot(scaled_attention, name=\'scaled_attention after transpose\', \n           shape_desc=\'\\n(batch_size, seq_len_q, num_heads, depth)\',\n           single_plot_size=8, h_dist_ratio=0.3, w_dist_ratio=0.3, width_prec=1.0 / 2 / 2, bottom_prec=1.0 / 8 * 2)\n    \n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n    if draw and not only_draw_reshape:\n      plot(concat_attention, name=\'concat_attention\', shape_desc=\'\\n(batch_size, seq_len_q, d_model)\')\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n    if draw and not only_draw_reshape:\n      plot(output, name=\'output\', shape_desc=\'\\n(batch-size, seq_len_q, d_model)\')\n        \n    return output, attention_weights\n```\n\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "```python\n# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\ndef point_wise_feed_forward_network(d_model, dff):\n  \n  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '```python\nclass EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, training, mask):\n    attn_output, attn = self.mha(x, x, x, mask)  \n    attn_output = self.dropout1(attn_output, training=training) \n    out1 = self.layernorm1(x + attn_output)  \n    \n    ffn_output = self.ffn(out1) \n    ffn_output = self.dropout2(ffn_output, training=training)  \n    out2 = self.layernorm2(out1 + ffn_output)\n    \n    return out2\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '# Decoder 裡頭會有 N 個 DecoderLayer，\n# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\nclass DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    # 3 個 sub-layers 的主角們\n    self.mha1 = MultiHeadAttention(d_model, num_heads)\n    self.mha2 = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n \n    # 定義每個 sub-layer 用的 LayerNorm\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    # 定義每個 sub-layer 用的 Dropout\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n  def call(self, x, enc_output, training, \n           combined_mask, inp_padding_mask):\n    # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n    # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n    # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n    # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n\n    # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n    # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n    # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n    attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n    attn1 = self.dropout1(attn1, training=training)\n    out1 = self.layernorm1(attn1 + x)\n    \n    # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n    # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n    attn2, attn_weights_block2 = self.mha2(\n        enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n    attn2 = self.dropout2(attn2, training=training)\n    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n    \n    # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n\n    ffn_output = self.dropout3(ffn_output, training=training)\n    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n    \n    # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n    return out3, attn_weights_block1, attn_weights_block2\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               rate=0.1):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    \n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n    \n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n\n    self.dropout = tf.keras.layers.Dropout(rate)\n        \n  def call(self, x, training, mask):\n    input_seq_len = tf.shape(x)[1]\n    \n    x = self.embedding(x)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :input_seq_len, :]\n\n    x = self.dropout(x, training=training)\n    \n    for i, enc_layer in enumerate(self.enc_layers):\n      x = enc_layer(x, training, mask)\n      \n    return x\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "class Decoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n               rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    \n    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n    \n    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(rate)\n  \n  def call(self, x, enc_output, training, \n           combined_mask, inp_padding_mask):\n    \n    tar_seq_len = tf.shape(x)[1]\n    attention_weights = {}\n    \n    x = self.embedding(x)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :tar_seq_len, :]\n    x = self.dropout(x, training=training)\n\n    \n    for i, dec_layer in enumerate(self.dec_layers):\n      x, block1, block2 = dec_layer(x, enc_output, training,\n                                    combined_mask, inp_padding_mask)\n      \n      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n    \n    return x, attention_weights\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '```python\n# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\nclass Transformer(tf.keras.Model):\n  # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, rate=0.1):\n    super(Transformer, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                           input_vocab_size, rate)\n\n    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                           target_vocab_size, rate)\n    # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n  \n  def call(self, inp, tar, training, enc_padding_mask, \n           combined_mask, dec_padding_mask):\n\n    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n    \n    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n    dec_output, attention_weights = self.decoder(\n        tar, enc_output, training, combined_mask, dec_padding_mask)\n    \n    # 將 Decoder 輸出通過最後一個 linear layer\n    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n    \n    return final_output, attention_weights\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def loss_function(real, pred):\n  # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  # 照樣計算所有位置的 cross entropy 但不加總\n  loss_ = loss_object(real, pred)\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask  # 只計算非 <pad> 位置的損失 \n  \n  return tf.reduce_mean(loss_)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '# 為 Transformer 的 Encoder / Decoder 準備遮罩\ndef create_masks(inp, tar):\n  # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n  enc_padding_mask = create_padding_mask(inp)\n  \n  # Decoder layer 的 MHA1 在做自注意力機制用的\n  # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n  return enc_padding_mask, combined_mask```\n\n\nUnused functions:\n- `create_padding_mask`\n- `create_look_ahead_mask`\n- `create_padding_mask`'}, {'reason': 'stop', 'result': '@tf.function\ndef train_step(inp, tar):\n  tar_inp = tar[:, :-1]\n  tar_real = tar[:, 1:]\n  \n  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  \n  with tf.GradientTape() as tape:\n    predictions, _ = transformer(inp, tar_inp, \n                                 True, \n                                 enc_padding_mask, \n                                 combined_mask, \n                                 dec_padding_mask)\n    loss = loss_function(tar_real, predictions)\n\n  gradients = tape.gradient(loss, transformer.trainable_variables)    \n  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n  \n  train_loss(loss)\n  train_accuracy(tar_real, predictions)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\ndef evaluate(inp_sentence):\n  \n  # 準備英文句子前後會加上的 <start>, <end>\n  start_token = [subword_encoder_en.vocab_size]\n  end_token = [subword_encoder_en.vocab_size + 1]\n  \n  # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n  # 並在前後加上 BOS / EOS\n  inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n  encoder_input = tf.expand_dims(inp_sentence, 0)\n  \n  # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n  # 是一個只包含一個中文 <start> token 的序列\n  decoder_input = [subword_encoder_zh.vocab_size]\n  output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n  \n  # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n  for i in range(MAX_LENGTH):\n    # 每多一個生成的字就得產生新的遮罩\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        encoder_input, output)\n  \n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(encoder_input, \n                                                 output,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n\n    # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n    predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n    \n    # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n    if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n      return tf.squeeze(output, axis=0), attention_weights\n    \n    #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n    # 下個中文字的時候關注到最新的 `predicted_id`\n    output = tf.concat([output, predicted_id], axis=-1)\n\n  # 將 batch 的維度去掉後回傳預測的中文索引序列\n  return tf.squeeze(output, axis=0), attention_weights\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'import matplotlib as mpl\n# 你可能會需要自行下載一個中文字體檔案以讓 matplotlib 正確顯示中文\nzhfont = mpl.font_manager.FontProperties(fname=\'/usr/share/fonts/SimHei/simhei.ttf\')\nplt.style.use("seaborn-whitegrid")\n\ndef plot_attention_weights(attention_weights, sentence, predicted_seq, layer_name, max_len_tar=None):\n    \n  fig = plt.figure(figsize=(17, 7))\n  \n  sentence = subword_encoder_en.encode(sentence)\n  \n  # 只顯示中文序列前 `max_len_tar` 個字以避免畫面太過壅擠\n  if max_len_tar:\n    predicted_seq = predicted_seq[:max_len_tar]\n  else:\n    max_len_tar = len(predicted_seq)\n  \n  # 將某一個特定 Decoder layer 裡頭的 MHA 1 或 MHA2 的注意權重拿出來並去掉 batch 維度\n  attention_weights = tf.squeeze(attention_weights[layer_name], axis=0)  \n  # (num_heads, tar_seq_len, inp_seq_len)\n  \n  # 將每個 head 的注意權重畫出\n  for head in range(attention_weights.shape[0]):\n    ax = fig.add_subplot(2, 4, head + 1)\n\n    # [注意]我為了將長度不短的英文子詞顯示在 y 軸，將注意權重做了 transpose\n    attn_map = np.transpose(attention_weights[head][:max_len_tar, :])\n    ax.matshow(attn_map, cmap=\'viridis\')  # (inp_seq_len, tar_seq_len)\n    \n    fontdict = {"fontproperties": zhfont}\n    \n    ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n    ax.set_xlim(-0.5, max_len_tar -1.5)\n    \n    ax.set_yticks(range(len(sentence) + 2))\n    ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n                        if i < subword_encoder_zh.vocab_size], \n                       fontdict=fontdict, fontsize=18)    \n    \n    ax.set_yticklabels(\n        [\'<start>\'] + [subword_encoder_en.decode([i]) for i in sentence] + [\'<end>\'], \n        fontdict=fontdict)\n    \n    ax.set_xlabel(\'Head {}\'.format(head + 1))\n    ax.tick_params(axis="x", labelsize=12)\n    ax.tick_params(axis="y", labelsize=12)\n  \n  plt.tight_layout()\n  plt.show()\n  plt.close(fig)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class Dataset(object):\n    PAD = 0\n    SOS = 1\n    EOS = 2\n    UNK = 3\n    constants = [\'PAD\', \'SOS\', \'EOS\', \'UNK\']\n    hu_alphabet = list("aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz-+._")\n\n    def __init__(self, fn, config, src_alphabet=None, tgt_alphabet=None):\n        self.config = config\n        self.create_tables(src_alphabet, tgt_alphabet)\n        self.load_and_preproc_dataset(fn)\n\n    def create_tables(self, src_alphabet, tgt_alphabet):\n        if src_alphabet is None:\n            self.src_vocab = Dataset.constants + Dataset.hu_alphabet\n        else:\n            self.src_vocab = Dataset.constants + alphabet\n        self.src_table = lookup_ops.index_table_from_tensor(\n            tf.constant(self.src_vocab), default_value=Dataset.UNK\n        )\n        if self.config.share_vocab:\n            self.tgt_vocab = self.src_vocab\n            self.tgt_table = self.src_table\n        else:\n            if tgt_alphabet is None:\n                self.tgt_vocab = Dataset.constants + Dataset.hu_alphabet\n            else:\n                self.tgt_vocab = Dataset.constants + alphabet\n            self.tgt_table = lookup_ops.index_table_from_tensor(\n                tf.constant(self.tgt_vocab), default_value=Dataset.UNK\n            )\n        self.src_vocab_size = len(self.src_vocab)\n        self.tgt_vocab_size = len(self.tgt_vocab)\n\n    def load_and_preproc_dataset(self, fn):\n        dataset = tf.contrib.data.TextLineDataset(fn)\n        dataset = dataset.repeat()\n        dataset = dataset.map(lambda s: tf.string_split([s], delimiter=\'\\t\').values)\n\n        src = dataset.map(lambda s: s[0])\n        tgt = dataset.map(lambda s: s[1])\n\n        src = src.map(lambda s: tf.string_split([s], delimiter=\' \').values)\n        src = src.map(lambda s: s[:self.config.src_maxlen])\n        tgt = tgt.map(lambda s: tf.string_split([s], delimiter=\' \').values)\n        tgt = tgt.map(lambda s: s[:self.config.tgt_maxlen])\n\n        src = src.map(lambda words: self.src_table.lookup(words))\n        tgt = tgt.map(lambda words: self.tgt_table.lookup(words))\n\n        dataset = tf.contrib.data.Dataset.zip((src, tgt))\n        dataset = dataset.map(\n            lambda src, tgt: (\n                src,\n                tf.concat(([Dataset.SOS], tgt), 0),\n                tf.concat((tgt, [Dataset.EOS]), 0),\n            )\n        )\n        dataset = dataset.map(\n            lambda src, tgt_in, tgt_out: (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in))\n        )\n        batched = dataset.padded_batch(\n            self.config.batch_size,\n            padded_shapes=(\n                tf.TensorShape([self.config.src_maxlen]),\n                tf.TensorShape([self.config.tgt_maxlen+2]),\n                tf.TensorShape([None]),\n                tf.TensorShape([]),\n                tf.TensorShape([]),\n            )\n        )\n        self.batched_iter = batched.make_initializable_iterator()\n        s = self.batched_iter.get_next()\n        self.src_ids = s[0]\n        self.tgt_in_ids = s[1]\n        self.tgt_out_ids = s[2]\n        self.src_size = s[3]\n        self.tgt_size = s[4)\n\n    def run_initializers(self, session):\n        session.run(tf.tables_initializer())\n        session.run(self.batched_iter.initializer)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class Config(object):\n    default_fn = os.path.join(\n        PROJECT_DIR, "config", "seq2seq", "default.yaml"\n    )\n    \n    @staticmethod\n    def load_defaults(fn=default_fn):\n        with open(fn) as f:\n            return yaml.load(f)\n    \n    def __init__(self, **kwargs):\n        defaults = Config.load_defaults()\n        for param, val in defaults.items():\n            setattr(self, param, val)\n        for param, val in kwargs.items():\n            setattr(self, param, val)\n        \nconfig = Config(src_maxlen=30, tgt_maxlen=33)\ndataset = Dataset(input_fn, config)```\n\nUnused functions:\n- from_yaml'}, {'reason': 'stop', 'result': 'import random\n\ndef compute_fitness(solution):\n    solution_fitness = 0.0\n    \n    for index in range(len(solution)):\n        waypoint1 = solution[index - 1]\n        waypoint2 = solution[index]\n        solution_fitness += waypoint_distances[frozenset([waypoint1, waypoint2])]\n        \n    return solution_fitness\n\ndef generate_random_agent():\n    new_random_agent = list(all_waypoints)\n    random.shuffle(new_random_agent)\n    return tuple(new_random_agent)\n\ndef mutate_agent(agent_genome, max_mutations=3):\n    agent_genome = list(agent_genome)\n    num_mutations = random.randint(1, max_mutations)\n    \n    for mutation in range(num_mutations):\n        swap_index1 = random.randint(0, len(agent_genome) - 1)\n        swap_index2 = swap_index1\n\n        while swap_index1 == swap_index2:\n            swap_index2 = random.randint(0, len(agent_genome) - 1)\n\n        agent_genome[swap_index1], agent_genome[swap_index2] = agent_genome[swap_index2], agent_genome[swap_index1]\n            \n    return tuple(agent_genome)\n\ndef shuffle_mutation(agent_genome):\n    agent_genome = list(agent_genome)\n    \n    start_index = random.randint(0, len(agent_genome) - 1)\n    length = random.randint(2, 20)\n    \n    genome_subset = agent_genome[start_index:start_index + length]\n    agent_genome = agent_genome[:start_index] + agent_genome[start_index + length:]\n    \n    insert_index = random.randint(0, len(agent_genome) + len(genome_subset) - 1)\n    agent_genome = agent_genome[:insert_index] + genome_subset + agent_genome[insert_index:]\n    \n    return tuple(agent_genome)\n\ndef generate_random_population(pop_size):\n    random_population = []\n    for agent in range(pop_size):\n        random_population.append(generate_random_agent())\n    return random_population\n    \ndef run_genetic_algorithm(generations=5000, population_size=100):\n    population_subset_size = int(population_size / 10.)\n    generations_10pct = int(generations / 10.)\n    \n    population = generate_random_population(population_size)\n\n    for generation in range(generations):\n        population_fitness = {}\n\n        for agent_genome in population:\n            if agent_genome in population_fitness:\n                continue\n\n            population_fitness[agent_genome] = compute_fitness(agent_genome)\n\n        new_population = []\n        for rank, agent_genome in enumerate(sorted(population_fitness,\n                                                   key=population_fitness.get)[:population_subset_size]):\n            \n            if (generation % generations_10pct == 0 or generation == generations - 1) and rank == 0:\n                print("Generation %d best: %d | Unique genomes: %d" % (generation,\n                                                                       population_fitness[agent_genome],\n                                                                       len(population_fitness)))\n                print(agent_genome)\n                print("")\n\n            new_population.append(agent_genome)\n\n            for offspring in range(2):\n                new_population.append(mutate_agent(agent_genome, 3))\n                \n            for offspring in range(7):\n                new_population.append(shuffle_mutation(agent_genome))\n\n        for i in range(len(population))[::-1]:\n            del population[i]\n\n        population = new_population\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from IPython.display import display\nfrom IPython.display import HTML\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# funciones necesarias\n\n# esta es la funcion de cuota mensual total\ndef get_cuota_mensual_total(p, i, n):\n    num = p * i * (i+1)**n\n    den = (i+1)**n - 1\n    return num/den\n\n# esta es la funcion de deuda capital en el mes t\ndef get_deuda_capital(p, i, n, t):\n    coef = (((1+i)**t) - 1) / (((1+i)**n) - 1)\n    return p * ( 1 - coef )\n\n# esta funcion muestra la evolucion mes a mes del credito\ndef mostrar_evolucion_credito(P, I, N):\n    \n    p = P\n    i = I / 12.0 * 0.01\n    n = N * 12\n    \n    # calculo cuota mensual total\n    cuota_total = get_cuota_mensual_total(p, i, n)\n    print("Tu cuota mensual es de AR$ " + str(cuota_total) + "\\n")\n\n    cuota_capital = []\n    cuota_interes = []\n    deuda_capital = []\n    t_array = []\n    print("Este es tu listado de cuotas por mes: \\n")\n    print("MES \\t CAPITAL \\t INTERESES \\t CAPITAL ADEUDADO")\n\n    for t in range(1, n+1):    \n\n        t_array.append(t)\n        cuota_capital.append(get_deuda_capital(p, i, n, t-1) - get_deuda_capital(p, i, n, t))\n        cuota_interes.append(cuota_total - cuota_capital[t-1])\n        deuda_capital.append(get_deuda_capital(p, i, n, t))\n\n        print(str(t) + "         " + \\\n              str(\'%.2f\'%(cuota_capital[t-1])) + "         " + \\\n              str(\'%.2f\'%(cuota_interes[t-1])) + "         " + \\\n              str(\'%.2f\'%(deuda_capital[t-1]))\n             )\n\ndef get_cuota_mensual_total_2(xs):\n    print("hi")\n    p=1000000\n    i=0.14\n    out = []\n    for n in xs:\n        #print(n)\n        num = p * i * (i+1)**n\n        den = (i+1)**n - 1\n        out.append(num/den)\n    return out\n```\n\nUnused functions:\n- None'}, {'reason': 'length', 'result': 'import collections\nimport itertools\nimport numpy as np\nfrom operator import itemgetter\nfrom scipy.spatial import distance\nimport re\n\nDEBUG = False\n\n"""Vector stuff"""\n\ndef v_same_orientation(v1, v2):\n    return np.dot(v1, v2) > 0\n    \n"""Division by zero problem!"""\ndef v_angle(v1, v2):\n    length_v1 = np.linalg.norm(v1)\n    length_v2 = np.linalg.norm(v2)\n    \n    if length_v1 == 0 or length_v2 == 0:\n        return 0\n    \n    return np.round(np.degrees(np.arccos(np.dot(v1, v2) /  length_v1 * length_v2)))\n\ndef v_perpendicular(v1, v2, tolerance_deg = 0):\n    return in_range(v_angle(v1, v2), 90, 5)\n\ndef v_parallel(v1, v2, tolerance_deg = 0):       \n    return in_range(v_angle(v1, v2), 0, 5) or in_range(v_angle(v1, v2), 180, 5)\n\ndef in_range(value, target, tolerance):\n    return target - tolerance <= value <= target + tolerance\n\ndef v_rotate(matrix, angle):\n    """rotates the given matrix by angle in degrees, counter clockwise."""\n    angle = np.radians(angle)\n    rot_matrix = np.array( [ [ np.cos(angle), -np.sin(angle)], [ np.sin(angle), np.cos(angle)] ] )\n    return np.dot(rot_matrix, matrix)\n    \n\n"""Helpers"""\n\ndef log(message):\n    """Prints a message only if DEBUG = True, so that all printing to stdout can be easily disabled."""\n    if DEBUG:\n        print(message)\n\ndef are_same(reference, value, percentage):\n    min_value = reference - reference * percentage\n    max_value = reference + reference * percentage\n    result = min_value < value < max_value\n    \n    return result\n\n"""Heavy stuff"""\n\ndef string_to_coords(coord_string):\n    """\n    Checks and decodes a coordinates string (that is passed to the API on the command line) into coordinates.\n    Returns an empty list if it is not well formed.\n    """\n    if not isinstance(coord_string, str):\n        return []\n    \n    coord_string = re.sub(r\'\\s+\', \'\', coord_string, flags=re.UNICODE)\n    is_well_formed = re.match(r\'\\[(\\(\\d+,\\d+\\),){0,}(\\(\\d+,\\d+\\))\\]\', coord_string)\n    \n    return eval(coord_string) if is_well_formed else []\n\n\ndef approximates(ref_point, point, max_deviation):\n    """Helper function to check if two points are the same within the specified deviation."""\n    \n    x = ref_point[0] - max_deviation <= point[0] <= ref_point[0] + max_deviation\n    y = ref_point[1] - max_deviation <= point[1] <= ref_point[1] + max_deviation\n    \n    return x and y\n\n\ndef get_orientation_marks(points):\n    """\n    Extract the reference system (o, vx, vy) from a set of points. \n    \n    Returns None if no reference system found.\n    """\n    p_threshold = 0.10\n\n    # no touchcode if there are not enough points\n    if points is None or len(points) < 3:\n        return None\n    \n    # calculate all possible distances between all points\n    vectors = [(p1, p2, distance.euclidean(p1, p2)) for p1, p2 in list(itertools.combinations(points, 2))]\n\n    # get the two points that have the longest distance (those are vx and vy)\n    v1, v2, longest_distance = max(vectors, key=itemgetter(2))\n    \n    log("v1: {0}, v2: {1}, dst(v1, v2): {2}]".format(v1, v2, longest_distance))\n    \n    origin = None\n    candidates = []\n    \n    # find the origin candidates by getting all distances that are longest_distance / sqrt(2)    \n    for vector in vectors:\n        if are_same(longest_distance / np.sqrt(2), vector[2], p_threshold):\n            if np.array_equal(vector[0], v1) or np.array_equal(vector[0], v2):\n                candidates.append((vector[1][0], vector[1][1]))\n            if np.array_equal(vector[1], v1) or np.array_equal(vector[1], v2):\n                candidates.append((vector[0][0], vector[0][1]))\n    \n    # find the origin (the point that we got twice)\n    try:\n        origin = np.array([k for k, v in collections.Counter(candidates).items() if v == 2])[0]\n    except:\n        return None\n    \n    return find_vx_vy_new(np.array([origin,v1,v2]))\n\ndef find_vx_vy_new(m):\n    """\n    Given three points (origin, v1, v2), finds out which of v1, v2 is vx and vy.\n    \n    Input: A 2x3 matrix (origin, v1, v2)\n    Output: A 2x3 matrix (origin, vx, vy)\n    """\n    \n    # The standard coordinate system\n    positive_x = np.array([1,0])\n    positive_y = np.array([0,1])\n    real_origin = np.array([0,0])\n    \n    # The origin of our touchcode system\n    origin = m[0]\n    \n    # Translate the touchcode coordinate system to have its origin at the standard origin (0,0)\n    translation_vec = real_origin - origin\n    mt = m + translation_vec\n    \n    v1, v2 = mt[1], mt[2]\n    log("v1 is {0}".format(v1))\n    \n    # Pick v1 as a pivot and check if it is in first or fourth quadrant.\n    # If so, rotate by angle(v1, positive_y) to align v2 with the x-axis.\n    # Next, check whether v2 has the same orientation as the positive x-axis, v1 then being vx. \n    # In the other case, v1 is the vx.\n    if v_same_orientation(v1, positive_x):\n        log("v1 is oriented with positive_x")\n        angle = v_angle(v1, positive_y)\n        log("angle: {0}".format(angle))\n        v1 = v_rotate(v1, angle)\n        v2 = v_rotate(v2, angle)\n    else:\n        log("v1 is NOT oriented with positive_x")\n        angle = 360 - v_angle(v1, positive_y)\n        v1 = v_rotate(v1, angle)\n        v2 = v_rotate(v2, angle)\n    \n    log(v_same_orientation(v2, positive_x))\n    log("after rot: v1 = {0} and v2 = {1}".format(v1, v2))\n    if v_same_orientation(v2, positive_x):\n        return np.array([m[0],m[2],m[1]])\n    else:\n        return m\n\ndef norm(reference, point):\n    """Given a reference system (o, vx, vy), normalize a set of points to new coordinates."""\n    o = reference[0]\n    x = reference[1]\n    y = reference[2]\n    s = point\n    \n    # Richtungsvektoren entlang der Kanten vom Referenzsystem\n    vx = x - o\n    vy = y - o\n    # Ortsvektor des Punkts bzgl o (wo ist s bgzl des neuen Ursprungs o)\n    so = s - o\n    \n    # Normierung der Richtungsvektoren    \n    vx = (vx/(np.linalg.norm(vx)))/(np.linalg.norm(vx))*3\n    vy = (vy/(np.linalg.norm(vy)))/(np.linalg.norm(vy))*3\n   \n    xcor = np.dot(vx, so)\n    ycor = np.dot(vy, so)\n    \n    log("s.x: {0}, s.y: {1}".format(xcor, ycor))\n    \n    return (round(xcor, 1), round(ycor, 1))\n\n\ndef touchcode_from_points(points):   \n    """Generate touchcode for a set of normalized touchpoints."""\n    \n    touchcode = 0\n    touchpoint_map = {\n        (1,3):'}, {'reason': 'length', 'result': ''}, {'reason': 'stop', 'result': 'import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import ConvexHull\nfrom scipy.spatial import distance\n\ndef sample_to_ndarray(sample):\n    points = []\n    \n    for point in sample:\n        points.append(point[0])\n        points.append(point[1])\n    \n    return np.array(points)\n\n\ndef render_sample(index):\n    df = pd.DataFrame(samples[index], columns=[\'x\', \'y\'])\n    print(df)\n    fig = df.plot.scatter(x=\'x\', y=\'y\', color=\'red\', figsize=(15,15))\n\n    fig.set_xlim([0, 2000])\n    fig.set_ylim([0, 2000])\n\n    plt.gca().set_aspect(\'equal\', adjustable=\'box\')\n    plt.ylabel(\'some numbers\')\n\n    plt.show()\n\n\ndef convex_hull(sample):\n    # convert points of sample to ndarray\n    points = np.asarray(sample)\n    \n    # find the convex hull\n    hull = ConvexHull(points)    \n   \n    plt.figure(num=None, figsize=(18, 16), dpi=320, facecolor=\'w\', edgecolor=\'r\')\n    \n    plt.xlim([0,2000]) \n    plt.ylim([0,2000]) \n\n    # plot the original points\n    plt.plot(points[:, 0], points[:, 1], \'o\')\n\n    # plot the convex hull around it\n    for simplex in hull.simplices:\n        plt.plot(points[simplex, 0], points[simplex, 1], \'r\')\n\n    # adjustment to coordinate system\n    plt.gca().set_aspect(\'equal\', adjustable=\'box\')\n    \n    # display that crap\n    plt.show()\n\n\ndef get_distances_for_sample(sample):\n    oy = sample[0]\n    ox = sample[1]\n    tc = sample[2] # part of the code\n    origin = sample[3] # origin\n\n    return[distance.euclidean(origin, ox), distance.euclidean(origin, oy),distance.euclidean(ox, oy),distance.euclidean(tc, origin) ]\n    \n\ndef get_statistics_for_sampleset(sampleset):\n    dst_origin_ox = []\n    dst_origin_oy = []\n    dst_ox_oy = []\n    dst_origin_tc = []\n\n    for i in range(0,9): \n        sample = samples[0b10000000][i]\n        distances = get_distances_for_sample(sample)\n        \n        dst_origin_ox.append(distances[0])\n        dst_origin_oy.append(distances[1])\n        dst_ox_oy.append(distances[2])\n        dst_origin_tc.append(distances[3])\n\n    print("dst(origin,x): mean({0}), max({1}, min({2}))".format(np.mean(dst_origin_ox), np.max(dst_origin_ox), np.min(dst_origin_ox)))\n    print("dst(origin,y): mean({0}), max({1}, min({2}))".format(np.mean(dst_origin_oy), np.max(dst_origin_oy), np.min(dst_origin_oy)))\n    print("dst(ox,oy): mean({0}), max({1}, min({2}))".format(np.mean(dst_ox_oy), np.max(dst_ox_oy), np.min(dst_ox_oy)))\n    print("dst(origin,tc): mean({0}), max({1}, min({2}))".format(np.mean(dst_origin_tc), np.max(dst_origin_tc), np.min(dst_origin_tc)))\n\n\nsample = samples[0x10][3]\nom = get_orientation_marks(sample)\nprint(norm(om, sample[2]))\n\nconvex_hull(sample)\n#get_statistics_for_sampleset(samples[0x80])\n```\n\nUnused functions:\n- sample_to_ndarray\n- render_sample\n- get_statistics_for_sampleset'}, {'reason': 'stop', 'result': 'def max_corr(corr_list):\n    max = 0\n    for item in corr_list.iteritems():\n        if item[1][1] > max:\n            max = item[1][1]\n            max_factor = item[1]\n    return max_factor\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def factor_with_high_corr(corr_list):\n    return_list = []\n    for i in range(len(corr_list)):\n        return_list.append(corr_list[i+1][0])\n    return return_list\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class getMovieByGendre(object):\n    def __init__(self):\n        self.site = \'http://www.imdb.com/search/title?\' \n        \n    def gendre(self,type_movie):\n\n        #type_movie could be  Action,Adventure,Animation,Biography,Comedy,Crime,Documentary,\n        #Drama,Family,Fantasy,Film-Noir,History,Horror,Music,Musical,Mystery,Romance,Sci-Fi,\n        #Short,Sport,Thriller,War,Western\n        \n        reference_list = []\n        for i in range(0,2):\n            link = self.site + \'genres=\' + type_movie + \'&page=\'+ str(i)\n            html = requests.get(link)\n            soup = BeautifulSoup(html.text,"html.parser")   \n            for reference in soup.find_all("div", {"class": "lister-item-image float-left"}):\n                reference_list.append(reference.find(\'img\',alt = True).get(\'data-tconst\'))\n        return reference_list \n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'class getReviewPerMovie(object):\n    def __init__(self,movieNumber):\n        self.site = \'http://www.imdb.com/title/\'+str(movieNumber)+\'/reviews\'\n        self.movie_no = movieNumber\n        \n    def getLength(self,type_re):\n        first_page = self.site +\'?filter=\'+ type_re + \';filter=\'+ type_re +\';start=\'+\'0\'\n        html_l = requests.get(first_page)\n        soup_l = BeautifulSoup(html_l.text,"html.parser")   \n        tbl = soup_l.findAll(\'table\')[1]\n        row_n = tbl.find_all(\'tr\')\n        str_len = row_n[0].find_all(\'td\')[0].text\n        regex = r"(Page).(\\d).(of).(\\d*)(.+)"\n        len_review = re.sub(regex, \'\\\\4\',str_len)\n        return(len_review)\n    \n    def category(self,ca_re):\n        reviews = []\n        rev=[]\n        try:        \n            print(self.movie_no)\n            page_length = int(self.getLength(str(ca_re)))\n         \n            for i in range(0,page_length):\n\n                link= self.site +\'?filter=\'+ ca_re + \';filter=\'+ ca_re +\';start=\'+ str(i*10)\n                print(link)\n                html_link = requests.get(link)\n                soup = BeautifulSoup(html_link.text,"html.parser")   \n                review_soup = soup.find("div", {"id": "tn15content"})\n\n                for div_re in review_soup.find_all(\'div\'):\n                    user_info_dict = {}\n                    for user_info in div_re.find_all(\'img\',alt=True):\n                        user_info_dict[\'rating\']=user_info.get(\'alt\')\n                        user_info_dict[\'author\']=re.sub(r"(/user/)(ur\\d+)(/)",\'\\\\2\',user_info.a.get(\'href\'))\n                        user_info_dict[\'title\']=div_re.h2.text\n                        user_info_dict[\'movie\']=self.movie_no\n                        if ca_re == \'love\':\n                            user_info_dict[\'categorie\']=\'postive\'\n                        else:\n                            user_info_dict[\'categorie\']=\'negative\'\n                        if user_info.small is not None:\n                            \n                            reg = r"(<small>)(\\d+)( out of )(\\d+)(.+)"\n                            user_info_dict[\'usefulness\'] = re.sub(reg,\'\\\\2/\\\\4\',str(div_re.small))\n                            reviews.append(user_info_dict)\n                            #print(user_info_dict)\n\n                for re_text in review_soup.find_all(\'p\'):\n                    review_text = {}\n                    if re_text.getText() not in [\'*** This review may contain spoilers ***\',\n                                                 \'Add another review\']:\n                        review_text[\'review\'] =re_text.get_text().replace(\'\\n\',\' \')\n                        rev.append(review_text)\n\n            for i in range(0,len(reviews)):\n                if len(reviews) == len(rev):\n                    reviews[i][\'review\'] = rev[i][\'review\']\n                else:\n                    print(\'the length of the user info and review is not the same\',\'...\\n\',\n                         \'user info has length: \',str(len(reviews)),\'...\\n\',\n                          \'review has length: \',str(len(rev)),\'...\\n\')\n\n            return reviews\n\n\n                \n        except ValueError:\n            pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "class ReviewsJason(object):\n    def __init__(self,type_movie,hate_love):\n        self.type_movie = type_movie\n        self.hate_love = hate_love\n    \n    def export(self):\n        reviews = [getReviewPerMovie(i).category(self.hate_love) for i in getMovieByGendre().gendre(self.type_movie)]\n        json_name = self.type_movie + '_'+self.hate_love + '_reviews_100'+ '.json'\n        \n        with open(json_name, 'w') as fp:\n            json.dump(reviews, fp)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '#METHOD PART\n\ndef get_name(IATA_code):\n  """\n    Function to return the name of the airport linked to IATA_code\n    \n    @IATA_code : String object which is a IATA_code\n   \n    @return    : String object which is the name of the airport\n  """\n  \n  #If IATE CODE exists in GEO_O\n  try:\n    result = GEO_O.get(IATA_code.replace(" ",""), \'name\')\n  #Else we just specify that we cannot found the IATA CODE\n  except KeyError as e:\n    result = "NOT FOUND IATA CODE"\n   \n  return result\n  \n\ndef get_airports_arrival_sorted(dataframe):\n  """\n  Method to print the get arrivals airports in 2013 from searches file\n  \n  @dataframe : the dataframe containing the data\n  \n  @return    : a new dataframe\n  """\n  \n  #Created dataframe grouped by \'arr_port\' aggregated by sum\n  result_dataframe = dataframe.groupby([\'arr_port\']).sum()\n  #Sorted the result in a descending way\n  result_dataframe = result_dataframe.sort_values(by=[\'pax\'], ascending=False)\n  \n  return result_dataframe\n\n\ndef add_airports_name(dataframe):\n  """\n  Method to add a column in a dataframe containing the full name of airports\n  thanks to the IATA CODE\n  \n  @dataframe : the dataframe to modify\n  \n  @return    : the dataframe modified\n  """\n  \n  #Reset the index of the dataframe in order to apply a lambda method\n  dataframe = dataframe.reset_index()\n  \n  #Add the column and its values\n  dataframe[\'airport_name\'] = dataframe[\'arr_port\'].apply(lambda x: get_name(x))\n\n  return dataframe\n\ndef print_top_n_arrival_airport(dataframe,n):\n  """\n  Method to print the top n of arrival airports in 2013\n  \n  @dataframe : the preformatted dataframe by columns containing the data\n  @n         : the number of airports to show\n  """\n  \n  df = get_airports_arrival_sorted(dataframe)\n  df = add_airports_name(df)\n  \n  print(df.head(n))```\n\nUnused functions:\n- get_df_cols'}, {'reason': 'stop', 'result': 'import ipywidgets as widgets\nimport json\nimport time\nfrom threading import Thread\nfrom traitlets import Int, Unicode\n\n\nclass newcyjsWidget(widgets.DOMWidget):\n    \n    _view_name = Unicode(\'CyjsView\').tag(sync=True)\n    _view_module = Unicode(\'cyjs\').tag(sync=True)\n    frameWidth = Int(400).tag(sync=True)\n    frameHeight = Int(300).tag(sync=True)\n    msgFromKernel = Unicode("{}").tag(sync=True)\n    msgToKernel = Unicode("{}").tag(sync=True)\n    status = "initial status message\\n"\n    selectedNodes = [];\n    incomingMessageArrivedAndParsed = False;\n    globalStatus = "blank"\n\n    class MyThread(Thread):\n       owner = None\n       def __init__(self, owner, group=None, target=None, name=None,\n                    args=(), kwargs=None, *, daemon=None):\n          Thread.__init__(self, group, target, name, args, kwargs, daemon=daemon)\n          self.owner = owner\n\n       def run(self):\n          maxLoops = 5\n          counter = 0\n          while((self.owner.incomingMessageArrivedAndParsed == False) and (counter < maxLoops)):\n             counter += 1\n             print("thread, no message yet, sleeping, loop %d" % counter)\n             time.sleep(1.0)\n          self.owner.status += "thread owner\'s selectedNodes: %s\\n" % self.owner.selectedNodes;\n          self.owner.status += "MyThread ending loop\\n";\n\n       def result(self):\n          return("imaap? %s" % self.owner.incomingMessageArrivedAndParsed)\n            \n    def testThread(self):\n      for i in range(4):\n         threadName = "Thread-%s" % (i + 1)\n         mythread = self.MyThread(name=threadName, owner=self)\n         mythread.start()\n    \n    def setSize(self, width, height):\n      self.status += "setSize(%d, %d)\\n" % (width, height)\n      self.frameWidth = width\n      self.frameHeight = height\n        \n    def fit(self, margin=50):\n      self.status += "entering fit (%d)\\n" % margin\n      self.msgFromKernel = json.dumps({"cmd": "fit", "status": "request",\n                                       "callback": "", "payload": margin});\n    def getSelectedNodes(self):\n      self.incomingMessageArrivedAndParsed = False;\n      self.status += "entering getSelectedNodes\\n";\n      self.msgFromKernel = json.dumps({"cmd": "cleanSlate", "status": "request", "callback": "", "payload":""});\n      self.msgFromKernel = json.dumps({"cmd": "getSelectedNodes", "status": "request",\n                                       "callback": "", "payload": ""});\n      \n      observingThread = self.MyThread(name="getSelectedNodes-thread", owner=self)\n      print("getSelectedNodes about to start observingThread")\n      observingThread.start()\n      self.status += "getSelectedNodes, observingThread now started: %s\\n" %  self.selectedNodes\n      self.status += "getSelectedNodes, incomingMessageArrivedAndParsed? %s\\n" % self.incomingMessageArrivedAndParsed\n      return(observingThread.result())\n        \n    def selectNodes(self, nodes):\n      self.msgFromKernel = json.dumps({"cmd": "selectNodes", "status": "request",\n                                       "callback": "", "payload": nodes});\n       \n    def clearSelection(self):\n      self. msgFromKernel = json.dumps({"cmd": "clearSelection", "status": "request",\n                                        "callback": "", "payload": ""});\n        \n    def msg_arrived(self, change):\n        self.status += "---- python - msg arrived\\n"\n        tmp = change[\'new\']\n        self.status += "len of tmp: %d\\n" % len(tmp)\n        self.status += "type of tmp: %s\\n" % type(tmp)\n        self.msgToKernel = tmp\n        self.status += "%s\\n" % tmp\n        self.incomingMessageArrived = True\n        self.dispatch(self.msgToKernel)\n \n    def dispatch(self, msgRaw):\n        self.msg = json.loads(msgRaw)\n        self.status += "entering dispatch\\n"\n        self.status += "dispatch this msg: %s\\n" % self.msg\n        self.status += "msg.cmd: %s\\n" % self.msg["cmd"]\n        if self.msg["cmd"] == \'storeSelectedNodes\':\n            self.status += "storing selected nodes to self.selectedNodes %s\\n" % msg["payload"]\n            self.selectedNodes = msg["payload"]\n        elif self.msg["cmd"] == \'clearCircles\':\n            self.circles = []\n        else:\n          print("unknown cmd: %s" % self.msg["cmd"])\n        self.incomingMessageArrivedAndParsed = True\n        \n    def getResponse(self):\n        return(self.msg["payload"])\n```'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef portfolio_annualised_performance(weights, mean_returns, cov_matrix):\n    """This function calculates annualised portfolio returns and volatility (risk).\n    \n    Parameters\n    ----------\n      `weights`: randomly generated weights for each stock in a portfolio\n      `mean_returns`: mean of stock daily_returns\n      `cov_matrix`: coeffecient matrix of all the daily_returns\n    \n    Returns\n    -------\n      Both annualised `standard_deviation` and `returns`.\n    """\n    annual_returns = np.sum(mean_returns * weights) * 252\n    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    return std, annual_returns\n\n\ndef random_portfolio(mean_returns, cov_matrix, num_portfolios=10000, risk_free_rate=0.00):\n    """This function generates portfolios with random weights for each stock.\n    \n    Parameters\n    ----------\n      `mean_returns`: mean of stock daily_returns\n      `cov_matrix`: coeffecient matrix of all daily_returns\n      `num_portfolios`: default is `10000` but can be tuned\n      `risk_free_rate`: default is `0.00` can vary from 0 through 1\n    \n    Returns\n    -------\n      `results` and `weights_record`.\n    """\n\n    results = np.zeros((3, num_portfolios))\n    weights_record = []\n    for i in range(num_portfolios):\n        weights = np.random.rand(4)\n        weights /= np.sum(weights)\n        weights_record.append(weights)\n\n        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(\n            weights=weights, mean_returns=mean_returns, cov_matrix=cov_matrix)\n\n        results[0, i] = portfolio_std_dev\n        results[1, i] = portfolio_return\n        results[2, i] = (portfolio_return - risk_free_rate) / portfolio_std_dev\n    return results, weights_record\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def padRightDownCorner(img, stride, padValue):\n    h = img.shape[0]\n    w = img.shape[1]\n\n    pad = 4 * [None]\n    pad[0] = 0 # up\n    pad[1] = 0 # left\n    pad[2] = 0 if (h%stride==0) else stride - (h % stride) # down\n    pad[3] = 0 if (w%stride==0) else stride - (w % stride) # right\n\n    img_padded = img\n    pad_up = np.tile(img_padded[0:1,:,:]*0 + padValue, (pad[0], 1, 1))\n    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n    pad_left = np.tile(img_padded[:,0:1,:]*0 + padValue, (1, pad[1], 1))\n    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n    pad_down = np.tile(img_padded[-2:-1,:,:]*0 + padValue, (pad[2], 1, 1))\n    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n    pad_right = np.tile(img_padded[:,-2:-1,:]*0 + padValue, (1, pad[3], 1))\n    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n\n    return img_padded, pad\n\nclass DataBatch(object):\n    def __init__(self, data, label, pad=0):\n        self.data = [data]\n        self.label = [label]\n        self.pad = pad\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "from scipy.optimize import fsolve\n\ntsym, dsym, ssym, test_sym = symbols('tsym dsym ssym test_sym')\n\nmodel_test_diag = [\n    tsym - ( ssym + (1 - A - U)*test_sym ),\n    dsym - ( A*ssym*p_true_pos + U*ssym*p_false_pos + (1 - A - U)*test_sym*p_true_pos )\n    ]\n\nsol_test_diag = solve(model_test_diag, tsym, dsym)\ntest_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[tsym])\ndiag_fun = lambdify((A, U, ssym, test_sym), sol_test_diag[dsym])\n\ndef test_diag_fun(parms):\n    # parms = (incidence, screening rate)\n    inc = parms[0]\n    scr = parms[1]\n    \n    A = A_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1 - p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n    U = U_fun(inc*p_asymp, sc + scr*p_true_pos, inc*(1 - p_asymp), scr*p_true_pos + att_symp*p_true_pos)\n    return [test_fun(A, U, scr, att_symp), diag_fun(A, U, scr, att_symp)]\n\n\n# set up a function to simulate system dynamics when perturbed from steady state\nfrom scipy.integrate import odeint\n\ndef dydt(y, t, parms):\n    return([\n    parms[1]*y[1] + parms[3]*y[2] - (parms[0] + parms[2])*y[0],\n    parms[0]*y[0] - parms[1]*y[1],\n    parms[2]*y[0] - parms[3]*y[2]\n    ])```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def removal_evolution(removal_list, G):\n    disting_bet_average_degree = []\n    hist = nx.degree_histogram(G)\n    last_average_degree = float(sum(hist[i] * i for i in range(len(hist)))) / float(sum(hist))\n    idx_node = 0\n    while last_average_degree > 0 and idx_node < len(removal_list):\n        disting_bet_average_degree.append(last_average_degree)\n        hist = nx.degree_histogram(G)\n        last_average_degree = float(sum(hist[i] * i for i in range(len(hist)))) / float(sum(hist))\n        G.remove_node(removal_list[idx_node][0])\n        idx_node = idx_node + 1\n    return disting_bet_average_degree\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def rv_plot(r, v, t0, t1, epsilon=500, title='', redrange=None):\n    #rとvはそれぞれ対応するarray, t0とt1は第１と第２の極小値近傍時刻, itimeオプションは後のため\n    ti = t0 - epsilon\n    tf = t1 + epsilon\n    tm = ti + (tf - ti) // 2    #tm は ti ~ tf の中間インデックス\n    xi = np.where(r==np.min(r[ti:tm]))[0][0]    #xi は ti ~ tm までの r の最小値のインデックス\n    xf = np.where(r==np.min(r[tm:tf]))[0][0]    #xf は tm ~ tf までの r の最小値のインデックス\n    xm = xi + (xf - xi) // 2    #xm は xi ~ xf の中間インデックス\n    plt.plot(r[xi:xm], v[xi:xm], c='black')\n    plt.plot(r[xm:xf], v[xm:xf], c='gray')\n    \n    if redrange != None:\n        plt.plot(r[redrange[0]:redrange[1]], v[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた\n    plt.title(f'{title}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def rv_plot2(r1, v1, r2, v2, t0, t1, epsilon=500, title1='', title2='', redrange=None):\n    #0. r1とv2はそれぞれ対応する膝屈曲角とそれの角速度, t0とt1は第１と第２の極小値近傍時刻, itimeオプションは後のため\n    ti = t0 - epsilon\n    tf = t1 + epsilon\n    tm = ti + (tf - ti) // 2    #tm は ti ~ tf の中間インデックス\n    xi = np.where(r1==np.min(r1[ti:tm]))[0][0]    #xi は ti ~ tm までの r の最小値のインデックス\n    xf = np.where(r1==np.min(r1[tm:tf]))[0][0]    #xf は tm ~ tf までの r の最小値のインデックス\n    xm = xi + (xf - xi) // 2    #xm は xi ~ xf の中間インデックス\n    \n    #1. 膝屈曲角プロット\n    plt.subplot(1, 2, 1)\n    plt.plot(r1[xi:xm], v1[xi:xm], c='black')\n    plt.plot(r1[xm:xf], v1[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r1[redrange[0]:redrange[1]], v1[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた\n    plt.title(f'{title1}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()\n    \n    #2. 大腿(下腿)傾き角プロット\n    plt.subplot(1, 2, 2)\n    plt.plot(r2[xi:xm], v2[xi:xm], c='black')\n    plt.plot(r2[xm:xf], v2[xm:xf], c='gray')\n    if redrange != None:\n        plt.plot(r2[redrange[0]:redrange[1]], v2[redrange[0]:redrange[1]], c='red', lw=3) #指定個所に赤い曲線を上描き\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms',  f'{redrange[0]}-{redrange[1]}ms'])\n    else:\n        plt.legend([f'{xi}-{xm}ms', f'{xm}-{xf}ms'])    #すぐ上のコードがあるのでエラーを出さないためにelse内部に書いた       \n    plt.title(f'{title2}')\n    plt.xlabel('angle [deg]')\n    plt.ylabel('angular velocity [deg/sec]')                                \n    plt.grid()```\n\nUnused functions:\nNone"}, {'reason': 'stop', 'result': 'def performance_metric(y_true, y_predict):\n    """ Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. """\n    \n    # TODO: Calculate the performance score between \'y_true\' and \'y_predict\'\n    score = None\n    \n    # Return the score\n    return score\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import cv2\nimport numpy as np\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((*img.shape, 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': 'def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n\n    return result\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def clean_election_data():\n    \'\'\'\n    Function to clean election data \n    \'\'\'\n    import math\n        \n    # read in dirty data \n    df = pd.read_csv("2014_election_results.csv")\n    dfClean = df.dropna(subset=["STATE", "D", "GENERAL PERCENT"]).copy()\n\n    for i in range(len(dfClean)):\n        row = dfClean.iloc[i]  \n        row["GENERAL PERCENT"] = np.float(row["GENERAL PERCENT"].strip("%").replace(",", "."))\n        if(pd.isnull(row["CANDIDATE NAME"]) or (row["CANDIDATE NAME"] == \'Scattered\')):\n            if(pd.isnull(row["CANDIDATE NAME (Last)"]) or (row["CANDIDATE NAME (Last)"] == \'Scattered\')):\n                row["CANDIDATE NAME"] = "UNKNOWN" \n            else:\n                row["CANDIDATE NAME"] = row["CANDIDATE NAME (Last)"]\n    \n    dfClean = dfClean[["STATE", "D", "CANDIDATE NAME", "GENERAL PERCENT"]]\n    return dfClean\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "#Pass bigquery.SourceFormat.NEWLINE_DELIMITED_JSON for JSON in delimiter type and bigquery.SourceFormat.CSV for CSV\ndef loadfiletobigquery(file_name, dataset_id, table_name, delimitertype, loadtype, skipheader):\n    client = bigquery.Client()\n    table_ref = client.dataset(dataset_id).table(table_name)\n    job_config = bigquery.LoadJobConfig()\n    if skipheader is not None:\n        job_config.skip_leading_rows = skipheader\n    job_config.source_format = delimitertype\n    if delimitertype == bigquery.SourceFormat.CSV:\n        job_config.autodetect = True\n    job_config.write_disposition = loadtype\n\n    load_job = client.load_table_from_uri(\n        file_name,\n        table_ref,\n        job_config=job_config)  # API request\n\n    assert load_job.job_type == 'load'\n\n    load_job.result()  # Waits for table load to complete.\n\n    assert load_job.state == 'DONE'```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '# returns "True" if face is detected in image stored at img_path\ndef face_detector(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray)\n    return len(faces) > 0```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)```\n\nUnused functions:\n- paths_to_tensor'}, {'reason': 'stop', 'result': 'from keras.applications.resnet50 import preprocess_input, decode_predictions\n\ndef ResNet50_predict_labels(img_path):\n    # returns prediction vector for image located at img_path\n    img = preprocess_input(path_to_tensor(img_path))\n    return np.argmax(ResNet50_model.predict(img))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '### returns "True" if a dog is detected in the image stored at img_path\ndef dog_detector(img_path):\n    prediction = ResNet50_predict_labels(img_path)\n    return ((prediction <= 268) & (prediction >= 151)) \n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from extract_bottleneck_features import *\n\ndef VGG16_predict_breed(img_path):\n    # extract bottleneck features\n    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n    # obtain predicted vector\n    predicted_vector = VGG16_model.predict(bottleneck_feature)\n    # return dog breed that is predicted by the model\n    return dog_names[np.argmax(predicted_vector)]```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'import numpy as np\n```\n\nUnused functions:\n- nCr'}, {'reason': 'stop', 'result': 'from matplotlib.colors import ListedColormap\n\ndef plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n    x1s = np.linspace(axes[0], axes[1], 100)\n    x2s = np.linspace(axes[2], axes[3], 100)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    X_new = np.c_[x1.ravel(), x2.ravel()]\n    y_pred = clf.predict(X_new).reshape(x1.shape)\n    custom_cmap = ListedColormap([\'#fafab0\',\'#9898ff\',\'#a0faa0\'])\n    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap, linewidth=10)\n    if contour:\n        custom_cmap2 = ListedColormap([\'#7d7d58\',\'#4c4c7f\',\'#507d50\'])\n        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", alpha=alpha)\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", alpha=alpha)\n    plt.axis(axes)\n    plt.xlabel(r"$x_1$", fontsize=18)\n    plt.ylabel(r"$x_2$", fontsize=18, rotation=0)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def plot_digit(data):\n    image = data.reshape(28, 28)\n    plt.imshow(image, cmap = matplotlib.cm.hot,\n               interpolation="nearest")\n    plt.axis("off")```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# will try to convert a value to a float, followed by string,\n# if the float conversion fails\n\ndef convert_invalid_values(value):\n    \n    try:\n        new_value = float(value)\n    except:\n        new_value = str(value)\n\n    return new_value\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import math\n\nleftline=[(0,0,0,0)]\nrightline=[(0,0,0,0)]\n   \ndef grayscale(img):\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    mask = np.zeros_like(img)   \n    if len(img.shape) > 2:\n        channel_count = img.shape[2]\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef draw_lines_roi(img, lines, vertices, color=[255, 0, 0], thickness=2):\n    global leftline\n    global rightline\n    \n    imshape = img.shape\n    y_min = np.int(imshape[0]*0.61)\n    y_max = imshape[0]\n    left_x1 = []\n    left_x2 = []\n    right_x1 = []\n    right_x2 = []\n    left_count=0\n    right_count=0\n    \n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            m = (y2-y1)/(x2-x1)\n            if (m > 0.3) & (m<7):\n                fit = np.polyfit([x1,x2],[y1,y2],1)\n                right_x1.append(np.int(np.float((y_min - fit[1]))/np.float(fit[0])))\n                right_x2.append(np.int(np.float((y_max - fit[1]))/np.float(fit[0])))\n                right_count+=1\n            if (m < -0.3) & (m>-7):\n                fit = np.polyfit([x1,x2],[y1,y2],1)\n                left_x1.append(np.int(np.float((y_min - fit[1]))/np.float(fit[0])))\n                left_x2.append(np.int(np.float((y_max - fit[1]))/np.float(fit[0])))\n                left_count+=1\n    \n    if (left_count>0):\n        left_line_x1 = np.int(np.nanmedian(left_x1))\n        left_line_x2 = np.int(np.nanmedian(left_x2))\n                \n        if leftline[0][2]!=0:\n            if (abs(leftline[0][2]-left_line_x2)<100) & (abs(leftline[0][0]-left_line_x1)<50):\n                leftline=[(left_line_x1,y_min,left_line_x2,y_max)]\n        else:\n            leftline=[(left_line_x1,y_min,left_line_x2,y_max)]\n    \n    if (right_count>0):        \n        right_line_x1 = np.int(np.nanmedian(right_x1))\n        right_line_x2 = np.int(np.nanmedian(right_x2))\n    \n        if rightline[0][2]!=0:\n            if (abs(rightline[0][2]-right_line_x2)<100) & (abs(rightline[0][0]-right_line_x1)<50):\n                rightline=[(right_line_x1,y_min,right_line_x2,y_max)]\n        else:\n            rightline=[(right_line_x1,y_min,right_line_x2,y_max)]\n    \n    newlines = [leftline,rightline]\n           \n    draw_lines(img, newlines,color,thickness)\n  \n    return img\n                         \ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap,vertices):\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n   \n    result = draw_lines_roi(line_img, lines,vertices,[255,0,0],14)\n    return result\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': 'def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n\n    gray = grayscale(image)\n\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray,kernel_size)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    vertices = np.array([[(0,imshape[0]),(imshape[1]*0.48, imshape[0]*0.6), (imshape[1]*0.52, imshape[0]*0.6), (imshape[1],imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges,vertices)\n \n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    rho = 1 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 90     # minimum number of votes (intersections in Hough grid cell)\n    min_line_length = 30 #minimum number of pixels making up a line\n    max_line_gap = 30    # maximum gap in pixels between connectable line segments\n    line_image = np.copy(image)*0 # creating a blank to draw lines on\n\n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, max_line_gap,vertices)\n\n    # Draw the lines on the edge image\n    result = weighted_img(lines, image, α=0.8, β=1., λ=0.)\n    \n    return result\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '```python\ndef performance_metric(y_true, y_predict):\n    """ Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. """\n    \n    # TODO: Calculate the performance score between \'y_true\' and \'y_predict\'\n    score = None\n    \n    # Return the score\n    return score\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# TODO: Import \'make_scorer\', \'DecisionTreeRegressor\', and \'GridSearchCV\'\n\ndef fit_model(X, y):\n    """ Performs grid search over the \'max_depth\' parameter for a \n        decision tree regressor trained on the input data [X, y]. """\n    \n    # Create cross-validation sets from the training data\n    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '# 2 1d ODEs\ndef ode(state,t):\n    # x and y are first two components of state vector\n    x = state[0]\n\n    # Compute state derivatives.  Mess around here! \n    dx = .4* np.square(x) - 2\n\n    # Return the state derivatives\n    return [dx]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import math\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    (assuming your grayscaled image is called \'gray\')\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=10):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    \n    \n    left_lines = []\n    right_lines = []\n    \n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            # Draw line segments in blue for error checking.\n            cv2.line(img, (x1, y1), (x2, y2), [0, 0, 255], 6)\n            \n            if x1 == x2:\n                continue\n            slope = get_slope(x1,y1,x2,y2)\n            # the slope for the left is less than 0 because of the way the coordinates are arranged.\n            if slope < 0:\n                # Ignore obviously invalid lines\n                if slope > -0.5 or slope < -0.8:\n                    continue\n                left_lines.append(line)\n            elif slope >= 0:\n                # Ignore obviously invalid lines\n                if slope < 0.5 or slope > 0.8:\n                    continue\n                right_lines.append(line)\n                \n    draw_lines_oneside(img, left_lines, color, thickness)\n    draw_lines_oneside(img, right_lines, color, thickness)\n\ndef get_slope(x1,y1,x2,y2):\n    return ((y2-y1)/(x2-x1))    \n    \ndef draw_lines_oneside(img, lines, color, thickness):\n    if len(lines) == 0:\n        return\n    \n    top = 330\n    bottom = img.shape[0]\n    \n    x1_avg, y1_avg, x2_avg, y2_avg = averaging_lines(lines)\n    \n    slope = (y2_avg - y1_avg) / (x2_avg - x1_avg)\n    \n    bias = y1_avg - slope * x1_avg\n    \n    x1 = (bottom - bias) / slope\n    y1 = bottom\n    x2 = (top - bias) / slope\n    y2 = top\n    \n    cv2.line(img, (int(x1), y1), (int(x2), y2), color, thickness)\n    \ndef averaging_lines(lines):\n    lines_size = len(lines)\n    x1_sum, y1_sum, x2_sum, y2_sum = 0, 0, 0, 0\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            x1_sum += x1\n            y1_sum += y1\n            x2_sum += x2\n            y2_sum += y2\n    \n    x1_avg = x1_sum / lines_size\n    y1_avg = y1_sum / lines_size\n    x2_avg = x2_sum / lines_size\n    y2_avg = y2_sum / lines_size\n    return x1_avg, y1_avg, x2_avg, y2_avg\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\n# Python 3 has support for cool math symbols.\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': 'def lane_detection(image):\n    gray = grayscale(image)\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray, 5)\n\n    # Define our parameters for Canny and apply\n    low_threshold = 60\n    high_threshold = 180\n    edges = canny(blur_gray, low_threshold, high_threshold) \n\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    # print(\'This image is:\', type(image), \'with dimensions:\', image.shape)\n    vertices = np.array([[(0,imshape[0]),(450, 320), (490, 320), (imshape[1],imshape[0])]], dtype=np.int32)\n\n    masked_edges = region_of_interest(edges, vertices)\n\n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    rho = 2 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 15     # minimum number of votes (intersections in Hough grid cell)\n    min_line_len = 40 #minimum number of pixels making up a line\n    max_line_gap = 20    # maximum gap in pixels between connectable line segments\n\n    # Run Hough on edge detected image\n    # Here the returned image already has the lane line drawn (not in segments)\n    line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\n\n    # Create a "color" binary image to combine with line image\n    color_edges = np.dstack((edges, edges, edges)) \n\n    # Draw the lines on the edge image\n    lines_edges = weighted_img(line_image, image, α=0.8, β=1., λ=0.)\n    # plt.imshow(lines_edges)\n    return lines_edges\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image where lines are drawn on lanes)\n    return lane_detection(image)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '# returns "True" if face is detected in image stored at img_path\ndef face_detector(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray)\n    return len(faces) > 0```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def detect_face(img_path, cascade):\n    ''' Alternate implementation of Face Detector - using given Haar Cascade file\n        :img_path - path to image file\n        :cascade - cascade algo to use\n        :returns - True if face found, else False\n    '''\n    img = cv2.imread(img_path)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = cascade.detectMultiScale(img_gray)\n    return len(faces) > 0\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path, height=224, width=224):\n    ''' Loads RGB image as PIL.Image.Image type of given Height x Width dimensions\n    '''\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(height, width))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'from keras.applications.resnet50 import preprocess_input, decode_predictions\n\ndef ResNet50_predict_labels(img_path):\n    # returns prediction vector for image located at img_path\n    img = preprocess_input(path_to_tensor(img_path))\n    return np.argmax(ResNet50_model.predict(img))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': '### returns "True" if a dog is detected in the image stored at img_path\ndef dog_detector(img_path):\n    prediction = ResNet50_predict_labels(img_path)\n    return ((prediction <= 268) & (prediction >= 151)) \n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "## Show history\n\ndef show_history_graph(history):\n    '''  Graphically show the history of model fitting\n    '''\n    \n    plt.figure(figsize=(8,8))\n    plt.subplot(221)\n    # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    \n    # summarize history for loss\n    plt.subplot(222)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    \n    plt.tight_layout()\n    plt.show() \n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'from extract_bottleneck_features import *\n\ndef VGG16_predict_breed(img_path):\n    # extract bottleneck features\n    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n    # obtain predicted vector\n    predicted_vector = VGG16_model.predict(bottleneck_feature)\n    # return dog breed that is predicted by the model\n    return dog_names[np.argmax(predicted_vector)]```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "from keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as preprocess_inception_input\n\ndef detect_dog_breed(img_path, given_model, use_bottleneck=True, img_H=224, img_W=224):\n    ''' Detect dog breed given image in the img_path,\n        using given model, using either bottleneck features (or not)\n        with given img Height and Width\n        \n        @return: Dog breed (str)\n    '''\n    print('Detecting dog breed...')\n    tensor = path_to_tensor(img_path, img_H, img_W)\n    \n    # using given image, extract its bottleneck features by running thru InceptionV3 n/w first\n    if use_bottleneck: \n        tensor = extract_InceptionV3(tensor)\n    else:\n        tensor = preprocess_inception_input(tensor)\n    \n    # print('  [input tensor shape: {}]'.format(tensor.shape))\n    # make predictions (probabilities)\n    predicted_vector = given_model.predict(tensor)\n    # get max index\n    y_hat = np.argmax(predicted_vector)\n    chance = 100. * predicted_vector[0][y_hat]  # probability of correctness\n    # print('  [y_hat:{}]'.format(y_hat))\n    # print('  prob:{:.2f}%'.format(chance))\n\n    # return dog breed and probability \n    return dog_names[y_hat], chance\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "def whose_a_good_doggy(img_path):\n    ''' Using the given image (in img_path), returns either:\n        - Dog breed (if it's a dog)\n        - Dog breed that resembles a human (if it's a human face)\n        \n        Uses the transfer-learned CNN model from Step 5\n    '''\n    print('.'*60)\n    print('Given image:', img_path)\n    \n    # human face?\n    human_found = face_detector(img_path)\n    print('Found human:', human_found)\n    \n    # find breed of dog\n    breed, chance = detect_dog_breed(img_path, inception_bneck, use_bottleneck=True, img_H=229, img_W=229)\n    print()\n    print('Image is dog breed: {} ({:.2f}% prob)'.format(breed, chance))\n    print('🐶 Woof!') if not human_found else print('Hellooo, 🐱👩🏻👦🏻👧🏻 animal 🤔')\n    print('='*60)"}, {'reason': 'stop', 'result': "import matplotlib.image as mpimg\n\ndef disp_image(img_path):\n    img = mpimg.imread(img_path)\n    fig = plt.figure()\n    plt.subplot()\n    plt.imshow(img)\n    plt.axis('off')\n    plt.plot()\n    plt.show()```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': '## TODO: Execute your algorithm from Step 6 on\n## at least 6 images on your computer.\n## Feel free to use as many code cells as needed.'}, {'reason': 'stop', 'result': 'import math\nprevious = {\n    "right": [0, 0, 0, 0], # x1, y1, x2, y2\n    "left": [0, 0, 0, 0]\n}\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    # Or use BGR2GRAY if you read an image with cv2.imread()\n    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=5):\n    global previous\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    sides = {\n        "left": [],\n        "right": []\n    }\n    yMin = img.shape[0]\n    xHalf = img.shape[1] / 2\n    yMax = 315\n    drawn = []\n    if lines != None:\n        for line in lines:\n            for x1,y1,x2,y2 in line:\n                slope = ((y2 - y1) / (x2 - x1))\n                if slope > 0.50 and slope < 0.8: # Right line, because of positive slope (y-reversed)\n                    sides["right"].append([x1,y1,x2,y2]) \n                elif slope < -0.50 and slope > -0.8: # Left line because of negative slope\n                    sides["left"].append([x1,y1,x2,y2])\n    yHalf = None\n    for side in sides:\n        avgSlope = None\n        totalSlope = 0\n        totalWeight = 0\n        xAvg = None\n        yAvg = None\n        for x1,y1,x2,y2 in sides[side]:\n            slope = (y2 - y1) / (x2 - x1)\n            length = math.sqrt(abs(x2-x1)^2+abs(y2 - y1)^2)\n            if xAvg == None:\n                #avgSlope = slope\n                xAvg = (x1 + x2) / 2\n                yAvg = (y1 + y2) / 2\n            else:\n                #avgSlope = (avgSlope + slope) / 2\n                xAvg = (xAvg + ((x1 + x2) / 2)) / 2\n                yAvg = (yAvg + ((y1 + y2) / 2)) / 2\n            totalSlope += slope * length\n            totalWeight += length\n        if totalWeight > 0:\n            avgSlope = totalSlope / totalWeight\n        if avgSlope != None and xAvg != None and yAvg != None:\n            yIntercept = -(avgSlope * xAvg) + yAvg\n            xMax = (yMax - yIntercept) / avgSlope\n            xMin = (yMin - yIntercept) / avgSlope\n            if side == "right":\n                offset = 20\n            else:\n                offset = -20\n            _yHalf = avgSlope * (xHalf + offset) + yIntercept \n            if yHalf == None:\n                yHalf = _yHalf\n            else:\n                xHalf = ((yHalf - yIntercept) / avgSlope) - offset\n            # cv2.line(img, (int(xMin), int(yMin)), (int(xMax), int(yMax)), color, thickness)\n            \'\'\' Switched to X at half calculation to avoid crossing lines in the extra task \'\'\'\n            points = unjitter(side, [xMin, yMin, (xHalf + offset), yHalf])\n            cv2.line(img, (int(points[0]), int(points[1])), (int(points[2]), int(points[3])), color, thickness)\n        else:\n            \'\'\' Couldn\'t accurately calculate line so going with the line of the previous frame \'\'\'\n            points = unjitter(side, previous[side])\n            cv2.line(img, (int(points[0]), int(points[1])), (int(points[2]), int(points[3])), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((*img.shape, 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\n# Python 3 has support for cool math symbols.\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': "def process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    ''' Filter color '''\n    color_select = np.copy(image)\n    rgb_threshold = [200, 150, 95]\n    thresholds = (image[:,:,0] < rgb_threshold[0]) \\\n    | (image[:,:,1] < rgb_threshold[1]) \\\n    | (image[:,:,2] < rgb_threshold[2])\n    color_select[thresholds] = [0,0,0]\n    gray = grayscale(color_select)\n    blurred = gaussian_blur(gray, 3)\n    edges = canny(blurred, 50, 150)\n    xsize = image.shape[1]\n    ysize = image.shape[0]\n    vertices = np.array([[(0,ysize),(xsize/2,ysize/1.71),(xsize/2,ysize/1.71),(xsize,ysize)]], dtype=np.int32)\n    regioned = region_of_interest(edges, vertices)\n    hough = hough_lines(regioned, 1, np.pi/180, 35, 35, 20)\n    result = weighted_img(hough, image)\n    return result\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'class VGGNetwork:\n    \'\'\'\n    Helper class to load VGG and its weights to the FastNet model\n    \'\'\'\n\n    def __init__(self, img_width=384, img_height=384, vgg_weight=1.0):\n        self.img_height = img_height\n        self.img_width = img_width\n        self.vgg_weight = vgg_weight\n\n        self.vgg_layers = None\n\n    def append_vgg_network(self, x_in, true_X_input, pre_train=False):\n\n        # Append the initial inputs to the outputs of the SRResNet\n        x = concatenate([x_in, true_X_input], axis=0)\n\n        # Normalize the inputs via custom VGG Normalization layer\n        x = Normalize(name="normalize_vgg")(x)\n\n        # Begin adding the VGG layers\n        x = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv1_1\')(x)\n\n        x = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv1_2\')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name=\'vgg_maxpool1\')(x)\n\n        x = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv2_1\')(x)\n\n        vgg_regularizer2 = ContentVGGRegularizer(weight=self.vgg_weight)\n        x = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\',\n                              activity_regularizer=vgg_regularizer2, name=\'vgg_conv2_2\')(x)\n        \n        x = MaxPooling2D((2, 2), strides=(2, 2), name=\'vgg_maxpool2\')(x)\n\n        x = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv3_1\')(x)\n        x = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv3_2\')(x)\n\n        x = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv3_3\')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name=\'vgg_maxpool3\')(x)\n\n        x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv4_1\')(x)\n        x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv4_2\')(x)\n\n        x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv4_3\')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name=\'vgg_maxpool4\')(x)\n\n        x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv5_1\')(x)\n        x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv5_2\')(x)\n\n        x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'vgg_conv5_3\')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name=\'vgg_maxpool5\')(x)\n\n        return x\n\n    def load_vgg_weight(self, model):\n        # Loading VGG 16 weights\n        if K.image_dim_ordering() == "th":\n            weights = get_file(\'vgg16_weights_th_dim_ordering_th_kernels_notop.h5\', THEANO_WEIGHTS_PATH_NO_TOP,\n                                   cache_subdir=\'models\')\n        else:\n            weights = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\', TF_WEIGHTS_PATH_NO_TOP,\n                                   cache_subdir=\'models\')\n        f = h5py.File(weights)\n\n        layer_names = [name for name in f.attrs[\'layer_names\']]\n\n        if self.vgg_layers is None:\n            self.vgg_layers = [layer for layer in model.layers\n                               if \'vgg_\' in layer.name]\n\n        for i, layer in enumerate(self.vgg_layers):\n            g = f[layer_names[i]]\n            weights = [g[name] for name in g.attrs[\'weight_names\']]\n            layer.set_weights(weights)\n\n        # Freeze all VGG layers\n        for layer in self.vgg_layers:\n            layer.trainable = False\n\n        return model\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class DiscriminatorNetwork:\n\n    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.adversarial_loss_weight = adversarial_loss_weight\n        self.small_model = small_model\n\n        self.k = 3\n        self.mode = 2\n        self.weights_path = "weights/Discriminator weights.h5"\n\n        self.gan_layers = None\n\n    def append_gan_network(self, true_X_input):\n\n        # Normalize the inputs via custom VGG Normalization layer\n        x = Normalize(type="gan", value=127.5, name="gan_normalize")(true_X_input)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_1\', kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name="gan_lrelu1_1")(x)\n\n        x = Conv2D(64, (self.k, self.k), padding=\'same\', name=\'gan_conv1_2\', strides=(2, 2), kernel_initializer="glorot_uniform")(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu1_2\')(x)\n        x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm1_1\')(x)\n\n        filters = [128, 256] if self.small_model else [128, 256, 512]\n\n        for i, num_filters in enumerate(filters):\n            for j in range(2):\n                strides = (2, 2) if j == 1 else (1, 1)\n            \n                x = Conv2D(num_filters, (self.k, self.k), padding=\'same\', strides=strides,\n                                  name=\'gan_conv%d_%d\' % (i + 2, j + 1), kernel_initializer="glorot_uniform")(x)\n                x = LeakyReLU(0.3, name=\'gan_lrelu_%d_%d\' % (i + 2, j + 1))(x)\n                x = BatchNormalization(axis=channel_axis, name=\'gan_batchnorm%d_%d\' % (i + 2, j + 1))(x)\n\n        x = Flatten(name=\'gan_flatten\')(x)\n\n        output_dim = 128 if self.small_model else 1024\n\n        x = Dense(output_dim, name=\'gan_dense1\')(x)\n        x = LeakyReLU(0.3, name=\'gan_lrelu5\')(x)\n\n        gan_regulrizer = AdversarialLossRegularizer(weight=self.adversarial_loss_weight)\n        x = Dense(2, activation="softmax", activity_regularizer=gan_regulrizer, name=\'gan_output\')(x)\n\n        return x\n\n    def set_trainable(self, model, value=True):\n        if self.gan_layers is None:\n            disc_model = [layer for layer in model.layers\n                          if \'model\' in layer.name][0] # Only disc model is an inner model\n\n            self.gan_layers = [layer for layer in disc_model.layers\n                               if \'gan_\' in layer.name]\n\n        for layer in self.gan_layers:\n            layer.trainable = value\n\n    def load_gan_weights(self, model):\n        f = h5py.File(self.weights_path)\n\n        layer_names = [name for name in f.attrs[\'layer_names\']]\n        layer_names = layer_names[1:] # First is an input layer. Not needed.\n\n        if self.gan_layers is None:\n            self.gan_layers = [layer for layer in model.layers\n                                if \'gan_\' in layer.name]\n\n        for i, layer in enumerate(self.gan_layers):\n            g = f[layer_names[i]]\n            weights = [g[name] for name in g.attrs[\'weight_names\']]\n            layer.set_weights(weights)\n\n        print("GAN Model weights loaded.")\n        return model\n\n    def save_gan_weights(self, model):\n        print(\'GAN Weights are being saved.\')\n        model.save_weights(self.weights_path, overwrite=True)\n        print(\'GAN Weights saved.\')'}, {'reason': 'stop', 'result': 'class GenerativeNetwork:\n\n    def __init__(self, img_width=96, img_height=96, batch_size=16, num_upscales=2, small_model=False,\n                 content_weight=1, tv_weight=2e5, gen_channels=64):\n        self.img_width = img_width\n        self.img_height = img_height\n        self.batch_size = batch_size\n        self.small_model = small_model\n        self.num_scales = num_upscales\n\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n\n        self.filters = gen_channels\n        self.mode = 2\n        self.init = \'glorot_uniform\'\n\n        self.sr_res_layers = None\n        self.sr_weights_path = "weights/SRGAN.h5"\n\n        self.output_func = None\n\n    def create_sr_model(self, ip):\n\n        x = Conv2D(self.filters, (5, 5), activation=\'linear\', padding=\'same\', name=\'sr_res_conv1\',\n                          kernel_initializer=self.init)(ip)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_1\')(x)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_lr1\')(x)\n\n        x = Conv2D(self.filters, (5, 5), activation=\'linear\', padding=\'same\', name=\'sr_res_conv2\', kernel_initializer="glorot_uniform")(x)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_2\')(x)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_lr2\')(x)\n\n        num_residual = 5 if self.small_model else 15\n\n        for i in range(num_residual):\n            x = self._residual_block(x, i + 1)\n\n        for scale in range(self.num_scales):\n            x = self._upscale_block(x, scale + 1)\n    \n        scale = 2 ** self.num_scales\n        tv_regularizer = TVRegularizer(img_width=self.img_width * scale, img_height=self.img_height * scale,\n                                       weight=self.tv_weight) #self.tv_weight)\n        \n        x = Conv2D(3, (5, 5), activation=\'tanh\', padding=\'same\', activity_regularizer=tv_regularizer, \n                   name=\'sr_res_conv_final\', kernel_initializer=self.init)(x)\n        \n        x = Denormalize(name=\'sr_res_conv_denorm\')(x)\n        return x\n\n    def _residual_block(self, ip, id):\n        init = ip\n\n        x = Conv2D(self.filters, (3, 3), activation=\'linear\', padding=\'same\', name=\'sr_res_conv_\' + str(id) + \'_1\',\n                          kernel_initializer=self.init)(ip)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_\' + str(id) + \'_1\')(x)\n        x = LeakyReLU(alpha=0.25, name="sr_res_activation_" + str(id) + "_1")(x)\n\n        x = Conv2D(self.filters, (3, 3), activation=\'linear\', padding=\'same\', name=\'sr_res_conv_\' + str(id) + \'_2\',\n                          kernel_initializer=self.init)(x)\n        x = BatchNormalization(axis=channel_axis, name=\'sr_res_bn_\' + str(id) + \'_2\')(x)\n\n        m = add([x, init],name="sr_res_merge_" + str(id))\n\n        return m\n\n    def _upscale_block(self, ip, id):\n        \'\'\'\n        As per suggestion from http://distill.pub/2016/deconv-checkerboard/, I am swapping out\n        SubPixelConvolution to simple Nearest Neighbour Upsampling\n        \'\'\'\n        init = ip\n        \n        x = Conv2D(128, (3, 3), activation="linear", padding=\'same\', name=\'sr_res_upconv1_%d\' % id,\n                          kernel_initializer=self.init)(init)\n        x = LeakyReLU(alpha=0.25, name=\'sr_res_up_lr_%d_1_1\' % id)(x)\n        x = UpSampling2D(name=\'sr_res_upscale_%d\' % id)(x)\n        #x = SubPixelUpscaling(r=2, channels=32)(x)\n        x = Conv2D(128, (3, 3), activation="linear", padding=\'same\', name=\'sr_res_filter1_%d\' % id,\n                          kernel_initializer=self.init)(x)\n        x = LeakyReLU(alpha=0.3, name=\'sr_res_up_lr_%d_1_2\' % id)(x)\n\n        return x\n\n    def set_trainable(self, model, value=True):\n        if self.sr_res_layers is None:\n            self.sr_res_layers = [layer for layer in model.layers\n                                    if \'sr_res_\' in layer.name]\n\n        for layer in self.sr_res_layers:\n            layer.trainable = value\n\n    def get_generator_output(self, input_img, srgan_model):\n        if self.output_func is None:\n            gen_output_layer = [layer for layer in srgan_model.layers\n                                if layer.name == "sr_res_conv_denorm"][0]\n            self.output_func = K.function([srgan_model.layers[0].input],\n                                          [gen_output_layer.output])\n\n        return self.output_func([input_img])'}, {'reason': 'length', 'result': ''}, {'reason': 'stop', 'result': 'from keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport sys\nsys.path.append("..")\n\n#import models\nfrom loss import PSNRLoss, psnr\n\nimport os\nimport time\nimport numpy as np\nfrom imageio import imwrite as imsave\nfrom scipy.misc import imresize\nfrom scipy.ndimage.filters import gaussian_filter\n\nbase_weights_path = "weights/"\nbase_val_images_path = "val_images/"\nbase_test_images = "test_images/"\n\nset5_path = "tests/set5"\nset14_path = "tests/set14"\nbsd100_path = "tests/bsd100"\n\nif not os.path.exists(base_weights_path):\n    os.makedirs(base_weights_path)\n\nif not os.path.exists(base_val_images_path):\n    os.makedirs(base_val_images_path)\n\nif not os.path.exists(base_test_images):\n    os.makedirs(base_test_images)\n\ndef test_set5(model : Model, img_width=32, img_height=32, batch_size=1):\n    datagen = ImageDataGenerator(rescale=1. / 255)\n    large_img_width = img_width * 4\n    large_img_height = img_height * 4\n\n    iteration = 0\n    total_psnr = 0.0\n\n    print("Testing model on Set 5 Validation images")\n    total_psnr = _test_loop(set5_path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width,\n                            model, total_psnr, "Set5", 5)\n\n    print("Average PSNR of Set5 validation images : ", total_psnr / 5)\n    print()\n\n\ndef test_set14(model : Model, img_width=32, img_height=32, batch_size=1):\n    datagen = ImageDataGenerator(rescale=1. / 255)\n    large_img_width = img_width * 4\n    large_img_height = img_height * 4\n\n    iteration = 0\n    total_psnr = 0.0\n\n    print("Testing model on Set 14 Validation images")\n    total_psnr = _test_loop(set14_path, batch_size, datagen, img_height, img_width, iteration, large_img_height,\n                            large_img_width, model, total_psnr, "Set14", 14)\n\n    print("Average PSNR of Set5 validation images : ", total_psnr / 14)\n    print()\n\ndef test_bsd100(model : Model, img_width=32, img_height=32, batch_size=1):\n    datagen = ImageDataGenerator(rescale=1. / 255)\n    large_img_width = img_width * 4\n    large_img_height = img_height * 4\n\n    iteration = 0\n    total_psnr = 0.0\n\n    print("Testing model on BSD 100 Validation images")\n    total_psnr = _test_loop(bsd100_path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width,\n                            model, total_psnr, "bsd100", 100)\n\n    print("Average PSNR of BSD100 validation images : ", total_psnr / 100)\n    print()\n\n\ndef _test_loop(path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width, model,\n               total_psnr, prefix, num_images):\n    print("Path " + path)\n    print("Height " + str(img_height))\n    print("img_width " + str(img_width))\n    print("large_img_height " + str(large_img_height))\n    print("large_img_width " + str(large_img_width))\n    print("num_images " + str(num_images))\n    for x in datagen.flow_from_directory(path, class_mode=None, batch_size=batch_size,\n                                         target_size=(large_img_width, large_img_height)):\n        t1 = time.time()\n\n        # resize images\n        x_temp = x.copy()\n        x_temp = x_temp.transpose((0, 2, 3, 1))\n\n        x_generator = np.empty((batch_size, img_width, img_height, 3))\n\n        for j in range(batch_size):\n            img = imresize(x_temp[j], (img_width, img_height))\n            x_generator[j, :, :, :] = img\n\n        #x_generator = x_generator.transpose((0, 3, 1, 2))\n\n        output_image_batch = model.predict_on_batch(x_generator)\n\n        average_psnr = 0.0\n        for x_i in range(batch_size):\n            average_psnr += psnr(x[x_i], output_image_batch[x_i] / 255.)\n            total_psnr += average_psnr\n\n        average_psnr /= batch_size\n\n        iteration += batch_size\n        t2 = time.time()\n\n        print("Time required : %0.2f. Average validation PSNR over %d samples = %0.2f" %\n              (t2 - t1, batch_size, average_psnr))\n\n        for x_i in range(batch_size):\n            real_path = base_test_images + prefix + "_iteration_%d_num_%d_real_.png" % (iteration, x_i + 1)\n            generated_path = base_test_images + prefix + "_iteration_%d_num_%d_generated.png" % (iteration, x_i + 1)\n\n            val_x = x[x_i].copy() * 255.\n            #val_x = val_x.transpose((1, 2, 0))\n            val_x = np.clip(val_x, 0, 255).astype(\'uint8\')\n\n            output_image = output_image_batch[x_i]\n            #output_image = output_image.transpose((1, 2, 0))\n            output_image = np.clip(output_image, 0, 255).astype(\'uint8\')\n            \n            imsave(real_path, val_x[:,:,0])\n            imsave(generated_path, output_image[:,:,0])\n\n        if iteration >= num_images:\n            break\n    return total_psnr\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class SRResNetTest:\n\n    def __init__(self, img_width=96, img_height=96, batch_size=16):\n        assert img_width >= 16, "Minimum image width must be at least 16"\n        assert img_height >= 16, "Minimum image height must be at least 16"\n\n        self.img_width = img_width\n        self.img_height = img_height\n        self.batch_size = batch_size\n\n        self.model = None # type: Model\n        self.weights_path = base_weights_path + "sr_resnet_weights.h5"\n\n    def build_model(self, load_weights=False) -> Model:\n        sr_resnet = GenerativeNetwork(self.img_width, self.img_height, self.batch_size)\n\n        ip = Input(shape=(self.img_width, self.img_height, 3), name=\'x_generator\')\n        output = sr_resnet.create_sr_model(ip)\n\n        self.model = Model(ip, output)\n\n        optimizer = Adam(lr=1e-4)\n        self.model.compile(optimizer, loss=\'mse\', metrics=[PSNRLoss])\n\n        if load_weights:\n            try:\n                self.model.load_weights(self.weights_path)\n                print("SR ResNet model weights loaded.")\n            except Exception:\n                print("Weight for SR ResNet model not found or are incorrect size. Cannot load weights.")\n\n                response = input("Continue without loading weights? \'y\' or \'n\' ")\n                if response == \'n\':\n                    exit()\n\n        return self.model\n\n    def train_model(self, image_dir, num_images=50000, epochs=1):\n        datagen = ImageDataGenerator(rescale=1. / 255)\n        img_width = self.img_width * 4\n        img_height = self.img_height * 4\n\n        early_stop = False\n        iteration = 0\n        prev_improvement = -1\n\n        print("Training SR ResNet network")\n        for i in range(epochs):\n            print()\n            print("Epoch : %d" % (i + 1))\n\n            for x in datagen.flow_from_directory(image_dir, class_mode=None, batch_size=self.batch_size,\n                                                 target_size=(img_width, img_height)):\n\n                try:\n                    t1 = time.time()\n\n                    # resize images\n                    x_temp = x.copy()\n                    x_temp = x_temp.transpose((0, 2, 3, 1))\n\n                    x_generator = np.empty((self.batch_size, self.img_width, self.img_height, 3))\n\n                    for j in range(self.batch_size):\n                        img = gaussian_filter(x_temp[j], sigma=0.5)\n                        img = imresize(img, (self.img_width, self.img_height))\n                        x_generator[j, :, :, :] = img\n\n                    if iteration % 50 == 0 and iteration != 0 :\n                        print("Random Validation image..")\n                        output_image_batch = self.model.predict_on_batch(x_generator)\n\n                        print("Pred Max / Min: %0.2f / %0.2f" % (output_image_batch.max(),\n                                                                 output_image_batch.min()))\n\n                        average_psnr = 0.0\n                        for x_i in range(self.batch_size):\n                            average_psnr += psnr(x[x_i], output_image_batch[x_i] / 255.)\n\n                        average_psnr /= self.batch_size\n\n                        iteration += self.batch_size\n                        t2 = time.time()\n\n                        print("Time required : %0.2f. Average validation PSNR over %d samples = %0.2f" %\n                              (t2 - t1, self.batch_size, average_psnr))\n\n                        for x_i in range(self.batch_size):\n                            real_path = base_val_images_path + "epoch_%d_iteration_%d_num_%d_real_.png" % \\\n                                                               (i + 1, iteration, x_i + 1)\n\n                            generated_path = base_val_images_path + \\\n                                             "epoch_%d_iteration_%d_num_%d_generated.png" % (i + 1,\n                                                                                            iteration,\n                                                                                            x_i + 1)\n\n                            val_x = x[x_i].copy() * 255.\n                            val_x = np.clip(val_x, 0, 255).astype(\'uint8\')\n\n                            output_image = output_image_batch[x_i]\n                            output_image = np.clip(output_image, 0, 255).astype(\'uint8\')\n\n                            imsave(real_path, val_x[:,:,0])\n                            imsave(generated_path, output_image[:,:,0])\n\n                        continue\n\n                    hist = self.model.fit(x_generator, x * 255, batch_size=self.batch_size, epochs=1, verbose=0)\n                    psnr_loss_val = hist.history[\'PSNRLoss\'][0]\n\n                    if prev_improvement == -1:\n                        prev_improvement = psnr_loss_val\n\n                    improvement = (prev_improvement - psnr_loss_val) / prev_improvement * 100\n                    prev_improvement = psnr_loss_val\n\n                    iteration += self.batch_size\n                    t2 = time.time()\n\n                    print("%d / %d | Improvement : %0.2f %% | Time required : %0.2f s/step | "\n                          "PSNR : %0.3f" % (iteration, num_images, improvement, t2 - t1, psnr_loss_val))\n\n                    if iteration % 1000 == 0 and iteration != 0:\n                        print("Saving weights")\n                        self.model.save_weights(self.weights_path, overwrite=True)\n\n                    if iteration >= num_images:\n                        break\n\n                except KeyboardInterrupt:\n                    print("Keyboard interrupt detected. Stopping early.")\n                    early_stop = True\n                    break\n\n            iteration = 0\n\n            if early_stop:\n                break\n\n        print("Finished training SRGAN network. Saving model weights.")'}, {'reason': 'stop', 'result': 'def translate(s, dico_to_token, to_token=True):\n    new_s = []\n    if to_token:\n        for w in s:\n            token = dico_to_token[w]\n            new_s.append(token)\n    else:\n        inverted_dico = {token: word for word, token in dico_to_token.items()}\n        for t in s:\n            word = inverted_dico[t]\n            if word in ["PADD","START","STOP"]:\n                continue\n            else:\n                new_s.append(word)\n    return new_s\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def sample(preds, temperature=1.):\n    # helper function to sample an index from a probability array\n    if temperature == 0.:\n        return np.argmax(preds)\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def term_output(term, input_value):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def output_at(list_of_terms, x_value):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def delta_f(list_of_terms, x_value, delta_x):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "def delta_f_trace(list_of_terms, x_value, delta_x):\n    initial_f_value = output_at(list_of_terms, x_value)\n    delta_f_value = delta_f(list_of_terms, x_value, delta_x)\n    if initial_f_value and delta_f_value:\n        trace =  trace_values(x_values=[x_value + delta_x, x_value + delta_x], \n                              y_values=[initial_f_value, initial_f_value + delta_f_value], mode = 'lines',\n                              name = 'delta f = ' + str(delta_x))\n        return trace```"}, {'reason': 'stop', 'result': "def delta_x_trace(list_of_terms, x_value, delta_x):\n    initial_f_value = output_at(list_of_terms, x_value)\n    if initial_f_value:\n        trace = trace_values(x_values=[x_value, x_value + delta_x],\n                            y_values=[initial_f_value, initial_f_value], mode = 'lines', \n                            name = 'delta x = ' + str(delta_x))\n        return trace```"}, {'reason': 'stop', 'result': 'def derivative_of(list_of_terms, x_value, delta_x):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def derivative_trace(list_of_terms, x_value, line_length = 4, delta_x = .01):\n    derivative_at = derivative_of(list_of_terms, x_value, delta_x)\n    y = output_at(list_of_terms, x_value)\n    if derivative_at and y:\n        x_minus = x_value - line_length/2\n        x_plus = x_value + line_length/2\n        y_minus = y - derivative_at * line_length/2\n        y_plus = y + derivative_at * line_length/2\n        return trace_values([x_minus, x_value, x_plus],[y_minus, y, y_plus], name = "f\' (x) = " + str(derivative_at), mode = \'lines\')```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def delta_traces(list_of_terms, x_value, line_length = 4, delta_x = .01):\n    tangent = derivative_trace(list_of_terms, x_value, line_length, delta_x)\n    delta_f_line = delta_f_trace(list_of_terms, x_value, delta_x)\n    delta_x_line = delta_x_trace(list_of_terms, x_value, delta_x)\n    return [tangent, delta_f_line, delta_x_line]```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def move_dimension_first(f, index):\n    return np.moveaxis(f, index, 0).copy()\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def get_neighbour_messages(sender, receiver):\n    messages = []\n    for n in sender.neighbours:\n        if n is not receiver:\n            messages.append(sender.in_msgs[n])\n    return messages\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def calc_other_neighbour_msg_prod(sender, receiver):\n    vectors = get_neighbour_messages(sender, receiver)\n    return np.multiply.reduce(np.ix_(*vectors))\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def calculate_factor(f_neighb_first, neighbour_msg_prod):\n    #Numpy by default broadcast the first dimension,\n    #therefore it is not necessary to expand the messages tensor manually\n\n    return f_neighb_first * neighbour_msg_prod\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def calc_sum_product_factor_to_variable_msg(factor, variable):\n    neighbour_msg_prod = calc_other_neighbour_msg_prod(factor, variable)\n    \n    f_neighb_first = move_dimension_first(factor.f, factor.neighbours.index(variable))\n    \n    return marginalize(calculate_factor(f_neighb_first, neighbour_msg_prod), 0)\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def calc_sum_product_variable_to_factor_msg(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    if len(neighbour_msg_prod) > 0:\n        return np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        return np.ones(variable.num_states)\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def sum_product(node_list):\n\n    for n in node_list:\n        while len(n.pending) > 0:            \n            f = next(iter(n.pending))\n            n.send_sp_msg(f)\n            \n    for n in reversed(node_list):\n        while len(n.pending) > 0:\n            f = next(iter(n.pending))\n            n.send_sp_msg(f)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def calc_sum_product_variable_to_factor_msg(variable, factor):\n    \n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    \n    \n    if len(neighbour_msg_prod) > 0:\n        message = np.prod(np.array(neighbour_msg_prod), axis=0)\n    else:\n        message = np.ones(variable.num_states)\n    \n    message = message * variable.observed_state\n    \n    return message\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def max_sum(node_list):\n    \n    for n in node_list:\n        while len(n.pending) > 0:            \n            f = next(iter(n.pending))\n            n.send_ms_msg(f)\n            \n    for n in reversed(node_list):\n        while len(n.pending) > 0:\n            f = next(iter(n.pending))\n            n.send_ms_msg(f)\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def regression_formula(x):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def error(x_values, y_values, m, b, x):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def error_line_trace(x_values, y_values, m, b, x):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def error_line_traces(x_values, y_values, m, b):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def squared_error(x_values, y_values, m, b, x):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def squared_errors(x_values, y_values, m, b):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def residual_sum_squares(x_values, y_values, m, b):\n    pass\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import math\n\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    print(x)\n    pd.reset_option('display.max_rows')```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'def logit(z):\n    return 1 / (1 + np.exp(-z))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def leaky_relu(z, alpha=0.01):\n    return np.maximum(alpha*z, z)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def shuffle_batch(X, y, batch_size):\n    rnd_idx = np.random.permutation(len(X))\n    n_batches = len(X) // batch_size\n    for batch_idx in np.array_split(rnd_idx, n_batches):\n        X_batch, y_batch = X[batch_idx], y[batch_idx]\n        yield X_batch, y_batch\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def elu(z, alpha=1):\n    return np.where(z < 0, alpha * (np.exp(z) - 1), z)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def selu(z,\n         scale=1.0507009873554804934193349852946,\n         alpha=1.6732632423543772848170429916717):\n    return scale * elu(z, alpha)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def selu(z,\n         scale=1.0507009873554804934193349852946,\n         alpha=1.6732632423543772848170429916717):\n    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def max_norm_regularizer(threshold, axes=1, name="max_norm",\n                         collection="max_norm"):\n    def max_norm(weights):\n        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n        clip_weights = tf.assign(weights, clipped, name=name)\n        tf.add_to_collection(collection, clip_weights)\n        return None # 규제 손실을 위한 항이 없습니다\n    return max_norm\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "### Train your model here.\n### Calculate and report the accuracy on the training and validation set.\n### Once a final model architecture is selected, \n### the accuracy on the test set should be calculated and reported as well.\n\ndef LeNet6(x, n_classes):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # Layer 1: Convolutional. Input = 32x32x3 (rgb). Output = 28x28x6. shape (5,5,1,6)->(5,5,3,6)\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    \n    # Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Flatten. Input = 5x5x16. Output = 400.\n    fc0   = flatten(conv2)\n    \n    # Layer 3: Fully Connected. Input = 400. Output = 252.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean = mu, stddev = sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n    \n    # Activation.\n    fc1    = tf.nn.relu(fc1)\n    \n    # Dropout, to prevent overfitting\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n\n    # Layer 4: Fully Connected. Input = 252. Output = 168.\n    fc2_W  = tf.Variable(tf.truncated_normal(shape=(252, 168), mean = mu, stddev = sigma))\n    fc2_b  = tf.Variable(tf.zeros(168))\n    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n    \n    # Activation.\n    fc2    = tf.nn.relu(fc2)\n    \n    # Dropout, to prevent overfitting\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    \n    # Layer 5: Fully Connected. Input = 168. Output = 84.\n    fc3_W  = tf.Variable(tf.truncated_normal(shape=(168, 84), mean = mu, stddev = sigma))\n    fc3_b  = tf.Variable(tf.zeros(84))\n    fc3    = tf.matmul(fc2, fc3_W) + fc3_b\n    \n    # Activation.\n    fc3    = tf.nn.relu(fc3)\n    \n    # Dropout, to prevent overfitting\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n\n    # Layer 6: Fully Connected. Input = 84. Output = n_classes, which is 43.\n    fc4_W  = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n    fc4_b  = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    \n    return logits\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "### Model from Yann LeCun paper\n\ndef LeNetTrafficSign(x, n_classes):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # Layer 1: Convolutional. Input = 32x32x3 (rgb). Output = 28x28x6. shape (5,5,1,6)->(5,5,3,6)\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    \n    # Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    \n    # Layer 3: Convolutional. Output = 1x1x400.\n    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 400), mean = mu, stddev = sigma))\n    conv3_b = tf.Variable(tf.zeros(400))\n    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='VALID') + conv3_b\n    \n    # Activation.\n    conv3 = tf.nn.relu(conv3)\n    \n    # Flatten. Input = 5x5x16. Output = 400.\n    fc00   = flatten(conv2)\n    # Flatten. Input = 1x1x400. Output = 400.\n    fc01   = flatten(conv3)\n    \n    # Concatenate the two convs, Output = 800\n    fc0 = tf.concat_v2([fc01, fc00], 1)\n    \n    fc0 = tf.nn.dropout(fc0, keep_prob)\n    \n    # Layer 4: Fully Connected. Input = 800. Output = 400.\n    fc1_W  = tf.Variable(tf.truncated_normal(shape=(800, 400), mean = mu, stddev = sigma))\n    fc1_b  = tf.Variable(tf.zeros(400))\n    fc1    = tf.matmul(fc0, fc1_W) + fc1_b\n    \n    # Activation.\n    fc1    = tf.nn.relu(fc1)\n    \n    # Dropout, to prevent overfitting\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    \n    # Layer 5: Fully Connected. Input = 400. Output = n_classes, which is 43.\n    fc2_W  = tf.Variable(tf.truncated_normal(shape=(400, n_classes), mean = mu, stddev = sigma))\n    fc2_b  = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc1, fc2_W) + fc2_b\n    \n    return logits\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\naccuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsaver = tf.train.Saver()\n\ndef evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        batch_x = np.expand_dims(batch_x, axis=3)\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from sklearn.model_selection import StratifiedShuffleSplit\n\ndef nparray_to_list(nparray):\n    return [x for x in nparray]\n\n# Make sure the input and output formats are the same for X and y\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    X_np = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(X_np, y_np):\n        X_train, X_valid = nparray_to_list(X_np[train_index]), nparray_to_list(X_np[valid_index])\n        y_train, y_valid = nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index])\n    train = X_train, y_train\n    valid = X_valid, y_valid\n    return train, valid\n```\n\nUnused functions:\n- StratifiedKFold'}, {'reason': 'stop', 'result': 'def featurize_state(state):\n    """\n    Returns the featurized representation for a state.\n    """\n    scaled = scaler.transform([state])\n    featurized = featurizer.transform(scaled)\n    return featurized[0]```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class PolicyEstimator():\n    """\n    Policy Function approximator. \n    """\n    \n    def __init__(self, learning_rate=0.01, scope="policy_estimator"):\n        with tf.variable_scope(scope):\n            self.state = tf.placeholder(tf.float32, [400], "state")\n            self.target = tf.placeholder(dtype=tf.float32, name="target")\n\n            # This is just linear classifier\n            self.mu = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n            self.mu = tf.squeeze(self.mu)\n            \n            self.sigma = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n            \n            self.sigma = tf.squeeze(self.sigma)\n            self.sigma = tf.nn.softplus(self.sigma) + 1e-5\n            self.normal_dist = tf.contrib.distributions.Normal(self.mu, self.sigma)\n            self.action = self.normal_dist._sample_n(1)\n            self.action = tf.clip_by_value(self.action, env.action_space.low[0], env.action_space.high[0])\n\n            # Loss and train op\n            self.loss = -self.normal_dist.log_prob(self.action) * self.target\n            # Add cross entropy cost to encourage exploration\n            self.loss -= 1e-1 * self.normal_dist.entropy()\n            \n            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            self.train_op = self.optimizer.minimize(\n                self.loss, global_step=tf.contrib.framework.get_global_step())\n    \n    def predict(self, state, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        return sess.run(self.action, { self.state: state })\n\n    def update(self, state, target, action, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        feed_dict = { self.state: state, self.target: target, self.action: action  }\n        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n        return loss\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'class ValueEstimator():\n    """\n    Value Function approximator. \n    """\n    \n    def __init__(self, learning_rate=0.1, scope="value_estimator"):\n        with tf.variable_scope(scope):\n            self.state = tf.placeholder(tf.float32, [400], "state")\n            self.target = tf.placeholder(dtype=tf.float32, name="target")\n\n            # This is just linear classifier\n            self.output_layer = tf.contrib.layers.fully_connected(\n                inputs=tf.expand_dims(self.state, 0),\n                num_outputs=1,\n                activation_fn=None,\n                weights_initializer=tf.zeros_initializer)\n\n            self.value_estimate = tf.squeeze(self.output_layer)\n            self.loss = tf.squared_difference(self.value_estimate, self.target)\n\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            self.train_op = self.optimizer.minimize(\n                self.loss, global_step=tf.contrib.framework.get_global_step())        \n    \n    def predict(self, state, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        return sess.run(self.value_estimate, { self.state: state })\n\n    def update(self, state, target, sess=None):\n        sess = sess or tf.get_default_session()\n        state = featurize_state(state)\n        feed_dict = { self.state: state, self.target: target }\n        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n        return loss\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n    """\n    Actor Critic Algorithm. Optimizes the policy \n    function approximator using policy gradient.\n    \n    Args:\n        env: OpenAI environment.\n        estimator_policy: Policy Function to be optimized \n        estimator_value: Value function approximator, used as a critic\n        num_episodes: Number of episodes to run for\n        discount_factor: Time-discount factor\n    \n    Returns:\n        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n    """\n\n    # Keeps track of useful statistics\n    stats = plotting.EpisodeStats(\n        episode_lengths=np.zeros(num_episodes),\n        episode_rewards=np.zeros(num_episodes))    \n    \n    Transition = collections.namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])\n    \n    for i_episode in range(num_episodes):\n        # Reset the environment and pick the fisrst action\n        state = env.reset()\n        \n        episode = []\n        \n        # One step in the environment\n        for t in itertools.count():\n            \n            # env.render()\n            \n            # Take a step\n            action = estimator_policy.predict(state)\n            next_state, reward, done, _ = env.step(action)\n            \n            # Keep track of the transition\n            episode.append(Transition(\n              state=state, action=action, reward=reward, next_state=next_state, done=done))\n            \n            # Update statistics\n            stats.episode_rewards[i_episode] += reward\n            stats.episode_lengths[i_episode] = t\n            \n            # Calculate TD Target\n            value_next = estimator_value.predict(next_state)\n            td_target = reward + discount_factor * value_next\n            td_error = td_target - estimator_value.predict(state)\n            \n            # Update the value estimator\n            estimator_value.update(state, td_target)\n            \n            # Update the policy estimator\n            # using the td error as our advantage estimate\n            estimator_policy.update(state, td_error, action)\n            \n            # Print out which step we\'re on, useful for debugging.\n            print("\\rStep {} @ Episode {}/{} ({})".format(\n                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end="")\n\n            if done:\n                break\n                \n            state = next_state\n    \n    return stats\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '# Delete this cell to re-enable tracebacks\nimport sys\nipython = get_ipython()\n\ndef hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n                   exception_only=False, running_compiled_code=False):\n    etype, value, tb = sys.exc_info()\n    value.__cause__ = None  # suppress chained exceptions\n    return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))```\n\nUnused functions:\n- None'}, {'reason': 'length', 'result': 'import math\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    (assuming your grayscaled image is called \'gray\')\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    # Or use BGR2GRAY if you read an image with cv2.imread()\n    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \ndef canny(img, low_threshold, high_threshold, L2gradient):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold, L2gradient = L2gradient)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\n\n    \ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), \n                            minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img, lines\n\n# Python 3 has support for cool math symbols.\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ) \n\ndef extend_lines(img, lines, angle = np.pi / 4, angle_thresh = np.pi / 8):\n    \'\'\'\n    Draws extended lines on the original image.\n    \n    Returns the combined image.\n    Args:\n    - img: Should be the original image used for lane line detection\n    - lines: The second output of hough_lines; a collection of line-end vertices    \n    - angle: Approximate angle (in radians) that left lines are expected to form \n    with the horizontal. For right lines, the function uses pi - angle (assumed\n    symetry).\n    - angle_thresh: Used to filter out lines whose angle to the horizontal is too\n    far from either angle or pi - angle. Only left lines (resp. right lines) within \n    angle +/- angle_thresh (resp. pi - angle +/- angle_thresh) are retained. Recommended\n    values are pi/8 to pi/20. Extreme values will cause problems.\n    \'\'\'\n    \n    left_lines = list()\n    right_lines = list()\n    middles_left = list()\n    middles_right = list()\n    slopes_left = list()\n    slopes_right = list()\n    y_min_left = img.shape[0]\n    y_min_right = img.shape[0]\n    \n     \n    ## Collect left and right lines into separate lists based on slope\n    for line in lines:  \n        slope = (line[0, 3] - line[0, 1]) / (line[0, 2] - line[0, 0])\n        # Filter and classify lines as left or right based on their slope:\n        if (np.tan(np. pi - angle - angle_thresh)) < slope < (np.tan(np.pi - angle + angle_thresh)):\n            left_lines.append(line)  # Add line to our collection of left lines\n            slopes_left.append(slope)  # Also collect slopes of left lines\n            if min((line[0, 3], line[0, 1])) < y_min_left:\n                y_min_left = min((line[0, 3], line[0, 1]))  # and minimum y values of left lanes\n        elif (np.tan(angle - angle_thresh)) < slope < (np.tan(angle + angle_thresh)):\n            right_lines.append(line)  # Same for right lanes\n            slopes_right.append(slope)  # and their slopes\n            if min((line[0, 3], line[0, 1])) < y_min_right:\n                y_min_right = min((line[0, 3], line[0, 1])) # and their min y values\n       \n    ## Calculate middle positions of left and right lines\n    for line in left_lines:\n        middles_left.append(\n            (min(line[0, 2], line[0, 0]) + np.abs(line[0, 2] - line[0, 0]) / 2,\n             min(line[0, 3], line[0, 1]) + np.abs(line[0, 3] - line[0, 1]) / 2))\n    for line in right_lines:\n        middles_right.append(\n            (min(line[0, 2], line[0, 0]) + np.abs(line[0, 2] - line[0, 0]) / 2,\n             min(line[0, 3], line[0, 1]) + np.abs(line[0, 3] - line[0, 1]) / 2))\n    \n    ## Convert to np.array and calculate medians of middle positions (less sensitive to outliers\n    ## than the mean):\n    middles_left = np.array(middles_left)\n    middles_right = np.array(middles_right)\n    median_left = np.zeros(2)\n    median_right = np.zeros(2)\n    for j in range(2):\n        median_left[j] = np.median(middles_left[:, j])\n        median_right[j] = np.median(middles_right[:, j])\n    \n    ## Calculate parameters of the left and right average lines:\n    slopes_left = np.array(slopes_left)\n    slopes_right = np.array(slopes_right)\n    slope_left = np.median(slopes_left'}, {'reason': 'stop', 'result': "```python\ndef lane_detection_ppline(image, \n                          k_size = 3,\n                          vertex_ratio_h = .45,\n                          vertex_ratio_v = 0.60,\n                          low_thresh = 50,\n                          high_thresh = 200,\n                          L2gradient = False,\n                          rho = 2,\n                          theta = 1 * np.pi / 180.,\n                          min_votes = 15,\n                          min_line_len = 40,\n                          max_line_gap = 20,\n                          angle = 3 * np.pi / 16,\n                          angle_thresh = np.pi / 16,\n                          debug = False\n                         ):\n    '''\n    Takes an image and parameters and applies the lane detection pipeline.\n    Returns an image combining the original and the extended lines detected\n    by the algorithm.\n     - debug: Whether or not to display the images after each step of the process, for\n     debugging or tuning purposes.\n    '''\n    max_y, max_x = image.shape[:2]  # Grab image dimensions\n    # Define 4-sided polygon in relation to image dimensions:\n    roi = np.array([[(0, max_y), \n                  (round(max_x * vertex_ratio_h), round(max_y * vertex_ratio_v)),\n                  (round(max_x * (1 - vertex_ratio_h)), round(max_y * vertex_ratio_v)),\n                  (max_x, max_y)]])\n    if debug:\n        plt.subplot(4, 2, 1)\n        plt.imshow(image)\n    \n    ## 1. Conversion to grayscale ##\n    gray = grayscale(image)\n    if debug:\n        plt.subplot(4, 2, 2)\n        plt.imshow(gray, cmap = 'gray')\n\n    ## 2. Gaussian smoothing with custom parameters ##\n    blur_gray = gaussian_blur(gray, k_size)\n    if debug:\n        plt.subplot(4, 2, 3)\n        plt.imshow(blur_gray, cmap = 'gray')\n    \n    ## 3. Canny filter ##\n    edges = canny(blur_gray, low_thresh, high_thresh, L2gradient = L2gradient)\n    if debug:\n        plt.subplot(4, 2, 4)\n        plt.imshow(edges)\n    \n    ## 4. Mask for Region of interest ##\n    masked_edges = region_of_interest(edges, roi)\n    if debug:\n        plt.subplot(4, 2, 5)\n        plt.imshow(masked_edges)\n    \n    ## 5. Hough transform ##\n    line_img, lines = hough_lines(masked_edges, rho, theta, min_votes, min_line_len, max_line_gap)\n    if debug:\n        plt.subplot(4, 2, 6)\n        plt.imshow(line_img)\n    \n    ## 6. Extend lines and create the combined image ##\n    try:\n        combined = extend_lines(image, lines, angle = angle, angle_thresh = angle_thresh)\n        if debug:\n            plt.subplot(4, 2, 7)\n            plt.imshow(combined)\n    except IndexError:\n        print('Error. Try relaxing your angle parameters a litte.')\n    return combined\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': '# Set up parameters:    \nk_size = 3\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n\ndef process_image(image):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline(image, \n                                   k_size = k_size,\n                                   low_thresh = low_thresh,\n                                   high_thresh = high_thresh,\n                                   L2gradient = L2gradient,\n                                   rho = rho,\n                                   theta = theta,\n                                   min_votes = min_votes,\n                                   min_line_len = min_line_len,\n                                   max_line_gap = max_line_gap,\n                                   angle = angle,\n                                   angle_thresh = angle_threshold,\n                                   debug = False)\n    return result\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def process_image(image, \n                  k_size = k_size,\n                  low_thresh = low_thresh,\n                  high_thresh = high_thresh,\n                  L2gradient = L2gradient,\n                  rho = rho,\n                  theta = theta,\n                  min_votes = min_votes,\n                  min_line_len = min_line_len,\n                  max_line_gap = max_line_gap,\n                  angle = angle,\n                  angle_thresh = angle_threshold,\n                  debug = False):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline(image, \n                                   k_size = k_size,\n                                   low_thresh = low_thresh,\n                                   high_thresh = high_thresh,\n                                   L2gradient = L2gradient,\n                                   rho = rho,\n                                   theta = theta,\n                                   min_votes = min_votes,\n                                   min_line_len = min_line_len,\n                                   max_line_gap = max_line_gap,\n                                   angle = angle,\n                                   angle_thresh = angle_threshold,\n                                   debug = debug)\n    return result\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def rbg_to_hls(img):\n    ''' \n    Takes an RGB image and converts it to HLS.\n    Returns the converted image (3 channels)\n    '''\n    hls_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    return hls_image\n\ndef merge_channels(images):\n    '''\n    Merge images from three different channels\n     - images: a list of 3 images, each in a channel\n    '''\n    merged = weighted_img(images[0], images[1], α = .5, β = .5, λ = 0.)        \n    merged = weighted_img(merged, images[2], α = 1., β = .5, λ = 0.)\n    return merged\n\n\ndef lane_detection_ppline_3_channels(image, \n                                     k_size = 3,\n                                     vertex_ratio_h = .45,\n                                     vertex_ratio_v = 0.60,\n                                     low_thresh = 50,\n                                     high_thresh = 200,\n                                     L2gradient = False,\n                                     rho = 2,\n                                     theta = 1 * np.pi / 180.,\n                                     min_votes = 15,\n                                     min_line_len = 40,\n                                     max_line_gap = 20,\n                                     angle = 3 * np.pi / 16,\n                                     angle_thresh = np.pi / 16,\n                                     debug = False):\n    '''\n    Takes an image and parameters and applies the lane detection pipeline.\n    Returns an image combining the original and the extended lines detected\n    by the algorithm.\n     - debug: Whether or not to display the images after each step of the process, for\n     debugging or tuning purposes.\n    '''\n    max_y, max_x = image.shape[:2]  # Grab image dimensions\n    # Define 4-sided polygon in relation to image dimensions:\n    roi = np.array([[(0, max_y), \n                  (round(max_x * vertex_ratio_h), round(max_y * vertex_ratio_v)),\n                  (round(max_x * (1 - vertex_ratio_h)), round(max_y * vertex_ratio_v)),\n                  (max_x, max_y)]])\n    if debug:\n        plt.subplot(5, 3, 1)\n        plt.imshow(image)\n    \n    ## 1. Gaussian smoothing with custom parameters ##\n    blur = gaussian_blur(image, k_size)\n    if debug:\n        plt.subplot(5, 3, 2)\n        plt.imshow(blur)\n    \n    ## 2. Conversion to HLS ##\n    hls = rbg_to_hls(blur)\n    if debug:\n        plt.subplot(5, 3, 3)\n        plt.imshow(hls)\n    \n    ## 3.1. Canny filter, Hue ##\n    edges_list = []\n    for chan in range(0, 3):\n        edges_list.append(canny(hls[:, :, chan], low_thresh, high_thresh, L2gradient = L2gradient))\n        if debug:\n            plt.subplot(5, 3, chan + 4)\n            plt.imshow(edges_list[chan])\n    \n    ## 4. Mask for Region of interest ##\n    masked_edges_list = []\n    for chan in range(0, 3):\n        masked_edges_list.append(region_of_interest(edges_list[chan], roi))\n        if debug:\n            plt.subplot(5, 3, chan + 7)\n            plt.imshow(masked_edges_list[chan])\n    \n    ## 5. Hough transform ##\n    lines_list = []\n    for chan in range(0, 3):\n        # Note that each element of lines_list is a tuple of (image, lines collection):\n        lines_list.append(hough_lines(masked_edges_list[chan], rho, theta, \n                                      min_votes, min_line_len, max_line_gap))\n        if debug:\n            plt.subplot(5, 3, chan + 10)\n            plt.imshow(lines_list[chan][0])\n    \n    ## 6. Combine the 3 outputs from the Hough tranform into a single images (for debugging)\n    # and a single collection:\n    lines = np.zeros((1, 1, 4))\n    for chan in range(0, 3):\n        lines = np.concatenate((lines, lines_list[chan][1]), axis = 0)\n    \n    if debug:\n        hls_lines_image = merge_channels([lines_list[0][0], lines_list[1][0], lines_list[2][0]])\n        plt.subplot(5, 3, 13)\n        plt.imshow(hls_lines_image)\n    \n    ## 7. Extend lines and create the combined image ##\n    try:\n        combined = extend_lines(image, lines, angle = angle, angle_thresh = angle_thresh)\n        if debug:\n            plt.subplot(5, 3, 14)\n            plt.imshow(combined)\n    except IndexError:\n        print('Error. Try relaxing your angle parameters a litte.')\n    return combined\n\n\ndef process_image_3_channels(image, \n                  k_size = 3,\n                  vertex_ratio_h = .45,\n                  vertex_ratio_v = 0.60,\n                  low_thresh = 50,\n                  high_thresh = 200,\n                  L2gradient = False,\n                  rho = 2,\n                  theta = 1 * np.pi / 180.,\n                  min_votes = 15,\n                  min_line_len = 40,\n                  max_line_gap = 20,\n                  angle = 3 * np.pi / 16,\n                  angle_thresh = np.pi / 16,\n                  debug = False):\n    # NOTE: The output you return should be a color image (3 channel) for processing video below\n    # TODO: put your pipeline here,\n    # you should return the final output (image with lines are drawn on lanes)\n    result = lane_detection_ppline_3_channels(image, \n                                   k_size = k_size,\n                                   low_thresh = low_thresh,\n                                   high_thresh = high_thresh,\n                                   L2gradient = L2gradient,\n                                   rho = rho,\n                                   theta = theta,\n                                   min_votes = min_votes,\n                                   min_line_len = min_line_len,\n                                   max_line_gap = max_line_gap,\n                                   angle = angle,\n                                   angle_thresh = angle_threshold,\n                                   debug = debug)\n    return result\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': '```python\ndef true_mean_function(x):\n    return np.sin(2*pi*(x+1))\n\ndef add_noise(y, sigma):\n    return y + sigma*np.random.randn(len(y))\n```\n\nUnused functions:\n- generate_t'}, {'reason': 'stop', 'result': 'def k_n_m(xn, xm, thetas):\n    k = thetas[0] * np.exp(-thetas[1]/2 * np.linalg.norm(xn-xm)**2) + thetas[2] + thetas[3]*xn * xm\n    \n    return k\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'import numpy as np\n\ndef computeK(x1, x2, thetas):\n    K = []\n    for i in range(len(x1)):\n        innerList = []\n        for j in range(len(x2)):\n            innerList.append(k_n_m(x1[i], x2[j], thetas))\n        K.append(innerList)\n    return np.array(K)```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C=None):\n    \n    if C is None:\n        K = computeK(x_train, x_train, theta)\n        C = K + (1/beta)*np.eye(len(K))\n    \n    C_inv = np.linalg.inv(C)\n    \n    mean_test = np.zeros(len(x_test))\n    covar_test = np.zeros((len(x_test), len(x_test)))\n    \n    for i, x_t in enumerate(x_test):\n        # kernel evaluated between all training points and the new x_test \n        k = np.array([k_n_m(x_n, x_t, theta) for x_n in x_train])   \n        # kernel evaluated in the x_test with itself added of variance\n        c = k_n_m(x_t, x_t, theta) + 1/beta        \n        k_dot_C = np.matmul(k.T, C_inv) \n        \n        mean_test[i] = np.matmul(k_dot_C,t_train)\n        covar_test[i][i] = c - np.matmul(k_dot_C, k)\n\n    return mean_test, covar_test, C\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def gp_log_likelihood(x_train, t_train, theta, beta, C=None, invC=None):\n    \n    if C is None:\n        K = computeK(x_train, x_train, theta)\n        C = K + (1/beta)*np.eye(len(K))\n    \n    if invC is None:\n        invC = np.linalg.inv(C)\n    \n    lp = np.log(1/np.sqrt((np.linalg.det(2 * np.pi * C))) * np.exp((-1/2) * t_train.T.dot(invC).dot(t_train)))\n    \n    return lp, C, invC\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def gp_plot( x_test, y_test, mean_test, covar_test, x_train, t_train, theta, beta ):\n    # x_test: \n    # y_test:    the true function at x_test\n    # mean_test: predictive mean at x_test\n    # var_test:  predictive covariance at x_test \n    # t_train:   the training values\n    # theta:     the kernel parameters\n    # beta:      the precision (known)\n    \n    # the reason for the manipulation is to allow plots separating model and data stddevs.\n    std_total = np.sqrt(np.diag(covar_test))       # includes all uncertainty, model and target noise \n    std_model = np.sqrt(std_total**2 - 1.0/beta) # remove data noise to get model uncertainty in stddev\n    std_combo = std_model + np.sqrt(1.0/beta)    # add stddev (note: not the same as full)\n    \n    plt.plot(x_test, y_test, 'b', lw=3)\n    plt.plot(x_test, mean_test, 'k--', lw=2)\n    plt.fill_between(x_test, mean_test+2*std_combo,mean_test-2*std_combo, color='k', alpha=0.25)\n    plt.fill_between(x_test, mean_test+2*std_model,mean_test-2*std_model, color='r', alpha=0.25)\n    plt.plot(x_train, t_train, 'ro', ms=10)"}, {'reason': 'stop', 'result': 'def create_X_and_t(X1, X2):\n    X = np.concatenate((X1, X2))\n    t = np.concatenate((-np.ones(len(X1)), np.ones(len(X2))))\n    \n    return X, t\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def computeK(X):\n    K = X.dot(X.T).astype(double)\n    return K\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "import cvxopt\n\ndef compute_multipliers(X, t):\n    N = X.shape[0]\n    K = computeK(t.reshape(-1,1) * X)\n    \n    P = cvxopt.matrix(K)\n    q = cvxopt.matrix(-np.ones(N))\n    G = cvxopt.matrix(-np.eye(N))\n    h = cvxopt.matrix(np.zeros(N))\n    A = cvxopt.matrix(t).T\n    b = cvxopt.matrix(0.0)\n\n    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n    a = np.array(sol['x'])\n\n    return a\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'def logprob(predictions, labels):\n  """Log-probability of the true labels in a predicted batch."""\n  predictions[predictions < 1e-10] = 1e-10\n  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\ndef sample(prediction):\n  """Turn a (column) prediction into 1-hot encoded samples."""\n  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n  p[0, sample_distribution(prediction[0])] = 1.0\n  return p\n\ndef random_distribution():\n  """Generate a random column of probabilities."""\n  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n  return b/np.sum(b, 1)[:,None]```\n\nUnused functions:\n- sample_distribution'}, {'reason': 'stop', 'result': 'def get_mjdmax_BessellV(sn):\n    v = sn.lcfit.spline["BessellV"]    \n    mjd_spline = np.arange(np.nanmin(sn.phot.data["BessellV"]["MJD"]), \n                 np.nanmax(sn.phot.data["BessellV"]["MJD"]),\n                 0.001)\n    w = np.where(v(mjd_spline) == np.nanmax(v(mjd_spline)))\n\n    mjdmax = mjd_spline[w]\n    \n    return mjdmax\n```\n\nUnused functions:\n- convert_column_string_encoding'}, {'reason': 'stop', 'result': 'from scipy.integrate import simps\n\ndef calc_spectrum_filter_flux(filter_name, SpecClass):\n    filter_object = pcc.functions.load_filter("/Users/berto/Code/CoCo/data/filters/" + filter_name + ".dat")\n    filter_object.resample_response(new_wavelength = SpecClass.wavelength)\n    filter_area = simps(filter_object.throughput, filter_object.wavelength)\n    \n    transmitted_spec = filter_object.throughput * SpecClass.flux\n\n    integrated_flux = simps(transmitted_spec, SpecClass.wavelength)\n    \n    return  integrated_flux/filter_area\n\ndef calc_specphot(sn, filtername):\n\n    specphot = np.array([])\n    specepoch = np.array([])\n\n    for spec in sn.mangledspec:\n        specphot = np.append(specphot, calc_spectrum_filter_flux(filtername, sn.mangledspec[spec]))\n        specepoch = np.append(specepoch, sn.mangledspec[spec].mjd_obs)\n    \n    return specepoch, specphot\n\ndef compare_phot_specphot(sn, filtername):\n    """"""\n    specepoch, specphot =  calc_specphot(sn, filtername)\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    ax.scatter(specepoch, specphot, label = "specphot")\n    ax.scatter(sn.phot.data[filtername]["MJD"], sn.phot.data[filtername]["flux"], label = filtername)\n\n    ax.set_ylim(0, 1.05 * np.nanmax(np.append(sn.phot.data[filtername]["flux"], specphot)))\n    ax.legend()\n#     plt.show()```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'from astropy.table import Table\ndef load_coords(filename = "sncoordinates.list"):\n    path = os.path.abspath(os.path.join(pcc.__path__[0], os.path.pardir, filename))\n    coordtable = Table.read(path, format = \'ascii.commented_header\')\n    return coordtable\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'tags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\n# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\nfrom collections import defaultdict\n\nmfc_table = defaultdict(list)\n\nclass MFCTagger:\n    # NOTE: You should not need to modify this class or any of its methods\n    missing = FakeState(name="<MISSING>")\n    \n    def __init__(self, table):\n        self.table = defaultdict(lambda: MFCTagger.missing)\n        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n        \n    def viterbi(self, seq):\n        """This method simplifies predictions by matching the Pomegranate viterbi() interface"""\n        return 0., list(enumerate(["<start>"] + [self.table[w] for w in seq] + ["<end>"]))\n\n\n# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n# the same as the emission probabilities) and use it to fill the mfc_table\n\nword_counts = pair_counts(words, tags)\n\nfor key, val in word_counts.items():\n    mfc_table[key] = val.most_common(1)[0][0]\n\n\n# DO NOT MODIFY BELOW THIS LINE\nmfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance\n\nassert len(mfc_table) == len(data.training_set.vocab), ""\nassert all(k in data.training_set.vocab for k in mfc_table.keys()), ""\nassert sum(int(k not in mfc_table) for k in data.testing_set.vocab) == 5521, ""\nHTML(\'<div class="alert alert-block alert-success">Your MFC tagger has all the correct words!</div>\')```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': 'def simplify_decoding(X, model):\n    """X should be a 1-D sequence of observations for the model to predict"""\n    _, state_path = model.viterbi(replace_unknown(X))\n    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions```\n\nUnused functions:\n- replace_unknown'}, {'reason': 'stop', 'result': 'def accuracy(X, Y, model):\n    """Calculate the prediction accuracy by using the model to decode each sequence\n    in the input X and comparing the prediction with the true labels in Y.\n    \n    The X should be an array whose first dimension is the number of sentences to test,\n    and each element of the array should be an iterable of the words in the sequence.\n    The arrays X and Y should have the exact same shape.\n    \n    X = [("See", "Spot", "run"), ("Run", "Spot", "run", "fast"), ...]\n    Y = [(), (), ...]\n    """\n    correct = total_predictions = 0\n    for observations, actual_tags in zip(X, Y):\n        \n        # The model.viterbi call in simplify_decoding will return None if the HMM\n        # raises an error (for example, if a test sentence contains a word that\n        # is out of vocabulary for the training set). Any exception counts the\n        # full sentence as an error (which makes this a conservative estimate).\n        try:\n            most_likely_tags = simplify_decoding(observations, model)\n            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n        except:\n            pass\n        total_predictions += len(observations)\n    return correct / total_predictions\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "# Authors: Eustache Diemert <eustache@diemert.fr>\n#          @FedericoV <https://github.com/FedericoV/>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom glob import glob\nimport itertools\nimport os.path\nimport re\nimport tarfile\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nfrom sklearn.externals.six.moves import html_parser\nfrom sklearn.externals.six.moves import urllib\nfrom sklearn.datasets import get_data_home\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import MultinomialNB\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return '__file__' in globals()```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': 'class ReutersParser(html_parser.HTMLParser):\n    """Utility class to parse a SGML file and yield documents one at a time."""\n\n    def __init__(self, encoding=\'latin-1\'):\n        html_parser.HTMLParser.__init__(self)\n        self._reset()\n        self.encoding = encoding\n\n    def handle_starttag(self, tag, attrs):\n        method = \'start_\' + tag\n        getattr(self, method, lambda x: None)(attrs)\n\n    def handle_endtag(self, tag):\n        method = \'end_\' + tag\n        getattr(self, method, lambda: None)()\n\n    def _reset(self):\n        self.in_title = 0\n        self.in_body = 0\n        self.in_topics = 0\n        self.in_topic_d = 0\n        self.title = ""\n        self.body = ""\n        self.topics = []\n        self.topic_d = ""\n\n    def parse(self, fd):\n        self.docs = []\n        for chunk in fd:\n            self.feed(chunk.decode(self.encoding))\n            for doc in self.docs:\n                yield doc\n            self.docs = []\n        self.close()\n\n    def handle_data(self, data):\n        if self.in_body:\n            self.body += data\n        elif self.in_title:\n            self.title += data\n        elif self.in_topic_d:\n            self.topic_d += data\n\n    def end_reuters(self):\n        self.body = re.sub(r\'\\s+\', r\' \', self.body)\n        self.docs.append({\'title\': self.title,\n                          \'body\': self.body,\n                          \'topics\': self.topics})\n        self._reset()\n\n    def start_title(self, attributes):\n        self.in_title = 1\n\n    def end_title(self):\n        self.in_title = 0\n\n    def start_body(self, attributes):\n        self.in_body = 1\n\n    def end_body(self):\n        self.in_body = 0\n\n    def start_topics(self, attributes):\n        self.in_topics = 1\n\n    def end_topics(self):\n        self.in_topics = 0\n\n    def start_d(self, attributes):\n        self.in_topic_d = 1\n\n    def end_d(self):\n        self.in_topic_d = 0\n        self.topics.append(self.topic_d)\n        self.topic_d = ""\n\n\ndef stream_reuters_documents(data_path=None):\n    """Iterate over documents of the Reuters dataset.\n\n    The Reuters archive will automatically be downloaded and uncompressed if\n    the `data_path` directory does not exist.\n\n    Documents are represented as dictionaries with \'body\' (str),\n    \'title\' (str), \'topics\' (list(str)) keys.\n\n    """\n\n    DOWNLOAD_URL = (\'http://archive.ics.uci.edu/ml/machine-learning-databases/\'\n                    \'reuters21578-mld/reuters21578.tar.gz\')\n    ARCHIVE_FILENAME = \'reuters21578.tar.gz\'\n\n    if data_path is None:\n        data_path = os.path.join(get_data_home(), "reuters")\n    if not os.path.exists(data_path):\n        """Download the dataset."""\n        print("downloading dataset (once and for all) into %s" %\n              data_path)\n        os.mkdir(data_path)\n\n        def progress(blocknum, bs, size):\n            total_sz_mb = \'%.2f MB\' % (size / 1e6)\n            current_sz_mb = \'%.2f MB\' % ((blocknum * bs) / 1e6)\n            if _not_in_sphinx():\n                print(\'\\rdownloaded %s / %s\' % (current_sz_mb, total_sz_mb),\n                      end=\'\')\n\n        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n                                   reporthook=progress)\n        if _not_in_sphinx():\n            print(\'\\r\', end=\'\')\n        print("untarring Reuters dataset...")\n        tarfile.open(archive_path, \'r:gz\').extractall(data_path)\n        print("done.")\n\n    parser = ReutersParser()\n    for filename in glob(os.path.join(data_path, "*.sgm")):\n        for doc in parser.parse(open(filename, \'rb\')):\n            yield doc\n```\n\nUnused functions:\n- start_reuters'}, {'reason': 'stop', 'result': '# Reuters-21578 dataset downloader and parser\n#\n# Author:  Eustache Diemert <eustache@diemert.fr> \n# http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html\n#\n# Modified by @herrfz, get pandas DataFrame from the orig SGML\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport re\nimport os.path\nimport fnmatch\nimport sgmllib\nimport urllib\nimport tarfile\nimport itertools\nfrom pandas import DataFrame\n\n###############################################################################\n# Reuters Dataset related routines\n###############################################################################\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return \'__file__\' in globals()\n\n\nclass ReutersParser(sgmllib.SGMLParser):\n    """Utility class to parse a SGML file and yield documents one at a time."""\n    def __init__(self, verbose=0):\n        sgmllib.SGMLParser.__init__(self, verbose)\n        self._reset()\n\n    def _reset(self):\n        self.in_title = 0\n        self.in_body = 0\n        self.in_topics = 0\n        self.in_topic_d = 0\n        self.title = ""\n        self.body = ""\n        self.topics = []\n        self.topic_d = ""\n\n    def parse(self, fd):\n        self.docs = []\n        for chunk in fd:\n            self.feed(chunk)\n            for doc in self.docs:\n                yield doc\n            self.docs = []\n        self.close()\n\n    def handle_data(self, data):\n        if self.in_body:\n            self.body += data\n        elif self.in_title:\n            self.title += data\n        elif self.in_topic_d:\n            self.topic_d += data\n\n    def start_reuters(self, attributes):\n        pass\n\n    def end_reuters(self):\n        self.body = re.sub(r\'\\s+\', r\' \', self.body)\n        self.docs.append({\'title\': self.title,\n                          \'body\': self.body,\n                          \'topics\': self.topics})\n        self._reset()\n\n    def start_title(self, attributes):\n        self.in_title = 1\n\n    def end_title(self):\n        self.in_title = 0\n\n    def start_body(self, attributes):\n        self.in_body = 1\n\n    def end_body(self):\n        self.in_body = 0\n\n    def start_topics(self, attributes):\n        self.in_topics = 1\n\n    def end_topics(self):\n        self.in_topics = 0\n\n    def start_d(self, attributes):\n        self.in_topic_d = 1\n\n    def end_d(self):\n        self.in_topic_d = 0\n        self.topics.append(self.topic_d)\n        self.topic_d = ""\n\n\nclass ReutersStreamReader():\n\n    """Iterate over documents of the Reuters dataset.\n    The Reuters archive will automatically be downloaded and uncompressed if\n    the `data_path` directory does not exist.\n    Documents are represented as dictionaries with \'body\' (str),\n    \'title\' (str), \'topics\' (list(str)) keys.\n    """\n\n    DOWNLOAD_URL = (\'http://archive.ics.uci.edu/ml/machine-learning-databases/\'\n                    \'reuters21578-mld/reuters21578.tar.gz\')\n    ARCHIVE_FILENAME = \'reuters21578.tar.gz\'\n\n    def __init__(self, data_path):\n        self.data_path = data_path\n        if not os.path.exists(self.data_path):\n            self.download_dataset()\n\n    def download_dataset(self):\n        """Download the dataset."""\n        print("downloading dataset (once and for all) into %s" %\n              self.data_path)\n        os.mkdir(self.data_path)\n\n        def progress(blocknum, bs, size):\n            total_sz_mb = \'%.2f MB\' % (size / 1e6)\n            current_sz_mb = \'%.2f MB\' % ((blocknum * bs) / 1e6)\n            if _not_in_sphinx():\n                print(\'\\rdownloaded %s / %s\' % (current_sz_mb, total_sz_mb),\n                      end=\'\')\n        urllib.urlretrieve(self.DOWNLOAD_URL,\n                           filename=os.path.join(self.data_path,\n                                                 self.ARCHIVE_FILENAME),\n                           reporthook=progress)\n        if _not_in_sphinx():\n            print(\'\\r\', end=\'\')\n        print("untaring data ...")\n        tfile = tarfile.open(os.path.join(self.data_path,\n                                          self.ARCHIVE_FILENAME),\n                             \'r:gz\')\n        tfile.extractall(self.data_path)\n        print("done !")\n\n    def iterdocs(self):\n        """Iterate doc by doc, yield a dict."""\n        for root, _dirnames, filenames in os.walk(self.data_path):\n            for filename in fnmatch.filter(filenames, \'*.sgm\'):\n                path = os.path.join(root, filename)\n                parser = ReutersParser()\n                for doc in parser.parse(open(path)):\n                    yield doc\n\n\ndef get_minibatch(doc_iter, size):\n    """Extract a minibatch of examples, return a tuple X, y.\n    Note: size is before excluding invalid docs with no topics assigned.\n    """\n    data = [(\'{title}\\n\\n{body}\'.format(**doc), doc[\'topics\'])\n            for doc in itertools.islice(doc_iter, size)\n            if doc[\'topics\']]\n    if not len(data):\n        return DataFrame([])\n    else:\n        return DataFrame(data, columns=[\'text\', \'tags\'])\n```\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "import nltk\nfrom nltk.corpus import stopwords\n\ndef cleanupDoc(s):\n     stopset = set(stopwords.words('english'))\n     tokens = nltk.word_tokenize(s)\n     cleanup = [token.lower() for token in tokens if token.lower() not in stopset and  len(token)>2]\n     return cleanup```\n\nUnused functions:\n- sys"}, {'reason': 'stop', 'result': "def word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n    features = {\n        'word_lower':word.lower(),\n        'suffix3':word[-3:],\n        'suffix2':word[-2:],\n        'prefix1':word[:2],\n        'prefix2':word[:3],\n        'wordIsupper':word.isupper(),\n        'wordIstitle':word.istitle(),\n        'word.isdigit':word.isdigit(),\n        'postag':postag,\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.update({\n            '-1word.lower': word1.lower(),\n            '-1word.istitle': word1.istitle(),\n            '-1word.isupper': word1.isupper(),\n            '-1word.isdigit':word1.isdigit(),\n            '-1postag' :postag1,\n        })\n    else:\n        word1 = 'bos'\n        postag1 = 'bos'\n        features.update({\n            '-1word.lower': word1.lower(),\n            '-1word.istitle': word1.istitle(),\n            '-1word.isupper': word1.isupper(),\n            '-1postag' :postag1,\n        })\n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.update({\n            '1word.lower': word1.lower(),\n            '1word.istitle': word1.istitle(),\n            '1word.isupper': word1.isupper(),\n            '1postag' :postag1,\n        })\n    else:\n        word1 = 'eos'\n        postag1 = 'eos'\n        features.update({\n            '1word.lower': word1.lower(),\n            '1word.istitle': word1.istitle(),\n            '1word.isupper': word1.isupper(),\n            '1postag' :postag1,\n        })\n\n                \n    return features\n\n\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]"}, {'reason': 'stop', 'result': 'import sys\nfrom collections import defaultdict\nimport math\n\n"""\nCount n-gram frequencies in a CoNLL NER data file and write counts to\nstdout. \n"""\n\ndef simple_conll_corpus_iterator():\n    """\n    Get an iterator object over the corpus file. The elements of the\n    iterator contain (word, ne_tag) tuples. Blank lines, indicating\n    sentence boundaries return (None, None).\n    """\n    \n    for line in train_sents:\n        if line: # Nonempty line\n            # Extract information from line.\n            # Each line has the format\n            # word pos_tag phrase_tag ne_tag\n            for i in range(len(line)):\n                word = line[i][0]\n                ne_tag = line[i][2]\n                #print(word,ne_tag)\n                yield word, ne_tag\n        else: # Empty line\n            yield (None, None)                        \n      \n    \ndef sentence_iterator(corpus_iterator):\n    """\n    Return an iterator object that yields one sentence at a time.\n    Sentences are represented as lists of (word, ne_tag) tuples.\n    """  \n    \n    current_sentence = [] #Buffer for the current sentence\n    for l in corpus_iterator:        \n            if l==(None, None):\n                if current_sentence:  #Reached the end of a sentence\n                    yield current_sentence\n                    current_sentence = [] #Reset buffer\n                else: # Got empty input stream\n                    sys.stderr.write("WARNING: Got empty input file/stream.\\n")\n                    raise StopIteration\n            else:\n                current_sentence.append(l) #Add token to the buffer\n\n    if current_sentence: # If the last line was blank, we\'re done\n        yield current_sentence  #Otherwise when there is no more token\n                                # in the stream return the last sentence.\n\ndef get_ngrams(sent_iterator, n):\n    """\n    Get a generator that returns n-grams over the entire corpus,\n    respecting sentence boundaries and inserting boundary tokens.\n    Sent_iterator is a generator object whose elements are lists\n    of tokens.\n    """\n          \n    for sent in sent_iterator:\n         #Add boundary symbols to the sentence\n         w_boundary = (n-1) * [(None, "*")]\n         w_boundary.extend(sent)\n         w_boundary.append((None, "STOP"))\n         #Then extract n-grams\n         ngrams = (tuple(w_boundary[i:i+n]) for i in xrange(len(w_boundary)-n+1))\n         for n_gram in ngrams: #Return one n-gram at a time\n            yield n_gram'}, {'reason': 'stop', 'result': 'class Hmm(object):\n    """\n    Stores counts for n-grams and emissions. \n    """\n\n    def __init__(self, n=3):\n        assert n>=2, "Expecting n>=2."\n        self.n = n\n        self.emission_counts = defaultdict(int)\n        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n        self.all_states = set()\n        self.count_y = dict() \n        #[(\'O\', 0), (\'I-MISC\', 0), (\'I-PER\', 0), (\'I-ORG\', 0), (\'I-LOC\', 0), (\'B-MISC\', 0), (\'B-PER\', 0), (\'B-ORG\', 0), (\'B-LOC\', 0)])\n        self.count_xy = dict()\n        self.trigram_counts = dict()\n        self.bigram_counts = dict()\n        \n\n    def train(self):\n        """\n        Count n-gram frequencies and emission probabilities from a corpus file.\n        """\n        ngram_iterator = \\\n            get_ngrams(sentence_iterator(simple_conll_corpus_iterator()), self.n)\n\n        for ngram in ngram_iterator:\n            #Sanity check: n-gram we get from the corpus stream needs to have the right length\n            assert len(ngram) == self.n, "ngram in stream is %i, expected %i" % (len(ngram, self.n))\n\n            tagsonly = tuple([ne_tag for word, ne_tag in ngram]) #retrieve only the tags            \n            for i in xrange(2, self.n+1): #Count NE-tag 2-grams..n-grams\n                self.ngram_counts[i-1][tagsonly[-i:]] += 1\n            \n            if ngram[-1][0] is not None: # If this is not the last word in a sentence\n                self.ngram_counts[0][tagsonly[-1:]] += 1 # count 1-gram\n                self.emission_counts[ngram[-1]] += 1 # and emission frequencies\n\n            # Need to count a single n-1-gram of sentence start symbols per sentence\n            if ngram[-2][0] is None: # this is the first n-gram in a sentence\n                self.ngram_counts[self.n - 2][tuple((self.n - 1) * ["*"])] += 1\n    \n    def calc_count_xy_y(self):\n        \n        for word, ne_tag in self.emission_counts:\n            count = self.emission_counts[(word,ne_tag)]\n            label = ne_tag\n            if label in self.count_y:\n                self.count_y[label] = self.count_y[label]+int(float(count))\n            else:\n                self.count_y.update({label:count})\n\n            if word in self.count_xy:\n                self.count_xy[word].update({label : count})\n            else:\n                self.count_xy[word] = {label : count}\n\n    def calc_transition_count(self,printngrams=[1,2,3]):\n        \n        for n in printngrams:            \n                for ngram in self.ngram_counts[n-1]:\n                    count = self.ngram_counts[n-1][ngram]\n                    ngramstr = " ".join(ngram)\n                    if (n == 2):\n                        self.bigram_counts[ngramstr] = count\n                    elif (n == 3):\n                        self.trigram_counts[ngramstr] = count\n```\n\n\nUnused functions:\n- None'}, {'reason': 'stop', 'result': "import numpy as np\n\ndef word2embedding(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n            \n    if word in model.vocab:\n        worde = word\n    else:\n        worde = 'null'\n    \n    res = model[worde]\n     \n    if i > 0:\n        word1 = sent[i-1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n    \n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n  \n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        if word1 in model.vocab:\n            worde = word1\n        else:\n            worde = 'null'\n        res.extend(model[worde])\n    \n    else:\n        res.extend(model['null'])\n    \n    res.shape = (1,900)\n    return res\n        \ndef sent2embedding(sent):\n    rese = []\n    for  i in range(len(sent)):\n        line = word2embedding(sent,i)\n        rese.append(line)\n        \n    resee = np.vstack(rese)\n    return resee\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': "import cv2\nimport numpy as np\n\ndef transform_image(img,ang_range,shear_range,trans_range):\n    '''\n    This function transforms images to generate new images.\n    The function takes in following arguments,\n    1- Image\n    2- ang_range: Range of angles for rotation\n    3- shear_range: Range of values to apply affine transform to\n    4- trans_range: Range of values to apply translations over. \n    \n    A Random uniform distribution is used to generate different parameters for transformation\n    \n    '''\n    # Rotation\n\n    ang_rot = np.random.uniform(ang_range)-ang_range/2\n    rows,cols,ch = img.shape    \n    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n\n    # Translation\n    tr_x = trans_range*np.random.uniform()-trans_range/2\n    tr_y = trans_range*np.random.uniform()-trans_range/2\n    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n\n    # Shear\n    pts1 = np.float32([[5,5],[20,5],[5,20]])\n\n    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n\n    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n\n    shear_M = cv2.getAffineTransform(pts1,pts2)\n        \n    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n    img = cv2.warpAffine(img,shear_M,(cols,rows))\n    \n    return img\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "def get_augmented_images(X,y,img_ct_per_class = 2500):\n    '''\n    This function creates the necessary number of images so that each class\n    has the same number of images (img_ct_per_class)\n    '''\n    aug_images = []\n    aug_labels = []\n    for cl in range(0,n_classes):\n        y_class = np.where(y == cl)\n        idx_class = y_class[0]\n        X_class = X[idx_class]\n        tmp_x,tmp_y = get_augmented_images_by_class(X=X_class, cl=cl, final_length = img_ct_per_class)\n\n        aug_images.extend(tmp_x)\n        aug_labels.extend(tmp_y)\n        \n    X_aug = np.stack(aug_images, axis = 0)\n    Y_aug = np.stack(aug_labels, axis = 0)\n    \n    return (X_aug,Y_aug)\n    \n\ndef get_augmented_images_by_class(X,cl,final_length):\n    tmp_x = []\n    for im in X:\n        tmp_x.append(im)\n    original_length = X.shape[0]\n    tmp_y = [cl] * original_length\n    \n    add_length = final_length - original_length\n    for _ in range(add_length):\n        img_to_augment = X[np.random.randint(0,original_length)]\n        new_img = transform_image(img_to_augment, 20, 10, 5)\n        tmp_x.append(new_img)\n    tmp_y.extend([cl] * add_length)\n    return tmp_x, tmp_y\n```\n\nUnused functions:\n- None"}, {'reason': 'stop', 'result': '```python\nimport numpy as np\nimport tensorflow as tf\n\ndef get_conv_bounds(input_feature_map, output_feature_map, filter_height, filter_width):\n    fan_in = input_feature_map * filter_height * filter_width \n    fan_out = output_feature_map * filter_height * filter_width\n\n    W_bound = np.sqrt(2. / (fan_in + fan_out))\n    \n    return W_bound\n\ndef create_conv_weight(input_feature_map, output_feature_map, filter_height, filter_width, weight_name):\n    W_bound = get_conv_bounds(input_feature_map, output_feature_map, filter_height, filter_height)\n    return tf.Variable(tf.truncated_normal(\n            [filter_height, filter_width, input_feature_map, output_feature_map], stddev=W_bound), name=weight_name)```'}, {'reason': 'stop', 'result': "def conv2d(x, W, b, strides=1, is_relu=True):\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n    x += b\n    if is_relu:\n        return tf.nn.relu(x)\n    else:\n        return x\n\ndef maxpool2d(x, k=2):\n    return tf.nn.max_pool(\n        x,\n        ksize=[1, k, k, 1],\n        strides=[1, k, k, 1],\n        padding='SAME')"}, {'reason': 'stop', 'result': "def define_graph():\n    \n    tf_train_dataset = tf.placeholder(tf.float32, \n                          shape=[None, image_size, image_size, num_channels])\n    tf_train_labels = tf.placeholder(tf.float32, \n                          shape=[None, n_classes])\n    tf_train_labels_cls = tf.argmax(tf_train_labels, dimension=1)\n\n    tf_beta = tf.placeholder(tf.float32)\n    tf_keep_prob = tf.placeholder(tf.float32)\n    tf_learning_rate = tf.placeholder(tf.float32)\n\n    layer_width = {\n        'layer_1': 48,\n        'layer_2': 64,\n        'layer_3': 128,\n        'fc1': 512,\n        'fc2': 512\n    }\n\n    weights = {\n        'layer_1': create_conv_weight(input_feature_map = num_channels, output_feature_map = layer_width['layer_1']\n                                      , filter_height = 3, filter_width = 3, weight_name = 'W_L1'),\n\n        'layer_2': create_conv_weight(input_feature_map = layer_width['layer_1'], output_feature_map = layer_width['layer_2']\n                                      , filter_height = 3, filter_width = 3, weight_name = 'W_L2'),\n\n        'layer_3': create_conv_weight(input_feature_map = layer_width['layer_2'], output_feature_map = layer_width['layer_3']\n                                      , filter_height = 3, filter_width = 3, weight_name = 'W_L3'),\n\n        'fc1': create_fc_weight(input_feature_map = 2048, output_feature_map = layer_width['fc1']\n                                    , weight_name = 'W_F1'),\n\n        'fc2': create_fc_weight(input_feature_map = layer_width['fc1'], output_feature_map = layer_width['fc2']\n                                    , weight_name = 'W_F2'),\n\n        'out': create_fc_weight(input_feature_map = layer_width['fc2'], output_feature_map = n_classes\n                                    , weight_name = 'W_out')\n    }\n\n    biases = {\n        'layer_1': tf.Variable(tf.constant(0.0, shape=[layer_width['layer_1']]), name='b_L1'),\n        'layer_2': tf.Variable(tf.constant(0.0, shape=[layer_width['layer_2']]), name='b_L2'),\n        'layer_3': tf.Variable(tf.constant(0.0, shape=[layer_width['layer_3']]), name='b_L3'),\n        'fc1': tf.Variable(tf.constant(0.0, shape=[layer_width['fc1']]), name='b_F1'),\n        'fc2': tf.Variable(tf.constant(0.0, shape=[layer_width['fc2']]), name='b_F2'),\n        'out': tf.Variable(tf.constant(0.0, shape=[n_classes]), name='b_out')\n    }\n\n    # Layer 1\n    conv1 = conv2d(tf_train_dataset, weights['layer_1'], biases['layer_1'], is_relu = True)\n    conv1 = maxpool2d(conv1)\n    #conv1_drop = tf.nn.dropout(conv1, tf_keep_prob)\n    #print(conv1)\n\n    # Layer 2\n    conv2 = conv2d(conv1, weights['layer_2'], biases['layer_2'], is_relu = True)\n    conv2 = maxpool2d(conv2)\n    #conv2_drop = tf.nn.dropout(conv2, tf_keep_prob)\n    #print('conv2')\n    #print(conv2)\n\n    # Layer 3\n    conv3 = conv2d(conv2, weights['layer_3'], biases['layer_3'], is_relu = True)\n    conv3 = maxpool2d(conv3)\n    conv3_drop = tf.nn.dropout(conv3, tf_keep_prob)\n    #print('conv3')\n    #print(conv3)\n\n    # Flatten\n    flat, num_fc_layers = flatten_layer(conv3_drop)\n\n    # Fully connected layer\n    fc1 = tf.add(tf.matmul(flat, weights['fc1']), biases['fc1'])\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, tf_keep_prob)\n    #print(fc1)\n\n    fc2 = tf.add(tf.matmul(fc1, weights['fc2']), biases['fc2'])\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, tf_keep_prob)\n    #print(fc2)\n\n    # Output Layer - class prediction\n    logits = tf.add(tf.matmul(fc2, weights['out']), biases['out'], name='logits')\n    train_prediction = tf.nn.softmax(logits) \n\n    regularizers = (tf.nn.l2_loss(weights['layer_1']) \n                + tf.nn.l2_loss(weights['layer_2']) \n                + tf.nn.l2_loss(weights['layer_3']) \n                + tf.nn.l2_loss(weights['fc1'])\n                + tf.nn.l2_loss(weights['fc2'])\n                + tf.nn.l2_loss(weights['out']))\n\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n                                                    labels=tf_train_labels)\n\n    loss = tf.reduce_mean(cross_entropy) + tf_beta*regularizers \n    tf.summary.scalar('loss', loss) \n\n    with tf.name_scope('SGD'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=tf_learning_rate).minimize(loss) #AdamOptimizer #GradientDescentOptimizer\n\n    with tf.name_scope('accuracy'):\n        with tf.name_scope('correct_prediction'):\n\n          labels_pred_cls = tf.argmax(train_prediction,dimension = 1) \n          correct_prediction = tf.equal(labels_pred_cls, tf_train_labels_cls)\n\n        with tf.name_scope('accuracy'):\n          accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n          tf.summary.scalar('accuracy', accuracy_operation) \n\n    summary_op = tf.summary.merge_all()\n    \n    return tf_train_dataset, tf_train_labels, tf_train_labels_cls, tf_beta, tf_keep_prob, tf_learning_rate, weights, biases,conv1, conv2, conv3, flat, fc1, fc2, logits, train_prediction, regularizers,  cross_entropy, loss, optimizer, labels_pred_cls, correct_prediction, accuracy_operation, accuracy_operation, summary_op  \n```"}, {'reason': 'stop', 'result': 'def evaluate(X_data, y_data, b, lr):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        summary, accuracy = sess.run([summary_op,accuracy_operation], feed_dict={tf_train_dataset : batch_x, tf_train_labels : batch_y, \n                         tf_beta: b, tf_keep_prob : 1, tf_learning_rate : lr})\n        total_accuracy += (accuracy * len(batch_x))\n    return summary, total_accuracy / num_examples\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'def train_model(beta, lr, name_ext, learn_decay):\n    best_model_validation_acc = 0\n    valid_acc_list = []\n    train_acc_list = []\n    \n    save_file = \'./train_model_\' + \'best\' + \'.ckpt\'\n    num_examples = X_train.shape[0]\n    step = 0\n    \n    saver = tf.train.Saver()\n    \n    with tf.Session() as session:\n        \n        if(RELOAD_MODEL):\n            # Load the weights and bias\n            saver.restore(session, save_file)\n        else:         \n            session.run(tf.global_variables_initializer())\n            \n        print(\'tensorboard loc:\', \'log-directory/\' + str.replace(name_ext,\'_\',\'/\') + \'/train\')\n        train_writer = tf.summary.FileWriter(\'log-directory/\' + str.replace(name_ext,\'_\',\'/\') + \'/train\',session.graph)\n        valid_writer =  tf.summary.FileWriter(\'log-directory/\' + str.replace(name_ext,\'_\',\'/\') + \'/valid\')\n\n        for epoch in range(EPOCHS):\n\n            if(learn_decay and epoch > 0 and epoch % 30 == 0 and lr > 1e-6):\n                lr = lr/2\n            \n            # shuffling\n            X_train_subset, y_train_subset = shuffle(X_train, y_train)\n\n            # Train for all the mini-batches in the epoch\n            for offset in range(0, num_examples, BATCH_SIZE):\n                step += 1\n                \n                end = offset + BATCH_SIZE\n                batch_x, batch_y = X_train_subset[offset:end], y_train_subset[offset:end]\n\n                feed_dict = {tf_train_dataset : batch_x, tf_train_labels : batch_y, \n                             tf_beta: BETA, tf_keep_prob : 0.5, tf_learning_rate : lr}\n\n                session.run([optimizer], feed_dict = feed_dict)\n\n            # at the end of the epoch, gather statistics\n            training_summary, training_accuracy = evaluate(X_train, y_train, b = beta, lr = lr)\n            train_writer.add_summary(training_summary, step)\n            valid_summary, validation_accuracy = evaluate(X_valid, y_valid, b = beta, lr = lr)\n            valid_writer.add_summary(valid_summary, step)\n            print(\'Step %s / Epoch %s: Training accuracy: %s, Validation accuracy: %s\' % (step, epoch, training_accuracy, validation_accuracy))\n            valid_acc_list.append(validation_accuracy)\n            train_acc_list.append(training_accuracy)\n\n            if(epoch == 9 and validation_accuracy<0.9):\n                print(\'break\')\n                break\n\n            if(validation_accuracy > best_model_validation_acc):\n                best_model_validation_acc = validation_accuracy\n                \n                saver.save(session, save_file) \n                print("Model saved at: ", save_file) \n\n        test_summary, test_accuracy = evaluate(X_test, y_test, b = beta, lr = lr)  \n        print(\'Step %s / Epoch %s: Test accuracy: %s\' % (step, epoch,test_accuracy))\n        train_writer.close()   \n        valid_writer.close()   \n        \n    return train_acc_list, valid_acc_list\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def get_name_ext(timestr, beta, learning_rate, learn_decay, separator = '_'):\n    return timestr + separator + str(beta) + separator + str(learning_rate) + separator + str(learn_decay)"}, {'reason': 'stop', 'result': '# transform character-based input/output into equivalent numerical versions\ndef encode_io_pairs(text,window_size,step_size):\n    # number of unique chars\n    chars = sorted(list(set(text)))\n    num_chars = len(chars)\n    \n    # cut up text into character input/output pairs\n    inputs, outputs = window_transform_text(text,window_size,step_size)\n    \n    # create empty vessels for one-hot encoded input/output\n    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n    \n    # loop over inputs/outputs and transform and store in X/y\n    for i, sentence in enumerate(inputs):\n        for t, char in enumerate(sentence):\n            X[i, t, chars_to_indices[char]] = 1\n        y[i, chars_to_indices[outputs[i]]] = 1\n        \n    return X,y\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, len(chars)))\n        for t, char in enumerate(input_chars):\n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars\n```\n\nUnused functions: None"}, {'reason': 'stop', 'result': "### Updated code:\n\n```python\nfrom tensorflow.contrib.layers import flatten\n\ndef LeNet(x):    \n    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n    mu = 0\n    sigma = 0.1\n    \n    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n\n    # SOLUTION: Activation.\n    conv1 = tf.nn.relu(conv1)\n\n    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    \n    # SOLUTION: Activation.\n    conv2 = tf.nn.relu(conv2)\n\n    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n    fc0   = flatten(conv2)\n    \n    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n    fc1_b = tf.Variable(tf.zeros(120))\n    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n    \n    # SOLUTION: Activation.\n    fc1    = tf.nn.relu(fc1)\n\n    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n    fc2_b  = tf.Variable(tf.zeros(84))\n    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n    \n    # SOLUTION: Activation.\n    fc2    = tf.nn.relu(fc2)\n\n    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n    fc3_b  = tf.Variable(tf.zeros(43))\n    logits = tf.matmul(fc2, fc3_W) + fc3_b\n    \n    return logits\n```\n\nUnused functions:\n- model_arc"}, {'reason': 'stop', 'result': 'correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\naccuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsaver = tf.train.Saver()\n\ndef evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15,15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap)) # displays the feature map number\n        if activation_min != -1 & activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min !=-1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")```'}, {'reason': 'stop', 'result': 'import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeature_dict = {i:label for i,label in zip(\n                range(4),\n                  (\'sepal length in cm\',\n                  \'sepal width in cm\',\n                  \'petal length in cm\',\n                  \'petal width in cm\', ))}\n\nlabel_dict = {0: \'Setosa\', 1: \'Versicolor\', 2:\'Virginica\'}\n\n\ndef plot_data(X, title):\n\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))\n\n    for ax,cnt in zip(axes.ravel(), range(4)):  \n\n        # set bin sizes\n        min_b = math.floor(np.min(X[:,cnt]))\n        max_b = math.ceil(np.max(X[:,cnt]))\n        bins = np.linspace(min_b, max_b, 25)\n\n        # plottling the histograms\n        for lab,col in zip(range(0,3), (\'blue\', \'red\', \'green\')):\n            ax.hist(X[y==lab, cnt],\n                   color=col,\n                   label=\'%s\' %label_dict[lab],\n                   bins=bins,\n                   alpha=0.5,)\n        ylims = ax.get_ylim()\n\n        # plot annotation\n        leg = ax.legend(loc=\'upper right\', fancybox=True, fontsize=8)\n        leg.get_frame().set_alpha(0.5)\n        ax.set_ylim([0, max(ylims)+2])\n        ax.set_xlabel(feature_dict[cnt])\n        ax.set_title(\'Iris histogram #%s\' %str(cnt+1))\n\n        # hide axis ticks\n        ax.tick_params(axis="both", which="both", bottom="off", top="off",  \n            labelbottom="on", left="off", right="off", labelleft="on")\n\n        # remove axis spines\n        ax.spines["top"].set_visible(False)  \n        ax.spines["right"].set_visible(False)\n        ax.spines["bottom"].set_visible(False)\n        ax.spines["left"].set_visible(False)    \n\n    axes[0][0].set_ylabel(\'count\')\n    axes[1][0].set_ylabel(\'count\')\n\n    fig.tight_layout()       \n\n    plt.show()\n\ndef plot_pca(X,title):\n\n    ax = plt.subplot(111)\n\n    for label,marker,color in zip(\n        range(0,3),(\'^\', \'s\', \'o\'),(\'blue\', \'red\', \'green\')):\n\n        plt.scatter(x=X[:,0][y == label],\n                y=X[:,1][y == label],\n                marker=marker,\n                color=color,\n                alpha=0.5,\n                label=label_dict[label]\n                )\n\n    plt.xlabel(\'PC1\')\n    plt.ylabel(\'PC2\')\n\n    leg = plt.legend(loc=\'upper left\', fancybox=True)\n    leg.get_frame().set_alpha(0.5)\n    plt.title(title)\n\n    # hide axis ticks\n    plt.tick_params(axis="both", which="both", bottom="off", top="off",  \n            labelbottom="on", left="off", right="off", labelleft="on")\n\n    # remove axis spines\n    ax.spines["top"].set_visible(False)  \n    ax.spines["right"].set_visible(False)\n    ax.spines["bottom"].set_visible(False)\n    ax.spines["left"].set_visible(False)    \n\n    plt.tight_layout\n    plt.grid()\n\n    plt.show()'}, {'reason': 'stop', 'result': '### Visualize your network\'s feature maps here.\n### Feel free to use as many code cells as needed.\n\n# image_input: the test image being fed into the network to produce the feature maps\n# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n\ndef outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n    featuremaps = activation.shape[3]\n    plt.figure(plt_num, figsize=(15,15))\n    for featuremap in range(featuremaps):\n        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n        plt.title(\'FeatureMap \' + str(featuremap)) # displays the feature map number\n        if activation_min != -1 & activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")\n        elif activation_max != -1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")\n        elif activation_min !=-1:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")\n        else:\n            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")```'}, {'reason': 'stop', 'result': '```python\nfrom scipy.special import logsumexp \nimport numpy as np\n\ndef logprob(x, w, b):\n    ln_q = np.matmul(x, w) + b     \n    ln_Z = logsumexp(ln_q)     \n   \n    ln_p = ln_q - ln_Z\n    return ln_q, ln_Z, ln_p   \n\ndef logreg_gradient(x, t, w, b):\n    num_classes = len(b)\n    \n    ln_q, ln_Z, ln_p = logprob(x, w, b)\n    t_oh = one_hot(t, num_classes)\n    \n    delta = t_oh - np.exp(ln_q)/np.exp(ln_Z)   #\n    \n    dL_db = delta\n    dL_dw = np.matmul(x.T, delta)\n    logp = ln_p\n    \n    # here the statement contains logp[:,t] where logp is meant as a matrix of shape 1x10\n    return logp[:,t].squeeze(), dL_dw, dL_db.squeeze()\n```\n\nUnused functions:\n- `one_hot`'}, {'reason': 'stop', 'result': 'def sgd_iter(x_train, t_train, W, b):\n    \n    #every day I am shufflin`\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    lr = 1e-4\n    \n    logp = np.zeros(len(x_train))\n    for i in indices:\n        x = x_train[i:i+1]\n        t = t_train[i]\n        logp[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\n        W = W + lr*grad_w #grad ascent\n        b = b + lr*grad_b\n    \n    logp_train = logp.mean()\n    return logp_train, W, b\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': '# Write all helper functions here\nfrom scipy.special import logsumexp\nfrom scipy.special import expit\nimport numpy as np\n\ndef mlp_logprob(x, W, b, V, a):\n    h = expit(np.matmul(x, V) + a)\n    \n    ln_q = np.matmul(h, W) + b     \n    ln_Z = logsumexp(ln_q)       \n    ln_p = ln_q - ln_Z\n    \n    return ln_p, ln_q, ln_Z, h   \n\ndef mlp_gradient(x, t, W, b, V, a):\n    num_classes = len(b)\n    \n    ln_p, ln_q, ln_Z, h = mlp_logprob(x, W, b, V, a)\n    t_oh = one_hot(t, num_classes)\n    \n    delta_q = t_oh - np.exp(ln_q)/np.exp(ln_Z)\n    delta_h = np.matmul(delta_q, W.T)\n    \n    dL_db = delta_q\n    dL_dW = np.matmul(h.T, delta_q)\n    dL_da = delta_h * h * (1 - h)\n    dL_dV = np.matmul(x.T, dL_da)\n    \n    return ln_p[:,t].squeeze(), dL_dW, dL_db.squeeze(), dL_dV, dL_da.squeeze()\n\ndef init_params(input_size, num_classes, hidden_units):\n    W = np.random.normal(size=(hidden_units, num_classes), scale=0.1)\n    b = np.zeros(num_classes)\n    V = np.random.normal(size=(input_size, hidden_units), scale=0.1)\n    a = np.zeros(hidden_units)\n    return W, b, V, a\n\ndef mlp_sgd_iter(x_train, t_train, W, b, V, a, lr):\n    \n    #every day I am shufflin`\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    \n    logp = np.zeros(len(x_train))\n    for i in indices:\n        x = x_train[i:i+1]\n        t = t_train[i]\n        logp[i], grad_W, grad_b, grad_V, grad_a = mlp_gradient(x, t, W, b, V, a)\n        #grad ascent\n        W = W + lr*grad_W \n        b = b + lr*grad_b\n        V = V + lr*grad_V\n        a = a + lr*grad_a\n    \n    logp_train = logp.mean()\n    return logp_train, W, b, V, a\n\ndef eval_mean_logp(xs, ts, W, b, V, a):\n    logps = []\n    for x,t in zip(xs, ts):\n        logp, _, _, _ = mlp_logprob(x, W, b, V, a)\n        logps.append(logp[t].squeeze())\n    return mean(logps)\n\n```'}, {'reason': 'stop', 'result': 'accuracies_training = []\naccuracies_validation = []\naucs_training = []\naucs_validation = []\n\nX_dev_np = np.array(X_dev)\ny_dev_np = np.array(y_dev).ravel()\n\nX_eval_np = np.array(X_eval)\ny_eval_np = np.array(y_eval).ravel()\n\narbol.fit(X_dev_np, y_dev_np)\n\ndef get_positive_class_probabilities(arr):\n    arr_aux = []\n    for entry in arr:\n        arr_aux.append(entry[1])\n    return arr_aux\n\ndef get_accuracy(y_pred, y_eval_np):\n    return np.mean(y_pred == y_eval_np)\n    \ndef show_prediction_accuracy(y_pred, y_eval_np, x_eval_np):\n    print("Predicciones sobre el test set:\\n {}".format(y_pred))\n    print("Score sobre el test set: {:.2f}".format(np.mean(y_pred == y_eval_np))) # A mano\n    print("Score sobre el test set: {:.2f}".format(arbol.score(x_eval_np, y_eval_np))) # Usando el método score.\n\n#Generamos los 5 folds\nkf = KFold(n_splits=5)\n\naccuracy_train      = []\naccuracy_validation = []\nroc_train      = []\nroc_validation = []\n\nfor train_index, test_index in kf.split(X_dev_np):\n    #print("TRAIN:", train_index, "TEST:", test_index)\n    kf_X_train, kf_X_test = X_dev_np[train_index], X_dev_np[test_index]\n    kf_y_train, kf_y_test = y_dev_np[train_index], y_dev_np[test_index]\n    \n    # Entrenamos el arbol con el fold actual\n    arbol.fit(kf_X_train, kf_y_train)\n    \n    # Testeamos contra el fold de test para calcular accuracy\n    kf_y_pred     = arbol.predict(kf_X_test)\n    kf_y_pred_dev = arbol.predict(kf_X_train)\n        \n    # Calculamos accuracy\n    accuracy_validation.append(get_accuracy(kf_y_pred, kf_y_test) )\n    accuracy_train.append(get_accuracy(kf_y_pred_dev, kf_y_train) )\n\n    # Testeamos contra el fold de test para calcular el score roc\n    kf_y_pred_proba     = arbol.predict_proba(kf_X_test)\n    kf_y_pred_dev_proba = arbol.predict_proba(kf_X_train)\n    \n    # Calculamos roc score\n    roc_train.append(sklearn.metrics.roc_auc_score(kf_y_train, get_positive_class_probabilities(kf_y_pred_dev_proba)))\n    roc_validation.append(sklearn.metrics.roc_auc_score(kf_y_test, get_positive_class_probabilities(kf_y_pred_proba)))\n    \ndf = pd.DataFrame(index=range(1,6))\ndf.index.name = "Permutación"\n                  \ndf["Accuracy (training)"]   = accuracy_train\ndf["Accuracy (validación)"] = accuracy_validation\ndf["AUC ROC (training)"]    = roc_train\ndf["AUC ROC (validación)"]  = roc_validation\n\ndisplay(HTML("<h3> TABLA 1 </h3>"))\ndisplay(df)\n\n# Descomentar las siguientes líneas para graficar el resultado\n# df.plot(kind="bar")\n# plt.legend(loc=\'upper left\', bbox_to_anchor=(1.0, 1.0))\n# plt.show()```'}, {'reason': 'stop', 'result': 'import time\n\nfrom sklearn.model_selection import GridSearchCV\n\n##############################################\n################# Auxiliares #################\n##############################################\n\ndef top_resultados(grid, top=5):\n    print("Top {} combinaciones".format(top))\n    df = pd.DataFrame(grid.cv_results_["params"])\n    df["mean_score_validation"] = grid.cv_results_["mean_test_score"]\n    df["mean_score_training"] = grid.cv_results_["mean_train_score"]\n    display(df.sort_values(by="mean_score_validation", ascending=False).head(top))\n\ndef bot_resultados(grid, bot=5):\n    print("Bot {} combinaciones".format(bot))\n    df = pd.DataFrame(grid.cv_results_["params"])\n    df["mean_score_validation"] = grid.cv_results_["mean_test_score"]\n    df["mean_score_training"] = grid.cv_results_["mean_train_score"]\n    display(df.sort_values(by="mean_score_validation", ascending=True).head(bot))\n    \ndef correr_y_mostrar(estimator, parameters, folds, top):\n    grid = GridSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\')\n    time_before = time.time()\n    grid.fit(X_dev_np, y_dev_np)\n    time_after = time.time()\n    top_resultados(grid, top)\n    bot_resultados(grid, top)\n    runtime = (time_after - time_before) * 1000.0 \n    return (runtime, grid)\n\n# Para usar en caso de tener probabilidades a priori\npriors = [(0.1,0.9),(0.2,0.8),(0.3,0.7),(0.4,0.6),(0.5,0.5),(0.6,0.4),(0.7,0.3),(0.8,0.2),(0.9,0.1)]'}, {'reason': 'stop', 'result': '```python\n#Auxiliares para correr randomized search\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\n\ndef correr_randomized_y_mostrar(estimator, parameters, folds, top,  iteraciones=None):\n    random_search = None\n    \n    if(iteraciones is None):\n        random_search = RandomizedSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\')\n    else:\n        random_search = RandomizedSearchCV(estimator, parameters, cv=folds, scoring=\'roc_auc\', n_iter=iteraciones)\n        \n    time_before = time.time()\n    random_search.fit(X_dev_np, y_dev_np)\n    time_after = time.time()\n    runtime = (time_after - time_before) * 1000.0\n    \n    top_resultados(random_search, top)\n    bot_resultados(random_search, top)\n    \n    return (runtime, random_search)\n\ndef verTiempo(original, random):\n    display("########### Timepos ###########")\n    display("original: {:f}".format(original))\n    display("random: {:f}".format(random))\n    display("diferencia: {:f}".format( np.absolute(original-random) ))```'}, {'reason': 'stop', 'result': 'from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\ndef plot_learning_curve(estimator, title, X, y, cv=None):\n    plt.figure()\n    plt.title(title)\n    plt.xlabel("Training examples")\n    plt.ylabel("Roc Auc Score")\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5))\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    \n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color="r")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color="g")\n    plt.plot(train_sizes, train_scores_mean, \'o-\', color="r",\n             label="Training score")\n    plt.plot(train_sizes, test_scores_mean, \'o-\', color="g",\n             label="Cross-validation score")\n\n    plt.legend(loc="best")\n    plt.show()'}, {'reason': 'stop', 'result': 'import cv2\nimport numpy as np\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    (assuming your grayscaled image is called \'gray\')\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    #defining a blank mask to start with\n    mask = np.zeros_like(img)   \n    \n    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n        \n    #filling pixels inside the polygon defined by "vertices" with the fill color    \n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    \n    #returning the image only where mask pixels are nonzero\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n    """\n    `img` should be the output of a Canny transform.\n        \n    Returns an image with hough lines drawn.\n    """\n    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n    draw_lines(line_img, lines)\n    return line_img\n\ndef weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n    """\n    `img` is the output of the hough_lines(), An image with lines drawn on it.\n    Should be a blank image (all black) with lines drawn on it.\n    \n    `initial_img` should be the image before any processing.\n    \n    The result image is computed as follows:\n    \n    initial_img * α + img * β + λ\n    NOTE: initial_img and img must be the same shape!\n    """\n    return cv2.addWeighted(initial_img, α, img, β, λ)'}, {'reason': 'stop', 'result': 'def process_image(image):\n    # 1. grayscale\n    gray = grayscale(image)\n\n    # 2. Blur\n    # Define a kernel size and apply Gaussian smoothing\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray, kernel_size)\n\n    # 3. Canny edge\n    # Define our parameters for Canny and apply\n    low_threshold = 50\n    high_threshold = 150\n    edges = canny(blur_gray, low_threshold, high_threshold)\n\n\n    # 4. Region of interest (4-sided polygon)\n    # This time we are defining a four sided polygon to mask\n    imshape = image.shape\n    xPct = 0.05\n    yPct = 0.60\n\n    xbl = imshape[1] * xPct\n    xbr = imshape[1] * (1 - xPct)\n    xtl = imshape[1] * (0.5 - xPct)\n    xtr = imshape[1] * (0.5 + xPct)\n\n    yb = imshape[0]\n    yt = imshape[0] * yPct\n\n    vertices = np.array([[(xbl,yb),(xtl, yt), (xtr, yt), (xbr, yb)]],\n                        dtype=np.int32)\n\n    masked_image = region_of_interest(edges, vertices)\n\n\n    # 5. Hough lines\n    # Define the Hough transform parameters\n    # Make a blank the same size as our image to draw on\n    rho = 2 # distance resolution in pixels of the Hough grid\n    theta = np.pi/180 # angular resolution in radians of the Hough grid\n    threshold = 15     # minimum number of votes (intersections in Hough grid cell)\n    min_line_len = 20 #minimum number of pixels making up a line\n    max_line_gap = 30     # maximum gap in pixels between connectable line segments\n    \n    # Run Hough on edge detected image\n    # Output "lines" is an array containing endpoints of detected line segments\n    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\n\n\n    # 6. Overlay Hough lines with original image\n    # Create a "color" binary image to combine with line image\n    overlayedImg = weighted_img(line_img, image, 0.8, 1, 0)\n\n    return overlayedImg\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': "def fit_odd_shedule(learn, lr):\n    n = len(learn.data.train_dl)\n    phases = [TrainingPhase(n).schedule_hp('lr', lr, anneal=annealing_cos), \n              TrainingPhase(n*2).schedule_hp('lr', lr, anneal=annealing_poly(2))]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    total_epochs = 3\n    learn.fit(total_epochs)```\n\nUnused functions: None"}, {'reason': 'stop', 'result': 'import math\nimport numpy as np\nfrom numpy.random import randn\n\ndef compute_dog_data(z_var, process_var, count=1, dt=1.):\n    "returns track, measurements 1D ndarrays"\n    x, vel = 0., 1.\n    z_std = math.sqrt(z_var) \n    p_std = math.sqrt(process_var)\n    xs, zs = [], []\n    for _ in range(count):\n        v = vel + (randn() * p_std)\n        x += v*dt        \n        xs.append(x)\n        zs.append(x + randn() * z_std)        \n    return np.array(xs), np.array(zs)```\n\nUnused functions:\nNone'}, {'reason': 'stop', 'result': 'from filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\ndef pos_vel_filter(x, P, R, Q=0., dt=1.0):\n    """ Returns a KalmanFilter which implements a\n    constant velocity model for a state [x dx].T\n    """\n    \n    kf = KalmanFilter(dim_x=2, dim_z=1)\n    kf.x = np.array([x[0], x[1]]) # location and velocity\n    kf.F = np.array([[1., dt],\n                     [0.,  1.]])  # state transition matrix\n    kf.H = np.array([[1., 0]])    # Measurement function\n    kf.R *= R                     # measurement uncertainty\n    if np.isscalar(P):\n        kf.P *= P                 # covariance matrix \n    else:\n        kf.P[:] = P               # [:] makes deep copy\n    if np.isscalar(Q):\n        kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=Q)\n    else:\n        kf.Q[:] = Q\n    return kf\n```\n\nUnused functions: None'}, {'reason': 'stop', 'result': 'from kf_book.mkf_internal import plot_track\n\ndef run(x0=(0.,0.), P=500, R=0, Q=0, dt=1.0, \n        track=None, zs=None,\n        count=0, do_plot=True, **kwargs):\n    """\n    track is the actual position of the dog, zs are the \n    corresponding measurements. \n    """\n\n    # Simulate dog if no data provided. \n    if zs is None:\n        track, zs = compute_dog_data(R, Q, count)\n\n    # create the Kalman filter\n    kf = pos_vel_filter(x0, R=R, P=P, Q=Q, dt=dt)  \n\n    # run the kalman filter and store the results\n    xs, cov = [], []\n    for z in zs:\n        kf.predict()\n        kf.update(z)\n        xs.append(kf.x)\n        cov.append(kf.P)\n\n    xs, cov = np.array(xs), np.array(cov)\n    if do_plot:\n        plot_track(xs[:, 0], track, zs, cov, **kwargs)\n    return xs, cov\n```\n\nUnused functions:\n- None'}]