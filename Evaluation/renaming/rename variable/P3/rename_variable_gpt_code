[{'reason': 'stop', 'result': '```python\nnum_nodes = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, num_nodes]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    output_variable_def = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), output_variable_def, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n    train_prediction = tf.nn.softmax(logits)\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, output_variable_def, b))\n```'}, {'reason': 'stop', 'result': "```python\nbottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\ntrain_VGG16 = bottleneck_features['train']\nvalidation_bottleneck_features = bottleneck_features['valid']\ntest_VGG16 = bottleneck_features['test']\n```"}, {'reason': 'stop', 'result': "```python\nnum_steps = 7001\nsummary_frequency = 100\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    mean_loss = 0\n    for step in range(num_steps):\n        batch_data = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = batch_data[i]\n        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss = mean_loss / summary_frequency\n            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(batch_data)[1:])\n            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                print('=' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print('=' * 80)\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                b = valid_batches.next()\n                predictions = sample_prediction.eval({sample_input: b[0]})\n                valid_logprob = valid_logprob + logprob(predictions, b[1])\n            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n```"}, {'reason': 'stop', 'result': "```python\ndef get_model_params():\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n\ndef restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    assign_operations = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_operations.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(assign_operations, feed_dict=feed_dict)\n```"}, {'reason': 'stop', 'result': "```python\nX = Variable(name='X', num_states=2)\nvariable_definition = Factor(name='p(X)', f=np.array([0.95, 0.05]), neighbours=[X])\nZ = Variable(name='Z', num_states=2)\nZ_prior = Factor(name='p(Z)', f=np.array([0.8, 0.2]), neighbours=[Z])\nY = Variable(name='Y', num_states=2)\nf_Y_cond = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\nY_cond = Factor(name='p(Y |X, Z)', f=np.array(f_Y_cond), neighbours=[Y, X, Z])\n```"}, {'reason': 'stop', 'result': "```python\nimport pyprind\nimport pandas as pd\nimport os\nbasepath = 'aclImdb'\nlabels = {'pos': 1, 'neg': 0}\npbar = pyprind.ProgBar(50000)\ndf = pd.DataFrame()\nfor s in ('test', 'train'):\n    for l in ('pos', 'neg'):\n        path = os.path.join(basepath, s, l)\n        for file in sorted(os.listdir(path)):\n            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n                review_text = infile.read()\n            df = df.append([[review_text, labels[l]]], ignore_index=True)\n            pbar.update()\ndf.columns = ['review', 'sentiment']\n```"}, {'reason': 'stop', 'result': "```python\ndef LeNet6(x, n_classes):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    flattened_conv2 = flatten(conv2)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1 = tf.matmul(flattened_conv2, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(252, 168), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(168))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(168, 84), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(84))\n    fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n    fc3 = tf.nn.relu(fc3)\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean=mu, stddev=sigma))\n    fc4_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    return logits\n```"}, {'reason': 'stop', 'result': '```python\nlearning_rate = 0.02\n```'}, {'reason': 'stop', 'result': '```python\nimage_size = 28\npixel_depth = 255.0\n\ndef load_letter(folder, min_num_images):\n    """Load the data for a single letter label."""\n    image_files = os.listdir(folder)\n    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n    print(folder)\n    num_images = 0\n    for image in image_files:\n        image_file_path = os.path.join(folder, image)\n        try:\n            image_data = (ndimage.imread(image_file_path).astype(float) - pixel_depth / 2) / pixel_depth\n            if image_data.shape != (image_size, image_size):\n                raise Exception(\'Unexpected image shape: %s\' % str(image_data.shape))\n            dataset[num_images, :, :] = image_data\n            num_images = num_images + 1\n        except IOError as e:\n            print(\'Could not read:\', image_file_path, \':\', e, "- it\'s ok, skipping.")\n    dataset = dataset[0:num_images, :, :]\n    if num_images < min_num_images:\n        raise Exception(\'Many fewer images than expected: %d < %d\' % (num_images, min_num_images))\n    print(\'Full dataset tensor:\', dataset.shape)\n    print(\'Mean:\', np.mean(dataset))\n    print(\'Standard deviation:\', np.std(dataset))\n    return dataset\n\ndef maybe_pickle(data_folders, min_num_images_per_class, force=False):\n    dataset_names = []\n    for folder in data_folders:\n        set_filename = folder + \'.pickle\'\n        dataset_names.append(set_filename)\n        if os.path.exists(set_filename) and (not force):\n            print(\'%s already present - Skipping pickling.\' % set_filename)\n        else:\n            print(\'Pickling %s.\' % set_filename)\n            dataset = load_letter(folder, min_num_images_per_class)\n            try:\n                with open(set_filename, \'wb\') as f:\n                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n            except Exception as e:\n                print(\'Unable to save data to\', set_filename, \':\', e)\n    return dataset_names\ntrain_datasets = maybe_pickle(train_folders, 45000)\ntest_datasets = maybe_pickle(test_folders, 1800)\n```'}, {'reason': 'stop', 'result': "```python\nimport pickle\ntraining_file = '../traffic-signs-data/train.p'\nvalidation_file = '../traffic-signs-data/valid.p'\ntesting_file = '../traffic-signs-data/test.p'\nwith open(training_file, mode='rb') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode='rb') as f:\n    test = pickle.load(f)\nX_train, y_train = (train['features'], train['labels'])\nvalidation_features_labels, y_valid = (valid['features'], valid['labels'])\nX_test, y_test = (test['features'], test['labels'])\n```"}, {'reason': 'stop', 'result': "```python\nfrom my_model_selectors import SelectorDIC\ntraining = asl.build_training(features_ground)\nsequences = training.get_all_sequences()\nXlengths = training.get_all_Xlengths()\nfor word in words_to_train:\n    training_time_start = timeit.default_timer()\n    model = SelectorDIC(sequences, Xlengths, word, min_n_components=2, max_n_components=15, random_state=14).select()\n    end = timeit.default_timer() - training_time_start\n    if model is not None:\n        print('Training complete for {} with {} states with time {} seconds'.format(word, model.n_components, end))\n    else:\n        print('Training failed for {}'.format(word))\n```"}, {'reason': 'stop', 'result': "```python\noptimizer = tf.train.GradientDescentOptimizer(0.0001)\nmodel = two_layer_nn(output_size=1, loss_type='regression')\nnum_epochs = 300\nmodel.fit(X, y, optimizer, num_epochs=num_epochs, track_accuracy=False)\n```"}, {'reason': 'stop', 'result': '```python\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\ndef nparray_to_list(nparray):\n    return [x for x in nparray]\n\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    X_train_valid = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(X_train_valid, y_np):\n        X_train, X_valid = (nparray_to_list(X_train_valid[train_index]), nparray_to_list(X_train_valid[valid_index]))\n        y_train, y_valid = (nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index]))\n    train = (X_train, y_train)\n    valid = (X_valid, y_valid)\n    return (train, valid)\n```'}, {'reason': 'stop', 'result': "```python\nsoftmax_logits = tf.nn.softmax(logits)\ntop_k = tf.nn.top_k(softmax_logits, k=3)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.import_meta_graph('./lenet.meta')\n    saver.restore(sess, './lenet')\n    batch_x = np.expand_dims(my_images_normalized, axis=3)\n    sample_softmax_logits = sess.run(softmax_logits, feed_dict={x: batch_x, keep_prob: 1.0})\n    my_top_k = sess.run(top_k, feed_dict={x: batch_x, keep_prob: 1.0})\n    fig, axs = plt.subplots(len(my_images), 1, figsize=(20, 25))\n    fig.subplots_adjust(hspace=1.0, wspace=0.6)\n    axs = axs.ravel()\n    for i, image in enumerate(my_images):\n        axs[i].axis('off')\n        axs[i].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n        top_predictions = my_top_k[1][i][0]\n        index1 = np.argwhere(y_valid == top_predictions)[0]\n        guess2 = my_top_k[1][i][1]\n        index2 = np.argwhere(y_valid == guess2)[0]\n        guess3 = my_top_k[1][i][2]\n        index3 = np.argwhere(y_valid == guess3)[0]\n        title = ''\n        title += 'guess 1: class ' + sign_dict[str(top_predictions)] + ', probability: ' + str(100 * my_top_k[0][i][0]) + '\\n'\n        title += 'guess 2: class ' + sign_dict[str(guess2)] + ', probability: ' + str(100 * my_top_k[0][i][1]) + '\\n'\n        title += 'guess 3: class ' + sign_dict[str(guess3)] + ', probability: ' + str(100 * my_top_k[0][i][2])\n        axs[i].set_title(title)\n```"}, {'reason': 'stop', 'result': "```python\nbatch_size = 128\nembedding_size = 128\nskip_window = 1\nnum_skips = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    sampled_softmax_loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(sampled_softmax_loss)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n```"}, {'reason': 'stop', 'result': "```python\npoem_text = 'Take this kiss upon the brow!\\nAnd, in parting from you now,\\nThus much let me avow —\\nYou are not wrong, who deem\\nThat my days have been a dream;\\nYet if hope has flown away\\nIn a night, or in a day,\\nIn a vision, or in none,\\nIs it therefore the less gone?  \\nAll that we see or seem\\nIs but a dream within a dream.\\n\\nI stand amid the roar\\nOf a surf-tormented shore,\\nAnd I hold within my hand\\nGrains of the golden sand —\\nHow few! yet how they creep\\nThrough my fingers to the deep,\\nWhile I weep — while I weep!\\nO God! Can I not grasp \\nThem with a tighter clasp?\\nO God! can I not save\\nOne from the pitiless wave?\\nIs all that we see or seem\\nBut a dream within a dream?'\n```"}, {'reason': 'stop', 'result': "```python\nfiles = os.listdir('challenge/')\nk_size = 7\nvertex_ratio_h = 0.45\nvertex_ratio_v = 0.6\nlow_thresh = 60\nhigh_thresh = 100\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.0\nmin_votes_threshold = 15\nmin_line_len = 20\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_thresh = np.pi / 8\ndebug = True\nfor file in files:\n    frame = mpimg.imread('challenge/' + file)\n    fig = plt.figure(figsize=(15, 10))\n    fig.text(0.1, 1, file)\n    result = process_image_3_channels(frame, k_size=k_size, low_thresh=low_thresh, high_thresh=high_thresh, L2gradient=L2gradient, rho=rho, theta=theta, min_votes=min_votes_threshold, min_line_len=min_line_len, max_line_gap=max_line_gap, angle=angle, angle_thresh=angle_threshold, debug=debug)\n```"}, {'reason': 'stop', 'result': "```python\nimg[...] = 0\nX = (x - 400) / 30\ny_coordinate_normalized = -(y - 300) / 30\nR = np.sqrt(X ** 2 + y_coordinate_normalized ** 2)\nt = np.arctan2(y_coordinate_normalized, X)\nimg[R < 5] = (1, 0, 0)\nimg[(t > 0) & (t < 3.14 / 4)] = (0, 0, 1)\nplt.imshow(img, interpolation='bilinear')\n```"}, {'reason': 'stop', 'result': "```python\ndef temporal_layer(x, n_neurons, dilation_rate, conv1d_kwargs, normalize=False, dropout=0):\n    h = Conv1D(n_neurons, dilation_rate=dilation_rate, **conv1d_kwargs)(x)\n    if normalize:\n        h = BatchNormalization()(h)\n    h = Activation('relu')(h)\n    if dropout:\n        h = Dropout(dropout)(h)\n    return h\n\ndef temporal_block(h0, n_neurons, dilation_rate, conv1d_kwargs, normalize=False, dropout=0):\n    h1 = temporal_layer(h0, n_neurons, dilation_rate, conv1d_kwargs, normalize=normalize, dropout=dropout)\n    hidden_layer_output = temporal_layer(h1, n_neurons, dilation_rate, conv1d_kwargs, normalize=normalize, dropout=dropout)\n    res = Conv1D(n_neurons, kernel_size=1)(h0) if h0.shape != hidden_layer_output.shape else h0\n    block = Add()([res, hidden_layer_output])\n    return Activation('relu')(block)\nprint(input_shape, 2 ** n_hidden)\nloss = msig.classification_type + '_crossentropy'\nfc_act = classifier_activation[msig.classification_type]\nout_neurons = 1 if msig.classification_type == 'binary' else n_classes\nconv1d_kwargs = dict(kernel_size=kernel_size, padding='causal')\ncompile_kwargs = dict(loss=loss, optimizer='adam', metrics=['accuracy'])\nx = Input(shape=input_shape)\nh = temporal_block(x, n_neurons, 1, conv1d_kwargs, normalize=False)\nfor d in range(1, n_hidden):\n    h = temporal_block(h, n_neurons, 2 ** d, conv1d_kwargs, normalize=False)\nz = Dense(out_neurons, activation=fc_act)(h)\nmodel = Model(inputs=[x], outputs=[z])\nmodel.compile(**compile_kwargs)\nmodel.summary()\n```"}, {'reason': 'stop', 'result': "```python\nfor i in range(len(data)):\n    biodiversity_index = 0.0\n    for x in range(len(data.iloc[0]))[4:last]:\n        if data.iloc[i][x] > 0:\n            biodiversity_index += -(data.iloc[i][x] / sum(data.iloc[i][4:last][data.iloc[i][4:last] > 0])) * math.log(data.iloc[i][x] / sum(data.iloc[i][4:last][data.iloc[i][4:last] > 0]))\n    data.loc[i, 'SWI_e'] = biodiversity_index\n```"}]