[{'identified': 'lstm_cell_weights', 'updated_code': 'num_nodes = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, num_nodes]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    lstm_cell_weights = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), lstm_cell_weights, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n    train_prediction = tf.nn.softmax(logits)\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, lstm_cell_weights, b))'}, {'identified': 'validation_bottleneck_features', 'updated_code': "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\ntrain_VGG16 = bottleneck_features['train']\nvalidation_bottleneck_features = bottleneck_features['valid']\ntest_VGG16 = bottleneck_features['test']"}, {'identified': 'next_training_batch', 'updated_code': "num_steps = 7001\nsummary_frequency = 100\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    mean_loss = 0\n    for step in range(num_steps):\n        next_training_batch = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = next_training_batch[i]\n        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss = mean_loss / summary_frequency\n            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(next_training_batch)[1:])\n            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                print('=' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print('=' * 80)\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                b = valid_batches.next()\n                predictions = sample_prediction.eval({sample_input: b[0]})\n                valid_logprob = valid_logprob + logprob(predictions, b[1])\n            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"}, {'identified': 'assign_operations', 'updated_code': "def get_model_params():\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n\ndef restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    assign_operations = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_operations.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(assign_operations, feed_dict=feed_dict)"}, {'identified': 'conditional_probability_factor', 'updated_code': "X = Variable(name='X', num_states=2)\nconditional_probability_factor = Factor(name='p(X)', f=np.array([0.95, 0.05]), neighbours=[X])\nZ = Variable(name='Z', num_states=2)\nZ_prior = Factor(name='p(Z)', f=np.array([0.8, 0.2]), neighbours=[Z])\nY = Variable(name='Y', num_states=2)\nf_Y_cond = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\nY_cond = Factor(name='p(Y |X, Z)', f=np.array(f_Y_cond), neighbours=[Y, X, Z])"}, {'identified': 'file_content', 'updated_code': "import pyprind\nimport pandas as pd\nimport os\nbasepath = 'aclImdb'\nlabels = {'pos': 1, 'neg': 0}\npbar = pyprind.ProgBar(50000)\ndf = pd.DataFrame()\nfor s in ('test', 'train'):\n    for l in ('pos', 'neg'):\n        path = os.path.join(basepath, s, l)\n        for file in sorted(os.listdir(path)):\n            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n                file_content = infile.read()\n            df = df.append([[file_content, labels[l]]], ignore_index=True)\n            pbar.update()\ndf.columns = ['review', 'sentiment']"}]