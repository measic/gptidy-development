[{'identified': 'hidden_dim', 'updated_code': 'hidden_dim = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, hidden_dim], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([hidden_dim, hidden_dim], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, hidden_dim]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, hidden_dim], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([hidden_dim, hidden_dim], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, hidden_dim]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, hidden_dim], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([hidden_dim, hidden_dim], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, hidden_dim]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, hidden_dim], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([hidden_dim, hidden_dim], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, hidden_dim]))\n    saved_output = tf.Variable(tf.zeros([batch_size, hidden_dim]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, hidden_dim]), trainable=False)\n    w = tf.Variable(tf.truncated_normal([hidden_dim, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n    train_prediction = tf.nn.softmax(logits)\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, hidden_dim]))\n    saved_sample_state = tf.Variable(tf.zeros([1, hidden_dim]))\n    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, hidden_dim])), saved_sample_state.assign(tf.zeros([1, hidden_dim])))\n    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))'}, {'identified': 'dog_bottleneck_features', 'updated_code': "dog_bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\ntrain_VGG16 = dog_bottleneck_features['train']\nvalid_VGG16 = dog_bottleneck_features['valid']\ntest_VGG16 = dog_bottleneck_features['test']"}, {'identified': 'feed_dict', 'updated_code': "num_steps = 7001\nsummary_frequency = 100\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    mean_loss = 0\n    for step in range(num_steps):\n        batches = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = batches[i]\n        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss = mean_loss / summary_frequency\n            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(batches)[1:])\n            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                print('=' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print('=' * 80)\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                b = valid_batches.next()\n                predictions = sample_prediction.eval({sample_input: b[0]})\n                valid_logprob = valid_logprob + logprob(predictions, b[1])\n            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"}, {'identified': 'model_variable_definitions', 'updated_code': "def get_model_params():\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n\ndef restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    model_variable_definitions = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + '/Assign') for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in model_variable_definitions.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(model_variable_definitions, feed_dict=feed_dict)"}, {'identified': 'z_conditional_distribution', 'updated_code': "X = Variable(name='X', num_states=2)\nX_prior = Factor(name='p(X)', f=np.array([0.95, 0.05]), neighbours=[X])\nZ = Variable(name='Z', num_states=2)\nz_conditional_distribution = Factor(name='p(Z)', f=np.array([0.8, 0.2]), neighbours=[Z])\nY = Variable(name='Y', num_states=2)\nf_Y_cond = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\nY_cond = Factor(name='p(Y |X, Z)', f=np.array(f_Y_cond), neighbours=[Y, X, Z])"}, {'identified': 'progress_bar', 'updated_code': "import pyprind\nimport pandas as pd\nimport os\nbasepath = 'aclImdb'\nlabels = {'pos': 1, 'neg': 0}\nprogress_bar = pyprind.ProgBar(50000)\ndf = pd.DataFrame()\nfor s in ('test', 'train'):\n    for l in ('pos', 'neg'):\n        path = os.path.join(basepath, s, l)\n        for file in sorted(os.listdir(path)):\n            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n                txt = infile.read()\n            df = df.append([[txt, labels[l]]], ignore_index=True)\n            progress_bar.update()\ndf.columns = ['review', 'sentiment']"}, {'identified': 'fc2_weights', 'updated_code': "def LeNet6(x, n_classes):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    fc0 = flatten(conv2)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    fc2_weights = tf.Variable(tf.truncated_normal(shape=(252, 168), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(168))\n    fc2 = tf.matmul(fc1, fc2_weights) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(168, 84), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(84))\n    fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n    fc3 = tf.nn.relu(fc3)\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean=mu, stddev=sigma))\n    fc4_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    return logits"}, {'identified': 'learning_rate', 'updated_code': 'learning_rate = 0.02'}, {'identified': 'image_data_max_value', 'updated_code': 'image_size = 28\nimage_data_max_value = 255.0\n\ndef load_letter(folder, min_num_images):\n    """Load the data for a single letter label."""\n    image_files = os.listdir(folder)\n    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n    print(folder)\n    num_images = 0\n    for image in image_files:\n        image_file = os.path.join(folder, image)\n        try:\n            image_data = (ndimage.imread(image_file).astype(float) - image_data_max_value / 2) / image_data_max_value\n            if image_data.shape != (image_size, image_size):\n                raise Exception(\'Unexpected image shape: %s\' % str(image_data.shape))\n            dataset[num_images, :, :] = image_data\n            num_images = num_images + 1\n        except IOError as e:\n            print(\'Could not read:\', image_file, \':\', e, "- it\'s ok, skipping.")\n    dataset = dataset[0:num_images, :, :]\n    if num_images < min_num_images:\n        raise Exception(\'Many fewer images than expected: %d < %d\' % (num_images, min_num_images))\n    print(\'Full dataset tensor:\', dataset.shape)\n    print(\'Mean:\', np.mean(dataset))\n    print(\'Standard deviation:\', np.std(dataset))\n    return dataset\n\ndef maybe_pickle(data_folders, min_num_images_per_class, force=False):\n    dataset_names = []\n    for folder in data_folders:\n        set_filename = folder + \'.pickle\'\n        dataset_names.append(set_filename)\n        if os.path.exists(set_filename) and (not force):\n            print(\'%s already present - Skipping pickling.\' % set_filename)\n        else:\n            print(\'Pickling %s.\' % set_filename)\n            dataset = load_letter(folder, min_num_images_per_class)\n            try:\n                with open(set_filename, \'wb\') as f:\n                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n            except Exception as e:\n                print(\'Unable to save data to\', set_filename, \':\', e)\n    return dataset_names\n\ntrain_datasets = maybe_pickle(train_folders, 45000)\ntest_datasets = maybe_pickle(test_folders, 1800)'}, {'identified': 'validation_features', 'updated_code': "import pickle\ntraining_file = '../traffic-signs-data/train.p'\nvalidation_file = '../traffic-signs-data/valid.p'\ntesting_file = '../traffic-signs-data/test.p'\nwith open(training_file, mode='rb') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode='rb') as f:\n    test = pickle.load(f)\nX_train, y_train = (train['features'], train['labels'])\nvalidation_features, y_valid = (valid['features'], valid['labels'])\nX_test, y_test = (test['features'], test['labels'])"}, {'identified': 'training_data', 'updated_code': "from my_model_selectors import SelectorDIC\ntraining_data = asl.build_training(features_ground)\nsequences = training_data.get_all_sequences()\nXlengths = training_data.get_all_Xlengths()\nfor word in words_to_train:\n    start = timeit.default_timer()\n    model = SelectorDIC(sequences, Xlengths, word, min_n_components=2, max_n_components=15, random_state=14).select()\n    end = timeit.default_timer() - start\n    if model is not None:\n        print('Training complete for {} with {} states with time {} seconds'.format(word, model.n_components, end))\n    else:\n        print('Training failed for {}'.format(word))"}, {'identified': 'num_training_epochs', 'updated_code': "optimizer = tf.train.GradientDescentOptimizer(0.0001)\nmodel = two_layer_nn(output_size=1, loss_type='regression')\nnum_training_epochs = 300\nmodel.fit(X, y, optimizer, num_epochs=num_training_epochs, track_accuracy=False)"}, {'identified': 'traffic_sign_images', 'updated_code': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\ndef nparray_to_list(nparray):\n    return [x for x in nparray]\n\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    traffic_sign_images = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(traffic_sign_images, y_np):\n        X_train, X_valid = (nparray_to_list(traffic_sign_images[train_index]), nparray_to_list(traffic_sign_images[valid_index]))\n        y_train, y_valid = (nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index]))\n    train = (X_train, y_train)\n    valid = (X_valid, y_valid)\n    return (train, valid)'}, {'identified': 'top_k_predictions', 'updated_code': "softmax_logits = tf.nn.softmax(logits)\ntop_k = tf.nn.top_k(softmax_logits, k=3)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.import_meta_graph('./lenet.meta')\n    saver.restore(sess, './lenet')\n    batch_x = np.expand_dims(my_images_normalized, axis=3)\n    sample_softmax_logits = sess.run(softmax_logits, feed_dict={x: batch_x, keep_prob: 1.0})\n    top_k_predictions = sess.run(top_k, feed_dict={x: batch_x, keep_prob: 1.0})\n    fig, axs = plt.subplots(len(my_images), 1, figsize=(20, 25))\n    fig.subplots_adjust(hspace=1.0, wspace=0.6)\n    axs = axs.ravel()\n    for i, image in enumerate(my_images):\n        axs[i].axis('off')\n        axs[i].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n        guess1 = top_k_predictions[1][i][0]\n        index1 = np.argwhere(y_valid == guess1)[0]\n        guess2 = top_k_predictions[1][i][1]\n        index2 = np.argwhere(y_valid == guess2)[0]\n        guess3 = top_k_predictions[1][i][2]\n        index3 = np.argwhere(y_valid == guess3)[0]\n        title = ''\n        title += 'guess 1: class ' + sign_dict[str(guess1)] + ', probability: ' + str(100 * top_k_predictions[0][i][0]) + '\\n'\n        title += 'guess 2: class ' + sign_dict[str(guess2)] + ', probability: ' + str(100 * top_k_predictions[0][i][1]) + '\\n'\n        title += 'guess 3: class ' + sign_dict[str(guess3)] + ', probability: ' + str(100 * top_k_predictions[0][i][2])\n        axs[i].set_title(title)"}, {'identified': 'similarity_matrix', 'updated_code': "batch_size = 128\nembedding_size = 128\nskip_window = 1\nnum_skips = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity_matrix = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"}, {'identified': "'poem_text' or 'text_data'", 'updated_code': "poem_text = 'Take this kiss upon the brow!\\nAnd, in parting from you now,\\nThus much let me avow —\\nYou are not wrong, who deem\\nThat my days have been a dream;\\nYet if hope has flown away\\nIn a night, or in a day,\\nIn a vision, or in none,\\nIs it therefore the less gone?  \\nAll that we see or seem\\nIs but a dream within a dream.\\n\\nI stand amid the roar\\nOf a surf-tormented shore,\\nAnd I hold within my hand\\nGrains of the golden sand —\\nHow few! yet how they creep\\nThrough my fingers to the deep,\\nWhile I weep — while I weep!\\nO God! Can I not grasp \\nThem with a tighter clasp?\\nO God! can I not save\\nOne from the pitiless wave?\\nIs all that we see or seem\\nBut a dream within a dream?'\n\ntext_data = 'Take this kiss upon the brow!\\nAnd, in parting from you now,\\nThus much let me avow —\\nYou are not wrong, who deem\\nThat my days have been a dream;\\nYet if hope has flown away\\nIn a night, or in a day,\\nIn a vision, or in none,\\nIs it therefore the less gone?  \\nAll that we see or seem\\nIs but a dream within a dream.\\n\\nI stand amid the roar\\nOf a surf-tormented shore,\\nAnd I hold within my hand\\nGrains of the golden sand —\\nHow few! yet how they creep\\nThrough my fingers to the deep,\\nWhile I weep — while I weep!\\nO God! Can I not grasp \\nThem with a tighter clasp?\\nO God! can I not save\\nOne from the pitiless wave?\\nIs all that we see or seem\\nBut a dream within a dream?'"}, {'identified': 'lane_lines_detected', 'updated_code': "files = os.listdir('challenge/')\nk_size = 7\nvertex_ratio_h = 0.45\nvertex_ratio_v = 0.6\nlow_thresh = 60\nhigh_thresh = 100\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.0\nmin_votes = 15\nmin_line_len = 20\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_thresh = np.pi / 8\ndebug = True\nfor file in files:\n    frame = mpimg.imread('challenge/' + file)\n    fig = plt.figure(figsize=(15, 10))\n    fig.text(0.1, 1, file)\n    lane_lines_detected = process_image_3_channels(frame, k_size=k_size, low_thresh=low_thresh, high_thresh=high_thresh, L2gradient=L2gradient, rho=rho, theta=theta, min_votes=min_votes, min_line_len=min_line_len, max_line_gap=max_line_gap, angle=angle, angle_thresh=angle_threshold, debug=debug)"}, {'identified': 'distance_from_origin', 'updated_code': "img[...] = 0\nX = (x - 400) / 30\nY = -(y - 300) / 30\ndistance_from_origin = np.sqrt(X ** 2 + Y ** 2)\nt = np.arctan2(Y, X)\nimg[distance_from_origin < 5] = (1, 0, 0)\nimg[(t > 0) & (t < 3.14 / 4)] = (0, 0, 1)\nplt.imshow(img, interpolation='bilinear')"}, {'identified': 'classifier_activation_type', 'updated_code': "def temporal_layer(x, n_neurons, dilation_rate, conv1d_kwargs, normalize=False, dropout=0):\n    h = Conv1D(n_neurons, dilation_rate=dilation_rate, **conv1d_kwargs)(x)\n    if normalize:\n        h = BatchNormalization()(h)\n    h = Activation('relu')(h)\n    if dropout:\n        h = Dropout(dropout)(h)\n    return h\n\ndef temporal_block(h0, n_neurons, dilation_rate, conv1d_kwargs, normalize=False, dropout=0):\n    h1 = temporal_layer(h0, n_neurons, dilation_rate, conv1d_kwargs, normalize=normalize, dropout=dropout)\n    h2 = temporal_layer(h1, n_neurons, dilation_rate, conv1d_kwargs, normalize=normalize, dropout=dropout)\n    res = Conv1D(n_neurons, kernel_size=1)(h0) if h0.shape != h2.shape else h0\n    block = Add()([res, h2])\n    return Activation('relu')(block)\nprint(input_shape, 2 ** n_hidden)\nloss = msig.classification_type + '_crossentropy'\nclassifier_activation_type = classifier_activation[msig.classification_type]\nout_neurons = 1 if msig.classification_type == 'binary' else n_classes\nconv1d_kwargs = dict(kernel_size=kernel_size, padding='causal')\ncompile_kwargs = dict(loss=loss, optimizer='adam', metrics=['accuracy'])\nx = Input(shape=input_shape)\nh = temporal_block(x, n_neurons, 1, conv1d_kwargs, normalize=False)\nfor d in range(1, n_hidden):\n    h = temporal_block(h, n_neurons, 2 ** d, conv1d_kwargs, normalize=False)\nz = Dense(out_neurons, activation=classifier_activation_type)(h)\nmodel = Model(inputs=[x], outputs=[z])\nmodel.compile(**compile_kwargs)\nmodel.summary()"}, {'identified': 'biodiversity_index_value', 'updated_code': "for i in range(len(data)):\n    biodiversity_index_value = 0.0\n    for x in range(len(data.iloc[0]))[4:last]:\n        if data.iloc[i][x] > 0:\n            biodiversity_index_value += -(data.iloc[i][x] / sum(data.iloc[i][4:last][data.iloc[i][4:last] > 0])) * math.log(data.iloc[i][x] / sum(data.iloc[i][4:last][data.iloc[i][4:last] > 0]))\n    data.loc[i, 'SWI_e'] = biodiversity_index_value"}, {'identified': 'pairwise_distances_euclidean', 'updated_code': "pairwise_distances_euclidean = pairwise_distances(tf_idf, tf_idf[0:3], metric='euclidean')\ncluster_assignment = np.argmin(pairwise_distances_euclidean, axis=1)"}, {'identified': 'UNKNOWN', 'updated_code': "class Dataset(object):\n    PAD = 0\n    SOS = 1\n    EOS = 2\n    UNKNOWN = 3\n    constants = ['PAD', 'SOS', 'EOS', 'UNK']\n    hu_alphabet = list('aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz-+._')\n\n    def __init__(self, fn, config, src_alphabet=None, tgt_alphabet=None):\n        self.config = config\n        self.create_tables(src_alphabet, tgt_alphabet)\n        self.load_and_preproc_dataset(fn)\n\n    def create_tables(self, src_alphabet, tgt_alphabet):\n        if src_alphabet is None:\n            self.src_vocab = Dataset.constants + Dataset.hu_alphabet\n        else:\n            self.src_vocab = Dataset.constants + alphabet\n        self.src_table = lookup_ops.index_table_from_tensor(tf.constant(self.src_vocab), default_value=Dataset.UNK)\n        if self.config.share_vocab:\n            self.tgt_vocab = self.src_vocab\n            self.tgt_table = self.src_table\n        else:\n            if tgt_alphabet is None:\n                self.tgt_vocab = Dataset.constants + Dataset.hu_alphabet\n            else:\n                self.tgt_vocab = Dataset.constants + alphabet\n            self.tgt_table = lookup_ops.index_table_from_tensor(tf.constant(self.tgt_vocab), default_value=Dataset.UNK)\n        self.src_vocab_size = len(self.src_vocab)\n        self.tgt_vocab_size = len(self.tgt_vocab)\n\n    def load_and_preproc_dataset(self, fn):\n        dataset = tf.contrib.data.TextLineDataset(fn)\n        dataset = dataset.repeat()\n        dataset = dataset.map(lambda s: tf.string_split([s], delimiter='\\t').values)\n        src = dataset.map(lambda s: s[0])\n        tgt = dataset.map(lambda s: s[1])\n        src = src.map(lambda s: tf.string_split([s], delimiter=' ').values)\n        src = src.map(lambda s: s[:self.config.src_maxlen])\n        tgt = tgt.map(lambda s: tf.string_split([s], delimiter=' ').values)\n        tgt = tgt.map(lambda s: s[:self.config.tgt_maxlen])\n        src = src.map(lambda words: self.src_table.lookup(words))\n        tgt = tgt.map(lambda words: self.tgt_table.lookup(words))\n        dataset = tf.contrib.data.Dataset.zip((src, tgt))\n        dataset = dataset.map(lambda src, tgt: (src, tf.concat(([Dataset.SOS], tgt), 0), tf.concat((tgt, [Dataset.EOS]), 0)))\n        dataset = dataset.map(lambda src, tgt_in, tgt_out: (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)))\n        batched = dataset.padded_batch(self.config.batch_size, padded_shapes=(tf.TensorShape([self.config.src_maxlen]), tf.TensorShape([self.config.tgt_maxlen + 2]), tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([])))\n        self.batched_iter = batched.make_initializable_iterator()\n        s = self.batched_iter.get_next()\n        self.src_ids = s[0]\n        self.tgt_in_ids = s[1]\n        self.tgt_out_ids = s[2]\n        self.src_size = s[3]\n        self.tgt_size = s[4]\n\n    def run_initializers(self, session):\n        session.run(tf.tables_initializer())\n        session.run(self.batched_iter.initializer)"}, {'identified': 'image_directory', 'updated_code': "image_directory = 'images'\ntest_images = glob(image_directory + '/*')\ntest_images"}, {'identified': 'audio_feature_size', 'updated_code': "N = Ngenres * Nclips * Nframes * 2\nsizeX = N * n / 2.0 ** 20\nsizeZ = N * m / 2.0 ** 20\nsizeD = n * m / 2.0 ** 10\naudio_feature_size = m * n / 2.0 ** 10\nprint('Size X: {:.1f} M --> {:.1f} MiB'.format(sizeX, sizeX * 4))\nprint('Size Z: {:.1f} M --> {:.1f} MiB'.format(sizeZ, sizeZ * 4))\nprint('Size D: {:.1f} k --> {:.1f} kiB'.format(sizeD, sizeD * 4))\nprint('Size E: {:.1f} k --> {:.1f} kiB'.format(audio_feature_size, audio_feature_size * 4))"}, {'identified': 'hidden_layer_data_mean', 'updated_code': "if msig.sequence_type == 'many2many':\n    hidden_layer_data_mean = y_score.sum(axis=1) / y_score.shape[1]\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size - 1:]\n    y_score_unshifted_clipped_mean = y_score_unshifted_clipped.sum(axis=1) / y_score.shape[1]\nelse:\n    hidden_layer_data_mean = y_score.sum(axis=1) / y_score.shape[1]\n    y_score_unshifted = np.zeros((msig.n_timestamps, msig.window_size))\n    for i in range(msig.window_size):\n        y_score_unshifted[i:i + msig.n_samples, i] = y_score[:, i]\n    y_score_unshifted_clipped = y_score_unshifted[msig.window_size - 1:]\n    y_score_unshifted_clipped_mean = y_score_unshifted_clipped.sum(axis=1) / y_score.shape[1]"}, {'identified': 'x_axis_values', 'updated_code': "county = df_county_data['County Name']\npov_rate = df_county_data['Poverty Rate']\nx_axis_values = np.arange(len(pov_rate))\nvariable_def = [value for value in x_axis_values]\nplt.bar(x_axis_values, pov_rate, color='r', align='center')\nplt.title('County Poverty Rates')\nplt.xlabel('Counties')\nplt.ylabel('Poverty Rates')\nplt.text(140, 30, 'Note:\\nPoverty Rates for all counties in NJ, NY, & PA.')\nplt.savefig('Images/County_Poverty_Rates.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'reservation_combinations', 'updated_code': "unique_hotels_names = most_checkins['Hotel Name'].unique()\nunique_checkins = most_checkins['Checkin Date'].unique()\nunique_discount_code = [1, 2, 3, 4]\nimport itertools\nimport sys\nreservation_combinations = []\nfor x in unique_hotels_names:\n    for y in unique_checkins:\n        for z in unique_discount_code:\n            reservation_combinations.append([x, y, z, sys.maxsize])\nnew_df = DataFrame.from_records(reservation_combinations, columns=['Hotel Name', 'Checkin Date', 'Discount Code', 'Discount Price'])\nmost_checkins = most_checkins.append(new_df)"}, {'identified': 'lecture_materials_count', 'updated_code': 'a = 840\nb = 2 * a\nlecture_materials_count = 12\nd = lecture_materials_count * b\nd'}, {'identified': 'error_detection_images', 'updated_code': "probas_patches_msr = np.reshape(get_acc_net_msr(y_pred_te).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_msr -= np.min(probas_patches_msr)\nprobas_patches_msr /= np.max(probas_patches_msr)\nprobas_patches_margin = np.reshape((1 - get_acc_net_max_margin(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_margin -= np.min(probas_patches_margin)\nprobas_patches_margin /= np.max(probas_patches_margin)\nprobas_patches_entropy = np.reshape((1 - get_acc_net_entropy(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_entropy -= np.min(probas_patches_entropy)\nprobas_patches_entropy /= np.max(probas_patches_entropy)\nprobas_patches_msr = np.reshape(get_acc_net_msr(y_pred_te).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_msr -= np.min(probas_patches_msr)\nprobas_patches_msr /= np.max(probas_patches_msr)\nprobas_patches_margin = np.reshape((1 - get_acc_net_max_margin(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_margin -= np.min(probas_patches_margin)\nprobas_patches_margin /= np.max(probas_patches_margin)\nprobas_patches_entropy = np.reshape((1 - get_acc_net_entropy(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_entropy -= np.min(probas_patches_entropy)\nprobas_patches_entropy /= np.max(probas_patches_entropy)\nacc_im_msr = convert_patches_to_image(data_test.imgs, probas_patches_msr[..., np.newaxis], 64, 64)\nacc_im_margin = convert_patches_to_image(data_test.imgs, probas_patches_margin[..., np.newaxis], 64, 64)\nerror_detection_images = convert_patches_to_image(data_test.imgs, probas_patches_entropy[..., np.newaxis], 64, 64)\nfor img_idx in range(5):\n    plt.figure(figsize=(8, 8))\n    plt.imshow(acc_im_msr[img_idx], cmap='RdYlGn')\n    plt.axis('off')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.savefig('../Figures/Zurich/Im_cert/ED/net_msr_im_' + str(img_idx) + '.pdf', bbox_inches='tight', pad_inches=0)\n    plt.close()\n    plt.figure(figsize=(8, 8))\n    plt.imshow(acc_im_margin[img_idx], cmap='RdYlGn')\n    plt.axis('off')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.savefig('../Figures/Zurich/Im_cert/ED/net_margin_im_' + str(img_idx) + '.pdf', bbox_inches='tight', pad_inches=0)\n    plt.close()\n    plt.figure(figsize=(8, 8))\n    plt.imshow(error_detection_images[img_idx], cmap='RdYlGn')\n    plt.axis('off')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.savefig('../Figures/Zurich/Im_cert/ED/net_entropy_im_' + str(img_idx) + '.pdf', bbox_inches='tight', pad_inches=0)\n    plt.close()"}, {'identified': 'friction_force', 'updated_code': 'def friction(vi, vf, F0):\n    """\n    Returns the friction of the bottom plate against blocks moving at a specific velocity\n    \n    Arguments:  vi - initial velocity of the block\n                vf - final velocity of the block\n                F0 - the static friction force\n    \n    Returned: The force due to friction\n    \n    Examples:\n    \n    >>> friction (0, 1, 20)\n    -20.0\n    \n    >>> friction (1, 1, 20)\n    -10.0\n    """\n    if vi == 0:\n        sign = 1\n    else:\n        sign = vi / abs(vi)\n    friction_force = -(F0 * sign / (1 + abs(vi / vf)))\n    return friction_force'}, {'identified': 'total_course_materials', 'updated_code': 'a = 840\nb = 2 * a\nc = 12\ntotal_course_materials = c * b\ntotal_course_materials'}, {'identified': 'color_counter', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\ncolor_counter = 0\nfor segment in segments:\n    segDF = data.loc[data[\'RiverSeg\'] == segment]\n    segDF = segDF[segDF["lam\'"] > 0]\n    segDF = segDF.sort_values(\'Year\')\n    x = segDF[\'Year\']\n    y = segDF["N10\'"]\n    y.name = segment\n    plt.plot(x, y, colors[color_counter])\n    plt.legend(loc=(1.05, 0.2))\n    color_counter += 1\nplt.title("Hill N10\', " + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel("N10\'")\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + "_Hill_N10\'.png", bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'stock_features_df', 'updated_code': "def basic_stock_features(input_df, mnemonic, new_time_index, inplace=False):\n    stock_features_df = input_df.loc[mnemonic]\n    if not inplace:\n        stock_features_df = input_df.loc[mnemonic].copy()\n    stock_features_df = stock_features_df.reindex(new_time_index)\n    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n    for f in features:\n        stock_features_df[f] = stock_features_df[f].fillna(method='ffill')\n    features = ['TradedVolume', 'NumberOfTrades']\n    for f in features:\n        stock_features_df[f] = stock_features_df[f].fillna(0.0)\n    stock_features_df['HourOfDay'] = stock_features_df.index.hour\n    stock_features_df['MinOfHour'] = stock_features_df.index.minute\n    stock_features_df['MinOfDay'] = stock_features_df.index.hour * 60 + stock_features_df.index.minute\n    stock_features_df['DayOfWeek'] = stock_features_df.index.dayofweek\n    stock_features_df['DayOfYear'] = stock_features_df.index.dayofyear\n    stock_features_df['MonthOfYear'] = stock_features_df.index.month\n    stock_features_df['WeekOfYear'] = stock_features_df.index.weekofyear\n    stock_features_df['Mnemonic'] = mnemonic\n    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n    return stock_features_df.drop(unwanted_features, axis=1)"}, {'identified': 'sign_language_word', 'updated_code': "sign_language_word = 'CHOCOLATE'\nmodel, logL = train_a_word(sign_language_word, 3, features_ground)\nshow_model_stats(sign_language_word, model)\nprint('logL = {}'.format(logL))"}, {'identified': 'numeric_features', 'updated_code': 'numeric_features = X.loc[:, X.dtypes == np.float64]\nZ2 = X.loc[:, X.dtypes == np.int64]\nX_numeric = pd.concat([numeric_features, Z2], axis=1)'}, {'identified': 'dog_breeds', 'updated_code': "from sklearn.datasets import load_files\nfrom keras.utils import np_utils\nimport numpy as np\nfrom glob import glob\n\ndef load_dataset(path):\n    data = load_files(path)\n    dog_files = np.array(data['filenames'])\n    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n    return (dog_files, dog_targets)\ntrain_files, train_targets = load_dataset('dogImages/train')\nvalid_files, valid_targets = load_dataset('dogImages/valid')\ntest_files, test_targets = load_dataset('dogImages/test')\ndog_breeds = [item[20:-1] for item in sorted(glob('dogImages/train/*/'))]\nprint('There are %d total dog categories.' % len(dog_breeds))\nprint('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\nprint('There are %d training dog images.' % len(train_files))\nprint('There are %d validation dog images.' % len(valid_files))\nprint('There are %d test dog images.' % len(test_files))"}, {'identified': 'gradient_descent_optimizer', 'updated_code': 'num_nodes = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, num_nodes]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n    gradient_descent_optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*gradient_descent_optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    gradient_descent_optimizer = gradient_descent_optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n    train_prediction = tf.nn.softmax(logits)\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))'}, {'identified': 'test_url', 'updated_code': "test_url = 'http://www.google.com/search?q=fuzzing'\nmutations = 50"}, {'identified': 'pca_results', 'updated_code': 'pca = None\npca_samples = None\npca_results = vs.pca_results(good_data, pca)'}, {'identified': 'pattern_matrix', 'updated_code': "pattern_matrix = B12_a[:, -1::-1]\nHoM = hit_or_miss(X, pattern_matrix)\nplt.figure(figsize=[10, 10])\nplt.subplot(1, 2, 1)\nplt.imshow(X, interpolation='nearest')\nplt.subplot(1, 2, 2)\nplt.imshow(X, interpolation='nearest', alpha=0.8)\nplt.imshow(HoM, interpolation='nearest', alpha=0.5)"}, {'identified': 'unnormalized_log_marginal_C', 'updated_code': "nodes = [ST, F, C, W, f_I, f_ST, f_F, f_C, f_W, I, B, f_B, S, f_S]\nfor n in nodes:\n    n.reset()\nC.pending.add(f_C)\nW.pending.add(f_W)\nf_I.pending.add(I)\nf_S.pending.add(S)\nST.pending.add(f_ST)\nF.pending.add(f_F)\nmax_sum(nodes)\nI_ulm = I.unnormalized_log_marginal()\nS_ulm = S.unnormalized_log_marginal()\nST_ulm = ST.unnormalized_log_marginal()\nF_ulm = F.unnormalized_log_marginal()\nB_ulm = B.unnormalized_log_marginal()\nunnormalized_log_marginal_C = C.unnormalized_log_marginal()\nW_ulm = W.unnormalized_log_marginal()\nprint('I', I_ulm)\nprint('S', S_ulm)\nprint('ST', ST_ulm)\nprint('F', F_ulm)\nprint('B', B_ulm)\nprint('C', unnormalized_log_marginal_C)\nprint('W', W_ulm)"}, {'identified': 'canvas_image', 'updated_code': "canvas_image = np.zeros((800, 800, 3))\ny, x = np.indices(canvas_image.shape[:2])\ncx, cy = (300, 300)\ncircle1 = (x - cx) ** 2 + (y - cy) ** 2 < 200 ** 2\ncx, cy = (500, 500)\ncircle2 = (x - cx) ** 2 + (y - cy) ** 2 < 200 ** 2\ncx, cy = (300, 500)\ncircle3 = (x - cx) ** 2 + (y - cy) ** 2 < 200 ** 2\ncx, cy = (500, 300)\ncircle4 = (x - cx) ** 2 + (y - cy) ** 2 < 200 ** 2\ncanvas_image[circle1] += (0.2, 0, 0)\ncanvas_image[circle2] += (0.2, 0, 0)\ncanvas_image[circle3] += (0.2, 0, 0)\ncanvas_image[circle4] += (0.2, 0, 0)\ncanvas_image[circle1 & circle2 & circle3 & circle4] = (0, 0, 1)\nplt.imshow(canvas_image, interpolation='bilinear')"}, {'identified': 'extracted_features', 'updated_code': "extracted_features = []\nresult_train = []\ntot = 0\nfor string in ['share', 'comment', 'zan', 'content_len', '链接', '//@', '@', '#', '【', '《', '\\\\[']:\n    temp = []\n    for i in test[string + '_histogram']:\n        if isinstance(i, int):\n            temp.append(np.zeros(shape=8))\n            tot += 1\n        else:\n            temp.append(i[0])\n    extracted_features.append(np.asarray(temp))\n    temp = []\n    for i in train[string + '_histogram']:\n        temp.append(i[0])\n    result_train.append(np.asarray(temp))\n    train.drop(string + '_histogram', axis=1, inplace=True)\n    test.drop(string + '_histogram', axis=1, inplace=True)\ntrain.drop(['pid', 'uid'], inplace=True, axis=1)\ntest.drop(['pid', 'uid'], inplace=True, axis=1)\ntrain_y = train[['share', 'comment', 'zan']].values\ntrain.drop(['share', 'comme·nt', 'zan'], axis=1, inplace=True)\ntrain_x = train.values\ntest_x = test.values\nfor i in result_train:\n    train_x = np.c_[train_x, i]\nfor i in extracted_features:\n    test_x = np.c_[test_x, i]\nnp.save('processed_data/train3_np', train_x)\nnp.save('processed_data/test3_np', test_x)\nnp.save('processed_data/target3_np', train_y)"}, {'identified': 'input_data', 'updated_code': 'num_nodes = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, num_nodes]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n    train_prediction = tf.nn.softmax(logits)\n    input_data = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(input_data, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))'}, {'identified': 'x_axis_limit', 'updated_code': "ncomponents = 2\npca = decomp.PCA(n_components=ncomponents)\nraw_pca = pca.fit(np.array(x_train.iloc[:, first_egemaps_feature:]).astype(np.float))\nx_pca = raw_pca.transform(x_train.iloc[:, first_egemaps_feature:])\ngroups = x_train['pos']\nshort_pos = {'nouns': 'nn', 'verbs': 'vrb', 'other': 'oth', 'function_words': 'fct', 'adjectives': 'adj'}\ngroup_values = [short_pos[i] for i in groups.unique()]\nposIDs = groups.factorize()[0]\nlabels = np.array(x_train['word'])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.set_xlabel('PC 1')\nax.set_ylabel('PC 2')\ncolormap = plt.get_cmap('gnuplot', len(group_values))\npc1 = x_pca[:, 0]\npc2 = x_pca[:, 1]\np = ax.scatter(pc1, pc2, c=posIDs, cmap=colormap, s=10)\ncb = plt.colorbar(p)\nloc = np.arange(0, max(posIDs), max(posIDs) / float(len(group_values)))\ncb.set_ticks(loc)\ncb.set_ticklabels(group_values)\nfor label, x, y in zip(labels, pc1, pc2):\n    if y == np.amax(y) or x == np.amax(x):\n        plt.annotate(label, fontsize=2, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nif pos_filter == None:\n    axins = zoomed_inset_axes(ax, 2.5, loc=2)\n    x1, x_axis_limit, y1, y2 = (-3.1, 2.5, 6.3, 9.8)\n    axins.set_xlim(x1, x_axis_limit)\n    axins.set_ylim(y1, y2)\n    axins.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\n    axins2 = zoomed_inset_axes(ax, 2.5, loc=4)\n    x1, x_axis_limit, y1, y2 = (-4, 0.5, -8.5, -6.5)\n    axins2.set_xlim(x1, x_axis_limit)\n    axins2.set_ylim(y1, y2)\n    axins2.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins2, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nfilename = corpus + '_full_zoom.pdf'\nif pos_filter != None and len(pos_filter) == 1:\n    filename = corpus + '_pca_' + pos_filter[0][1] + '_' + pos_filter[0][2] + '.pdf'\nplt.savefig(filename, bbox_inches='tight')"}, {'identified': 'flower_class_probabilities', 'updated_code': "n_outputs = len(flower_classes)\nwith tf.name_scope('new_output_layer'):\n    flower_logits = tf.layers.dense(prelogits, n_outputs, name='flower_logits')\n    flower_class_probabilities = tf.nn.softmax(flower_logits, name='Y_proba')"}, {'identified': 'traffic_sign_layer_output', 'updated_code': "from tensorflow.contrib.layers import flatten\n\ndef LeNet(x):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    fc0 = flatten(conv2)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(120))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(120, 84), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(84))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(84, 43), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(43))\n    logits = tf.matmul(fc2, fc3_W) + fc3_b\n    return logits\n\ndef model_arc(x):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 9), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(9))\n    conv1 = tf.nn.bias_add(tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID'), conv1_b)\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 9, 27), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(27))\n    conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID'), conv2_b)\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv3_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 27, 81), mean=mu, stddev=sigma))\n    conv3_b = tf.Variable(tf.zeros(81))\n    conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='VALID'), conv3_b)\n    conv3 = tf.nn.relu(conv3)\n    conv3 = tf.nn.dropout(conv3, keep_prob=0.5)\n    conv3 = tf.nn.relu(conv3)\n    fc0 = flatten(conv3)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(2916, 972), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(972))\n    fc1 = tf.nn.xw_plus_b(fc0, fc1_W, fc1_b)\n    fc1 = tf.nn.relu(fc1)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(972, 324), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(324))\n    fc2 = tf.nn.xw_plus_b(fc1, fc2_W, fc2_b)\n    fc2 = tf.nn.relu(fc2)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(324, 108), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(108))\n    traffic_sign_layer_output = tf.nn.xw_plus_b(fc2, fc3_W, fc3_b)\n    traffic_sign_layer_output = tf.nn.relu(traffic_sign_layer_output)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(108, 43), mean=mu, stddev=sigma))\n    fc4_b = tf.Variable(tf.zeros(43))\n    logits = tf.nn.xw_plus_b(traffic_sign_layer_output, fc4_W, fc4_b)\n    return logits"}, {'identified': 'regularization_parameter', 'updated_code': 'seed = 56\nregularization_parameter = 7\nsplit_ratio = 0.5\nridge_regression_demo(x, y, regularization_parameter, split_ratio, seed)'}, {'identified': 'plane_orientation_cube', 'updated_code': 'plane_orientation_cube = P.mycarta.CubeYF_7\nplane_orientation_cube.show_as_blocks()'}, {'identified': 'roc_curve_gmm', 'updated_code': "y_scores = -probas_gmm\nprecision_gmm, recall_gmm, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_gmm = metrics.auc(recall_gmm, precision_gmm)\nroc_curve_gmm, tpr_gmm, _ = metrics.roc_curve(y_true, y_scores)\nauroc_gmm = metrics.roc_auc_score(y_true, y_scores)\nprint('AUROC: %.2f, PR AUC: %.2f' % (auroc_gmm, pr_auc_gmm))"}, {'identified': 'heatmap_filepath', 'updated_code': 'if visualize_dataset == True and round_values == True:\n    heatmap_filepath = exportpath + timestamp + \'heatmap.png\'\n    correlation_dataframe = data_no_nulls.corr()\n    mask = numpy.zeros_like(correlation_dataframe)\n    mask[numpy.triu_indices_from(mask)] = True\n    seaborn.heatmap(data=correlation_dataframe, cmap=[\'#b2182b\', \'#ef8a62\', \'#fddbc7\', \'#f7f7f7\', \'#d1e5f0\', \'#67a9cf\', \'#2166ac\'], center=0, square=True, linewidth=1, mask=mask, annot=True).get_figure().savefig(heatmap_filepath)\n    print("Heatmap saved to \'{}\'".format(heatmap_filepath))\nelse:\n    print(\'No heatmap was produced. Dataset contains no numeric features or visualize_dataset variable was set to False.\')'}, {'identified': 'income_labels', 'updated_code': "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, income_labels = train_test_split(features_final, income, test_size=0.2, random_state=0)\nprint('Training set has {} samples.'.format(X_train.shape[0]))\nprint('Testing set has {} samples.'.format(X_test.shape[0]))"}, {'identified': 'weights_variable', 'updated_code': "batch_size = 128\nembedding_size = 128\nskip_window = 1\nnum_skips = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    weights_variable = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=weights_variable, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"}, {'identified': 'revenue_line_plot', 'updated_code': "revenue_line_plot = sns.lineplot(df.index.year, df['Revenue'], color='#2ecc71', label='Revenue')"}, {'identified': 'sne_subplot', 'updated_code': "from matplotlib.ticker import MultipleLocator\nfilters = ['LSST_g']\nalpha = 1.0\nxminorticks = 10\npcc.utils.setup_plot_defaults()\nfig = plt.figure(figsize=[8, 4])\nfig.subplots_adjust(left=0.1, bottom=0.13, top=0.93, right=0.91, hspace=0, wspace=0)\nxaxis_label_string = '$\\\\textnormal{Time, MJD (days)}$'\nyaxis_label_string = '$\\\\textnormal{Flux, erg s}^{-1}\\\\textnormal{\\\\AA}^{-1}\\\\textnormal{cm}^{-2}$'\nsne_subplot = fig.add_subplot(111)\naxes_list = [sne_subplot]\nfor filter_key in filters:\n    plot_label_string = '$\\\\rm{' + sn.phot.data_filters['BessellV'].filter_name.replace('_', '\\\\_') + '}$'\n    plot_label_string_fake = '$\\\\rm{' + sn_fake.phot.data_filters[filter_key].filter_name.replace('_', '\\\\_') + ', simulated}$'\n    sne_subplot.errorbar(sn.phot.data['BessellV']['MJD'], sn.phot.data['BessellV']['flux'], yerr=sn.phot.data['BessellV']['flux_err'], capsize=0, fmt='x', color=sn.phot.data_filters['BessellV']._plot_colour, label=plot_label_string, ecolor=pcc.hex['batman'], mec=pcc.hex['batman'], alpha=alpha)\n    sne_subplot.fill_between(sn.lcfit.data['BessellV']['MJD'], sn.lcfit.data['BessellV']['flux_upper'], sn.lcfit.data['BessellV']['flux_lower'], color=pcc.hex['batman'], alpha=0.8, zorder=0)\n    sne_subplot.errorbar(sn_fake.phot.data[filter_key]['MJD'], sn_fake.phot.data[filter_key]['flux'], yerr=sn_fake.phot.data[filter_key]['flux_err'], capsize=0, fmt='o', color=pcc.hex['LSST_g'], label=plot_label_string_fake, ecolor=pcc.hex['batman'], mec=pcc.hex['batman'], alpha=alpha)\nxminorLocator = MultipleLocator(xminorticks)\nsne_subplot.spines['top'].set_visible(True)\nsne_subplot.xaxis.set_minor_locator(xminorLocator)\nplot_legend = sne_subplot.legend(loc='upper right', scatterpoints=1, markerfirst=False, numpoints=1, frameon=False, bbox_to_anchor=(1.0, 1.0), fontsize=12.0)\nsne_subplot.set_ylabel(yaxis_label_string)\nsne_subplot.set_xlabel(xaxis_label_string)\nprint(sne_subplot.get_xlim())\noutpath = '/Users/berto/projects/LSST/cadence/SN2007uy_consistency_check_BessellV_LSSTg'"}, {'identified': 'weight_init_stddev', 'updated_code': "def LeNet6(x, n_classes):\n    mu = 0\n    weight_init_stddev = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=weight_init_stddev))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=weight_init_stddev))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    fc0 = flatten(conv2)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean=mu, stddev=weight_init_stddev))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(252, 168), mean=mu, stddev=weight_init_stddev))\n    fc2_b = tf.Variable(tf.zeros(168))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(168, 84), mean=mu, stddev=weight_init_stddev))\n    fc3_b = tf.Variable(tf.zeros(84))\n    fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n    fc3 = tf.nn.relu(fc3)\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean=mu, stddev=weight_init_stddev))\n    fc4_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    return logits"}, {'identified': 'model_selector_tests', 'updated_code': 'from asl_test_model_selectors import TestSelectors\nmodel_selector_tests = unittest.TestLoader().loadTestsFromModule(TestSelectors())\nunittest.TextTestRunner().run(model_selector_tests)'}, {'identified': 'transformation_matrix', 'updated_code': "cov = X.T @ X / (X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:, idx]\nproject_X = X @ vec\nproject_V = vec.T @ vec\ntransformation_matrix = np.linalg.inv(vec)\ntransformation_matrix = transformation_matrix - transformation_matrix.mean(1)\nrevert_X = project_X @ transformation_matrix\nrevertedV = project_V @ transformation_matrix.T\n\ndef plot():\n    plt.figure(figsize=(15, 5))\n    plt.subplot(131)\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\n    pca11 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca12 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n    plt.subplot(132)\n    plt.scatter(project_X[y == 0, 0], project_X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(project_X[y == 1, 0], project_X[y == 1, 1], color='blue', alpha=0.5)\n    pca21 = plt.arrow(0, 0, *project_V[:, 0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca22 = plt.arrow(0, 0, *project_V[:, 1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n    plt.subplot(133)\n    plt.scatter(revert_X[y == 0, 0], revert_X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(revert_X[y == 1, 0], revert_X[y == 1, 1], color='blue', alpha=0.5)\n    pca21 = plt.arrow(0, 0, *revertedV[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca22 = plt.arrow(0, 0, *revertedV[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n    plt.show()\nplot()"}, {'identified': 'rnn_model', 'updated_code': "print(input_shape, 2 ** n_hidden)\nloss = msig.classification_type + '_crossentropy'\nfc_act = classifier_activation[msig.classification_type]\nout_neurons = 1 if msig.classification_type == 'binary' else n_classes\nconv1d_kwargs = dict(kernel_size=kernel_size, padding='causal', activation='relu')\ncompile_kwargs = dict(loss=loss, optimizer='adam', metrics=['accuracy'])\nrnn_model = Sequential()\nrnn_model.add(Conv1D(n_neurons, dilation_rate=1, input_shape=input_shape, **conv1d_kwargs))\nfor d in range(1, n_hidden):\n    rnn_model.add(Conv1D(n_neurons, dilation_rate=2 ** d, **conv1d_kwargs))\nrnn_model.add(Dense(out_neurons, activation=fc_act))\nrnn_model.compile(**compile_kwargs)\nrnn_model.summary()"}, {'identified': 'input_data', 'updated_code': "predictions = list()\nfor i in range(len(test_scaled)):\n    input_data, y = (test_scaled[i, 0:-1], test_scaled[i, -1])\n    yhat = forecast_lstm(lstm_model, 1, input_data)\n    yhat = invert_scale(scaler, input_data, yhat)\n    yhat = inverse_difference(raw_values, yhat, len(test_scaled) + 1 - i)\n    predictions.append(yhat)\n    expected = raw_values[len(train) + i + 1]\n    print('Month=%d, Predicted=%f, Expected=%f' % (i + 1, yhat, expected))\nrmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\nprint('Test RMSE: %.3f' % rmse)\npyplot.plot(raw_values[-12:])\npyplot.plot(predictions)\npyplot.show()"}, {'identified': 'customs_data_array', 'updated_code': 'customs_data_array = np.array(lista1)\nstats.describe(customs_data_array)'}, {'identified': 'test_prediction', 'updated_code': "batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n    layer1_biases = tf.Variable(tf.zeros([depth]))\n    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n    def model(data):\n        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(conv + layer1_biases)\n        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(conv + layer2_biases)\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        return tf.matmul(hidden, layer4_weights) + layer4_biases\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))"}, {'identified': 'county_language_data', 'updated_code': "county = df_county_data['County Name']\ncounty_language_data = df_county_data['Speak a language other than English']\nx_axis = np.arange(len(county_language_data))\ntick_locations = [value for value in x_axis]\nplt.bar(x_axis, county_language_data, color='r', align='center')\nplt.title('County ESL')\nplt.xlabel('Counties')\nplt.ylabel('Speak a language other than English')\nplt.text(140, 40, 'Note:\\nSpoken languages beside English for all counties in NJ, NY, & PA.')\nplt.savefig('Images/County_Speak a language other than English.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'test_batch_size_def', 'updated_code': "DEBUG = False\nJPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\nimage_dir = '../data/images'\ntest_images_dir = '../data/test_images'\nstored_images_resized = '../data/images_resized'\nstored_bottlenecks = '../data/bottlenecks'\ntmp_dir = '/tmp'\nbottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\nimages_resized_dir = os.path.join(tmp_dir, 'images_resized')\nsummaries_dir = os.path.join(tmp_dir, 'retrain_logs')\nmodel_dir = os.path.join(tmp_dir, 'inception')\ninception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\noutput_graph_orig = 'output_graph_orig.pb'\noutput_graph = 'output_graph.pb'\noutput_labels = 'output_labels.txt'\narchitecture = 'inception_v3'\nfinal_tensor_name = 'final_result'\nhow_many_training_steps = 500\nlearning_rate = 0.01\ntesting_percentage = 10\nvalidation_percentage = 10\neval_step_interval = 10\ntrain_batch_size = 100\ntest_batch_size_def = -1\nvalidation_batch_size = 100\nprint_misclassified_test_images = False\nflip_left_right = False\nrandom_crop = 0\nrandom_scale = 0\nrandom_brightness = 0\nforce_inception_download = False\nFLAGS = type('FlagsObject', (object,), {'architecture': architecture, 'model_dir': model_dir, 'intermediate_store_frequency': 0, 'summaries_dir': summaries_dir, 'learning_rate': learning_rate, 'image_dir': images_resized_dir, 'testing_percentage': testing_percentage, 'validation_percentage': validation_percentage, 'random_scale': random_scale, 'random_crop': random_crop, 'flip_left_right': flip_left_right, 'random_brightness': random_brightness, 'bottleneck_dir': bottleneck_dir, 'final_tensor_name': final_tensor_name, 'how_many_training_steps': how_many_training_steps, 'train_batch_size': train_batch_size, 'test_batch_size': test_batch_size_def, 'eval_step_interval': eval_step_interval, 'validation_batch_size': validation_batch_size, 'print_misclassified_test_images': print_misclassified_test_images, 'output_graph': output_graph, 'output_labels': output_labels})\nretrain.FLAGS = FLAGS"}, {'identified': 'local_chlamydia_prevalence', 'updated_code': 'cov_2012 = 0.100807801953\nadpc_2012 = 0.0111652211547\n[incsol, scrsol] = fsolve(lambda x: [test_diag_fun(x)[0] - cov_2012, test_diag_fun(x)[1] - adpc_2012], [0.09, 0.25])\nU_2012 = U_fun(incsol * p_asymp, sc + scrsol * p_true_pos, incsol * (1 - p_asymp), scrsol * p_true_pos + att_symp * p_true_pos)\nA_2012 = A_fun(incsol * p_asymp, sc + scrsol * p_true_pos, incsol * (1 - p_asymp), scrsol * p_true_pos + att_symp * p_true_pos)\nlocal_chlamydia_prevalence = S_fun(incsol * p_asymp, sc + scrsol * p_true_pos, incsol * (1 - p_asymp), scrsol * p_true_pos + att_symp * p_true_pos)\ncov_2013 = 0.173269822929\nadpc_2013 = 0.0216211803756\n[incsol, scrsol] = fsolve(lambda x: [test_diag_fun(x)[0] - cov_2013, test_diag_fun(x)[1] - adpc_2013], [0.09, 0.25])\ninc = incsol\nscr = scrsol\nparms = [incsol * p_asymp, sc + scrsol * p_true_pos, incsol * (1 - p_asymp), scrsol * p_true_pos + att_symp * p_true_pos]\nsol_n_lincs = odeint(dydt, [U_2012, A_2012, local_chlamydia_prevalence], linspace(0, 10, 1000), args=(parms,))'}, {'identified': 'inception_v3_url', 'updated_code': "import sys\nimport tarfile\nfrom six.moves import urllib\n\nTF_MODELS_URL = 'http://download.tensorflow.org/models'\ninception_v3_url = TF_MODELS_URL + '/inception_v3_2016_08_28.tar.gz'\nINCEPTION_PATH = os.path.join('datasets', 'inception')\nINCEPTION_V3_CHECKPOINT_PATH = os.path.join(INCEPTION_PATH, 'inception_v3.ckpt')\n\ndef download_progress(count, block_size, total_size):\n    percent = count * block_size * 100 // total_size\n    sys.stdout.write('\\rDownloading: {}%'.format(percent))\n    sys.stdout.flush()\n\ndef fetch_pretrained_inception_v3(url=inception_v3_url, path=INCEPTION_PATH):\n    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\n        return\n    os.makedirs(path, exist_ok=True)\n    tgz_path = os.path.join(path, 'inception_v3.tgz')\n    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n    inception_tgz = tarfile.open(tgz_path)\n    inception_tgz.extractall(path=path)\n    inception_tgz.close()\n    os.remove(tgz_path)"}, {'identified': 'melted_pollution_data', 'updated_code': "melted_pollution_data = pd.melt(stats.reset_index(), id_vars='time', var_name='LSOA').dropna()"}, {'identified': 'human_image_paths', 'updated_code': "import random\nrandom.seed(8675309)\nhuman_image_paths = np.array(glob('lfw/*/*'))\nrandom.shuffle(human_image_paths)\nprint('There are %d total human images.' % len(human_image_paths))"}, {'identified': 'sequence_mask_variable', 'updated_code': "with tf.variable_scope('train'):\n    if is_time_major:\n        logits = tf.transpose(logits, [1, 0, 2])\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dataset.tgt_out_ids, logits=logits)\n        sequence_mask_variable = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n    else:\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dataset.tgt_out_ids, logits=logits)\n        sequence_mask_variable = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n    loss = tf.reduce_sum(crossent * sequence_mask_variable) / tf.to_float(batch_size)\n    tf.summary.scalar('loss', loss)\n    learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n    max_global_norm = tf.placeholder(dtype=tf.float32, name='max_global_norm')\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.5)\n    params = tf.trainable_variables()\n    gradients = tf.gradients(loss, params)\n    for grad, var in zip(gradients, params):\n        tf.summary.histogram(var.op.name + '/gradient', grad)\n    gradients, _ = tf.clip_by_global_norm(gradients, max_global_norm)\n    for grad, var in zip(gradients, params):\n        tf.summary.histogram(var.op.name + '/clipped_gradient', grad)\n    update = optimizer.apply_gradients(zip(gradients, params))"}, {'identified': 'max_text_length', 'updated_code': 'max_text_length = 1000\nvalid_text = text[:max_text_length]\ntrain_text = text[max_text_length:]\ntrain_size = len(train_text)\nprint(train_size, train_text[:64])\nprint(max_text_length, valid_text[:64])'}, {'identified': 'nuclear_energy_table', 'updated_code': '"""Function to read the blank-spaced column data into a Pandas data frame (table)"""\n\ndef read_table(file_name):\n    """\n    Read table data into a `pandas` data frame (table).  \n    \n    Parameters\n    ----------\n    file_name: str, required\n        File name and its path relative to this notebook.\n    \n    Returns\n    -------\n    df: pandas.df\n        `Pandas` data frame (table).\n\n    Examples\n    --------\n    """\n    import pandas as pd\n    nuclear_energy_table = pd.read_csv(file_name, skiprows=6, delim_whitespace=True)\n    for c in nuclear_energy_table.columns:\n        if c == \'Y(NO-DIM)\':\n            continue\n        nuclear_energy_table = nuclear_energy_table.astype({c: float}, copy=False)\n    return nuclear_energy_table'}, {'identified': 'num_iterations', 'updated_code': "n_epochs = 10\nbatch_size = 40\nnum_iterations = len(flower_paths_and_classes_train) // batch_size\nwith tf.Session() as sess:\n    init.run()\n    inception_saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n    for epoch in range(n_epochs):\n        print('Epoch', epoch, end='')\n        for iteration in range(num_iterations):\n            print('.', end='')\n            X_batch, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        print('  Train accuracy:', acc_train)\n        save_path = saver.save(sess, './my_flowers_model')"}, {'identified': 'cluster_prediction_heatmap', 'updated_code': "df_diffs = (np.abs(samples - true_centers.iloc[0]) < np.abs(samples - true_centers.iloc[1])).applymap(lambda x: 0 if x else 1)\ncluster_prediction_heatmap = pd.concat([df_diffs, pd.Series(sample_preds, name='PREDICTION')], axis=1)\nsns.heatmap(cluster_prediction_heatmap, annot=True, cbar=False, yticklabels=['sample 0', 'sample 1', 'sample 2'], linewidth=0.1, square=True)\nplt.title('Samples closer to\\ncluster 0 or 1?')\nplt.xticks(rotation=45, ha='center')\nplt.yticks(rotation=0)"}, {'identified': 'calibration_data', 'updated_code': "X, Annotations = marconi['Soccer']\nfig = plt.figure(figsize=(16, 4))\n\ndef get_annot_img(ax, camera, frame):\n    ax.set_title('Camera ' + str(camera) + ' at frame ' + str(frame))\n    im = X[camera, frame]\n    ax.imshow(im)\n    Annotations_for_cam = Annotations[camera]\n    Annot_on_frame_cam = Annotations_for_cam[frame]\n    COLORS = ['red', 'yellow']\n    for i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n        ax.plot(head_x, head_y, color=COLORS[i])\n        for jx, jy, visible in joints:\n            ax.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\nget_annot_img(fig.add_subplot(121), 0, 0)\nget_annot_img(fig.add_subplot(122), 1, 0)\nplt.axis('off')\nplt.show()\nPs, Ks, calibration_data = marconi.get_calibration('Soccer', split_intrinsic_extrinsic=True)"}, {'identified': 'input_image', 'updated_code': "from skimage.io import imread\nfrom skimage import feature\n\ninput_image = imread('https://upload.wikimedia.org/wikipedia/commons/5/5f/MRI_EGC_sagittal.png')[-1::-1, :, :]\ncanny = feature.canny(input_image[:, :, 0], low_threshold=0.1 * 255, high_threshold=0.4 * 255) * 255\nplt.figure(figsize=[10, 10])\nplt.imshow(canny[-1::-1, :])"}, {'identified': 'pooled_flat_tensor', 'updated_code': "import tensorflow as tf\nheight = 28\nwidth = 28\nchannels = 1\nn_inputs = height * width\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = 'SAME'\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 1\nconv2_pad = 'SAME'\nconv2_dropout_rate = 0.25\npool3_fmaps = conv2_fmaps\nn_fc1 = 128\nfc1_dropout_rate = 0.5\nn_outputs = 10\nreset_graph()\nwith tf.name_scope('inputs'):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n    y = tf.placeholder(tf.int32, shape=[None], name='y')\n    training = tf.placeholder_with_default(False, shape=[], name='training')\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name='conv1')\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name='conv2')\nwith tf.name_scope('pool3'):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    pooled_flat_tensor = tf.reshape(pool3, shape=[-1, pool3_fmaps * 14 * 14])\n    pool3_flat_drop = tf.layers.dropout(pooled_flat_tensor, conv2_dropout_rate, training=training)\nwith tf.name_scope('fc1'):\n    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name='fc1')\n    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\nwith tf.name_scope('output'):\n    logits = tf.layers.dense(fc1, n_outputs, name='output')\n    Y_proba = tf.nn.softmax(logits, name='Y_proba')\nwith tf.name_scope('train'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\nwith tf.name_scope('init_and_save'):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"}, {'identified': 'transfer_effect_mean_invalid', 'updated_code': "transfer_effect_mean_invalid = ACCrunanalysis.loc[ACCrunanalysis['Run'] == 2].Invalid.mean()\nsms.DescrStatsW(ACCrunanalysis.loc[ACCrunanalysis['Run'] == 2].Invalid).tconfint_mean()"}, {'identified': 'extinction_estimation', 'updated_code': 'extinction_estimation = ext_pnicer.discretize()\nextinction_estimation.extinction'}, {'identified': 'half_life_subplot', 'updated_code': '"""Plot function for nuclides half-life"""\n\ndef plot_nuclides(nuclides):\n    from matplotlib import pyplot as plt\n    fig, half_life_subplot = plt.subplots(figsize=(18, 7))\n    half_life_subplot.plot([nc.Z for nc in nuclides.values()], [nc.half_life / 3600 / 24 / 365 for nc in nuclides.values()], \' \', color=\'black\', marker=\'x\', markersize=10)\n    min_z = min([nc.Z for nc in nuclides.values()])\n    max_z = max([nc.Z for nc in nuclides.values()])\n    half_life_subplot.xaxis.set_ticks(range(min_z, max_z + 1, 2))\n    half_life_subplot.set_xlim((min_z - 1, max_z + 1))\n    plt.xlabel(\'Nuclide Z Number\', fontsize=18)\n    plt.ylabel(\'$T_{1/2} [a]$\', fontsize=18)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=16)\n    ay1 = half_life_subplot.twiny()\n    ay1.set_xlim(half_life_subplot.get_xlim())\n    ay1.set_xticks([])\n    from mendeleev import element\n    ay1.set_xticks(range(min_z, max_z + 1), [element(z).symbol for z in range(min_z, max_z + 1)])\n    ay1.set_xticklabels([element(z).symbol for z in range(min_z, max_z + 1)], minor=True, fontsize=12)\n    min_a = min([nc.A for nc in nuclides.values()])\n    max_a = max([nc.A for nc in nuclides.values()])\n    plt.title(\'%i Nuclides: $%i \\\\leq A \\\\leq %i$ \' % (len(nuclides), min_a, max_a), fontsize=22)\n    half_life_subplot.grid(True)\n    plt.yscale(\'log\')\n    plt.show()\n    return'}, {'identified': 'unique_chars', 'updated_code': 'def encode_io_pairs(text, window_size, step_size):\n    unique_chars = sorted(list(set(text)))\n    num_chars = len(unique_chars)\n    inputs, outputs = window_transform_text(text, window_size, step_size)\n    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n    for i, sentence in enumerate(inputs):\n        for t, char in enumerate(sentence):\n            X[i, t, chars_to_indices[char]] = 1\n        y[i, chars_to_indices[outputs[i]]] = 1\n    return (X, y)'}, {'identified': 'county_names', 'updated_code': "county_names = df_county_data['County Name']\nhouse_size = df_county_data['Speak a language other than English']\nx_axis = np.arange(len(house_size))\ntick_locations = [value for value in x_axis]\nplt.bar(x_axis, house_size, color='r', align='center')\nplt.title('County ESL')\nplt.xlabel('Counties')\nplt.ylabel('Speak a language other than English')\nplt.text(140, 40, 'Note:\\nSpoken languages beside English for all counties in NJ, NY, & PA.')\nplt.savefig('Images/County_Speak a language other than English.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'package_errors', 'updated_code': 'import sys\nimport platform\nfrom importlib.util import find_spec, module_from_spec\n\ndef check_newer_version(version_inst, version_nec):\n    version_inst_split = version_inst.split(\'.\')\n    package_errors = version_nec.split(\'.\')\n    for i in range(min(len(version_inst_split), len(package_errors))):\n        if int(package_errors[i]) > int(version_inst_split[i]):\n            return False\n        elif int(package_errors[i]) < int(version_inst_split[i]):\n            return True\n    return True\nmodule_list = [(\'jupyter\', \'1.0.0\'), (\'matplotlib\', \'2.0.2\'), (\'numpy\', \'1.13.1\'), (\'python\', \'3.6.2\'), (\'sklearn\', \'0.19.0\'), (\'scipy\', \'0.19.1\'), (\'nb_conda\', \'2.2.1\')]\npackages_correct = True\npackages_errors = []\nfor module_name, version in module_list:\n    if module_name == \'scikit-learn\':\n        module_name = \'sklearn\'\n    if module_name == \'pyyaml\':\n        module_name = \'yaml\'\n    if \'python\' in module_name:\n        python_version = platform.python_version()\n        if not check_newer_version(python_version, version):\n            packages_correct = False\n            error = f\'Update {module_name} to version {version}. Current version is {python_version}.\'\n            packages_errors.append(error)\n            print(error)\n    else:\n        spec = find_spec(module_name)\n        if spec is None:\n            packages_correct = False\n            error = f\'Install {module_name} with version {version} or newer, it is required for this assignment!\'\n            packages_errors.append(error)\n            print(error)\n        else:\n            x = __import__(module_name)\n            if hasattr(x, \'__version__\') and (not check_newer_version(x.__version__, version)):\n                packages_correct = False\n                error = f\'Update {module_name} to version {version}. Current version is {x.__version__}.\'\n                packages_errors.append(error)\n                print(error)\ntry:\n    from google.colab import drive\n    packages_correct = False\n    error = "Please, don\'t use google colab!\\nIt will make it much more complicated for us to check your homework as it merges all the cells into one."\n    packages_errors.append(error)\n    print(error)\nexcept:\n    pass\npackages_errors = \'\\n\'.join(packages_errors)'}, {'identified': 'forward_computation_result', 'updated_code': 'forward_computation_result, _, _ = solver.forward(vp=model0.vp)'}, {'identified': 'test_image_tensors', 'updated_code': "from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\ntrain_tensors = paths_to_tensor(train_files).astype('float32') / 255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32') / 255\ntest_image_tensors = paths_to_tensor(test_files).astype('float32') / 255"}, {'identified': 'shuffled_indices', 'updated_code': 'def sgd_iter(x_train, t_train, W, b):\n    shuffled_indices = np.arange(len(x_train))\n    np.random.shuffle(shuffled_indices)\n    lr = 0.0001\n    logp = np.zeros(len(x_train))\n    for i in shuffled_indices:\n        x = x_train[i:i + 1]\n        t = t_train[i]\n        logp[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\n        W = W + lr * grad_w\n        b = b + lr * grad_b\n    logp_train = logp.mean()\n    return (logp_train, W, b)'}, {'identified': "'num_channels' or 'input_channels'", 'updated_code': "import tensorflow as tf\nheight = 28\nwidth = 28\nnum_channels = 1\nn_inputs = height * width\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = 'SAME'\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 1\nconv2_pad = 'SAME'\nconv2_dropout_rate = 0.25\npool3_fmaps = conv2_fmaps\nn_fc1 = 128\nfc1_dropout_rate = 0.5\nn_outputs = 10\nreset_graph()\nwith tf.name_scope('inputs'):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, num_channels])\n    y = tf.placeholder(tf.int32, shape=[None], name='y')\n    training = tf.placeholder_with_default(False, shape=[], name='training')\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name='conv1')\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name='conv2')\nwith tf.name_scope('pool3'):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 14 * 14])\n    pool3_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\nwith tf.name_scope('fc1'):\n    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name='fc1')\n    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\nwith tf.name_scope('output'):\n    logits = tf.layers.dense(fc1, n_outputs, name='output')\n    Y_proba = tf.nn.softmax(logits, name='Y_proba')\nwith tf.name_scope('train'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\nwith tf.name_scope('init_and_save'):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"}, {'identified': 'num_epochs', 'updated_code': "num_epochs = 10\nbatch_size = 100\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(num_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n        save_path = saver.save(sess, './my_mnist_model')"}, {'identified': 'traffic_sign_bar_plot', 'updated_code': "sign_frequencies = get_frequencies(y_train_augmented, sign_dict)\nfig, ax = plt.subplots(figsize=(15, 10))\nclasses = list(sign_dict.values())\nind = np.arange(len(classes))\nwidth = 0.8\ntraffic_sign_bar_plot = ax.bar(ind, sign_frequencies.values(), width, align='edge', alpha=0.5)\nax.set_ylabel('Frequency')\nax.set_title('Traffic Sign Classes')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(sign_frequencies.keys(), rotation=90)\nplt.show()"}, {'identified': 'to_minutes', 'updated_code': "def build_index(non_empty_days, from_time, to_time):\n    date_ranges = []\n    for date in non_empty_days:\n        yyyy, mm, dd = date.split('-')\n        from_hour, from_min = from_time.split(':')\n        to_hour, to_minutes = to_time.split(':')\n        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour), int(from_min), 0)\n        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour), int(to_minutes), 0)\n        date_ranges.append(pd.DataFrame({'OrganizedDateTime': pd.date_range(t1, t2, freq='1Min').values}))\n    agg = pd.concat(date_ranges, axis=0)\n    agg.index = agg['OrganizedDateTime']\n    return agg"}, {'identified': 'plot_font_size', 'updated_code': "SMALL_SIZE = 10\nMEDIUM_SIZE = 12\nBIGGER_SIZE = 16\nplot_font_size = 20\nplt.rc('font', size=plot_font_size)\nplt.rc('axes', titlesize=plot_font_size)\nplt.rc('axes', labelsize=plot_font_size)\nplt.rc('xtick', labelsize=BIGGER_SIZE)\nplt.rc('ytick', labelsize=BIGGER_SIZE)\nplt.rc('legend', fontsize=MEDIUM_SIZE)\nplt.rc('figure', titlesize=plot_font_size)"}, {'identified': 'current_word', 'updated_code': "num_steps = 100001\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    average_loss = 0\n    for step in range(num_steps):\n        batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += l\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss = average_loss / 2000\n            print('Average loss at step %d: %f' % (step, average_loss))\n            average_loss = 0\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                current_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log = 'Nearest to %s:' % current_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = '%s %s,' % (log, close_word)\n                print(log)\n    final_embeddings = normalized_embeddings.eval()"}, {'identified': 'scaled_state', 'updated_code': 'def featurize_state(state):\n    """\n    Returns the featurized representation for a state.\n    """\n    scaled_state = scaler.transform([state])\n    featurized = featurizer.transform(scaled_state)\n    return featurized[0]'}, {'identified': 'invalid_trial_accuracy_mean', 'updated_code': "ACCrunanalysis = pd.DataFrame()\nnew_acclists = [[] for list in range(0, 5)]\nfor ID in range(10, 86):\n    sub = adat[adat.subject == ID]\n    for runID in range(0, 4):\n        run = sub[sub.RunCounter == runID]\n        new_acclists[0].append(ID)\n        new_acclists[1].append(runID)\n        validACC_trials = run[run.TrialType == 'Valid'].Accuracy.mean()\n        invalid_trial_accuracy_mean = run[run.TrialType == 'Invalid'].Accuracy.mean()\n        new_acclists[2].append(validACC_trials)\n        new_acclists[3].append(invalid_trial_accuracy_mean)\nACCrunanalysis['SubjectID'] = new_acclists[0]\nACCrunanalysis['Run'] = new_acclists[1]\nACCrunanalysis['Valid'] = new_acclists[2]\nACCrunanalysis['Invalid'] = new_acclists[3]"}, {'identified': 'convolution_result', 'updated_code': "from tensorflow.python.framework import ops\nops.reset_default_graph()\nbatch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden_full_1 = 96\nnum_hidden_full_2 = 96\ngraph = tf.Graph()\nwith graph.as_default():\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    layer3_weights = init_weights([image_size * image_size * 64, num_hidden_full_1])\n    layer3_biases = init_weights([num_hidden_full_1], method='ones')\n    keep3 = tf.placeholder('float')\n    layer4_weights = init_weights([num_hidden_full_1, num_hidden_full_2])\n    layer4_biases = init_weights([num_hidden_full_2], method='ones')\n    keep4 = tf.placeholder('float')\n    layer5_weights = init_weights([num_hidden_full_2, num_labels])\n    layer5_biases = init_weights([num_labels], method='ones')\n    inception_1x1_weights = tf.Variable(tf.truncated_normal([1, 1, num_channels, depth], stddev=0.1))\n    inception_1x1_biases = tf.Variable(tf.zeros([depth]))\n    pre_inception_1x1_weights = tf.Variable(tf.truncated_normal([1, 1, num_channels, depth], stddev=0.1))\n    pre_inception_1x1_biases = tf.Variable(tf.zeros([depth]))\n    inception_1x1_pool_weights = tf.Variable(tf.truncated_normal([1, 1, num_channels, depth], stddev=0.1))\n    inception_1x1_pool_biases = tf.Variable(tf.zeros([depth]))\n    inception_3x3_weights = tf.Variable(tf.truncated_normal([3, 3, depth, depth], stddev=0.1))\n    inception_3x3_biases = tf.Variable(tf.zeros([depth]))\n    inception_5x5_weights = tf.Variable(tf.truncated_normal([5, 5, depth, depth], stddev=0.1))\n    inception_5x5_biases = tf.Variable(tf.zeros([depth]))\n\n    def inception_layer(data):\n        conv_1x1 = tf.nn.conv2d(data, inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n        conv_1x1 = tf.nn.relu(conv_1x1 + inception_1x1_biases)\n        print('1x1', conv_1x1.get_shape())\n        convolution_result = tf.nn.conv2d(data, pre_inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n        convolution_result = tf.nn.relu(convolution_result + pre_inception_1x1_biases)\n        conv_pool = tf.nn.avg_pool(data, [1, 3, 3, 1], [1, 1, 1, 1], padding='SAME')\n        conv_pool = tf.nn.conv2d(conv_pool, inception_1x1_pool_weights, [1, 1, 1, 1], padding='SAME')\n        conv_pool = tf.nn.relu(conv_pool + inception_1x1_pool_biases)\n        print('pool', conv_pool.get_shape())\n        conv_3x3 = tf.nn.conv2d(convolution_result, inception_3x3_weights, [1, 1, 1, 1], padding='SAME')\n        conv_3x3 = tf.nn.relu(conv_3x3 + inception_3x3_biases)\n        print('3x3', conv_3x3.get_shape())\n        conv_5x5 = tf.nn.conv2d(convolution_result, inception_5x5_weights, [1, 1, 1, 1], padding='SAME')\n        conv_5x5 = tf.nn.relu(conv_5x5 + inception_5x5_biases)\n        print('5x5', conv_5x5.get_shape())\n        inception_result = tf.concat(3, [conv_1x1, conv_3x3, conv_5x5, conv_pool])\n        print(inception_result.get_shape())\n        return inception_result\n\n    def model(data):\n        hidden = inception_layer(data)\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        hidden = tf.nn.dropout(hidden, keep3)\n        hidden = tf.nn.elu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n        hidden = tf.nn.dropout(hidden, keep4)\n        output = tf.matmul(hidden, layer5_weights) + layer5_biases\n        return output\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))"}, {'identified': 'biodiversity_index_N21', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\nindex = 0\nfor segment in segments:\n    segDF = data.loc[data[\'RiverSeg\'] == segment]\n    segDF = segDF[segDF["lam\'"] > 0]\n    segDF = segDF.sort_values(\'Year\')\n    x = segDF[\'Year\']\n    biodiversity_index_N21 = segDF[\'N21\']\n    biodiversity_index_N21.name = segment\n    plt.plot(x, biodiversity_index_N21, colors[index])\n    plt.legend(loc=(1.05, 0.2))\n    index += 1\nplt.title(\'Hill N21, \' + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel(\'N21\')\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + \'_Hill_N21.png\', bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'rock_x_world', 'updated_code': "def process_image(img):\n    warped, mask = perspect_transform(img, source, destination)\n    threshed = color_thresh(warped)\n    obs_map = np.absolute(np.float32(threshed) - 1) * mask\n    xpix, ypix = rover_coords(threshed)\n    world_size = data.worldmap.shape[0]\n    scale = 2 * dst_size\n    xpos = data.xpos[data.count]\n    ypos = data.ypos[data.count]\n    yaw = data.yaw[data.count]\n    x_world, y_world = pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale)\n    obsxpix, obsypix = rover_coords(obs_map)\n    obs_x_world, obs_y_world = pix_to_world(obsxpix, obsypix, xpos, ypos, yaw, world_size, scale)\n    data.worldmap[y_world, x_world, 2] = 255\n    data.worldmap[obs_y_world, obs_x_world, 0] = 255\n    nav_pix = data.worldmap[:, :, 2] > 0\n    data.worldmap[nav_pix, 0] = 0\n    rock_map = find_rocks(warped, levels=(110, 110, 50))\n    if rock_map.any():\n        rock_x, rock_y = rover_coords(rock_map)\n        rock_x_world, rock_y_world = pix_to_world(rock_x, rock_y, xpos, ypos, yaw, world_size, scale)\n        data.worldmap[rock_y_world, rock_x_world, :] = 255\n    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1] * 2, 3))\n    output_image[0:img.shape[0], 0:img.shape[1]] = img\n    output_image[0:img.shape[0], img.shape[1]:] = warped\n    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n    cv2.putText(output_image, 'Populate this image with your analyses to make a video!', (20, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n    if data.count < len(data.images) - 1:\n        data.count += 1\n    return output_image"}, {'identified': 'line_threshold', 'updated_code': 'def process_image(image):\n    gray = grayscale(image)\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray, kernel_size)\n    low_threshold = 60\n    high_threshold = 100\n    edges = canny(blur_gray, low_threshold, high_threshold)\n    imshape = image.shape\n    vertices = np.array([[(0, imshape[0]), (imshape[1] * 0.48, imshape[0] * 0.6), (imshape[1] * 0.52, imshape[0] * 0.6), (imshape[1], imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges, vertices)\n    rho = 1\n    theta = np.pi / 180\n    threshold = 90\n    min_line_length = 30\n    line_threshold = 30\n    line_image = np.copy(image) * 0\n    lines = hough_lines(masked_edges, rho, theta, threshold, min_line_length, line_threshold, vertices)\n    result = weighted_img(lines, image, α=0.8, β=1.0, λ=0.0)\n    return result'}, {'identified': 'train_features', 'updated_code': "import pickle\ntraining_file = '../traffic-signs-data/train.p'\nvalidation_file = '../traffic-signs-data/valid.p'\ntesting_file = '../traffic-signs-data/test.p'\nwith open(training_file, mode='rb') as f:\n    train = pickle.load(f)\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\nwith open(testing_file, mode='rb') as f:\n    test = pickle.load(f)\ntrain_features, y_train = (train['features'], train['labels'])\nX_valid, y_valid = (valid['features'], valid['labels'])\nX_test, y_test = (test['features'], test['labels'])"}, {'identified': 'ridge_regression_results', 'updated_code': 'def ridge_regression_demo(x, y, degree, ratio, seed):\n    """ridge regression demo."""\n    lambdas = np.logspace(-5, 0, 15)\n    raise NotImplementedError\n    raise NotImplementedError\n    ridge_regression_results = []  # previously \'variable_def\'\n    rmse_te = []\n    for ind, lambda_ in enumerate(lambdas):\n        print(\'proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\'.format(p=ratio, d=degree, l=lambda_, tr=ridge_regression_results[ind], te=rmse_te[ind]))\n    plot_train_test(ridge_regression_results, rmse_te, lambdas, degree)\n    raise NotImplementedError'}, {'identified': 'playlist_name_vectorizer', 'updated_code': "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\ncv_description = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', stop_words=None, ngram_range=(1, 1), analyzer='word')\ndt_mat_description = cv_description.fit_transform(playlist_df.playlist_description)\nplaylist_df['playlist_description_frequency'] = list(dt_mat_description.toarray())\ncv_name = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', stop_words=None, ngram_range=(1, 1), analyzer='word')\nplaylist_name_vectorizer = cv_name.fit_transform(playlist_df.playlist_name)\nplaylist_df['playlist_name_frequency'] = list(playlist_name_vectorizer.toarray())"}, {'identified': 'augmented_train_data', 'updated_code': None}, {'identified': 'experimental_data_histogram', 'updated_code': "faltwPHSData = readFlu(heprowPath + faltwPHSName, delim_whitespace=True, names=['lowE', 'absPHS', 'absSigma'], skiprows=3)\nmeasPHSData = readFlu(heprowPath + measPHSName, delim_whitespace=True, names=['lowE', 'absPHS', 'absSigma'], skiprows=3)\nfaltwPHSHisto = Histogram()\nfaltwPHSHisto.build_histo(faltwPHSData['lowE'].tolist(), bin_differentiation(faltwPHSData['lowE'].tolist(), faltwPHSData['absPHS'].tolist()), uncert=faltwPHSData['absSigma'].tolist(), edgeLoc=heprowBinBounds, name='FALTW')\nexperimental_data_histogram = Histogram()\nexperimental_data_histogram.build_histo(measPHSData['lowE'].tolist(), bin_differentiation(measPHSData['lowE'].tolist(), measPHSData['absPHS'].tolist()), uncert=measPHSData['absSigma'].tolist(), edgeLoc=heprowBinBounds, name='Measured')\nfaltwPHSHisto.plot(experimental_data_histogram, logY=True, title='33MeV Deutrons on Ta', xLabel='Light Yield [MeVee]', yLabel='Counts per MeVee')"}, {'identified': 'dropout_keep_prob', 'updated_code': "from tensorflow.python.framework import ops\nops.reset_default_graph()\nbatch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden_full_1 = 512\nnum_hidden_full_2 = 64\n\ndef init_weights(shape, method='xavier'):\n    if method == 'zeros':\n        return tf.Variable(tf.zeros(shape, dtype=tf.float32))\n    elif method == 'ones':\n        return tf.Variable(tf.ones(shape, dtype=tf.float32))\n    elif method == 'uniform':\n        return tf.Variable(tf.random_normal(shape, stddev=0.01, dtype=tf.float32))\n    elif method == 'altxavier':\n        low = -4 * np.sqrt(6.0 / (shape[0] + shape[1]))\n        high = 4 * np.sqrt(6.0 / (shape[0] + shape[1]))\n        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n    elif method == 'xavier':\n        sd = np.sqrt(3.0 / (shape[0] + shape[1]))\n        return tf.Variable(tf.truncated_normal(shape, stddev=sd))\n    else:\n        sd = np.sqrt(2.0 / shape[0])\n        return tf.Variable(tf.truncated_normal(shape, stddev=sd))\ngraph = tf.Graph()\nwith graph.as_default():\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n    layer1_biases = tf.Variable(tf.zeros([depth]))\n    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth * 2], stddev=0.1))\n    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth * 2]))\n    layer3_weights = init_weights([image_size // 4 * image_size // 4 * depth * 2, num_hidden_full_1])\n    layer3_biases = init_weights([num_hidden_full_1], method='ones')\n    dropout_keep_prob = tf.placeholder('float')\n    layer4_weights = init_weights([num_hidden_full_1, num_hidden_full_2])\n    layer4_biases = init_weights([num_hidden_full_2], method='ones')\n    keep4 = tf.placeholder('float')\n    layer5_weights = init_weights([num_hidden_full_2, num_labels])\n    layer5_biases = init_weights([num_labels], method='ones')\n\n    def model(data):\n        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n        conv = tf.nn.elu(conv + layer1_biases)\n        hidden = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n        conv = tf.nn.elu(conv + layer2_biases)\n        hidden = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        hidden = tf.nn.dropout(hidden, dropout_keep_prob)\n        hidden = tf.nn.elu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n        hidden = tf.nn.dropout(hidden, keep4)\n        output = tf.matmul(hidden, layer5_weights) + layer5_biases\n        return output\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))"}, {'identified': 'project_image', 'updated_code': "project_image = plt.imread(test_path + '61060_1/pdi_heat.jpg')\nplt.figure(figsize=(15, 15))\nplt.imshow(project_image)"}, {'identified': 'pm_hospital_data', 'updated_code': "pm_hospital_data['day'] = pm_hospital_data.time.dt.day"}, {'identified': 'energy_bins', 'updated_code': 'gROOT.ProcessLine(\'SimulationManipulation sm("{}",0)\'.format(rspPath))\ngROOT.ProcessLine(\'HistogramOperations ops\')\ngROOT.ProcessLine(\'HistogramWriter writer;\')\ngROOT.ProcessLine(\'lightTables.setBirksParams(1.0,6.90)\')\nenergy_bins = np.arange(rspEmin, rspEmax, rspEwidth)\nenergy_bins = np.append(energy_bins, rspEmax)\nrspLbins = np.arange(rspLmin, rspLmax, rspLwidth)\nrspLbins = np.append(rspLbins, rspLmax)\ngROOT.ProcessLine(\'const Int_t EBINS = {}; const Int_t LBINS = {};\'.format(len(energy_bins) - 1, len(rspLbins) - 1))\ngROOT.ProcessLine(\'Double_t eEdges[EBINS + 1] = {}{}{};\'.format(\'{\', \', \'.join((str(e) for e in energy_bins)), \'}\'))\ngROOT.ProcessLine(\'Double_t lEdges[LBINS + 1] = {}{}{};\'.format(\'{\', \', \'.join((str(e) for e in rspLbins)), \'}\'))\ngROOT.ProcessLine(\'axis1 = TAxis(EBINS,eEdges);\')\ngROOT.ProcessLine(\'axis2 = TAxis(LBINS,lEdges);\')\ngROOT.ProcessLine(\'TH2* matrix1=sm.getNormalizedResponseMatrix(axis1,axis2)\')\ngROOT.ProcessLine(\'writer.ResponseToHEPROW(matrix1,"EJ309_resp_03_1")\')\nfor detNum, detName in detNames.iteritems():\n    params = CalibParams(calPath + calNames[detNum])\n    gROOT.ProcessLine(\'TH2* smearMatrix{0} = ops.skewedGausSmearMatrix(matrix1, {1}, {2}, {3})\'.format(detNum, params.alpha, params.beta, params.gamma))\n    gROOT.ProcessLine(\'smearMatrix{0}->Draw("colz")\'.format(detNum))\n    gROOT.ProcessLine(\'writer.ResponseToHEPROW(smearMatrix{0},"{1}_smearedResp_03_1")\'.format(detNum, detName))\n    pause()'}, {'identified': 'edges', 'updated_code': 'def process_image(image):\n    """ Filter color """\n    color_select = np.copy(image)\n    rgb_threshold = [200, 150, 95]\n    thresholds = (image[:, :, 0] < rgb_threshold[0]) | (image[:, :, 1] < rgb_threshold[1]) | (image[:, :, 2] < rgb_threshold[2])\n    color_select[thresholds] = [0, 0, 0]\n    gray = grayscale(color_select)\n    blurred = gaussian_blur(gray, 3)\n    edges = canny(blurred, 50, 150)\n    xsize = image.shape[1]\n    ysize = image.shape[0]\n    vertices = np.array([[(0, ysize), (xsize / 2, ysize / 1.71), (xsize / 2, ysize / 1.71), (xsize, ysize)]], dtype=np.int32)\n    regioned = region_of_interest(edges, vertices)\n    hough = hough_lines(regioned, 1, np.pi / 180, 35, 35, 20)\n    result = weighted_img(hough, image)\n    return result'}, {'identified': 'null_value_ratio', 'updated_code': 'null_value_ratio = null_count_series / totalcount\npercent_filled = filled_count_series / totalcount'}, {'identified': 'covariance_matrix', 'updated_code': "covariance_matrix = X.T @ X / (X.shape[0] - 1)\nval, vec = np.linalg.eigh(covariance_matrix)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:, idx]\nproject_X = X @ vec\nproject_V = vec.T @ vec\nrevert_X = project_X @ np.linalg.inv(vec)\nrevertedV = project_V @ np.linalg.inv(vec).T\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\nplt.subplot(132)\nplt.scatter(project_X[y == 0, 0], project_X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(project_X[y == 1, 0], project_X[y == 1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *project_V[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *project_V[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\nplt.subplot(133)\nplt.scatter(revert_X[y == 0, 0], revert_X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(revert_X[y == 1, 0], revert_X[y == 1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *revertedV[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *revertedV[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\nplt.show()"}, {'identified': 'corr_income_grad_rate', 'updated_code': "corr_pov = abs(df_county_data['Poverty Rate'].corr(grad_rate))\ncorr_hh = abs(df_county_data['Household Size'].corr(grad_rate))\ncorr_job = abs(df_county_data['Unemployment Rate'].corr(grad_rate))\ncorr_income_grad_rate = abs(df_county_data['Median Income'].corr(grad_rate))\ncorr_ESL = abs(df_county_data['Speak a language other than English'].corr(grad_rate))\nprint('Correlations')\nprint('Poverty:             ' + '{:.4f}'.format(corr_pov))\nprint('Median Income:       ' + '{:.4f}'.format(corr_income_grad_rate))\nprint('Unemployment Rates:  ' + '{:.4f}'.format(corr_job))\nprint('Non-English at Home: ' + '{:.4f}'.format(corr_ESL))\nprint('Household Size:      ' + '{:.4f}'.format(corr_hh))"}, {'identified': 'w_b_probabilities', 'updated_code': "I = Variable(name='I', num_states=2)\nS = Variable(name='S', num_states=2)\nST = Variable(name='ST', num_states=2)\nF = Variable(name='F', num_states=2)\nB = Variable(name='B', num_states=2)\nC = Variable(name='C', num_states=2)\nW = Variable(name='W', num_states=2)\nf_I = Factor(name='p(I)', f=np.array([0.95, 0.05]), neighbours=[I])\nf_S = Factor(name='p(S)', f=np.array([0.8, 0.2]), neighbours=[S])\nprob_ST = [[0.999, 0.7], [0.001, 0.3]]\nf_ST = Factor(name='p(ST |I)', f=np.array(prob_ST), neighbours=[ST, I])\nprob_F = [[0.95, 0.1], [0.05, 0.9]]\nf_F = Factor(name='p(F |I)', f=np.array(prob_F), neighbours=[F, I])\nprob_B = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\nf_B = Factor(name='p(B |I, S)', f=np.array(prob_B), neighbours=[B, I, S])\nprob_C = [[0.93, 0.2], [0.07, 0.8]]\nf_C = Factor(name='p(C |B)', f=np.array(prob_C), neighbours=[C, B])\nw_b_probabilities = [[0.999, 0.4], [0.001, 0.6]]\nf_W = Factor(name='p(W |B)', f=np.array(w_b_probabilities), neighbours=[W, B])"}, {'identified': 'tag_state_mapping', 'updated_code': 'basic_model = HiddenMarkovModel(name=\'base-hmm-tagger\')\ntags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\nemission_counts = pair_counts(tags, words)\ntag_state_mapping = {}\nfor tag, word_dict in emission_counts.items():\n    emission_dict = defaultdict(float)\n    for word in word_dict.keys():\n        emission_dict[word] = emission_counts[tag][word] / tag_unigrams[tag]\n    state_emission = DiscreteDistribution(dict(emission_dict))\n    tag_state_mapping[tag] = State(state_emission, name=tag)\nbasic_model.add_states(list(tag_state_mapping.values()))\nfor tag in data.training_set.tagset:\n    state = tag_state_mapping[tag]\n    basic_model.add_transition(basic_model.start, state, tag_starts[tag] / len(data.training_set))\n    basic_model.add_transition(state, basic_model.end, tag_ends[tag] / tag_unigrams[tag])\n    for next_tag in data.training_set.tagset:\n        next_state = tag_state_mapping[next_tag]\n        basic_model.add_transition(state, next_state, tag_bigrams[tag, next_tag] / tag_unigrams[tag])\nbasic_model.bake()\nassert all((tag in set((s.name for s in basic_model.states)) for tag in data.training_set.tagset)), \'Every state in your network should use the name of the associated tag, which must be one of the training set tags.\'\nassert basic_model.edge_count() == 168, \'Your network should have an edge from the start node to each state, one edge between every \' + \'pair of tags (states), and an edge from each state to the end node.\'\nHTML(\'<div class="alert alert-block alert-success">Your HMM network topology looks good!</div>\')'}, {'identified': 'pollution_subset', 'updated_code': 'pollution_subset = subset'}, {'identified': 'image_width', 'updated_code': 'def padRightDownCorner(img, stride, padValue):\n    h = img.shape[0]\n    image_width = img.shape[1]\n    pad = 4 * [None]\n    pad[0] = 0\n    pad[1] = 0\n    pad[2] = 0 if h % stride == 0 else stride - h % stride\n    pad[3] = 0 if image_width % stride == 0 else stride - image_width % stride\n    img_padded = img\n    pad_up = np.tile(img_padded[0:1, :, :] * 0 + padValue, (pad[0], 1, 1))\n    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n    pad_left = np.tile(img_padded[:, 0:1, :] * 0 + padValue, (1, pad[1], 1))\n    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n    pad_down = np.tile(img_padded[-2:-1, :, :] * 0 + padValue, (pad[2], 1, 1))\n    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n    pad_right = np.tile(img_padded[:, -2:-1, :] * 0 + padValue, (1, pad[3], 1))\n    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n    return (img_padded, pad)\n\nclass DataBatch(object):\n\n    def __init__(self, data, label, pad=0):\n        self.data = [data]\n        self.label = [label]\n        self.pad = pad'}, {'identified': 'keypoint_heatmap', 'updated_code': "for i in range(len(multiplier)):\n    scale = multiplier[i]\n    imageToTest = cv.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv.INTER_CUBIC)\n    imageToTest_padded, pad = padRightDownCorner(imageToTest, 8, 128)\n    transposeImage = np.transpose(np.float32(imageToTest_padded[:, :, :]), (2, 0, 1)) / 256 - 0.5\n    testimage = transposeImage\n    cmodel = mx.mod.Module(symbol=sym, label_names=[])\n    cmodel.bind(data_shapes=[('data', (1, 3, testimage.shape[1], testimage.shape[2]))])\n    cmodel.init_params(arg_params=arg_params, aux_params=aux_params)\n    onedata = DataBatch(mx.nd.array([testimage[:, :, :]]), 0)\n    cmodel.forward(onedata)\n    result = cmodel.get_outputs()\n    keypoint_heatmap = np.moveaxis(result[1].asnumpy()[0], 0, -1)\n    keypoint_heatmap = cv.resize(keypoint_heatmap, (0, 0), fx=model['stride'], fy=model['stride'], interpolation=cv.INTER_CUBIC)\n    keypoint_heatmap = keypoint_heatmap[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]\n    keypoint_heatmap = cv.resize(keypoint_heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv.INTER_CUBIC)\n    heatmap_avg = heatmap_avg + keypoint_heatmap / len(multiplier)\n    f = plt.figure(i)\n    plt.imshow(oriImg[:, :, [2, 1, 0]])\n    ax2 = plt.imshow(keypoint_heatmap[:, :, 18], alpha=0.5)\n    f.show()"}, {'identified': 'total_distance_traveled', 'updated_code': 'import random\n\ndef compute_fitness(solution):\n    """\n        This function returns the total distance traveled on the current road trip.\n        \n        The genetic algorithm will favor road trips that have shorter\n        total distances traveled.\n    """\n    total_distance_traveled = 0.0\n    for index in range(len(solution)):\n        waypoint1 = solution[index - 1]\n        waypoint2 = solution[index]\n        total_distance_traveled += waypoint_distances[frozenset([waypoint1, waypoint2])]\n    return total_distance_traveled\n\n# Rest of the code remains the same'}, {'identified': 'detected_faces', 'updated_code': 'def face_detector(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    detected_faces = face_cascade.detectMultiScale(gray)\n    return len(detected_faces) > 0'}, {'identified': 'lstm_variable_def', 'updated_code': 'num_nodes = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    variable_def = tf.Variable(tf.zeros([1, num_nodes]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n    Note that in this formulation, we omit the various connections between the\n    previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + variable_def)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,'}, {'identified': 'limb_part_index', 'updated_code': "connection_all = []\nspecial_k = []\nmid_num = 10\nfor k in range(len(mapIdx)):\n    score_mid = paf_avg[:, :, [x - 19 for x in mapIdx[k]]]\n    candA = all_peaks[limbSeq[k][0] - 1]\n    candB = all_peaks[limbSeq[k][1] - 1]\n    nA = len(candA)\n    nB = len(candB)\n    limb_part_index, indexB = limbSeq[k]\n    if nA != 0 and nB != 0:\n        connection_candidate = []\n        for i in range(nA):\n            for j in range(nB):\n                vec = np.subtract(candB[j][:2], candA[i][:2])\n                norm = math.sqrt(vec[0] * vec[0] + vec[1] * vec[1])\n                vec = np.divide(vec, norm)\n                startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), np.linspace(candA[i][1], candB[j][1], num=mid_num))\n                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] for I in range(len(startend))])\n                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] for I in range(len(startend))])\n                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                score_with_dist_prior = sum(score_midpts) / len(score_midpts) + min(0.5 * oriImg.shape[0] / norm - 1, 0)\n                criterion1 = len(np.nonzero(score_midpts > param['thre2'])[0]) > 0.8 * len(score_midpts)\n                criterion2 = score_with_dist_prior > 0\n                if criterion1 and criterion2:\n                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior + candA[i][2] + candB[j][2]])\n        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n        connection = np.zeros((0, 5))\n        for c in range(len(connection_candidate)):\n            i, j, s = connection_candidate[c][0:3]\n            if i not in connection[:, 3] and j not in connection[:, 4]:\n                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n                if len(connection) >= min(nA, nB):\n                    break\n        connection_all.append(connection)\n    else:\n        special_k.append(k)\n        connection_all.append([])"}, {'identified': 'playlist_freq_dist_visualizer', 'updated_code': 'from yellowbrick.text import FreqDistVisualizer\nplt.figure(figsize=(15, 20))\nplaylist_freq_dist_visualizer = FreqDistVisualizer(n=100, features=cv_description.get_feature_names())'}, {'identified': 'input_noise', 'updated_code': "batch_size = 100\nepochs = 100\nsamples = []\nlosses = []\nsaver = tf.train.Saver(var_list=g_vars)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for e in range(epochs):\n        for ii in range(mnist.train.num_examples // batch_size):\n            batch = mnist.train.next_batch(batch_size)\n            batch_images = batch[0].reshape((batch_size, 784))\n            batch_images = batch_images * 2 - 1\n            input_noise = np.random.uniform(-1, 1, size=(batch_size, z_size))\n            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: input_noise})\n            _ = sess.run(g_train_opt, feed_dict={input_z: input_noise})\n        train_loss_d = sess.run(d_loss, {input_z: input_noise, input_real: batch_images})\n        train_loss_g = g_loss.eval({input_z: input_noise})\n        print('Epoch {}/{}...'.format(e + 1, epochs), 'Discriminator Loss: {:.4f}...'.format(train_loss_d), 'Generator Loss: {:.4f}'.format(train_loss_g))\n        losses.append((train_loss_d, train_loss_g))\n        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n        gen_samples = sess.run(generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha), feed_dict={input_z: sample_z})\n        samples.append(gen_samples)\n        saver.save(sess, './checkpoints/generator.ckpt')\nwith open('train_samples.pkl', 'wb') as f:\n    pkl.dump(samples, f)"}, {'identified': 'pos_tags_generator', 'updated_code': 'pos_tags_generator = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\nprint(type(pos_tags_generator))\nprint(type(words))'}, {'identified': 'num_channels', 'updated_code': "height = 28\nwidth = 28\nnum_channels = 1\nn_inputs = height * width\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = 'SAME'\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 2\nconv2_pad = 'SAME'\npool3_fmaps = conv2_fmaps\nn_fc1 = 64\nn_outputs = 10\nreset_graph()\nwith tf.name_scope('inputs'):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, num_channels])\n    y = tf.placeholder(tf.int32, shape=[None], name='y')\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name='conv1')\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name='conv2')\nwith tf.name_scope('pool3'):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\nwith tf.name_scope('fc1'):\n    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name='fc1')\nwith tf.name_scope('output'):\n    logits = tf.layers.dense(fc1, n_outputs, name='output')\n    Y_proba = tf.nn.softmax(logits, name='Y_proba')\nwith tf.name_scope('train'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\nwith tf.name_scope('init_and_save'):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"}, {'identified': 'log_likelihood', 'updated_code': "my_testword = 'CHOCOLATE'\nmodel, log_likelihood = train_a_word(my_testword, 3, features_ground)\nshow_model_stats(my_testword, model)\nprint('logL = {}'.format(log_likelihood))"}, {'identified': 'age_of_acquisition', 'updated_code': "corpus = 'BrentProvidence'\nprovidence_data_file = os.path.join('data/words_sentences/providence_avg_prosody_pos.csv')\nbrent_data_file = os.path.join('data/words_sentences/brent_avg_prosody_pos.csv')\nboth_data_file = os.path.join('data/words_sentences/brentprovidence_avg_prosody_pos.csv')\nfeatures = list(range(1, 93))\npos_filter = None\ny = 'y'\nprint('extracting providence...')\nprovidence_x_train, age_of_acquisition, _, _, _, _, labels = get_data_from_tsv(providence_data_file, x_fields=features, y_field=y, x_filter=pos_filter, train_portion=1.0, shuffle=False)\nprint('extracting brent...')\nbrent_x_train, brent_y_train, _, _, _, _, labels = get_data_from_tsv(brent_data_file, x_fields=features, y_field=y, x_filter=pos_filter, train_portion=1.0, shuffle=False)\nprint('extracting brentprovidence...')\nboth_x_train, both_y_train, _, _, _, _, labels = get_data_from_tsv(both_data_file, x_fields=features, y_field=y, x_filter=pos_filter, train_portion=1.0, shuffle=False)\nif corpus == 'Providence':\n    x_train = providence_x_train\n    y_train = age_of_acquisition\nelif corpus == 'Brent':\n    x_train = brent_x_train\n    y_train = brent_y_train\nelif corpus == 'BrentProvidence':\n    x_train = both_x_train\n    y_train = both_y_train\nfirst_numeric_feature = x_train.columns.tolist().index('log_length')\nfirst_egemaps_feature = x_train.columns.tolist().index('F0semitoneFrom27.5Hz_sma3nz_amean')\nprint(first_numeric_feature, first_egemaps_feature)"}, {'identified': 'visualization_figure', 'updated_code': "X, Annotations = marconi['Soccer']\ncamera = 3\nframe = 79\nvisualization_figure = plt.figure(figsize=(16, 8))\nax = visualization_figure.add_subplot(111)\nim = X[camera, frame]\nax.imshow(im)\nAnnotations_for_cam = Annotations[camera]\nAnnot_on_frame_cam = Annotations_for_cam[frame]\nCOLORS = ['red', 'yellow']\nfor i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\n    head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n    ax.plot(head_x, head_y, color=COLORS[i])\n    for jx, jy, visible in joints:\n        plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\nplt.axis('off')\nplt.show()"}, {'identified': 'total_loss', 'updated_code': "num_steps = 100001\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    total_loss = 0\n    for step in range(num_steps):\n        batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        total_loss += l\n        if step % 2000 == 0:\n            if step > 0:\n                total_loss = total_loss / 2000\n            print('Average loss at step %d: %f' % (step, total_loss))\n            total_loss = 0\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log = 'Nearest to %s:' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = '%s %s,' % (log, close_word)\n                print(log)\n    final_embeddings = normalized_embeddings.eval()"}, {'identified': 'playlist_features', 'updated_code': "playlist_features = []\nnot_pid_importances = []\nfor feature in feature_names:\n    if 'playlist_pid' not in feature:\n        playlist_features.append(feature)\n        idx = feature_names.index(feature)\n        not_pid_importances.append(top50importance[idx])"}, {'identified': 'placeholder', 'updated_code': 'clusterer = None\npreds = None\ncenters = None\nsample_preds = None\nplaceholder = None'}, {'identified': 'feature_matrix', 'updated_code': 'import pyprind\npbar = pyprind.ProgBar(45)\nclasses = np.array([0, 1])\nfor _ in range(45):\n    feature_matrix, y_train = get_minibatch(doc_stream, size=1000)\n    if not feature_matrix:\n        break\n    feature_matrix = vect.transform(feature_matrix)\n    clf.partial_fit(feature_matrix, y_train, classes=classes)\n    pbar.update()'}, {'identified': 'day', 'updated_code': "def build_index(non_empty_days, from_time, to_time):\n    date_ranges = []\n    for date in non_empty_days:\n        yyyy, mm, day = date.split('-')\n        from_hour, from_min = from_time.split(':')\n        to_hour, to_min = to_time.split(':')\n        t1 = datetime(int(yyyy), int(mm), int(day), int(from_hour), int(from_min), 0)\n        t2 = datetime(int(yyyy), int(mm), int(day), int(to_hour), int(to_min), 0)\n        date_ranges.append(pd.DataFrame({'OrganizedDateTime': pd.date_range(t1, t2, freq='1Min').values}))\n    agg = pd.concat(date_ranges, axis=0)\n    agg.index = agg['OrganizedDateTime']\n    return agg"}, {'identified': 'extended_lines_image', 'updated_code': 'def rbg_to_hls(img):\n    """ \n    Takes an RGB image and converts it to HLS.\n    Returns the converted image (3 channels)\n    """\n    hls_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    return hls_image\n\ndef merge_channels(images):\n    """\n    Merge images from three different channels\n     - images: a list of 3 images, each in a channel\n    """\n    merged = weighted_img(images[0], images[1], α=0.5, β=0.5, λ=0.0)\n    merged = weighted_img(merged, images[2], α=1.0, β=0.5, λ=0.0)\n    return merged\n\ndef lane_detection_ppline_3_channels(image, k_size=3, vertex_ratio_h=0.45, vertex_ratio_v=0.6, low_thresh=50, high_thresh=200, L2gradient=False, rho=2, theta=1 * np.pi / 180.0, min_votes=15, min_line_len=40, max_line_gap=20, angle=3 * np.pi / 16, angle_thresh=np.pi / 16, debug=False):\n    """\n    Takes an image and parameters and applies the lane detection pipeline.\n    Returns an image combining the original and the extended lines detected\n    by the algorithm.\n     - debug: Whether or not to display the images after each step of the process, for\n     debugging or tuning purposes.\n    """\n    max_y, max_x = image.shape[:2]\n    roi = np.array([[(0, max_y), (round(max_x * vertex_ratio_h), round(max_y * vertex_ratio_v)), (round(max_x * (1 - vertex_ratio_h)), round(max_y * vertex_ratio_v)), (max_x, max_y)]])\n    if debug:\n        plt.subplot(5, 3, 1)\n        plt.imshow(image)\n    blur = gaussian_blur(image, k_size)\n    if debug:\n        plt.subplot(5, 3, 2)\n        plt.imshow(blur)\n    hls = rbg_to_hls(blur)\n    if debug:\n        plt.subplot(5, 3, 3)\n        plt.imshow(hls)\n    edges_list = []\n    for chan in range(0, 3):\n        edges_list.append(canny(hls[:, :, chan], low_thresh, high_thresh, L2gradient=L2gradient))\n        if debug:\n            plt.subplot(5, 3, chan + 4)\n            plt.imshow(edges_list[chan])\n    masked_edges_list = []\n    for chan in range(0, 3):\n        masked_edges_list.append(region_of_interest(edges_list[chan], roi))\n        if debug:\n            plt.subplot(5, 3, chan + 7)\n            plt.imshow(masked_edges_list[chan])\n    lines_list = []\n    for chan in range(0, 3):\n        lines_list.append(hough_lines(masked_edges_list[chan], rho, theta, min_votes, min_line_len, max_line_gap))\n        if debug:\n            plt.subplot(5, 3, chan + 10)\n            plt.imshow(lines_list[chan][0])\n    lines = np.zeros((1, 1, 4))\n    for chan in range(0, 3):\n        lines = np.concatenate((lines, lines_list[chan][1]), axis=0)\n    if debug:\n        hls_lines_image = merge_channels([lines_list[0][0], lines_list[1][0], lines_list[2][0]])\n        plt.subplot(5, 3, 13)\n        plt.imshow(hls_lines_image)\n    try:\n        extended_lines_image = extend_lines(image, lines, angle=angle, angle_thresh=angle_thresh)\n        if debug:\n            plt.subplot(5, 3, 14)\n            plt.imshow(extended_lines_image)\n    except IndexError:\n        print(\'Error. Try relaxing your angle parameters a litte.\')\n    return extended_lines_image\n\ndef process_image_3_channels(image, k_size=3, vertex_ratio_h=0.45, vertex_ratio_v=0.6, low_thresh=50, high_thresh=200, L2gradient=False, rho=2, theta=1 * np.pi / 180.0, min_votes=15, min_line_len=40, max_line_gap=20, angle=3 * np.pi / 16, angle_thresh=np.pi / 16, debug=False):\n    result = lane_detection_ppline_3_channels(image, k_size=k_size, low_thresh=low_thresh, high_thresh=high_thresh, L2gradient=L2gradient, rho=rho, theta=theta, min_votes=min_votes, min_line_len=min_line_len, max_line_gap=max_line_gap, angle=angle, angle_thresh=angle_threshold, debug=debug)\n    return result'}, {'identified': 'transformed_plane_mapping', 'updated_code': "fn = path.join(DATA, 'field-orientations.geojson')\nwith fiona.open(fn) as ds:\n    for i, item in ds.items():\n        p = item['properties']\n        if p['planeType'].strip() != 'Bedding':\n            continue\n        asm = p.get('aster_smoothed')\n        alt = asm\n        alt -= 40\n        center = (*item['geometry']['coordinates'], alt)\n        err = 0.1 * N.pi / 180\n        a = ReconstructedPlane(p['strike'], p['dip'], 0, err, err)\n        transformed_plane_mapping = a.to_mapping(center=center, color='#444', type='in-situ')\n        collection.append(transformed_plane_mapping)\nremovedUIDs = ['89636280', '6031fd6f']\ncollection = [c for c in collection if 1600 < c['center'][2] < 1680]\ncollection = [c for c in collection if c['uid'] not in removedUIDs]"}, {'identified': 'F_ulm', 'updated_code': "nodes = [ST, F, C, W, f_I, f_ST, f_F, f_C, f_W, I, B, f_B, S, f_S]\nfor n in nodes:\n    n.reset()\nC.pending.add(f_C)\nW.pending.add(f_W)\nf_I.pending.add(I)\nf_S.pending.add(S)\nST.pending.add(f_ST)\nF.pending.add(f_F)\nmax_sum(nodes)\nI_ulm = I.unnormalized_log_marginal()\nS_ulm = S.unnormalized_log_marginal()\nST_ulm = ST.unnormalized_log_marginal()\nF_ulm = F.unnormalized_log_marginal()\nB_ulm = B.unnormalized_log_marginal()\nC_ulm = C.unnormalized_log_marginal()\nW_ulm = W.unnormalized_log_marginal()\nprint('I', I_ulm)\nprint('S', S_ulm)\nprint('ST', ST_ulm)\nprint('F', F_ulm)\nprint('B', B_ulm)\nprint('C', C_ulm)\nprint('W', W_ulm)"}, {'identified': 'predicted_values', 'updated_code': "def r2(labels, predictions):\n    mean_labels = tf.reduce_mean(labels)\n    total_sum_squares = tf.reduce_sum((labels - mean_labels) ** 2)\n    residual_sum_squares = tf.reduce_sum((labels - predictions) ** 2)\n    r2_score = 1 - residual_sum_squares / total_sum_squares\n    return r2_score\npredicted_values = model.predict(X)\nr2_score = r2(y, predicted_values)\nprint('R2 score: ', r2_score.numpy())"}, {'identified': 'keypoint_definition', 'updated_code': "from numpy import ma\nkeypoint_definition = paf_avg[:, :, 16] * -1\nV = paf_avg[:, :, 17]\nX, Y = np.meshgrid(np.arange(keypoint_definition.shape[1]), np.arange(keypoint_definition.shape[0]))\nM = np.zeros(keypoint_definition.shape, dtype='bool')\nM[keypoint_definition ** 2 + V ** 2 < 0.3 * 0.3] = True\nkeypoint_definition = ma.masked_array(keypoint_definition, mask=M)\nV = ma.masked_array(V, mask=M)\nplt.figure()\nplt.imshow(oriImg[:, :, [2, 1, 0]], alpha=0.5)\ns = 5\nQ = plt.quiver(X[::s, ::s], Y[::s, ::s], keypoint_definition[::s, ::s], V[::s, ::s], scale=50, headaxislength=4, alpha=0.5, width=0.001, color='r')\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(10, 10)"}, {'identified': 'rmsprop_optimizer', 'updated_code': "from keras import optimizers\nsgd = optimizers.SGD(lr=0.001, decay=1e-06, momentum=0.9, nesterov=True)\nrmsprop_optimizer = optimizers.RMSprop(lr=0.0001)\nVGG16Seq.compile(optimizer=rmsprop_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"}, {'identified': 'stimulus_control_bars', 'updated_code': "barwidth = 0.75\nfig, ax = plt.subplots(figsize=(9, 7))\nstimulus_control_bars = ax.bar(0.5, RTanalysis.Valid.mean(), barwidth, yerr=RTstderror.Valid, ecolor='k', edgecolor=sns.xkcd_rgb['green'], linewidth=2, facecolor='none', error_kw=dict(lw=3))\nrects2 = ax.bar(1.5, RTanalysis.Invalid.mean(), barwidth, color=sns.xkcd_rgb['green'], yerr=RTstderror.Invalid, ecolor='k', error_kw=dict(lw=3))\nsns.set(context='notebook', style='white', font='Myriad Pro', font_scale=2, color_codes=False, rc=None)\nax.set_ylim(550, 610)\nax.set_xlim(0, 2.5)\nax.set_xticklabels(('Valid', 'Invalid'))\nax.set_xticks([0.5 + barwidth / 2, 1.5 + barwidth / 2])\nax.set_yticks(np.arange(550, 611, 10))\nplt.title('S-S Phase RT', fontsize=26, fontweight='bold')\nplt.ylabel('Reaction Time (ms)', fontsize=24, fontweight='bold')\nplt.xlabel('Trial Type', fontsize=24, fontweight='bold')\nsns.despine()\nplt.show()"}, {'identified': 'lane_lines', 'updated_code': 'def lane_detection(image):\n    gray = grayscale(image)\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray, 5)\n    low_threshold = 60\n    high_threshold = 180\n    edges = canny(blur_gray, low_threshold, high_threshold)\n    imshape = image.shape\n    vertices = np.array([[(0, imshape[0]), (450, 320), (490, 320), (imshape[1], imshape[0])]], dtype=np.int32)\n    masked_edges = region_of_interest(edges, vertices)\n    rho = 2\n    theta = np.pi / 180\n    threshold = 15\n    min_line_len = 40\n    max_line_gap = 20\n    lane_lines = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\n    color_edges = np.dstack((edges, edges, edges))\n    lines_edges = weighted_img(lane_lines, image, α=0.8, β=1.0, λ=0.0)\n    return lines_edges'}, {'identified': 'county_names', 'updated_code': "fig, ax1 = plt.subplots()\ntick_locations = [value for value in x_axis]\nplt.xticks(tick_locations, county_names, rotation=90)\ngrad_rate = df_county_data['Graduation Rate']\ncounty_names = df_county_data['County Name']\npov_rate = df_county_data['Speak a language other than English']\nt = np.arange(len(county_names))\nax1.plot(t, pov_rate, 'b-')\nax1.set_xlabel('county')\nax1.set_ylabel('Speak a language other than English', color='b')\nax1.tick_params('y', colors='b')\nplt.title('High School Graduation Rates and ESL by County')\nax2 = ax1.twinx()\nax2.plot(t, grad_rate, 'r*')\nax2.set_ylabel('Graduation Rate', color='r')\nax2.tick_params('y', colors='r')\nzoom = 5\nw, h = fig.get_size_inches()\nfig.set_size_inches(w * zoom, h * zoom / 2)\nfig.tight_layout()\nplt.savefig('Images/County_Grad_Speak a language other than English2.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'bottom_boundary_region', 'updated_code': 'def estimate_anottation_correct(img, coords, line_width=2, threshold=0.8):\n    """Make histograms of boundaries to estimate annotation error\n    (if boundaries are blank, high chance of dumb walk failure)"""\n    total = 0\n    cont = 0\n    xleft, ytop, xright, ybottom = [int(c) for c in coords]\n    leftside = img[ytop:ybottom, xleft:xleft + line_width - 1]\n    rightside = img[ytop:ybottom, xright - line_width:xright]\n    topside = img[ytop:ytop + line_width - 1, xleft:xright]\n    bottom_boundary_region = img[ybottom - line_width:ybottom, xleft:xright]\n    total = leftside.sum() + rightside.sum()\n    cont = leftside.size + rightside.size\n    \'for y in range(ytop, ybottom):\\n        for x in range(xleft, xright):\\n            total += img[y, xleft:xleft + line_width].sum()  # Left side\\n            total += img[y, xright - line_width:xright].sum()  # Right side\\n            total += img[ytop:ytop + line_width, x].sum()  # Top side\\n            total += img[ybottom - line_width:ybottom, x].sum()  # Bottom side\\n            cont +=1\\n    \'\n    percent_black = total / cont\n    return int(percent_black)'}, {'identified': 'first_feature_layer_search_result', 'updated_code': 'first_feature_layer_search_result = feature_layer_srch_results[0]\nfeature_layers = first_feature_layer_search_result.layers\nfeature_layer = feature_layers[0]\nfeature_layer.properties.name'}, {'identified': 'spectral_definitions', 'updated_code': "verbose = False\nfor i, f in enumerate(spectral_definitions):\n    spectral_definitions[i] = f.replace(b'SDSS', b'LSST').replace(b'BessellV', b'LSST_g')\nif verbose:\n    print(mjdmax)\n    print(mjd_to_sim)\n    print(spectral_definitions)"}, {'identified': 'neighbour_messages_product', 'updated_code': 'def calc_other_neighbour_msg_prod(sender, receiver):\n    neighbour_messages_product = get_neighbour_messages(sender, receiver)\n    return np.multiply.reduce(np.ix_(*neighbour_messages_product))'}, {'identified': 'tanh_output', 'updated_code': "import numpy as np\nfrom scipy.special import expit\nn = 100\nxs = np.linspace(-3, 3, n)\nReLu = np.maximum(xs, 0)\nd_ReLu = np.concatenate((np.zeros(int(n / 2)), np.ones(int(n / 2))))\ntanh_output = np.tanh(xs)\nd_tanh = 1 - tanh_output ** 2\nsig = expit(xs)\nd_sig = sig * (1 - sig)\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 3, 1)\nplt.plot(xs, ReLu, label='ReLu')\nplt.plot(xs, d_ReLu, label='d_Relu')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('ReLu(x) Plot')\nplt.ylim(-1.1, 1.1)\nplt.legend()\nplt.subplot(1, 3, 2)\nplt.plot(xs, tanh_output, label='tanh')\nplt.plot(xs, d_tanh, label='d_tanh')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('tanh(x) Plot')\nplt.ylim(-1.1, 1.1)\nplt.legend()\nplt.subplot(1, 3, 3)\nplt.plot(xs, sig, label='sigmoid')\nplt.plot(xs, d_sig, label='d_sigmoid')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sigmoid(x) Plot')\nplt.ylim(-1.1, 1.1)\nplt.legend()\nplt.suptitle('Activation functions')\nplt.show()"}, {'identified': 'image_slice', 'updated_code': "image_slice = X[:, -1::-1][-1::-1, :]\nplt.imshow(image_slice, interpolation='nearest', cmap=plt.cm.gray)"}, {'identified': 'start_time', 'updated_code': "import os\nimport sys\nimport tarfile\nimport time\nsource = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\ntarget = 'aclImdb_v1.tar.gz'\n\ndef reporthook(count, block_size, total_size):\n    global start_time\n    if count == 0:\n        start_time = time.time()\n        return\n    duration = time.time() - start_time\n    progress_size = int(count * block_size)\n    speed = progress_size / (1024.0 ** 2 * duration)\n    percent = count * block_size * 100.0 / total_size\n    sys.stdout.write('\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed' % (percent, progress_size / 1024.0 ** 2, speed, duration))\n    sys.stdout.flush()\nif not os.path.isdir('aclImdb') and (not os.path.isfile('aclImdb_v1.tar.gz')):\n    if sys.version_info < (3, 0):\n        import urllib\n        urllib.urlretrieve(source, target, reporthook)\n    else:\n        import urllib.request\n        urllib.request.urlretrieve(source, target, reporthook)"}, {'identified': 'mc_pileUp_efficiency_image', 'updated_code': "mc_pileUp_efficiency_image = 'pileUpFilterEfficiency_MC.png'\nDATA_fraction = 'pileUpFilterEfficiency_DATA.png'\nImage(filename=pathname + mc_pileUp_efficiency_image)"}, {'identified': 'sat_score_columns', 'updated_code': "sat_score_columns = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\nfor c in sat_score_columns:\n    data['sat_results'][c] = pd.to_numeric(data['sat_results'][c], errors='coerce')\ndata['sat_results']['sat_score'] = data['sat_results'][sat_score_columns[0]] + data['sat_results'][sat_score_columns[1]] + data['sat_results'][sat_score_columns[2]]\n\ndef find_lat(loc):\n    coords = re.findall('\\\\(.+, .+\\\\)', loc)\n    lat = coords[0].split(',')[0].replace('(', '')\n    return lat\n\ndef find_lon(loc):\n    coords = re.findall('\\\\(.+, .+\\\\)', loc)\n    lon = coords[0].split(',')[1].replace(')', '').strip()\n    return lon\ndata['hs_directory']['lat'] = data['hs_directory']['Location 1'].apply(find_lat)\ndata['hs_directory']['lon'] = data['hs_directory']['Location 1'].apply(find_lon)\ndata['hs_directory']['lat'] = pd.to_numeric(data['hs_directory']['lat'], errors='coerce')\ndata['hs_directory']['lon'] = pd.to_numeric(data['hs_directory']['lon'], errors='coerce')"}, {'identified': 'balanced_subset_indices', 'updated_code': 'tsne_pts_per_class = 200\nbalanced_subset_indices = get_balanced_subset_indices(data_test.gt_patches.flatten(), np.arange(1, 9), pts_per_class=tsne_pts_per_class)\nbalanced_subset_indices = np.concatenate(balanced_subset_indices)'}, {'identified': 'activations_overlap_removed', 'updated_code': 'activations_overlap_removed = get_activations_batch(model_unet, -2, data_train_overlap.im_patches, 20, verbose=True)\nactivations_overlap_removed = remove_overlap(data_train.imgs, activations_overlap_removed, patch_size=64, stride=32)\nact_train = activations_overlap_removed[pred_t_tr]'}, {'identified': 'power_of_3', 'updated_code': 'power_of_3, x3, x4 = powers(3)\nprint(x3)'}, {'identified': 'simulated_data', 'updated_code': "reload(pccsims)\ncoco = pccsims.pyCoCo(pcc.utils.b(filter_path), pcc.utils.b(coco_root_path))\nflux, simulated_data = coco.simulate(b'SN2007uy', z_obs, 0.0, 0.0, 0.0, 3.1, mjdmax, mjd_to_sim, filters_to_sim)"}, {'identified': 'SceneTreeFrequencyMean', 'updated_code': "SkyPresence = posttest.groupby(['subjID'])['Q2_SceneSkyPresence'].mean()\nSkyPresenceSEM = pd.Series.std(SkyPresence) / n\nColorScheme = posttest.groupby(['subjID'])['Q2_SceneColorScheme'].mean()\nColorSchemeSEM = pd.Series.std(ColorScheme) / n\nSceneTreeFrequencyMean = posttest.groupby(['subjID'])['Q2_SceneTreeFrequency'].mean()\nTreeFreqSEM = pd.Series.std(SceneTreeFrequencyMean) / n\nImageType = posttest.groupby(['subjID'])['Q2_ImageType'].mean()\nImageTypeSEM = pd.Series.std(ImageType) / n\nFeatureType = posttest.groupby(['subjID'])['Q2_FeatureType'].mean()\nFeatureTypeSEM = pd.Series.std(FeatureType) / n\nLightType = posttest.groupby(['subjID'])['Q2_LightType'].mean()\nLightTypeSEM = pd.Series.std(LightType) / n"}, {'identified': 'log_transformed_data', 'updated_code': "log_transformed_data = np.log(data.copy())\nlog_data2 = np.log(data2.copy())\nlog_samples = np.log(samples.copy())\npd.scatter_matrix(log_data2, alpha=0.3, figsize=(14, 8), diagonal='kde')"}, {'identified': 'hidden_layer_activations', 'updated_code': "def validate_hypothesis(model, diag_classifier, hypothesis, train_len=50, test_len=1, text_len=500, temperature=0.8, save_hyp=None, save_diag=None, save_resp=None):\n\n    def gen_hyp_data(model, N, text_len=500):\n        texts, hiddens, hyps = ([], [], [])\n        for i in range(N):\n            text, hidden = generate(model, '\\n\\n', text_len, temperature, True)\n            hidden = hidden.reshape(hidden.shape[0], -1)\n            hyp = hypothesis(text)\n            hiddens.append(hidden)\n            hyps.append(hyp)\n            texts.append(text)\n        return (''.join(texts), np.concatenate(hyps), np.concatenate(hiddens))\n    _, hidden_layer_activations, train_hiddens = gen_hyp_data(model, train_len)\n    test_texts, test_hyps, test_hiddens = gen_hyp_data(model, test_len)\n    print(pearsonr(train_hiddens, hidden_layer_activations))\n    print(pearsonr(test_hiddens, test_hyps))\n    diag_classifier.fit(train_hiddens, hidden_layer_activations)\n    pred_hyps = diag_classifier.predict(test_hiddens)\n    resp_neuron = np.argmax(np.abs(diag_classifier.coef_))\n    print(resp_neuron)\n    if save_hyp:\n        plot_colored_text(test_texts[:text_len], test_hyps[:text_len], title='Formed Hypothesis', save_file=save_hyp)\n    if save_diag:\n        plot_colored_text(test_texts[:text_len], pred_hyps[:text_len], title='Diagnostic Classifier Prediction', save_file=save_diag)\n    if save_resp:\n        plot_colored_text(test_texts[:text_len], test_hiddens[:text_len, resp_neuron], title='Most Responsible Neuron {}'.format(resp_neuron), save_file=save_resp)\n    del hidden_layer_activations\n    del train_hiddens\n    del test_texts\n    del test_hiddens\n    gc.collect()\n    return (test_hyps, pred_hyps)"}, {'identified': 'generated_data', 'updated_code': "t_min_max = (msig.timestamps[0], msig.timestamps[-1])\ngenerated_data, y_test = msig.generate()\nprint(generated_data.shape)\nprint(y_test.shape)\nscore = model.evaluate(generated_data, y_test, batch_size=batch_size)\nmodel.reset_states()\nprint(score)\ny_hat = model.predict(generated_data, batch_size=batch_size)\ny_pred = np.argmax(y_hat, axis=1)\nmodel.reset_states()\nprint(y_pred.shape)\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 6))\nax[0].plot(msig.timestamps, msig.mixed_signal, color='grey', alpha=0.3)\nax[0].scatter(msig.timestamps, msig.mixed_signal, marker='.')\nax[0].set_title('loss = {:<6.4f}, accuracy = {:<6.4f}'.format(*score))\nax[0].set_xlim(t_min_max)\nax[1].plot(msig.timestamps[msig.window_size - 1:], generated_data[:, -1, 0], color='grey', alpha=0.3)\nax[1].scatter(msig.timestamps[msig.window_size - 1:], generated_data[:, -1, 0], marker='.', c=y_pred)\nax[1].set_xlim(t_min_max)\nax[1].set_xlabel('time')\nplt.show()"}, {'identified': 'padded_img_bottom', 'updated_code': 'def padRightDownCorner(img, stride, padValue):\n    h = img.shape[0]\n    w = img.shape[1]\n    pad = 4 * [None]\n    pad[0] = 0\n    pad[1] = 0\n    pad[2] = 0 if h % stride == 0 else stride - h % stride\n    pad[3] = 0 if w % stride == 0 else stride - w % stride\n    img_padded = img\n    pad_up = np.tile(img_padded[0:1, :, :] * 0 + padValue, (pad[0], 1, 1))\n    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n    pad_left = np.tile(img_padded[:, 0:1, :] * 0 + padValue, (1, pad[1], 1))\n    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n    padded_img_bottom = np.tile(img_padded[-2:-1, :, :] * 0 + padValue, (pad[2], 1, 1))\n    img_padded = np.concatenate((img_padded, padded_img_bottom), axis=0)\n    pad_right = np.tile(img_padded[:, -2:-1, :] * 0 + padValue, (1, pad[3], 1))\n    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n    return (img_padded, pad)\n\nclass DataBatch(object):\n\n    def __init__(self, data, label, pad=0):\n        self.data = [data]\n        self.label = [label]\n        self.pad = pad'}, {'identified': 'convolution_output', 'updated_code': "from tensorflow.python.framework import ops\nops.reset_default_graph()\nbatch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n    layer1_biases = tf.Variable(tf.zeros([depth]))\n    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n    def model(data):\n        convolution_output = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n        pool = tf.nn.max_pool(convolution_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(pool + layer1_biases)\n        convolution_output = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n        pool = tf.nn.max_pool(convolution_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        hidden = tf.nn.relu(pool + layer2_biases)\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        return tf.matmul(hidden, layer4_weights) + layer4_biases\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))"}, {'identified': 'segment_data', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\nindex = 0\nfor segment in segments:\n    segment_data = data.loc[data[\'RiverSeg\'] == segment]\n    segment_data = segment_data[segment_data["lam\'"] > 0]\n    segment_data = segment_data.sort_values(\'Year\')\n    x = segment_data[\'Year\']\n    y = segment_data["1-lam\'"]\n    y.name = segment\n    plt.plot(x, y, colors[index])\n    plt.legend(loc=(1.05, 0.2))\n    index += 1\nplt.title("1-Lambda\', " + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel("1-Lambda\'")\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + "_1-Lambda\'.png", bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'base_url', 'updated_code': 'base_url = \'http://mattmahoney.net/dc/\'\n\ndef maybe_download(filename, expected_bytes):\n    """Download a file if not present, and make sure it\'s the right size."""\n    if not os.path.exists(filename):\n        filename, _ = urlretrieve(base_url + filename, filename)\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print(\'Found and verified %s\' % filename)\n    else:\n        print(statinfo.st_size)\n        raise Exception(\'Failed to verify \' + filename + \'. Can you get to it with a browser?\')\n    return filename\nfilename = maybe_download(\'text8.zip\', 31344016)'}, {'identified': 'input_dimension', 'updated_code': "input_dimension = 128\nembedding_size = 128\nskip_window = 1\nnum_skips = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[input_dimension])\n    train_labels = tf.placeholder(tf.int32, shape=[input_dimension, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"}, {'identified': 'tagged_words_iterator', 'updated_code': 'basic_model = HiddenMarkovModel(name=\'base-hmm-tagger\')\ntagged_words_iterator = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\nwords = (word for i, (word, tag) in enumerate(data.training_set.stream()))\nemission_counts = pair_counts(tagged_words_iterator, words)\nstates = {}\nfor tag, word_dict in emission_counts.items():\n    emission_dict = defaultdict(float)\n    for word in word_dict.keys():\n        emission_dict[word] = emission_counts[tag][word] / tag_unigrams[tag]\n    state_emission = DiscreteDistribution(dict(emission_dict))\n    states[tag] = State(state_emission, name=tag)\nbasic_model.add_states(list(states.values()))\nfor tag in data.training_set.tagset:\n    state = states[tag]\n    basic_model.add_transition(basic_model.start, state, tag_starts[tag] / len(data.training_set))\n    basic_model.add_transition(state, basic_model.end, tag_ends[tag] / tag_unigrams[tag])\n    for next_tag in data.training_set.tagset:\n        next_state = states[next_tag]\n        basic_model.add_transition(state, next_state, tag_bigrams[tag, next_tag] / tag_unigrams[tag])\nbasic_model.bake()\nassert all((tag in set((s.name for s in basic_model.states)) for tag in data.training_set.tagset)), \'Every state in your network should use the name of the associated tag, which must be one of the training set tags.\'\nassert basic_model.edge_count() == 168, \'Your network should have an edge from the start node to each state, one edge between every \' + \'pair of tags (states), and an edge from each state to the end node.\'\nHTML(\'<div class="alert alert-block alert-success">Your HMM network topology looks good!</div>\')'}, {'identified': 'row_indices', 'updated_code': "row_indices, x = np.indices((600, 800))\nimg[(x - 450) ** 2 + (row_indices - 350) ** 2 < 50 ** 2] = (1, 0, 0)\nplt.imshow(img, interpolation='bilinear')"}, {'identified': 'model_array', 'updated_code': 'nlay, nrow, ncol = (1, 1, 200)\ndelr = 50.0\ndelc = 1.0\nh1 = 23.0\nh2 = 5.0\nx = np.arange(0.0, float(ncol) * delr, delr) + delr / 2.0\nibound = np.ones((nlay, nrow, ncol), dtype=np.int)\nibound[:, :, 0] = -1\nibound[:, :, -1] = -1\nmodel_array = 25 * np.ones((nlay + 1, nrow, ncol), dtype=np.float)\nbase = 20.0\nfor j in range(ncol):\n    model_array[1, :, j] = base\n    if j + 1 in [40, 80, 120, 160]:\n        base -= 5\nstrt = h1 * np.ones((nlay, nrow, ncol), dtype=np.float)\nstrt[:, :, -1] = h2'}, {'identified': 'sequences_lengths', 'updated_code': "from my_model_selectors import SelectorDIC\ntraining = asl.build_training(features_ground)\nsequences = training.get_all_sequences()\nsequences_lengths = training.get_all_Xlengths()\nfor word in words_to_train:\n    start = timeit.default_timer()\n    model = SelectorDIC(sequences, sequences_lengths, word, min_n_components=2, max_n_components=15, random_state=14).select()\n    end = timeit.default_timer() - start\n    if model is not None:\n        print('Training complete for {} with {} states with time {} seconds'.format(word, model.n_components, end))\n    else:\n        print('Training failed for {}'.format(word))"}, {'identified': 'segmented_image', 'updated_code': 'segmented_image = get_segment_img(clusters_2, img, posterior_mu_2, mcmc=True)'}, {'identified': 'customer_data', 'updated_code': "customer_data = samples.copy()\ncustomer_data['Total'] = customer_data.sum(axis=1)\nsamples_percentage = samples.loc[:, 'Fresh':'Delicatessen'].div(customer_data['Total'], axis=0) * 100\nsamples_percentage['Total'] = samples_percentage.sum(axis=1)\nsamples_percentage"}, {'identified': 'heart_equation_result', 'updated_code': "img[...] = 0\nX = (x - 400) / 30\nheart_equation_result = -(y - 300) / 30\nheart = X ** 2 + (heart_equation_result - 2 * (X ** 2 + np.abs(X) - 6) / (3 * (X ** 2 + np.abs(X) + 2))) ** 2 < 36\nimg[heart] = (1, 0, 0)\nplt.imshow(img, interpolation='bilinear')"}, {'identified': 'output_layer', 'updated_code': "import tensorflow as tf\nheight = 28\nwidth = 28\nchannels = 1\nn_inputs = height * width\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = 'SAME'\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 1\nconv2_pad = 'SAME'\nconv2_dropout_rate = 0.25\npool3_fmaps = conv2_fmaps\nn_fc1 = 128\nfc1_dropout_rate = 0.5\nn_outputs = 10\nreset_graph()\nwith tf.name_scope('inputs'):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n    y = tf.placeholder(tf.int32, shape=[None], name='y')\n    training = tf.placeholder_with_default(False, shape=[], name='training')\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name='conv1')\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name='conv2')\nwith tf.name_scope('pool3'):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 14 * 14])\n    pool3_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\nwith tf.name_scope('fc1'):\n    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name='fc1')\n    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\nwith tf.name_scope('output'):\n    output_layer = tf.layers.dense(fc1, n_outputs, name='output')\n    Y_proba = tf.nn.softmax(output_layer, name='Y_proba')\nwith tf.name_scope('train'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output_layer, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(output_layer, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\nwith tf.name_scope('init_and_save'):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"}, {'identified': 'output_directory', 'updated_code': "output_directory = 'test_images_output'\nif not os.path.exists(output_directory):\n    os.makedirs(output_directory)\nimage = mpimg.imread('test_images/solidYellowLeft.jpg')\nprint('This image is:', type(image), 'with dimensions:', image.shape)\nplt.imshow(image)"}, {'identified': 'visualization_colors', 'updated_code': "pred_f = pred_f_te.flatten()[dataset_subset_indices]\npred_t = pred_t_te.flatten()[dataset_subset_indices]\nprobas_gmm_c = imgs_stretch_eq([probas_gmm[..., np.newaxis]])[0, ..., 0]\nvisualization_colors = plt.cm.YlOrRd(1 - probas_gmm_c)[:, :3][dataset_subset_indices]\nc_thresh_t = plt.cm.YlOrRd((probas_gmm_c[dataset_subset_indices] < np.sort(probas_gmm_c[dataset_subset_indices])[200]) * 255)[:, :3]\nc_thresh_f = plt.cm.YlOrRd((probas_gmm_c[dataset_subset_indices] > np.sort(probas_gmm_c[dataset_subset_indices])[200]) * 255)[:, :3]\n_, axes = plt.subplots(1, 2, figsize=(20, 10))\naxes[0].scatter(tsne_all[:, 0][pred_t], tsne_all[:, 1][pred_t], c=visualization_colors[pred_t], alpha=0.6)\naxes[1].scatter(tsne_all[:, 0][pred_t], tsne_all[:, 1][pred_t], c=c_thresh_t[pred_t])\naxes[0].scatter(tsne_all[:, 0][pred_f], tsne_all[:, 1][pred_f], c=visualization_colors[pred_f], marker='x')\naxes[1].scatter(tsne_all[:, 0][pred_f], tsne_all[:, 1][pred_f], c=c_thresh_f[pred_f], marker='x')"}, {'identified': 'dataset_index', 'updated_code': "X, Annotations = marconi['Kickbox']\ndataset_index = 0\nframe = 0\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(111)\nim = X[dataset_index, frame]\nax.imshow(im)\nAnnotations_for_cam = Annotations[dataset_index]\nAnnot_on_frame_cam = Annotations_for_cam[frame]\nCOLORS = ['red', 'yellow']\nfor i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\n    head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n    ax.plot(head_x, head_y, color=COLORS[i])\n    for jx, jy, visible in joints:\n        plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\nplt.axis('off')\nplt.show()"}, {'identified': 'dog_breed_classifier_model', 'updated_code': "K.clear_session()\ndog_breed_classifier_model = Sequential()\ndog_breed_classifier_model.add(GlobalAveragePooling2D(input_shape=train_incp_bn.shape[1:]))\ndog_breed_classifier_model.add(Activation('relu'))\ndog_breed_classifier_model.add(Dense(1024, activation='relu'))\ndog_breed_classifier_model.add(Dropout(0.5))\ndog_breed_classifier_model.add(Dense(512, activation='relu'))\ndog_breed_classifier_model.add(Dropout(0.5))\ndog_breed_classifier_model.add(Dense(NUM_CLASSES, activation='softmax'))\ndog_breed_classifier_model.summary()"}, {'identified': 'mean_absolute_error_flight_delays', 'updated_code': "pipe_knn = make_pipeline(MinMaxScaler(), SelectPercentile(), KNeighborsRegressor())\nparam_grid_knn = {'selectpercentile__percentile': range(10, 30, 5), 'kneighborsregressor__n_neighbors': range(1, 20), 'kneighborsregressor__weights': ['uniform', 'distance']}\nknn_grid = GridSearchCV(pipe_knn, param_grid_knn)\nknn_grid.fit(train_features, train_outcome)\nknn_best_params = knn_grid.best_params_\nknn_grid_score = knn_grid.score(test_features, test_outcome)\nmean_absolute_error_flight_delays = mean_absolute_error(knn_grid.predict(test_features), test_outcome)\nknn_evs = explained_variance_score(knn_grid.predict(test_features), test_outcome)"}, {'identified': 'annotation_colors', 'updated_code': "X, Annotations = marconi['Walk1']\ncamera = 0\nframe = 220\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(111)\nim = X[camera, frame]\nax.imshow(im)\nAnnotations_for_cam = Annotations[camera]\nAnnot_on_frame_cam = Annotations_for_cam[frame]\nannotation_colors = ['red', 'yellow']\nfor i, annot in enumerate(Annot_on_frame_cam):\n    if annot is not None:\n        (tl, br), joints = annot\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n        ax.plot(head_x, head_y, color=annotation_colors[i])\n        for jx, jy, visible in joints:\n            plt.scatter(jx, jy, color=annotation_colors[i], alpha=1 if visible == 1 else 0.4)\nplt.axis('off')\nplt.show()"}, {'identified': 'person_height_m', 'updated_code': "aditya_height_m = 1.21\nperson_height_m = 1.85\naverage_adult_human_height_m = 1.688\nbiggest_distance_m = max(abs(aditya_height_m - average_adult_human_height_m), abs(person_height_m - average_adult_human_height_m))\nprint('The biggest distance from the average height among these two people is', biggest_distance_m, 'meters.')"}, {'identified': 'data_path', 'updated_code': "heprowPath = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/'\nheprowName = 'mik_Det0_1.gru'\nunfanaName = 'unf_Det0_1.gru'\ngravelName = 'grv_out_Det0_1.flu'\nfaltwPHSName = 'faltw_Det0_1.phs'\nmeasPHSName = 'Inputs/Det0_stat_100_phs_03.phs'\nmtxName = 'MIEKE_Det0_1.MTX'\nheprowBinBounds = 'low'\nmeuldersPath = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/'\nmeuldersName = 'Meulders33MeVTaSpectrum_1.txt'\nmeuldersBinBounds = 'up'\nmcnpPath = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Simulated/PHS/33MeVTa/BeamOnly/Model/NoExtrap_Void_1deg/'\nmcnpName = '33MeVTaBeamOnly_Det.out'\nmcnpBinBounds = 'up'\ndata_path = '/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/'"}, {'identified': 'sorted_hotel_data', 'updated_code': "df['Hotel_Count'] = df.groupby('Hotel Name')['Hotel Name'].transform('count')\nsorted_hotel_data = df.sort_values(by=['Hotel_Count'], ascending=False).reset_index()\ndf_hotels = sorted_hotel_data['Hotel Name'].unique()[:150]\nmost_common_hotels = sorted_hotel_data[sorted_hotel_data['Hotel Name'].isin(df_hotels)]"}, {'identified': 'x_coordinates', 'updated_code': "def draw_person(ax, indv_left, indv_right):\n    HND_l = 0\n    ELB_l = 1\n    SHO_l = 2\n    HND_r = 5\n    ELB_r = 4\n    SHO_r = 3\n    FOT_l = 6\n    KNE_l = 7\n    HIP_l = 8\n    FOT_r = 11\n    KNE_r = 10\n    HIP_r = 9\n\n    def triangulate(elem):\n        a = indv_left[1][elem]\n        b = indv_right[1][elem]\n        a = a[:2]\n        b = b[:2]\n        A = np.array([a], 'float32').T\n        B = np.array([b], 'float32').T\n        result = cv2.triangulatePoints(P1, P2, A, B)\n        result /= result[3]\n        return result[:3]\n    left_hand = triangulate(HND_l)\n    left_elbow = triangulate(ELB_l)\n    left_shoulder = triangulate(SHO_l)\n    right_hand = triangulate(HND_r)\n    right_elbow = triangulate(ELB_r)\n    right_shoulder = triangulate(SHO_r)\n    right_hip = triangulate(HIP_r)\n    right_knee = triangulate(KNE_r)\n    right_foot = triangulate(FOT_r)\n    left_hip = triangulate(HIP_l)\n    left_knee = triangulate(KNE_l)\n    left_foot = triangulate(FOT_l)\n    ALL = np.squeeze(np.array([left_hand, left_elbow, left_shoulder, right_shoulder, right_elbow, right_hand, right_elbow, right_shoulder, right_hip, right_knee, right_foot, right_knee, right_hip, left_hip, left_knee, left_foot, left_knee, left_hip, left_shoulder]))\n    x_coordinates = ALL[:, 0]\n    Y = ALL[:, 1]\n    Z = ALL[:, 2]\n    ax.plot(x_coordinates, Y, Z)"}, {'identified': 'adaboost_model', 'updated_code': 'from sklearn.ensemble import AdaBoostClassifier\nadaboost_model = AdaBoostClassifier(random_state=42)\nadaboost_model.fit(X_train, y_train)\nimportances = adaboost_model.feature_importances_\nvs.feature_plot(importances, X_train, y_train)'}, {'identified': 'test_cases', 'updated_code': "seed = ['Hello World']\ncgi_runner = FunctionCoverageRunner(cgi_decode)\nm = MutationCoverageFuzzer(seed)\ntest_cases = m.runs(cgi_runner, 10000)"}, {'identified': 'image_processed_count', 'updated_code': 'from IPython.display import clear_output\n\ndef save_annotations(adict):\n    timestamp = datetime.datetime.today().strftime(\'%Y%m%d%H%M\')\n    filename = \'annotations_\' + timestamp + \'.csv\'\n    print(filename)\n    with open(os.path.join(\'.\', filename), \'w\', newline=\'\') as out:\n        writer = csv.writer(out)\n        for key, value in adict.items():\n            line = [key, *value]\n            writer.writerow(line)\n\ndef create_anottations(lista, save=True):\n    """Use dumb walk heuristic to create anottations\n    Args: \n    \n        lista: list of images\n        save: if true, save on current directory a csv <annottations_timestamp.csv>\n    \n    Returns: \n        \n        a dict with name of image: (xleft, ytop, xright, ytop) coordinates\n    """\n    image_processed_count = 0\n    result = {}\n    for img in lista:\n        try:\n            result[img] = find_conteiner(img)\n        except ValueError:\n            pass\n        image_processed_count += 1\n        if image_processed_count % 100 == 0:\n            clear_output()\n            print(\'...\', image_processed_count, \'...\')\n    if save:\n        save_annotations(result)\n    return result\n\ndef draw_anottation():\n    """Create red boxes on images for visual annotations checking\n    Will have to save RGB images inside of \'L\' greyscale"""\n    pass'}, {'identified': 'convolution_output', 'updated_code': "def LeNet6(x, n_classes):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    convolution_output = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    convolution_output = tf.nn.relu(convolution_output)\n    convolution_output = tf.nn.max_pool(convolution_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    fc0 = flatten(convolution_output)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(252, 168), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(168))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(168, 84), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(84))\n    fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n    fc3 = tf.nn.relu(fc3)\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean=mu, stddev=sigma))\n    fc4_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    return logits"}, {'identified': 'eroded_image', 'updated_code': "from skimage.morphology import disk, erosion, dilation, square\nB = square(3)\nX_dil = dilation(X, selem=B)\neroded_image = erosion(X, selem=B)\nplt.figure(figsize=[10, 5])\nplt.subplot(1, 3, 1)\nplt.imshow(X, interpolation='nearest', cmap=plt.cm.gray)\nplt.title('$X$')\nplt.subplot(1, 3, 2)\nplt.imshow(X_dil, interpolation='nearest', cmap=plt.cm.gray)\nplt.title('dilation of $X$ by $B$')\nplt.subplot(1, 3, 3)\nplt.imshow(eroded_image, interpolation='nearest', cmap=plt.cm.gray)\nplt.title('erosion of $X$ by $B$')\nplt.figure(figsize=[6, 6])\nplt.imshow(X, interpolation='nearest', cmap=plt.cm.gray, alpha=0.3)\nplt.imshow(eroded_image, interpolation='nearest', cmap=plt.cm.gray, alpha=0.3)\nplt.imshow(X_dil, interpolation='nearest', cmap=plt.cm.gray, alpha=0.3)"}, {'identified': 'feature_subset', 'updated_code': "def get_data_from_tsv(feature_file, x_fields, y_field, x_filter=None, seed=0, as_np_array=False, scale=False, shuffle=False, train_portion=0.6, test_portion=0.2):\n    my_data = pandas.read_csv(feature_file)\n    print(my_data.shape)\n    my_data = my_data.dropna()\n    my_data = my_data.reset_index(drop=True)\n    print(my_data.shape)\n    if not x_filter == None:\n        for ff in x_filter:\n            my_data = my_data[my_data[ff[0]].isin(ff[1:])]\n            print('filtering %s by ' % ff[0], ff[1:], 'num datapoints left: ', len(my_data))\n        my_data = my_data.reset_index(drop=True)\n    labels = my_data['word']\n    if 'freq' in x_fields or my_data.columns.get_loc('freq') in x_fields:\n        idx = my_data.columns.get_loc('freq')\n        my_data.iloc[:, idx] = np.log(my_data.iloc[:, idx])\n        my_data = my_data.rename(columns={'freq': 'log_freq'})\n    if 'length' in x_fields or my_data.columns.get_loc('length') in x_fields:\n        idx = my_data.columns.get_loc('length')\n        my_data.iloc[:, idx] = np.log(my_data.iloc[:, idx])\n        my_data = my_data.rename(columns={'length': 'log_length'})\n    if type(x_fields[0]) == str:\n        x_fields.append(y_field)\n        my_data = my_data[x_fields]\n    else:\n        x_fields.append(my_data.columns.get_loc(y_field))\n        my_data = my_data.iloc[:, x_fields]\n    del x_fields[-1]\n    x_train, y_train, x_dev, y_dev, feature_subset, y_test = train_validate_test_split(my_data, y_field, train_percent=train_portion, validate_percent=test_portion, seed=seed, shuffle=shuffle)\n    if scale:\n        x_train = pandas.DataFrame(preprocessing.scale(x_train), columns=x_train.columns, index=x_train.index)\n        x_dev = pandas.DataFrame(preprocessing.scale(x_dev), columns=x_dev.columns, index=x_dev.index)\n        feature_subset = pandas.DataFrame(preprocessing.scale(feature_subset), columns=feature_subset.columns, index=feature_subset.index)\n        y_train = pandas.DataFrame(preprocessing.scale(y_train), columns=y_train.columns, index=y_train.index)\n        y_dev = pandas.DataFrame(preprocessing.scale(y_dev), columns=y_dev.columns, index=y_dev.index)\n        y_test = pandas.DataFrame(preprocessing.scale(y_test), columns=y_test.columns, index=y_test.index)\n    if as_np_array:\n        x_train = np.array(x_train).astype(np.float)\n        y_train = np.array(y_train).astype(np.float)\n        x_dev = np.array(x_dev).astype(np.float)\n        y_dev = np.array(y_dev).astype(np.float)\n        feature_subset = np.array(feature_subset).astype(np.float)\n        y_test = np.array(y_test).astype(np.float)\n    return (x_train, y_train, x_dev, y_dev, feature_subset, y_test, labels)"}, {'identified': 'playlist_samples_vectorized', 'updated_code': "X_test_refined = pd.DataFrame([])\nr_precisions = []\npbar = tqdm(data_test.groupby(['playlist_pid']))\nfor pid, df in pbar:\n    p_info = df[playlist_df.columns].iloc[0]\n    labels = y_test.loc[df.index]\n    positive_tracks_idx = labels[labels == 1].index\n    positive_tracks = data_test.loc[positive_tracks_idx]\n    sp_positive_tracks = vectorizer.transform(positive_tracks.values)\n    negative_tracks_idx = ~np.isin(data_test.index, positive_tracks_idx)\n    negative_tracks = data_test[negative_tracks_idx].drop(playlist_df.columns, axis=1)\n    negative_playlist = np.array([p_info.values] * len(negative_tracks))\n    negative_playlist_samples = np.hstack([negative_tracks, negative_playlist])\n    playlist_samples_vectorized = vectorizer.transform(negative_playlist_samples)\n    test_tracks = vstack([playlist_samples_vectorized, sp_positive_tracks])\n    index_order = negative_tracks.index.append(positive_tracks_idx)\n    y_prob = AdaModel.predict_proba(test_tracks)\n    y_pred = np.argsort(-y_prob[:, 1])\n    best_pred = index_order[y_pred]\n    if len(positive_tracks_idx) > 0:\n        r_precisions.append(r_precision(positive_tracks_idx, best_pred))\n    pbar.set_description('{}'.format(np.mean(r_precisions)))"}, {'identified': 'url_to_test', 'updated_code': "url_to_test = 'http://www.google.com/search?q=fuzzing'\nmutation_fuzzer = MutationCoverageFuzzer(seed=[url_to_test])\nmutation_fuzzer.runs(http_runner, trials=10000)\nmutation_fuzzer.population"}, {'identified': 'control_state_transfer_data', 'updated_code': "control_state_transfer_data = pd.DataFrame()\nnew_RTlists = [[] for list in range(0, 5)]\nfor ID in range(10, 86):\n    sub = cdat[cdat.subject == ID]\n    for runID in range(0, 4):\n        run = sub[sub.RunCounter == runID]\n        new_RTlists[0].append(ID)\n        new_RTlists[1].append(runID)\n        validRT_trials = run[run.TrialType == 'Valid'].RT.mean()\n        invalidRT_trials = run[run.TrialType == 'Invalid'].RT.mean()\n        new_RTlists[2].append(validRT_trials)\n        new_RTlists[3].append(invalidRT_trials)\ncontrol_state_transfer_data['SubjectID'] = new_RTlists[0]\ncontrol_state_transfer_data['Run'] = new_RTlists[1]\ncontrol_state_transfer_data['Valid'] = new_RTlists[2]\ncontrol_state_transfer_data['Invalid'] = new_RTlists[3]"}, {'identified': 'batch_labels', 'updated_code': "num_steps = 10001\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    start = datetime.datetime.now()\n    print('Initialized')\n    for step in range(num_steps):\n        offset = step * batch_size % (train_labels.shape[0] - batch_size)\n        batch_data = train_dataset[offset:offset + batch_size, :, :, :]\n        batch_labels = train_labels[offset:offset + batch_size, :]\n        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, keep3: 0.9, keep4: 0.9}\n        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n        if step % 500 == 0:\n            ends = eta(start, step, num_steps)\n            valpred = valid_prediction.eval(feed_dict={keep3: 1.0, keep4: 1.0})\n            print('Step %d - Loss %f - Minibatch %.1f%% - Validation %.1f%% - ETA %s' % (step, l, accuracy(predictions, batch_labels), accuracy(valpred, valid_labels), ends))\n    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(feed_dict={keep3: 1.0, keep4: 1.0}), test_labels))"}, {'identified': 'initial_block_position_variation', 'updated_code': 'blockPositions = []\ntestBlockPositions = []\naverageSpacing = 1\nkp = 40\nkc = 250\nmass = 1\nF0 = 50\nv0 = 0.01\ninitial_block_position_variation = 0\nvf = 3.0\nblockNum = 25\nmaxTimeSteps = 50000\nminBlockV = 1e-08\ntimeStepShort = 0.005\ntimeStepLong = 1\nvariation = 0.001\nfor n in range(0, blockNum + 1):\n    blockPositions.append(n * averageSpacing + (random.random() - 0.5) * 2 * initial_block_position_variation)\n    testBlockPositions.append(n)'}, {'identified': 'graph_file_path', 'updated_code': "graph_file_path = os.path.join(folder, filename_graph)\nwith h5py.File(graph_file_path, 'r') as graph:\n    print('Attributes:')\n    for attr in graph.attrs:\n        print('  {} = {}'.format(attr, graph.attrs[attr]))\n    print('Datasets:')\n    for dname, dset in graph.items():\n        print('  {:10}: {:10}, {}'.format(dname, dset.shape, dset.dtype))\n    pars = []\n    for par in ('data', 'indices', 'indptr', 'shape'):\n        pars.append(graph.get('L_' + par))\n    L = scipy.sparse.csr_matrix(tuple(pars[:3]), shape=pars[3])\nif L.shape != (X.shape[0], X.shape[0]):\n    raise ValueError('Graph size does not correspond to data size.')"}, {'identified': 'principal_component_arrow', 'updated_code': "new_pc = eigvecs[:, -2:]\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\nprincipal_component_arrow = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\nnew_pc_cen = new_pc - new_pc.mean(0, keepdims=True)\ncov = new_pc_cen.T @ new_pc_cen / (new_pc_cen.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nplt.subplot(122)\nplt.scatter(new_pc[y == 0, 0], new_pc[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(new_pc[y == 1, 0], new_pc[y == 1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.005, head_length=0.005, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.005, head_length=0.005, color='magenta', label='Second PC')\nplt.grid(True)\nplt.show()"}, {'identified': 'x_coordinate_midpoint', 'updated_code': 'def process_image(image):\n    gray = grayscale(image)\n    kernel_size = 5\n    blur_gray = gaussian_blur(gray, kernel_size)\n    low_threshold = 50\n    high_threshold = 150\n    edges = canny(blur_gray, low_threshold, high_threshold)\n    imshape = image.shape\n    xPct = 0.05\n    yPct = 0.6\n    xbl = imshape[1] * xPct\n    xbr = imshape[1] * (1 - xPct)\n    xtl = imshape[1] * (0.5 - xPct)\n    x_coordinate_midpoint = imshape[1] * (0.5 + xPct)\n    yb = imshape[0]\n    yt = imshape[0] * yPct\n    vertices = np.array([[(xbl, yb), (xtl, yt), (x_coordinate_midpoint, yt), (xbr, yb)]], dtype=np.int32)\n    masked_image = region_of_interest(edges, vertices)\n    rho = 2\n    theta = np.pi / 180\n    threshold = 15\n    min_line_len = 20\n    max_line_gap = 30\n    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\n    overlayedImg = weighted_img(line_img, image, 0.8, 1, 0)\n    return overlayedImg'}, {'identified': 'biodiversity_index_N10', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\nindex = 0\nfor segment in segments:\n    segDF = data.loc[data[\'RiverSeg\'] == segment]\n    segDF = segDF[segDF["lam\'"] > 0]\n    segDF = segDF.sort_values(\'Year\')\n    x = segDF[\'Year\']\n    biodiversity_index_N10 = segDF[\'N10\']\n    biodiversity_index_N10.name = segment\n    plt.plot(x, biodiversity_index_N10, colors[index])\n    plt.legend(loc=(1.05, 0.2))\n    index += 1\nplt.title(\'Hill N10, \' + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel(\'N10\')\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + \'_Hill_N10.png\', bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'num_epochs', 'updated_code': 'import tensorflow as tf\nimport numpy as np\nfrom sklearn.utils import shuffle\nX_train_normal = np.array(X_train / 255 - 0.5)\nX_valid_normal = np.array(X_valid / 255 - 0.5)\nX_test_normal = np.array(X_test / 255 - 0.5)\nnum_epochs = 15\nBATCH_SIZE = 128'}, {'identified': 'class_names_file_content', 'updated_code': "import re\nCLASS_NAME_REGEX = re.compile('^n\\\\d+\\\\s+(.*)\\\\s*$', re.M | re.U)\n\ndef load_class_names():\n    with open(os.path.join('datasets', 'inception', 'imagenet_class_names.txt'), 'rb') as f:\n        class_names_file_content = f.read().decode('utf-8')\n        return CLASS_NAME_REGEX.findall(class_names_file_content)"}, {'identified': 'keypoint_detection_figure', 'updated_code': "from numpy import ma\nU = paf_avg[:, :, 16] * -1\nV = paf_avg[:, :, 17]\nX, Y = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))\nM = np.zeros(U.shape, dtype='bool')\nM[U ** 2 + V ** 2 < 0.3 * 0.3] = True\nU = ma.masked_array(U, mask=M)\nV = ma.masked_array(V, mask=M)\nplt.figure()\nplt.imshow(oriImg[:, :, [2, 1, 0]], alpha=0.5)\ns = 5\nQ = plt.quiver(X[::s, ::s], Y[::s, ::s], U[::s, ::s], V[::s, ::s], scale=50, headaxislength=4, alpha=0.5, width=0.001, color='r')\nkeypoint_detection_figure = matplotlib.pyplot.gcf()\nkeypoint_detection_figure.set_size_inches(10, 10)"}, {'identified': 'fc1_weights', 'updated_code': "def LeNet6(x, n_classes):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    fc0 = flatten(conv2)\n    fc1_weights = tf.Variable(tf.truncated_normal(shape=(400, 252), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1 = tf.matmul(fc0, fc1_weights) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(252, 168), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(168))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(168, 84), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(84))\n    fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n    fc3 = tf.nn.relu(fc3)\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean=mu, stddev=sigma))\n    fc4_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    return logits"}, {'identified': 'years_until_test_success', 'updated_code': 'years_until_test_success = seconds_until_success / 3600 / 24 / 365.25\nyears_until_test_success'}, {'identified': 'image_file_path', 'updated_code': "def resize_images(src_dir, dest_dir):\n    if not os.path.isdir(src_dir):\n        raise Exception(src_dir + ' is not a directory')\n    if not os.path.exists(dest_dir):\n        os.mkdir(dest_dir)\n    raw_images = {image for image in os.listdir(src_dir) if image.endswith(JPEG_EXTENSIONS)}\n    dest_images = {image for image in os.listdir(dest_dir)}\n    for image in raw_images - dest_images:\n        if DEBUG:\n            print('Resizing ' + image)\n        resize_image(image, src_dir, dest_dir)\n\ndef resize_image(image_file, src_dir, dest_dir):\n    image_file_path = os.path.join(src_dir, image_file)\n    with open(image_file_path, 'r+b') as fd_img:\n        with Image.open(fd_img) as img:\n            resized_image = resizeimage.resize_contain(img, [299, 299]).convert('RGB')\n            resized_image.save(os.path.join(dest_dir, image_file), img.format)\nif os.path.isdir(images_resized_dir):\n    shutil.rmtree(images_resized_dir)\nos.mkdir(images_resized_dir)\nsubdirs = ('house_with_pool', 'house_without_pool')\nfor subdir in subdirs:\n    dest_dir = os.path.join(images_resized_dir, subdir)\n    if not os.path.isdir(dest_dir):\n        os.mkdir(dest_dir)\n    if stored_images_resized:\n        source_dir = os.path.join(stored_images_resized, subdir)\n        if os.path.isdir(source_dir):\n            for f in os.listdir(source_dir):\n                path = os.path.join(source_dir, f)\n                if os.path.isfile(path):\n                    shutil.copy(path, dest_dir)\n    resize_images(os.path.join(image_dir, subdir), dest_dir)"}, {'identified': 'earnings_growth_32k_plus', 'updated_code': "fig = plt.figure()\nfig.add_subplot()\ny1 = earningsgrowth10k\ny2 = earningsgrowth10k_18k\ny3 = earningsgrowth18k_32k\nearnings_growth_32k_plus = earningsgrowth32kk\nx_axis = [x for x in range(1, 9)]\nx_axis2 = [x + 0.2 for x in range(1, 9)]\nx_axis3 = [x + 0.4 for x in range(1, 9)]\nx_axis4 = [x + 0.6 for x in range(1, 9)]\nplt.bar(x_axis, y1, width=0.2, label='Growth for Tier:10k')\nplt.bar(x_axis2, y2, width=0.2, label='Growth for Tier:10k-18k')\nplt.bar(x_axis3, y3, width=0.2, label='Growth for Tier:18k-32k')\nplt.bar(x_axis4, earnings_growth_32k_plus, width=0.2, label='Growth for Tier: 32k')\nplt.title('Earnings Growth years 6-10')\nplt.xlabel('Region')\nplt.ylabel('Earnings Growth')\nplt.legend(title=['Earnings Growth years 6-10'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"}, {'identified': 'lane_detected_image', 'updated_code': "image = mpimg.imread('test_images/solidWhiteCurve.jpg')\nlane_detected_image = lane_detection(image)\nplt.imshow(lane_detected_image)"}, {'identified': 'reconstructed_centers', 'updated_code': "reconstructed_centers = pca.inverse_transform(centers)\ntrue_centers = np.exp(reconstructed_centers)\nsegments = ['Segment {}'.format(i) for i in range(0, len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns=data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)"}, {'identified': 'image_filenames', 'updated_code': "import csv\nimport datetime\nimport os\nimport glob\nIMG_PATH = '/home/ivan/Área de trabalho/2017'\nimage_filenames = []\nfor filename in glob.iglob(IMG_PATH + '**/*/*/*/*stamp.jpg*', recursive=True):\n    image_filenames.append(filename)\nprint(image_filenames[:4])\nprint(len(image_filenames))\nprint(find_conteiner(image_filenames[1]))\nprint(find_conteiner(image_filenames[15]))"}, {'identified': 'patches_to_image_conversion_result', 'updated_code': "probas_patches_msr = np.reshape(get_acc_net_msr(y_pred_te).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_msr -= np.min(probas_patches_msr)\nprobas_patches_msr /= np.max(probas_patches_msr)\nprobas_patches_margin = np.reshape((1 - get_acc_net_max_margin(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_margin -= np.min(probas_patches_margin)\nprobas_patches_margin /= np.max(probas_patches_margin)\nprobas_patches_entropy = np.reshape((1 - get_acc_net_entropy(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_entropy -= np.min(probas_patches_entropy)\nprobas_patches_entropy /= np.max(probas_patches_entropy)\nprobas_patches_msr = np.reshape(get_acc_net_msr(y_pred_te).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_msr -= np.min(probas_patches_msr)\nprobas_patches_msr /= np.max(probas_patches_msr)\nprobas_patches_margin = np.reshape((1 - get_acc_net_max_margin(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_margin -= np.min(probas_patches_margin)\nprobas_patches_margin /= np.max(probas_patches_margin)\nprobas_patches_entropy = np.reshape((1 - get_acc_net_entropy(y_pred_te)).flatten(), np.shape(data_test.gt_patches))\nprobas_patches_entropy -= np.min(probas_patches_entropy)\nprobas_patches_entropy /= np.max(probas_patches_entropy)\npatches_to_image_conversion_result = convert_patches_to_image(data_test.imgs, probas_patches_msr[..., np.newaxis], 64, 64)\nacc_im_margin = convert_patches_to_image(data_test.imgs, probas_patches_margin[..., np.newaxis], 64, 64)\nacc_im_entropy = convert_patches_to_image(data_test.imgs, probas_patches_entropy[..., np.newaxis], 64, 64)\nfor img_idx in range(5):\n    plt.figure(figsize=(8, 8))\n    plt.imshow(patches_to_image_conversion_result[img_idx], cmap='RdYlGn')\n    plt.axis('off')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.savefig('../Figures/Zurich/Im_cert/ED/net_msr_im_' + str(img_idx) + '.pdf', bbox_inches='tight', pad_inches=0)\n    plt.close()\n    plt.figure(figsize=(8, 8))\n    plt.imshow(acc_im_margin[img_idx], cmap='RdYlGn')\n    plt.axis('off')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.savefig('../Figures/Zurich/Im_cert/ED/net_margin_im_' + str(img_idx) + '.pdf', bbox_inches='tight', pad_inches=0)\n    plt.close()\n    plt.figure(figsize=(8, 8))\n    plt.imshow(acc_im_entropy[img_idx], cmap='RdYlGn')\n    plt.axis('off')\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.savefig('../Figures/Zurich/Im_cert/ED/net_entropy_im_' + str(img_idx) + '.pdf', bbox_inches='tight', pad_inches=0)\n    plt.close()"}, {'identified': 'pascal_voc_writer_instance', 'updated_code': "from image_aq.utils.pascal_voc_io import PascalVocWriter\n\ndef gera_voc(file):\n    foldername, filename = os.path.split(file)\n    img = imageio.imread(file)\n    imgSize = img.shape\n    print(foldername, filename, imgSize)\n    pascal_voc_writer_instance = PascalVocWriter(foldername, filename, imgSize)\n    coords = find_conteiner(file)\n    pascal_voc_writer_instance.addBndBox(*coords, 'cc', '')\n    pascal_voc_writer_instance.save(file[:-4] + '.xml')\nfile = lista[0]\nprint(file)\ngera_voc(file)\nglob.glob(foldername + '/*')"}, {'identified': 'years', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\nindex = 0\nfor segment in segments:\n    segDF = data.loc[data[\'RiverSeg\'] == segment]\n    segDF = segDF[segDF["lam\'"] > 0]\n    segDF = segDF.sort_values(\'Year\')\n    years = segDF[\'Year\']\n    y = segDF[\'N1\']\n    y.name = segment\n    plt.plot(years, y, colors[index])\n    plt.legend(loc=(1.05, 0.2))\n    index += 1\nplt.title(\'Hill N1, \' + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel("N1\'")\nplt.xticks(np.arange(min(years), max(years) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + \'_Hill_N1.png\', bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'url_opener', 'updated_code': "if use_toy_data:\n    input_fn = 'toy_input.txt'\n    with open(input_fn, 'w') as f:\n        f.write('a b c\\td e f d e f\\n')\n        f.write('d e f\\ta b c a b c\\n')\nelse:\n    DATA_DIR = '../../data/'\n    input_fn = 'instrumental.full.train'\n    input_fn = os.path.join(DATA_DIR, input_fn)\n    if not os.path.exists(input_fn):\n        import urllib\n        url_opener = urllib.request.URLopener()\n        url_opener.retrieve('http://sandbox.mokk.bme.hu/~judit/resources/instrumental.full.train', input_fn)"}, {'identified': 'num_hidden_layers', 'updated_code': "xmin = 0\nxmax = x_test.shape[1]\nxindex = np.arange(xmin, xmax)\nnum_hidden_layers = 3\nfig, ax = plt.subplots(nrows=num_hidden_layers, ncols=1, figsize=(15, 3 * num_hidden_layers))\nax[0].scatter(xindex, x_test[0, :, 0], marker='.', c=y_true_colors)\nax[0].set_title('epoch = {}'.format(epoch))\nax[0].set_xlim((xmin, xmax))\nax[0].set_xticks([])\nax[0].grid(True)\nax[1].scatter(xindex, x_test[0, :, 0], marker='.', c=y_pred_colors)\nax[1].set_title('loss = {:<6.4f}, accuracy = {:<.2%}'.format(*score))\nax[1].set_xlim((xmin, xmax))\nax[1].set_xticks([])\nax[1].grid(True)\nlegend_labels = []\nfor wave in msig.waves:\n    ax[2].plot(xindex, wave.sample_full, color=wave.color, zorder=1)\n    legend_labels.append(wave.name)\nax[2].scatter(xindex[i_fail], x_test[0, i_fail, 0], marker='o', c=y_pred_colors[i_fail], zorder=2)\nax[2].set_xlim((xmin, xmax))\nax[2].grid(True)\nax[2].legend(legend_labels)\nplt.tight_layout()\nplt.savefig(os.path.join(msig.out_dir, 'prediction_analysis.png'), bbox_inches='tight')"}, {'identified': 'stimulus_control_chart', 'updated_code': "barwidth = 0.75\nfig, ax = plt.subplots(figsize=(9, 7))\nrects1 = ax.bar(0.5, SkyPresence.mean(), barwidth, color=sns.xkcd_rgb['green'], yerr=SkyPresenceSEM, ecolor='k', error_kw=dict(lw=3))\nrects2 = ax.bar(1.5, ColorScheme.mean(), barwidth, color=(0.3, 0.9, 0.3), yerr=ColorSchemeSEM, ecolor='k', error_kw=dict(lw=3))\nrects3 = ax.bar(2.5, TreeFreq.mean(), barwidth, color=(0.15, 1, 0.15), yerr=TreeFreqSEM, ecolor='k', error_kw=dict(lw=3))\nrects4 = ax.bar(4, ImageType.mean(), barwidth, yerr=ImageTypeSEM, ecolor='k', edgecolor=sns.xkcd_rgb['green'], linewidth=2, facecolor='none', error_kw=dict(lw=3))\nrects5 = ax.bar(5, FeatureType.mean(), barwidth, yerr=FeatureTypeSEM, ecolor='k', edgecolor=(0.3, 0.9, 0.3), linewidth=2, facecolor='none', error_kw=dict(lw=3))\nstimulus_control_chart = ax.bar(6, LightType.mean(), barwidth, yerr=LightTypeSEM, ecolor='k', edgecolor=(0.15, 1, 0.15), linewidth=2, facecolor='none', error_kw=dict(lw=3))\nsns.set(context='notebook', style='white', font='Myriad Pro', font_scale=2, color_codes=False, rc=None)\nax.set_ylim(0, 100)\nax.set_xlim(0, 7.5)\nax.set_xticklabels(('SP', 'CS', 'TF', 'IT', 'FT', 'LT'))\nax.set_xticks([0.5 + barwidth / 2, 1.5 + barwidth / 2, 2.5 + barwidth / 2, 4 + barwidth / 2, 5 + barwidth / 2, 6 + barwidth / 2])\nax.set_yticks(np.arange(0, 101, 10))\nplt.title('Q2: Rate the Frequency at Which These Perceptual Categories\\nPredicted an Easy/Hard Color-Word Trial', fontsize=18, fontweight='bold')\nplt.ylabel('<-- Less Likely      More Likely -->', fontsize=17, fontweight='bold')\nplt.xlabel('S-C Phase                 S-CT Phase', fontsize=17, fontweight='bold')\nsns.despine()\nplt.show()"}, {'identified': 'earnings_growth_tier_10k', 'updated_code': "fig = plt.figure()\nfig.add_subplot()\nearnings_growth_tier_10k = earningsgrowth10k\ny2 = earningsgrowth10k_18k\ny3 = earningsgrowth18k_32k\ny4 = earningsgrowth32kk\nx_axis = [x for x in range(1, 9)]\nx_axis2 = [x + 0.2 for x in range(1, 9)]\nx_axis3 = [x + 0.4 for x in range(1, 9)]\nx_axis4 = [x + 0.6 for x in range(1, 9)]\nplt.bar(x_axis, earnings_growth_tier_10k, width=0.2, label='Growth for Tier:10k')\nplt.bar(x_axis2, y2, width=0.2, label='Growth for Tier:10k-18k')\nplt.bar(x_axis3, y3, width=0.2, label='Growth for Tier:18k-32k')\nplt.bar(x_axis4, y4, width=0.2, label='Growth for Tier: 32k')\nplt.title('Earnings Growth years 6-10')\nplt.xlabel('Region')\nplt.ylabel('Earnings Growth')\nplt.legend(title=['Earnings Growth years 6-10'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"}, {'identified': 'bitmask', 'updated_code': 'def flip_random_character(s):\n    """Returns s with a random bit flipped in a random position"""\n    if s == \'\':\n        return s\n    pos = random.randint(0, len(s) - 1)\n    c = s[pos]\n    bitmask = 1 << random.randint(0, 6)\n    new_c = chr(ord(c) ^ bitmask)\n    return s[:pos] + new_c + s[pos + 1:]'}, {'identified': 'grid_search_cv', 'updated_code': "from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\n\ntfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\nparam_grid = [{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, {'vect__ngram_range': [(1, 1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}]\nlr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\ngrid_search_cv = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)"}, {'identified': 'error_detection_metrics', 'updated_code': 'y_scores = (-get_acc_net_msr(y_pred_te)).flatten()\ny_true = pred_f_te.flatten()\nprecision_msr, recall_msr, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_msr = metrics.average_precision_score(y_true, y_scores)\nerror_detection_metrics = metrics.roc_auc_score(y_true, y_scores)\nfpr_msr, tpr_msr, _ = metrics.roc_curve(y_true, y_scores)\ny_scores = (-get_acc_net_max_margin(y_pred_te)).flatten()\nprecision_margin, recall_margin, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_margin = metrics.average_precision_score(y_true, y_scores)\nauroc_margin = metrics.roc_auc_score(y_true, y_scores)\nfpr_margin, tpr_margin, _ = metrics.roc_curve(y_true, y_scores)\ny_scores = (-get_acc_net_entropy(y_pred_te)).flatten()\nprecision_entropy, recall_entropy, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_entropy = metrics.average_precision_score(y_true, y_scores)\nauroc_entropy = metrics.roc_auc_score(y_true, y_scores)\nfpr_entropy, tpr_entropy, _ = metrics.roc_curve(y_true, y_scores)'}, {'identified': 'y_coordinate', 'updated_code': "def plot(embeddings, labels):\n    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n    pylab.figure(figsize=(15, 15))\n    for i, label in enumerate(labels):\n        x, y_coordinate = embeddings[i, :]\n        pylab.scatter(x, y_coordinate)\n        pylab.annotate(label, xy=(x, y_coordinate), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n    pylab.show()\nwords = [reverse_dictionary[i] for i in range(1, num_points + 1)]\nplot(two_d_embeddings, words)"}, {'identified': 'lane_detected_image', 'updated_code': 'def process_image_3_channels(image, k_size=3, vertex_ratio_h=0.45, vertex_ratio_v=0.6, low_thresh=50, high_thresh=200, L2gradient=False, rho=2, theta=1 * np.pi / 180.0, min_votes=15, min_line_len=40, max_line_gap=20, angle=3 * np.pi / 16, angle_thresh=np.pi / 16, debug=False):\n    lane_detected_image = lane_detection_ppline_3_channels(image, k_size=k_size, low_thresh=low_thresh, high_thresh=high_thresh, L2gradient=L2gradient, rho=rho, theta=theta, min_votes=min_votes, min_line_len=min_line_len, max_line_gap=max_line_gap, angle=angle, angle_thresh=angle_threshold, debug=debug)\n    return lane_detected_image'}, {'identified': 'state_variable_ST', 'updated_code': "I = Variable(name='I', num_states=2)\nS = Variable(name='S', num_states=2)\nstate_variable_ST = Variable(name='ST', num_states=2)\nF = Variable(name='F', num_states=2)\nB = Variable(name='B', num_states=2)\nC = Variable(name='C', num_states=2)\nW = Variable(name='W', num_states=2)\nf_I = Factor(name='p(I)', f=np.array([0.95, 0.05]), neighbours=[I])\nf_S = Factor(name='p(S)', f=np.array([0.8, 0.2]), neighbours=[S])\nprob_ST = [[0.999, 0.7], [0.001, 0.3]]\nf_ST = Factor(name='p(ST |I)', f=np.array(prob_ST), neighbours=[state_variable_ST, I])\nprob_F = [[0.95, 0.1], [0.05, 0.9]]\nf_F = Factor(name='p(F |I)', f=np.array(prob_F), neighbours=[F, I])\nprob_B = [[[0.9999, 0.3], [0.1, 0.01]], [[0.0001, 0.7], [0.9, 0.99]]]\nf_B = Factor(name='p(B |I, S)', f=np.array(prob_B), neighbours=[B, I, S])\nprob_C = [[0.93, 0.2], [0.07, 0.8]]\nf_C = Factor(name='p(C |B)', f=np.array(prob_C), neighbours=[C, B])\nprob_W = [[0.999, 0.4], [0.001, 0.6]]\nf_W = Factor(name='p(W |B)', f=np.array(prob_W), neighbours=[W, B])"}, {'identified': "'phs_file_path' or 'phs_file_name'", 'updated_code': None}, {'identified': 'group_ticks', 'updated_code': "ncomponents = 2\npca = decomp.PCA(n_components=ncomponents)\nraw_pca = pca.fit(np.array(x_train.iloc[:, first_egemaps_feature:]).astype(np.float))\nx_pca = raw_pca.transform(x_train.iloc[:, first_egemaps_feature:])\ngroups = x_train['pos']\nshort_pos = {'nouns': 'nn', 'verbs': 'vrb', 'other': 'oth', 'function_words': 'fct', 'adjectives': 'adj'}\ngroup_values = [short_pos[i] for i in groups.unique()]\nposIDs = groups.factorize()[0]\nlabels = np.array(x_train['word'])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.set_xlabel('PC 1')\nax.set_ylabel('PC 2')\ncolormap = plt.get_cmap('gnuplot', len(group_values))\npc1 = x_pca[:, 0]\npc2 = x_pca[:, 1]\np = ax.scatter(pc1, pc2, c=posIDs, cmap=colormap, s=10)\ncb = plt.colorbar(p)\ngroup_ticks = np.arange(0, max(posIDs), max(posIDs) / float(len(group_values)))\ncb.set_ticks(group_ticks)\ncb.set_ticklabels(group_values)\nfor label, x, y in zip(labels, pc1, pc2):\n    if y == np.amax(y) or x == np.amax(x):\n        plt.annotate(label, fontsize=2, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nif pos_filter == None:\n    axins = zoomed_inset_axes(ax, 2.5, loc=2)\n    x1, x2, y1, y2 = (-3.1, 2.5, 6.3, 9.8)\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n    axins.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\n    axins2 = zoomed_inset_axes(ax, 2.5, loc=4)\n    x1, x2, y1, y2 = (-4, 0.5, -8.5, -6.5)\n    axins2.set_xlim(x1, x2)\n    axins2.set_ylim(y1, y2)\n    axins2.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins2, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nfilename = corpus + '_full_zoom.pdf'\nif pos_filter != None and len(pos_filter) == 1:\n    filename = corpus + '_pca_' + pos_filter[0][1] + '_' + pos_filter[0][2] + '.pdf'\nplt.savefig(filename, bbox_inches='tight')"}, {'identified': 'control_state_data', 'updated_code': "RTanalysis = pd.DataFrame()\ncontrol_state_data = [[] for list in range(0, 5)]\nfor ID in range(10, 86):\n    sub = cdat[cdat.subject == ID]\n    control_state_data[0].append(ID)\n    validRT_trials = sub[sub.TrialType == 'Valid'].RT.mean()\n    invalidRT_trials = sub[sub.TrialType == 'Invalid'].RT.mean()\n    control_state_data[1].append(validRT_trials)\n    control_state_data[2].append(invalidRT_trials)\nRTanalysis['SubjectID'] = control_state_data[0]\nRTanalysis['Valid'] = control_state_data[1]\nRTanalysis['Invalid'] = control_state_data[2]"}, {'identified': 'uncontrolled_charging_algorithm', 'updated_code': 'uncontrolled_charging_algorithm = algorithms.UncontrolledCharging()'}, {'identified': 'xB', 'updated_code': 'nA = 100\nxA = 20\nnB = 90\nxB = 25\nBayes_AB_test(nA=nA, xA=xA, nB=nB, xB=xB)'}, {'identified': 'mean_log_prob', 'updated_code': 'def sgd_iter(x_train, t_train, W, b):\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    lr = 0.0001\n    logp = np.zeros(len(x_train))\n    for i in indices:\n        x = x_train[i:i + 1]\n        t = t_train[i]\n        logp[i], grad_w, grad_b = logreg_gradient(x, t, W, b)\n        W = W + lr * grad_w\n        b = b + lr * grad_b\n    mean_log_prob = logp.mean()\n    return (mean_log_prob, W, b)'}, {'identified': 'generate_data', 'updated_code': 'n_neurons = 32\nkernel_size = 5\nn_hidden = 11\ngenerate_data = False'}, {'identified': 'input_data_batch', 'updated_code': "for i in range(len(multiplier)):\n    scale = multiplier[i]\n    imageToTest = cv.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv.INTER_CUBIC)\n    imageToTest_padded, pad = padRightDownCorner(imageToTest, 8, 128)\n    transposeImage = np.transpose(np.float32(imageToTest_padded[:, :, :]), (2, 0, 1)) / 256 - 0.5\n    testimage = transposeImage\n    cmodel = mx.mod.Module(symbol=sym, label_names=[])\n    cmodel.bind(data_shapes=[('data', (1, 3, testimage.shape[1], testimage.shape[2]))])\n    cmodel.init_params(arg_params=arg_params, aux_params=aux_params)\n    input_data_batch = DataBatch(mx.nd.array([testimage[:, :, :]]), 0)\n    cmodel.forward(input_data_batch)\n    result = cmodel.get_outputs()\n    heatmap = np.moveaxis(result[1].asnumpy()[0], 0, -1)\n    heatmap = cv.resize(heatmap, (0, 0), fx=model['stride'], fy=model['stride'], interpolation=cv.INTER_CUBIC)\n    heatmap = heatmap[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]\n    heatmap = cv.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv.INTER_CUBIC)\n    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n    f = plt.figure(i)\n    plt.imshow(oriImg[:, :, [2, 1, 0]])\n    ax2 = plt.imshow(heatmap[:, :, 18], alpha=0.5)\n    f.show()"}, {'identified': 'colormap', 'updated_code': "scores_auc = [auroc_msr, auroc_margin, auroc_entropy, auroc_dropout, auroc_gmm, auroc_svm, auroc_df]\nfprs = [fpr_msr, fpr_margin, fpr_entropy, fpr_dropout, fpr_gmm, fpr_svm, fpr_df]\ntprs = [tpr_msr, tpr_margin, tpr_entropy, tpr_dropout, tpr_gmm, tpr_svm, tpr_df]\nscores_order = np.argsort(scores_auc)\ncolormap = plt.cm.rainbow(np.linspace(0, 1, len(scores_auc)))[:, :3]\nfig = plt.figure(figsize=(6, 6))\nfor i in scores_order:\n    plt.step(fprs[i], tprs[i], where='post', c=colormap[i])\nplt.plot([0, 1], [0, 1], '--', c='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.grid(alpha=0.3)\nfig.axes[0].spines['right'].set_visible(False)\nfig.axes[0].spines['top'].set_visible(False)\nplt.legend([str.format('%s: %.2f') % (names_methods[i], scores_auc[i]) for i in scores_order], title='AUROC')\nplt.savefig('../Figures/Zurich/Metrics/ROC_pred_ED.pdf', bbox_inches='tight', pad_inches=0)"}, {'identified': 'predicted_class_index', 'updated_code': "softmax_logits = tf.nn.softmax(logits)\ntop_k = tf.nn.top_k(softmax_logits, k=3)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.import_meta_graph('./lenet.meta')\n    saver.restore(sess, './lenet')\n    batch_x = np.expand_dims(my_images_normalized, axis=3)\n    sample_softmax_logits = sess.run(softmax_logits, feed_dict={x: batch_x, keep_prob: 1.0})\n    my_top_k = sess.run(top_k, feed_dict={x: batch_x, keep_prob: 1.0})\n    fig, axs = plt.subplots(len(my_images), 1, figsize=(20, 25))\n    fig.subplots_adjust(hspace=1.0, wspace=0.6)\n    axs = axs.ravel()\n    for i, image in enumerate(my_images):\n        axs[i].axis('off')\n        axs[i].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n        guess1 = my_top_k[1][i][0]\n        index1 = np.argwhere(y_valid == guess1)[0]\n        guess2 = my_top_k[1][i][1]\n        predicted_class_index = np.argwhere(y_valid == guess2)[0]\n        guess3 = my_top_k[1][i][2]\n        index3 = np.argwhere(y_valid == guess3)[0]\n        title = ''\n        title += 'guess 1: class ' + sign_dict[str(guess1)] + ', probability: ' + str(100 * my_top_k[0][i][0]) + '\\n'\n        title += 'guess 2: class ' + sign_dict[str(guess2)] + ', probability: ' + str(100 * my_top_k[0][i][1]) + '\\n'\n        title += 'guess 3: class ' + sign_dict[str(guess3)] + ', probability: ' + str(100 * my_top_k[0][i][2])\n        axs[i].set_title(title)"}, {'identified': 'lowest_revenue_quarter', 'updated_code': "QHrev = dfq.Revenue.max()[3]\nQHLoss = dfq.Revenue.min()[3]\nQmax = max(dfq.idxmax())\nlowest_revenue_quarter = min(dfq.idxmin())\nprint('Highest Grossing Quarterly Revenue of $%.0f was observed on the %s %s-quarter.' % (QHrev, Qmax[0], Qmax[1]))\nprint('Biggest Quarterly Loss of $%.00f was observed on the %s %s-quarter.' % (QHLoss, lowest_revenue_quarter[0], lowest_revenue_quarter[1]))"}, {'identified': 'blurred_gray_image', 'updated_code': 'def process_image(image):\n    """ Filter color """\n    color_select = np.copy(image)\n    rgb_threshold = [200, 150, 95]\n    thresholds = (image[:, :, 0] < rgb_threshold[0]) | (image[:, :, 1] < rgb_threshold[1]) | (image[:, :, 2] < rgb_threshold[2])\n    color_select[thresholds] = [0, 0, 0]\n    gray = grayscale(color_select)\n    blurred_gray_image = gaussian_blur(gray, 3)\n    edges = canny(blurred_gray_image, 50, 150)\n    xsize = image.shape[1]\n    ysize = image.shape[0]\n    vertices = np.array([[(0, ysize), (xsize / 2, ysize / 1.71), (xsize / 2, ysize / 1.71), (xsize, ysize)]], dtype=np.int32)\n    regioned = region_of_interest(edges, vertices)\n    hough = hough_lines(regioned, 1, np.pi / 180, 35, 35, 20)\n    result = weighted_img(hough, image)\n    return result'}, {'identified': 'image_width', 'updated_code': 'def rbg_to_hls(img):\n    """ \n    Takes an RGB image and converts it to HLS.\n    Returns the converted image (3 channels)\n    """\n    hls_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    return hls_image\n\ndef merge_channels(images):\n    """\n    Merge images from three different channels\n     - images: a list of 3 images, each in a channel\n    """\n    merged = weighted_img(images[0], images[1], α=0.5, β=0.5, λ=0.0)\n    merged = weighted_img(merged, images[2], α=1.0, β=0.5, λ=0.0)\n    return merged\n\ndef lane_detection_ppline_3_channels(image, k_size=3, vertex_ratio_h=0.45, vertex_ratio_v=0.6, low_thresh=50, high_thresh=200, L2gradient=False, rho=2, theta=1 * np.pi / 180.0, min_votes=15, min_line_len=40, max_line_gap=20, angle=3 * np.pi / 16, angle_thresh=np.pi / 16, debug=False):\n    """\n    Takes an image and parameters and applies the lane detection pipeline.\n    Returns an image combining the original and the extended lines detected\n    by the algorithm.\n     - debug: Whether or not to display the images after each step of the process, for\n     debugging or tuning purposes.\n    """\n    max_y, image_width = image.shape[:2]\n    roi = np.array([[(0, max_y), (round(image_width * vertex_ratio_h), round(max_y * vertex_ratio_v)), (round(image_width * (1 - vertex_ratio_h)), round(max_y * vertex_ratio_v)), (image_width, max_y)]])\n    if debug:\n        plt.subplot(5, 3, 1)\n        plt.imshow(image)\n    blur = gaussian_blur(image, k_size)\n    if debug:\n        plt.subplot(5, 3, 2)\n        plt.imshow(blur)\n    hls = rbg_to_hls(blur)\n    if debug:\n        plt.subplot(5, 3, 3)\n        plt.imshow(hls)\n    edges_list = []\n    for chan in range(0, 3):\n        edges_list.append(canny(hls[:, :, chan], low_thresh, high_thresh, L2gradient=L2gradient))\n        if debug:\n            plt.subplot(5, 3, chan + 4)\n            plt.imshow(edges_list[chan])\n    masked_edges_list = []\n    for chan in range(0, 3):\n        masked_edges_list.append(region_of_interest(edges_list[chan], roi))\n        if debug:\n            plt.subplot(5, 3, chan + 7)\n            plt.imshow(masked_edges_list[chan])\n    lines_list = []\n    for chan in range(0, 3):\n        lines_list.append(hough_lines(masked_edges_list[chan], rho, theta, min_votes, min_line_len, max_line_gap))\n        if debug:\n            plt.subplot(5, 3, chan + 10)\n            plt.imshow(lines_list[chan][0])\n    lines = np.zeros((1, 1, 4))\n    for chan in range(0, 3):\n        lines = np.concatenate((lines, lines_list[chan][1]), axis=0)\n    if debug:\n        hls_lines_image = merge_channels([lines_list[0][0], lines_list[1][0], lines_list[2][0]])\n        plt.subplot(5, 3, 13)\n        plt.imshow(hls_lines_image)\n    try:\n        combined = extend_lines(image, lines, angle=angle, angle_thresh=angle_thresh)\n        if debug:\n            plt.subplot(5, 3, 14)\n            plt.imshow(combined)\n    except IndexError:\n        print(\'Error. Try relaxing your angle parameters a litte.\')\n    return combined\n\ndef process_image_3_channels(image, k_size=3, vertex_ratio_h=0.45, vertex_ratio_v=0.6, low_thresh=50, high_thresh=200, L2gradient=False, rho=2, theta=1 * np.pi / 180.0, min_votes=15, min_line_len=40, max_line_gap=20, angle=3 * np.pi / 16, angle_thresh=np.pi / 16, debug=False):\n    result = lane_detection_ppline_3_channels(image, k_size=k_size, low_thresh=low_thresh, high_thresh=high_thresh, L2gradient=L2gradient, rho=rho, theta=theta, min_votes=min_votes, min_line_len=min_line_len, max_line_gap=max_line_gap, angle=angle, angle_thresh=angle_threshold, debug=debug)\n    return result'}, {'identified': 'language_spoken_other_than_English', 'updated_code': "fig = plt.figure(figsize=(8, 6))\nlanguage_spoken_other_than_English = df_county_data['Speak a language other than English']\ny = df_county_data['Graduation Rate']\nplt.scatter(language_spoken_other_than_English, y, color='g', marker='o', alpha=0.9)\nmask = ~np.isnan(language_spoken_other_than_English) & ~np.isnan(y)\nsns.regplot(df_county_data['Speak a language other than English'], df_county_data['Graduation Rate'], color='r', label='Speak a language other than English')\nplt.title('High School Graduation Rates and ESL by County')\nplt.ylabel('Graduation Rate')\nplt.xlabel('Speak a language other than English')\nplt.legend(loc='best')\nplt.grid(True)\nsns.set_style('whitegrid')\nplt.text(65, 0.925, 'Note:\\nAreas with one or more Foreign languages beside English language \\ntend to have a Lower graduation rate.')\nplt.savefig('Images/County_Grad_Speak a language other than English3.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'feature_columns', 'updated_code': "def get_wide_deep():\n    is_male, mother_age, plurality, gestation_weeks = [tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False', 'Unknown']), tf.feature_column.numeric_column('mother_age'), tf.feature_column.categorical_column_with_vocabulary_list('plurality', ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)', 'Multiple(2+)']), tf.feature_column.numeric_column('gestation_weeks')]\n    age_buckets = tf.feature_column.bucketized_column(mother_age, boundaries=np.arange(15, 45, 1).tolist())\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, boundaries=np.arange(17, 47, 1).tolist())\n    wide = [is_male, plurality, age_buckets, gestation_buckets]\n    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n    embed = tf.feature_column.embedding_column(crossed, 3)\n    feature_columns = [mother_age, gestation_weeks, embed]\n    return (wide, feature_columns)"}, {'identified': 'resized_images_dir', 'updated_code': "DEBUG = False\nJPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\nimage_dir = '../data/images'\ntest_images_dir = '../data/test_images'\nresized_images_dir = '../data/images_resized'\nstored_bottlenecks = '../data/bottlenecks'\ntmp_dir = '/tmp'\nbottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\nimages_resized_dir = os.path.join(tmp_dir, 'images_resized')\nsummaries_dir = os.path.join(tmp_dir, 'retrain_logs')\nmodel_dir = os.path.join(tmp_dir, 'inception')\ninception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\noutput_graph_orig = 'output_graph_orig.pb'\noutput_graph = 'output_graph.pb'\noutput_labels = 'output_labels.txt'\narchitecture = 'inception_v3'\nfinal_tensor_name = 'final_result'\nhow_many_training_steps = 500\nlearning_rate = 0.01\ntesting_percentage = 10\nvalidation_percentage = 10\neval_step_interval = 10\ntrain_batch_size = 100\ntest_batch_size = -1\nvalidation_batch_size = 100\nprint_misclassified_test_images = False\nflip_left_right = False\nrandom_crop = 0\nrandom_scale = 0\nrandom_brightness = 0\nforce_inception_download = False\nFLAGS = type('FlagsObject', (object,), {'architecture': architecture, 'model_dir': model_dir, 'intermediate_store_frequency': 0, 'summaries_dir': summaries_dir, 'learning_rate': learning_rate, 'image_dir': images_resized_dir, 'testing_percentage': testing_percentage, 'validation_percentage': validation_percentage, 'random_scale': random_scale, 'random_crop': random_crop, 'flip_left_right': flip_left_right, 'random_brightness': random_brightness, 'bottleneck_dir': bottleneck_dir, 'final_tensor_name': final_tensor_name, 'how_many_training_steps': how_many_training_steps, 'train_batch_size': train_batch_size, 'test_batch_size': test_batch_size, 'eval_step_interval': eval_step_interval, 'validation_batch_size': validation_batch_size, 'print_misclassified_test_images': print_misclassified_test_images, 'output_graph': output_graph, 'output_labels': output_labels})\nretrain.FLAGS = FLAGS"}, {'identified': 'rover_y_position', 'updated_code': "def process_image(img):\n    warped, mask = perspect_transform(img, source, destination)\n    threshed = color_thresh(warped)\n    obs_map = np.absolute(np.float32(threshed) - 1) * mask\n    xpix, ypix = rover_coords(threshed)\n    world_size = data.worldmap.shape[0]\n    scale = 2 * dst_size\n    xpos = data.xpos[data.count]\n    rover_y_position = data.ypos[data.count]\n    yaw = data.yaw[data.count]\n    x_world, y_world = pix_to_world(xpix, ypix, xpos, rover_y_position, yaw, world_size, scale)\n    obsxpix, obsypix = rover_coords(obs_map)\n    obs_x_world, obs_y_world = pix_to_world(obsxpix, obsypix, xpos, rover_y_position, yaw, world_size, scale)\n    data.worldmap[y_world, x_world, 2] = 255\n    data.worldmap[obs_y_world, obs_x_world, 0] = 255\n    nav_pix = data.worldmap[:, :, 2] > 0\n    data.worldmap[nav_pix, 0] = 0\n    rock_map = find_rocks(warped, levels=(110, 110, 50))\n    if rock_map.any():\n        rock_x, rock_y = rover_coords(rock_map)\n        rock_x_world, rock_y_world = pix_to_world(rock_x, rock_y, xpos, rover_y_position, yaw, world_size, scale)\n        data.worldmap[rock_y_world, rock_x_world, :] = 255\n    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1] * 2, 3))\n    output_image[0:img.shape[0], 0:img.shape[1]] = img\n    output_image[0:img.shape[0], img.shape[1]:] = warped\n    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n    cv2.putText(output_image, 'Populate this image with your analyses to make a video!', (20, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n    if data.count < len(data.images) - 1:\n        data.count += 1\n    return output_image"}, {'identified': 'lane_detected_count', 'updated_code': 'import math\n\nleftline = [(0, 0, 0, 0)]\nrightline = [(0, 0, 0, 0)]\n\ndef grayscale(img):\n    """Applies the Grayscale transform\n    This will return an image with only one color channel\n    but NOTE: to see the returned image as grayscale\n    (assuming your grayscaled image is called \'gray\')\n    you should call plt.imshow(gray, cmap=\'gray\')"""\n    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\ndef canny(img, low_threshold, high_threshold):\n    """Applies the Canny transform"""\n    return cv2.Canny(img, low_threshold, high_threshold)\n\ndef gaussian_blur(img, kernel_size):\n    """Applies a Gaussian Noise kernel"""\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n\ndef region_of_interest(img, vertices):\n    """\n    Applies an image mask.\n    \n    Only keeps the region of the image defined by the polygon\n    formed from `vertices`. The rest of the image is set to black.\n    """\n    mask = np.zeros_like(img)\n    if len(img.shape) > 2:\n        channel_count = img.shape[2]\n        ignore_mask_color = (255,) * channel_count\n    else:\n        ignore_mask_color = 255\n    cv2.fillPoly(mask, vertices, ignore_mask_color)\n    masked_image = cv2.bitwise_and(img, mask)\n    return masked_image\n\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\ndef draw_lines_roi(img, lines, vertices, color=[255, 0, 0], thickness=2):\n    """\n    NOTE: this is the function you might want to use as a starting point once you want to \n    average/extrapolate the line segments you detect to map out the full\n    extent of the lane (going from the result shown in raw-lines-example.mp4\n    to that shown in P1_example.mp4).  \n    \n    Think about things like separating line segments by their \n    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n    line vs. the right line.  Then, you can average the position of each of \n    the lines and extrapolate to the top and bottom of the lane.\n    \n    This function draws `lines` with `color` and `thickness`.    \n    Lines are drawn on the image inplace (mutates the image).\n    If you want to make the lines semi-transparent, think about combining\n    this function with the weighted_img() function below\n    """\n    global leftline\n    global rightline\n    imshape = img.shape\n    y_min = np.int(imshape[0] * 0.61)\n    y_max = imshape[0]\n    left_x1 = []\n    left_x2 = []\n    right_x1 = []\n    right_x2 = []\n    left_count = 0\n    lane_detected_count = 0\n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            m = (y2 - y1) / (x2 - x1)\n            if (m > 0.3) & (m < 7):\n                fit ='}, {'identified': 'num_rows_subplot_grid', 'updated_code': "t_min_max = (vsig.timestamps[0], vsig.timestamps[-1])\nlayer = '2'\no_or_s = 'output'\nval_arrays = np.load(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, o_or_s])) + '.npy')\nn_generations, _, n_neurons = val_arrays.shape\nncols = 2\nnum_rows_subplot_grid = n_neurons // ncols\nfig, axes = plt.subplots(nrows=num_rows_subplot_grid, ncols=ncols, figsize=(16, 20))\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = axes[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(vsig.timestamps, x_val[0, :, 0], vsig.timestamps[vsig.window_size - 1:], marker='o', c=y_pred_colors, cmap=plt.get_cmap('coolwarm'), vmin=-1, vmax=1)\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, o_or_s, g + 1))\n    plt.savefig(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, o_or_s, 'gen', str(g + 1)]) + '.png'))\nplt.show()"}, {'identified': 'rnn_lm_model_lstm', 'updated_code': "rnn_lm_model_lstm = torch.load('models/linux_3x512_0d3_lstm_200l_40000E.model').cuda()\nmodel_gru = torch.load('models/linux_3x512_0d3_gru_200l_40000E.model').cuda()\nprint('Perplexity LSTM:', 2 ** np.mean([test_model(rnn_lm_model_lstm, 'data/linux/test.txt') for _ in range(1)]))\nprint('Perplexity GRU: ', 2 ** np.mean([test_model(model_gru, 'data/linux/test.txt') for _ in range(1)]))"}, {'identified': 'num_plots', 'updated_code': "from __future__ import division, print_function\nimport numpy as np\ntry:\n    from pylab import plt\nexcept ImportError:\n    print('Unable to import pylab. R_pca.plot_fit() will not work.')\ntry:\n    range = xrange\nexcept NameError:\n    pass\n\nclass R_pca:\n\n    def __init__(self, D, mu=None, lmbda=None):\n        self.D = D\n        self.S = np.zeros(self.D.shape)\n        self.Y = np.zeros(self.D.shape)\n        if mu:\n            self.mu = mu\n        else:\n            self.mu = np.prod(self.D.shape) / (4 * self.norm_p(self.D, 2))\n        self.mu_inv = 1 / self.mu\n        if lmbda:\n            self.lmbda = lmbda\n        else:\n            self.lmbda = 1 / np.sqrt(np.max(self.D.shape))\n\n    @staticmethod\n    def norm_p(M, p):\n        return np.sum(np.power(M, p))\n\n    @staticmethod\n    def shrink(M, tau):\n        return np.sign(M) * np.maximum(np.abs(M) - tau, np.zeros(M.shape))\n\n    def svd_threshold(self, M, tau):\n        U, S, V = np.linalg.svd(M, full_matrices=False)\n        return np.dot(U, np.dot(np.diag(self.shrink(S, tau)), V))\n\n    def fit(self, tol=None, max_iter=1000, iter_print=100):\n        iter = 0\n        err = np.Inf\n        Sk = self.S\n        Yk = self.Y\n        Lk = np.zeros(self.D.shape)\n        if tol:\n            _tol = tol\n        else:\n            _tol = 1e-07 * self.norm_p(np.abs(self.D), 2)\n        while err > _tol and iter < max_iter:\n            Lk = self.svd_threshold(self.D - Sk + self.mu_inv * Yk, self.mu_inv)\n            Sk = self.shrink(self.D - Lk + self.mu_inv * Yk, self.mu_inv * self.lmbda)\n            Yk = Yk + self.mu * (self.D - Lk - Sk)\n            err = self.norm_p(np.abs(self.D - Lk - Sk), 2)\n            iter += 1\n            if iter % iter_print == 0 or iter == 1 or iter > max_iter or (err <= _tol):\n                print('iteration: {0}, error: {1}'.format(iter, err))\n        self.L = Lk\n        self.S = Sk\n        return (Lk, Sk)\n\n    def plot_fit(self, size=None, tol=0.1, axis_on=True):\n        n, d = self.D.shape\n        if size:\n            nrows, ncols = size\n        else:\n            num_plots = np.ceil(np.sqrt(n))\n            nrows = int(num_plots)\n            ncols = int(num_plots)\n        ymin = np.nanmin(self.D)\n        ymax = np.nanmax(self.D)\n        print('ymin: {0}, ymax: {1}'.format(ymin, ymax))\n        numplots = np.min([n, nrows * ncols])\n        plt.figure()\n        for n in range(numplots):\n            plt.subplot(nrows, ncols, n + 1)\n            plt.ylim((ymin - tol, ymax + tol))\n            plt.plot(self.L[n, :] + self.S[n, :], 'r')\n            plt.plot(self.L[n, :], 'b')\n            if not axis_on:\n                plt.axis('off')"}, {'identified': 'grad_rate_vs_language', 'updated_code': "fig, grad_rate_vs_language = plt.subplots()\ntick_locations = [value for value in x_axis]\nplt.xticks(tick_locations, county, rotation=90)\ngrad_rate = df_county_data['Graduation Rate']\ncounty = df_county_data['County Name']\npov_rate = df_county_data['Speak a language other than English']\nt = np.arange(len(county))\ngrad_rate_vs_language.plot(t, pov_rate, 'b-')\ngrad_rate_vs_language.set_xlabel('county')\ngrad_rate_vs_language.set_ylabel('Speak a language other than English', color='b')\ngrad_rate_vs_language.tick_params('y', colors='b')\nplt.title('High School Graduation Rates and ESL by County')\nax2 = grad_rate_vs_language.twinx()\nax2.plot(t, grad_rate, 'r*')\nax2.set_ylabel('Graduation Rate', color='r')\nax2.tick_params('y', colors='r')\nzoom = 5\nw, h = fig.get_size_inches()\nfig.set_size_inches(w * zoom, h * zoom / 2)\nfig.tight_layout()\nplt.savefig('Images/County_Grad_Speak a language other than English2.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'cluster_means', 'updated_code': 'from sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\nx = 3\nclusterer = GaussianMixture(n_components=x)\nclusterer.fit(reduced_data)\npreds = clusterer.predict(reduced_data)\ncluster_means = clusterer.means_\nsample_preds = clusterer.predict(pca_samples)\nscore = silhouette_score(reduced_data, preds)\nprint(x, score)'}, {'identified': 'image_array', 'updated_code': "image_array[...] = 0\nX = (x - 400) / 30\nY = -(y - 300) / 30\nheart = X ** 2 + (Y - 2 * (X ** 2 + np.abs(X) - 6) / (3 * (X ** 2 + np.abs(X) + 2))) ** 2 < 36\nimage_array[heart] = (1, 0, 0)\nplt.imshow(image_array, interpolation='bilinear')"}, {'identified': 'image_data', 'updated_code': "image_data[...] = 0\nX = (x - 400) / 30\nY = -(y - 300) / 30\nR = np.sqrt(X ** 2 + Y ** 2)\nt = np.arctan2(Y, X)\nimage_data[R < 5] = (1, 0, 0)\nimage_data[(t > 0) & (t < 3.14 / 4)] = (0, 0, 1)\nplt.imshow(image_data, interpolation='bilinear')"}, {'identified': 'valid_traffic_sign_labels', 'updated_code': 'from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\ndef nparray_to_list(nparray):\n    return [x for x in nparray]\n\ndef cv_split(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05)\n    X_np = np.asarray(X)\n    y_np = np.asarray(y)\n    for train_index, valid_index in sss.split(X_np, y_np):\n        X_train, X_valid = (nparray_to_list(X_np[train_index]), nparray_to_list(X_np[valid_index]))\n        y_train, valid_traffic_sign_labels = (nparray_to_list(y_np[train_index]), nparray_to_list(y_np[valid_index]))\n    train = (X_train, y_train)\n    valid = (X_valid, valid_traffic_sign_labels)\n    return (train, valid)'}, {'identified': 'customer_segment_proportions', 'updated_code': "samples_w_total = samples.copy()\nsamples_w_total['Total'] = samples_w_total.sum(axis=1)\ncustomer_segment_proportions = samples.loc[:, 'Fresh':'Delicatessen'].div(samples_w_total['Total'], axis=0) * 100\ncustomer_segment_proportions['Total'] = customer_segment_proportions.sum(axis=1)\ncustomer_segment_proportions"}, {'identified': 'rover_height_offset', 'updated_code': "def perspect_transform(img, src, dst):\n    M = cv2.getPerspectiveTransform(src, dst)\n    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))\n    mask = cv2.warpPerspective(np.ones_like(img[:, :, 0]), M, (img.shape[1], img.shape[0]))\n    return (warped, mask)\ndst_size = 5\nrover_height_offset = 6\nsource = np.float32([[14, 140], [301, 140], [200, 96], [118, 96]])\ndestination = np.float32([[image.shape[1] / 2 - dst_size, image.shape[0] - rover_height_offset], [image.shape[1] / 2 + dst_size, image.shape[0] - rover_height_offset], [image.shape[1] / 2 + dst_size, image.shape[0] - 2 * dst_size - rover_height_offset], [image.shape[1] / 2 - dst_size, image.shape[0] - 2 * dst_size - rover_height_offset]])\nwarped, mask = perspect_transform(grid_img, source, destination)\nfig = plt.figure(figsize=(12, 3))\nplt.subplot(121)\nplt.imshow(warped)\nplt.subplot(122)\nplt.imshow(mask, cmap='gray')"}, {'identified': 'high_tuition_cost_regions', 'updated_code': "df0_10k = clean_info.loc[clean_info['tuition_cost_tier'] == 'less_10k']\ndf0_10k_edit0 = df0_10k[df0_10k.region != 0]\ndf0_10k_edit9 = df0_10k_edit0[df0_10k_edit0.region != 9]\ndf10_18k = clean_info.loc[clean_info['tuition_cost_tier'] == '10k_18k']\ndf10_18k_edit = df10_18k[df10_18k.region != 9]\ndf18_32 = clean_info.loc[clean_info['tuition_cost_tier'] == '18k_32k']\nhigh_tuition_cost_regions = clean_info.loc[clean_info['tuition_cost_tier'] == 'greater_32k']"}, {'identified': 'final_tensor_name', 'updated_code': "DEBUG = False\nJPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\nimage_dir = '../data/images'\ntest_images_dir = '../data/test_images'\nstored_images_resized = '../data/images_resized'\nstored_bottlenecks = '../data/bottlenecks'\ntmp_dir = '/tmp'\nbottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\nimages_resized_dir = os.path.join(tmp_dir, 'images_resized')\nsummaries_dir = os.path.join(tmp_dir, 'retrain_logs')\nmodel_dir = os.path.join(tmp_dir, 'inception')\ninception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\noutput_graph_orig = 'output_graph_orig.pb'\noutput_graph = 'output_graph.pb'\noutput_labels = 'output_labels.txt'\narchitecture = 'inception_v3'\nfinal_tensor_name = 'final_result'\nhow_many_training_steps = 500\nlearning_rate = 0.01\ntesting_percentage = 10\nvalidation_percentage = 10\neval_step_interval = 10\ntrain_batch_size = 100\ntest_batch_size = -1\nvalidation_batch_size = 100\nprint_misclassified_test_images = False\nflip_left_right = False\nrandom_crop = 0\nrandom_scale = 0\nrandom_brightness = 0\nforce_inception_download = False\nFLAGS = type('FlagsObject', (object,), {'architecture': architecture, 'model_dir': model_dir, 'intermediate_store_frequency': 0, 'summaries_dir': summaries_dir, 'learning_rate': learning_rate, 'image_dir': images_resized_dir, 'testing_percentage': testing_percentage, 'validation_percentage': validation_percentage, 'random_scale': random_scale, 'random_crop': random_crop, 'flip_left_right': flip_left_right, 'random_brightness': random_brightness, 'bottleneck_dir': bottleneck_dir, 'final_tensor_name': final_tensor_name, 'how_many_training_steps': how_many_training_steps, 'train_batch_size': train_batch_size, 'test_batch_size': test_batch_size, 'eval_step_interval': eval_step_interval, 'validation_batch_size': validation_batch_size, 'print_misclassified_test_images': print_misclassified_test_images, 'output_graph': output_graph, 'output_labels': output_labels})\nretrain.FLAGS = FLAGS"}, {'identified': 'logits', 'updated_code': "with tf.variable_scope('train'):\n    if is_time_major:\n        logits = tf.transpose(logits, [1, 0, 2])\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dataset.tgt_out_ids, logits=logits)\n        target_weights = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n    else:\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dataset.tgt_out_ids, logits=logits)\n        target_weights = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(batch_size)\n    tf.summary.scalar('loss', loss)\n    learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n    max_global_norm = tf.placeholder(dtype=tf.float32, name='max_global_norm')\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.5)\n    params = tf.trainable_variables()\n    gradients = tf.gradients(loss, params)\n    for grad, var in zip(gradients, params):\n        tf.summary.histogram(var.op.name + '/gradient', grad)\n    gradients, _ = tf.clip_by_global_norm(gradients, max_global_norm)\n    for grad, var in zip(gradients, params):\n        tf.summary.histogram(var.op.name + '/clipped_gradient', grad)\n    update = optimizer.apply_gradients(zip(gradients, params))"}, {'identified': 'recall_score_per_class', 'updated_code': 'def recall(labels, predictions, weights=None):\n    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=3)\n    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n    tp = tf.diag_part(conf_matrix)\n    recall_score_per_class = tp / tp_and_fn\n    if weights:\n        recall_score = tf.multiply(recall_score_per_class, weights) / tf.reduce_sum(weights)\n    else:\n        recall_score = tf.reduce_mean(recall_score_per_class)\n    return recall_score'}, {'identified': 'figure', 'updated_code': "X_train, y_train = shuffle(X_train_augmented, y_train_augmented)\n\ndef get_random_img(X_data, y_label):\n    index = random.randint(0, len(X_data))\n    image = X_data[index].squeeze()\n    return (image, y_label[index], index)\n\ndef plot_imgs(X_data, y_label):\n    figure, axarr = plt.subplots(3, 3, figsize=(16, 16))\n    rand_indices = []\n    for i in range(9):\n        image, label, index = get_random_img(X_data, y_label)\n        rand_indices.append(index)\n        label_str = str(label)\n        axarr[i // 3, i % 3].imshow(image, cmap='gray')\n        axarr[i // 3, i % 3].set_title(label_str + ': ' + sign_dict[label_str])\n        plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n        plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n    return rand_indices\nrand_img_indices = plot_imgs(X_train, y_train)\nprint(rand_img_indices)"}, {'identified': 'optimizer', 'updated_code': 'optimizer = tf.train.GradientDescentOptimizer(0.5)\nmodel = two_layer_nn(output_size=2)\nnum_epochs = 5\nmodel.fit(X, y, optimizer, num_epochs=num_epochs)'}, {'identified': 'feature_layers', 'updated_code': 'feature_layer_coll_item = feature_layer_srch_results[0]\nfeature_layers = feature_layer_coll_item.layers\nfeature_layer = feature_layers[0]\nfeature_layer.properties.name'}, {'identified': 'grayscale_image', 'updated_code': 'def process_image(image):\n    grayscale_image = grayscale(image)\n    kernel_size = 5\n    blur_gray = gaussian_blur(grayscale_image, kernel_size)\n    low_threshold = 50\n    high_threshold = 150\n    edges = canny(blur_gray, low_threshold, high_threshold)\n    imshape = image.shape\n    xPct = 0.05\n    yPct = 0.6\n    xbl = imshape[1] * xPct\n    xbr = imshape[1] * (1 - xPct)\n    xtl = imshape[1] * (0.5 - xPct)\n    xtr = imshape[1] * (0.5 + xPct)\n    yb = imshape[0]\n    yt = imshape[0] * yPct\n    vertices = np.array([[(xbl, yb), (xtl, yt), (xtr, yt), (xbr, yb)]], dtype=np.int32)\n    masked_image = region_of_interest(edges, vertices)\n    rho = 2\n    theta = np.pi / 180\n    threshold = 15\n    min_line_len = 20\n    max_line_gap = 30\n    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\n    overlayedImg = weighted_img(line_img, image, 0.8, 1, 0)\n    return overlayedImg'}, {'identified': 'image_size', 'updated_code': "image_size = 28\nnum_labels = 10\nnum_channels = 1\n\ndef reformat(dataset, labels):\n    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n    return (dataset, labels)\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint('Training set', train_dataset.shape, train_labels.shape)\nprint('Validation set', valid_dataset.shape, valid_labels.shape)\nprint('Test set', test_dataset.shape, test_labels.shape)"}, {'identified': 'features_file', 'updated_code': "if 'p' in globals().keys():\n    for key, value in p.items():\n        globals()[key] = value\nelse:\n    m = 64\n    ls = 1\n    ld = 10\n    le = None\n    lg = 1\n    rtol = 1e-05\n    N_inner = 500\n    N_outer = 50\n    Ngenres, Nclips, Nframes = (10, 100, 644)\n    noise_std = 0\n    folder = 'data'\n    filename_audio = 'audio.hdf5'\n    filename_graph = 'graph.hdf5'\n    features_file = 'features.hdf5'"}, {'identified': 'random_image', 'updated_code': "path = '../test_dataset/IMG/*'\nimg_list = glob.glob(path)\nidx = np.random.randint(0, len(img_list) - 1)\nrandom_image = mpimg.imread(img_list[idx])\nplt.imshow(random_image)"}, {'identified': 'selected_row_index', 'updated_code': 'selected_row_index = 0\nX[selected_row_index, :]'}, {'identified': 'county_names', 'updated_code': "fig, ax1 = plt.subplots()\ntick_locations = [value for value in x_axis]\nplt.xticks(tick_locations, county_names, rotation=90)\ngrad_rate = df_county_data['Graduation Rate']\ncounty_names = df_county_data['County Name']\npov_rate = df_county_data['Unemployment Rate']\nt = np.arange(len(county_names))\nax1.plot(t, pov_rate, 'b-')\nax1.set_xlabel('county')\nax1.set_ylabel('Unemployment Rate', color='b')\nax1.tick_params('y', colors='b')\nax2 = ax1.twinx()\nplt.title('High School Graduation Rates and Unemployment Rate by County')\nax2.plot(t, grad_rate, 'r*')\nax2.set_ylabel('Graduation Rate', color='r')\nax2.tick_params('y', colors='r')\nzoom = 5\nw, h = fig.get_size_inches()\nfig.set_size_inches(w * zoom, h * zoom / 2)\nfig.tight_layout()\nplt.savefig('Images/County_Grad_Unemployment Rate1.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'dataset_filepath', 'updated_code': "from pandas import DataFrame\nfrom pandas import Series\nfrom pandas import concat\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nfrom matplotlib import pyplot\nimport numpy\ndataset_filepath = '/Users/shengyuchen/Dropbox/Engagement - Business/My Hub/AI:ML:DL Playground/Local Python/AI-ML-DL Algorithms/LSTM Neural Networks/shampoo-sales.csv'\n\ndef parser(x):\n    return datetime.strptime('190' + x, '%Y-%b')\n\ndef timeseries_to_supervised(data, lag=1):\n    df = DataFrame(data)\n    columns = [df.shift(i) for i in range(1, lag + 1)]\n    columns.append(df)\n    df = concat(columns, axis=1)\n    df.fillna(0, inplace=True)\n    return df\n\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)"}, {'identified': 'inception_1x1_weights', 'updated_code': "from tensorflow.python.framework import ops\nops.reset_default_graph()\nbatch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden_full_1 = 96\nnum_hidden_full_2 = 96\ngraph = tf.Graph()\nwith graph.as_default():\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    layer3_weights = init_weights([image_size * image_size * 64, num_hidden_full_1])\n    layer3_biases = init_weights([num_hidden_full_1], method='ones')\n    keep3 = tf.placeholder('float')\n    layer4_weights = init_weights([num_hidden_full_1, num_hidden_full_2])\n    layer4_biases = init_weights([num_hidden_full_2], method='ones')\n    keep4 = tf.placeholder('float')\n    layer5_weights = init_weights([num_hidden_full_2, num_labels])\n    layer5_biases = init_weights([num_labels], method='ones')\n    inception_1x1_weights = tf.Variable(tf.truncated_normal([1, 1, num_channels, depth], stddev=0.1))\n    inception_1x1_biases = tf.Variable(tf.zeros([depth]))\n    inception_1x1_pool_weights = tf.Variable(tf.truncated_normal([1, 1, num_channels, depth], stddev=0.1))\n    inception_1x1_pool_biases = tf.Variable(tf.zeros([depth]))\n    inception_3x3_weights = tf.Variable(tf.truncated_normal([3, 3, depth, depth], stddev=0.1))\n    inception_3x3_biases = tf.Variable(tf.zeros([depth]))\n    inception_5x5_weights = tf.Variable(tf.truncated_normal([5, 5, depth, depth], stddev=0.1))\n    inception_5x5_biases = tf.Variable(tf.zeros([depth]))\n\n    def inception_layer(data):\n        conv_1x1 = tf.nn.conv2d(data, inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n        conv_1x1 = tf.nn.relu(conv_1x1 + inception_1x1_biases)\n        print('1x1', conv_1x1.get_shape())\n        conv_pre = tf.nn.conv2d(data, inception_1x1_weights, [1, 1, 1, 1], padding='SAME')\n        conv_pre = tf.nn.relu(conv_pre + inception_1x1_biases)\n        conv_pool = tf.nn.avg_pool(data, [1, 3, 3, 1], [1, 1, 1, 1], padding='SAME')\n        conv_pool = tf.nn.conv2d(conv_pool, inception_1x1_pool_weights, [1, 1, 1, 1], padding='SAME')\n        conv_pool = tf.nn.relu(conv_pool + inception_1x1_pool_biases)\n        print('pool', conv_pool.get_shape())\n        conv_3x3 = tf.nn.conv2d(conv_pre, inception_3x3_weights, [1, 1, 1, 1], padding='SAME')\n        conv_3x3 = tf.nn.relu(conv_3x3 + inception_3x3_biases)\n        print('3x3', conv_3x3.get_shape())\n        conv_5x5 = tf.nn.conv2d(conv_pre, inception_5x5_weights, [1, 1, 1, 1], padding='SAME')\n        conv_5x5 = tf.nn.relu(conv_5x5 + inception_5x5_biases)\n        print('5x5', conv_5x5.get_shape())\n        inception_result = tf.concat(3, [conv_1x1, conv_3x3, conv_5x5, conv_pool])\n        print(inception_result.get_shape())\n        return inception_result\n\n    def model(data):\n        hidden = inception_layer(data)\n        shape = hidden.get_shape().as_list()\n        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n        hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n        hidden = tf.nn.dropout(hidden, keep3)\n        hidden = tf.nn.elu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n        hidden = tf.nn.dropout(hidden, keep4)\n        output = tf.matmul(hidden, layer5_weights) + layer5_biases\n        return output\n    logits = model(tf_train_dataset)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    test_prediction = tf.nn.softmax(model(tf_test_dataset))"}, {'identified': 's3_csv_data', 'updated_code': "def engineer_date_range(dates):\n    s3_csv_data = read_s3_csv(dates)\n    print('Loaded CSV data set from S3')\n    cleaned_df = clean_data(s3_csv_data, inplace=True)\n    print('Cleaned CSV data set')\n    xgb_data = create_xgb_features(cleaned_df, 5, inplace=True)\n    xgb_data['NextMaxPrice'] = create_xgb_target(xgb_data)\n    print('Engineered CSV data set')\n    train_data, validate_data = train_test_split(xgb_data, train_size=0.8, test_size=0.2, shuffle=True)\n    cols = list(train_data.columns.values)\n    cols.remove('NextMaxPrice')\n    cols = ['NextMaxPrice'] + cols\n    train_data = pd.get_dummies(train_data[cols])\n    validate_data = pd.get_dummies(validate_data[cols])\n    print('Data split for training purposes')\n    return (train_data, validate_data)"}, {'identified': 'data_flow_graph', 'updated_code': "batch_size = 128\nembedding_size = 128\nskip_window = 1\ndata_flow_graph = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"}, {'identified': 'precision_recall_curve', 'updated_code': "y_scores = -probas_gmm\nprecision_recall_curve, recall_gmm, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_gmm = metrics.auc(recall_gmm, precision_recall_curve)\nfpr_gmm, tpr_gmm, _ = metrics.roc_curve(y_true, y_scores)\nauroc_gmm = metrics.roc_auc_score(y_true, y_scores)\nprint('AUROC: %.2f, PR AUC: %.2f' % (auroc_gmm, pr_auc_gmm))"}, {'identified': "'error_detection_metric' or 'novelty_detection_metric'", 'updated_code': 'y_scores = (-get_acc_net_msr(y_pred_te)).flatten()\ny_true = pred_f_te.flatten()\nprecision_msr, recall_msr, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_msr = metrics.average_precision_score(y_true, y_scores)\nauroc_msr = metrics.roc_auc_score(y_true, y_scores)\nfpr_msr, tpr_msr, _ = metrics.roc_curve(y_true, y_scores)\ny_scores = (-get_acc_net_max_margin(y_pred_te)).flatten()\nprecision_margin, recall_margin, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_margin = metrics.average_precision_score(y_true, y_scores)\nauroc_margin = metrics.roc_auc_score(y_true, y_scores)\nfpr_margin, tpr_margin, _ = metrics.roc_curve(y_true, y_scores)\ny_scores = (-get_acc_net_entropy(y_pred_te)).flatten()\nprecision_entropy, recall_entropy, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_entropy = metrics.average_precision_score(y_true, y_scores)\nerror_detection_metric = metrics.roc_auc_score(y_true, y_scores)\nfpr_entropy, tpr_entropy, _ = metrics.roc_curve(y_true, y_scores)'}, {'identified': 'image_data', 'updated_code': "X, Annotations = marconi['Soccer']\ncamera = 3\nframe = 79\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(111)\nimage_data = X[camera, frame]\nax.imshow(image_data)\nAnnotations_for_cam = Annotations[camera]\nAnnot_on_frame_cam = Annotations_for_cam[frame]\nCOLORS = ['red', 'yellow']\nfor i, ((tl, br), joints) in enumerate(Annot_on_frame_cam):\n    head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n    ax.plot(head_x, head_y, color=COLORS[i])\n    for jx, jy, visible in joints:\n        plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\nplt.axis('off')\nplt.show()"}, {'identified': 'projected_data', 'updated_code': "cov = X.T @ X / (X.shape[0] - 1)\nval, vec = np.linalg.eigh(cov)\nidx = np.argsort(val)[::-1]\nval = val[idx]\nvec = vec[:, idx]\nproject_X = X @ vec\nproject_V = vec.T @ vec\ninversed_vec = np.linalg.inv(vec)\ninversed_vec = inversed_vec - inversed_vec.mean(1)\nprojected_data = project_X @ inversed_vec\nrevertedV = project_V @ inversed_vec.T\n\ndef plot():\n    plt.figure(figsize=(15, 5))\n    plt.subplot(131)\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\n    pca11 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca12 = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n    plt.subplot(132)\n    plt.scatter(project_X[y == 0, 0], project_X[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(project_X[y == 1, 0], project_X[y == 1, 1], color='blue', alpha=0.5)\n    pca21 = plt.arrow(0, 0, *project_V[:, 0] * val_reduced[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca22 = plt.arrow(0, 0, *project_V[:, 1] * val_reduced[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n    plt.subplot(133)\n    plt.scatter(projected_data[y == 0, 0], projected_data[y == 0, 1], color='red', alpha=0.5)\n    plt.scatter(projected_data[y == 1, 0], projected_data[y == 1, 1], color='blue', alpha=0.5)\n    pca21 = plt.arrow(0, 0, *revertedV[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\n    pca22 = plt.arrow(0, 0, *revertedV[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\n    plt.grid(True)\n    plt.show()\nplot()"}, {'identified': 'validation_batches', 'updated_code': "num_steps = 7001\nsummary_frequency = 100\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    mean_loss = 0\n    for step in range(num_steps):\n        batches = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = batches[i]\n        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss = mean_loss / summary_frequency\n            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(batches)[1:])\n            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                print('=' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print('=' * 80)\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                validation_batches = valid_batches.next()  # Renamed variable here\n                predictions = sample_prediction.eval({sample_input: validation_batches[0]})  # Renamed variable here\n                valid_logprob = valid_logprob + logprob(predictions, validation_batches[1])  # Renamed variable here\n            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"}, {'identified': 'predicted_class_index', 'updated_code': "softmax_logits = tf.nn.softmax(logits)\ntop_k = tf.nn.top_k(softmax_logits, k=3)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.import_meta_graph('./lenet.meta')\n    saver.restore(sess, './lenet')\n    batch_x = np.expand_dims(my_images_normalized, axis=3)\n    sample_softmax_logits = sess.run(softmax_logits, feed_dict={x: batch_x, keep_prob: 1.0})\n    my_top_k = sess.run(top_k, feed_dict={x: batch_x, keep_prob: 1.0})\n    fig, axs = plt.subplots(len(my_images), 1, figsize=(20, 25))\n    fig.subplots_adjust(hspace=1.0, wspace=0.6)\n    axs = axs.ravel()\n    for i, image in enumerate(my_images):\n        axs[i].axis('off')\n        axs[i].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n        guess1 = my_top_k[1][i][0]\n        index1 = np.argwhere(y_valid == guess1)[0]\n        guess2 = my_top_k[1][i][1]\n        index2 = np.argwhere(y_valid == guess2)[0]\n        guess3 = my_top_k[1][i][2]\n        predicted_class_index = np.argwhere(y_valid == guess3)[0]\n        title = ''\n        title += 'guess 1: class ' + sign_dict[str(guess1)] + ', probability: ' + str(100 * my_top_k[0][i][0]) + '\\n'\n        title += 'guess 2: class ' + sign_dict[str(guess2)] + ', probability: ' + str(100 * my_top_k[0][i][1]) + '\\n'\n        title += 'guess 3: class ' + sign_dict[str(guess3)] + ', probability: ' + str(100 * my_top_k[0][i][2])\n        axs[i].set_title(title)"}, {'identified': 'variable_definition', 'updated_code': 'def hypothesis_inlinecounter(text):\n    hyp = np.concatenate([np.linspace(1, -1, len(x) + 1) for x in text.split(\'\\n\')])[:-1]\n    return hyp\n\ndef hypothesis_inside_one(text, single):\n    hyp = re.sub(\'\\\\{}.*?\\\\{}\'.format(single, single), lambda m: single + \'#\' * (len(m.group()) - 2) + single, text)\n    return np.array([1 if x == \'#\' else -1 for x in hyp])\n\ndef hypothesis_inside_two(text, left, right):\n    hyp = np.full(len(text), -1)\n    inside = False\n    for i in range(len(text) - 1):\n        if text[i] == left:\n            inside = True\n        elif text[i] == right:\n            inside = False\n        if inside:\n            hyp[i + 1] = 1\n    return hyp\nhypothesis_inside_quotation = lambda x: hypothesis_inside_one(x, \'"\')\nhypothesis_inside_parantheses = lambda x: hypothesis_inside_two(x, \'(\', \')\')\n\ndef hypothesis_comments(text):\n    hyp = np.full(len(text), -1)\n    in_brac_comment = False\n    variable_definition = False\n    for i in range(len(text)):\n        if text[i:i + 2] == \'//\':\n            variable_definition = True\n        elif text[i] == \'\\n\':\n            variable_definition = False\n        elif text[i:i + 2] == \'/*\':\n            in_brac_comment = True\n        elif text[i:i + 2] == \'*/\':\n            in_brac_comment = False\n        if in_brac_comment:\n            hyp[i:i + 3] = 1\n        if variable_definition:\n            hyp[i:i + 1] = 1\n    return hyp\n\ndef hypothesis_indentation(text, level):\n    hyp = np.full(len(text), -1)\n    cur_level = 0\n    for i, char in enumerate(text):\n        if char == \'\\n\':\n            cur_level = 0\n        elif char == \'\\t\':\n            cur_level += 1\n        if cur_level >= level:\n            hyp[i] = 1\n    return hyp'}, {'identified': 'roc_auc_error_detection', 'updated_code': 'y_scores = -probas_svm\nprecision_svm, recall_svm, _ = metrics.precision_recall_curve(y_true, y_scores)\npr_auc_svm = metrics.auc(recall_svm, precision_svm)\nfpr_svm, tpr_svm, _ = metrics.roc_curve(y_true, y_scores)\nroc_auc_error_detection = metrics.roc_auc_score(y_true, y_scores)'}, {'identified': 'one_hot_encoder', 'updated_code': 'def sframe_to_scipy(x, column_name):\n    """\n    Convert a dictionary column of an SFrame into a sparse matrix format where\n    each (row_id, column_id, value) triple corresponds to the value of\n    x[row_id][column_id], where column_id is a key in the dictionary.\n       \n    Example\n    >>> sparse_matrix, map_key_to_index = sframe_to_scipy(sframe, column_name)\n    """\n    assert x[column_name].dtype() == dict, \'The chosen column must be dict type, representing sparse data.\'\n    x = x.add_row_number()\n    x = x.stack(column_name, [\'feature\', \'value\'])\n    one_hot_encoder = graphlab.feature_engineering.OneHotEncoder(features=[\'feature\'])\n    one_hot_encoder.fit(x)\n    x = one_hot_encoder.transform(x)\n    mapping = one_hot_encoder[\'feature_encoding\']\n    x[\'feature_id\'] = x[\'encoded_features\'].dict_keys().apply(lambda x: x[0])\n    i = np.array(x[\'id\'])\n    j = np.array(x[\'feature_id\'])\n    v = np.array(x[\'value\'])\n    width = x[\'id\'].max() + 1\n    height = x[\'feature_id\'].max() + 1\n    mat = csr_matrix((v, (i, j)), shape=(width, height))\n    return (mat, mapping)'}, {'identified': 'softmax_logits', 'updated_code': 'num_nodes = 64\ngraph = tf.Graph()\nwith graph.as_default():\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, num_nodes]))\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    def lstm_cell(i, o, state):\n        """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n        Note that in this formulation, we omit the various connections between the\n        previous state and the gates."""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return (output_gate * tf.tanh(state), state)\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n    softmax_logits = tf.nn.softmax(logits)\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))'}, {'identified': 'optimizer_application', 'updated_code': "with tf.variable_scope('train'):\n    if is_time_major:\n        logits = tf.transpose(logits, [1, 0, 2])\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dataset.tgt_out_ids, logits=logits)\n        target_weights = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n    else:\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dataset.tgt_out_ids, logits=logits)\n        target_weights = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(batch_size)\n    tf.summary.scalar('loss', loss)\n    learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n    max_global_norm = tf.placeholder(dtype=tf.float32, name='max_global_norm')\n    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.5)\n    params = tf.trainable_variables()\n    gradients = tf.gradients(loss, params)\n    for grad, var in zip(gradients, params):\n        tf.summary.histogram(var.op.name + '/gradient', grad)\n    gradients, _ = tf.clip_by_global_norm(gradients, max_global_norm)\n    for grad, var in zip(gradients, params):\n        tf.summary.histogram(var.op.name + '/clipped_gradient', grad)\n    optimizer_application = optimizer.apply_gradients(zip(gradients, params))"}, {'identified': 'pm_measurement_subset', 'updated_code': "for f in tqdm(feats):\n    rasterized_image = features.rasterize([(shape(f['geometry']), 1)], out_shape=out_shape, transform=new_aff, fill=0, all_touched=True)\n    pm_measurement_subset = data.where(rasterized_image == 1)\n    res = pm_measurement_subset.stack(allpoints=['x', 'y']).mean(dim='allpoints').to_dataframe(name=f['properties']['LSOA11CD'])\n    dfs.append(res)\nstats = pd.concat(dfs, axis=1)"}, {'identified': 'language_model_pca_component', 'updated_code': "ncomponents = 2\npca = decomp.PCA(n_components=ncomponents)\nraw_pca = pca.fit(np.array(x_train.iloc[:, first_egemaps_feature:]).astype(np.float))\nx_pca = raw_pca.transform(x_train.iloc[:, first_egemaps_feature:])\ngroups = x_train['pos']\nshort_pos = {'nouns': 'nn', 'verbs': 'vrb', 'other': 'oth', 'function_words': 'fct', 'adjectives': 'adj'}\ngroup_values = [short_pos[i] for i in groups.unique()]\nposIDs = groups.factorize()[0]\nlabels = np.array(x_train['word'])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.set_xlabel('PC 1')\nax.set_ylabel('PC 2')\ncolormap = plt.get_cmap('gnuplot', len(group_values))\npc1 = x_pca[:, 0]\nlanguage_model_pca_component = x_pca[:, 1]\np = ax.scatter(pc1, language_model_pca_component, c=posIDs, cmap=colormap, s=10)\ncb = plt.colorbar(p)\nloc = np.arange(0, max(posIDs), max(posIDs) / float(len(group_values)))\ncb.set_ticks(loc)\ncb.set_ticklabels(group_values)\nfor label, x, y in zip(labels, pc1, language_model_pca_component):\n    if y == np.amax(y) or x == np.amax(x):\n        plt.annotate(label, fontsize=2, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nif pos_filter == None:\n    axins = zoomed_inset_axes(ax, 2.5, loc=2)\n    x1, x2, y1, y2 = (-3.1, 2.5, 6.3, 9.8)\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n    axins.scatter(pc1, language_model_pca_component, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, language_model_pca_component):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\n    axins2 = zoomed_inset_axes(ax, 2.5, loc=4)\n    x1, x2, y1, y2 = (-4, 0.5, -8.5, -6.5)\n    axins2.set_xlim(x1, x2)\n    axins2.set_ylim(y1, y2)\n    axins2.scatter(pc1, language_model_pca_component, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins2, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, language_model_pca_component):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nfilename = corpus + '_full_zoom.pdf'\nif pos_filter != None and len(pos_filter) == 1:\n    filename = corpus + '_pca_' + pos_filter[0][1] + '_' + pos_filter[0][2] + '.pdf'\nplt.savefig(filename, bbox_inches='tight')"}, {'identified': 'earnings_growth_y6_y10_region', 'updated_code': "earningscost10k = [earningscost for earningscost in df0_10k_grouped['earnings_cost_ratio']]\nearnings_growth_y6_y10_region = [earnings for earnings in df0_10k_grouped['earnings_growth_y6_y10']]\nweighted_growth10k = [worthit for worthit in df0_10k_grouped['weighted_growth_to_tuition']]\nearningscost10k_18k = [earningscost for earningscost in df10_18k_grouped['earnings_cost_ratio']]\nearningsgrowth10k_18k = [earnings for earnings in df10_18k_grouped['earnings_growth_y6_y10']]\nweighted_growth10k_18k = [worthit for worthit in df10_18k_grouped['weighted_growth_to_tuition']]\nearningscost18k_32k = [earningscost for earningscost in df18_32_grouped['earnings_cost_ratio']]\nearningsgrowth18k_32k = [earnings for earnings in df18_32_grouped['earnings_growth_y6_y10']]\nweighted_growth18k_32k = [worthit for worthit in df18_32_grouped['weighted_growth_to_tuition']]\nearningscost32k = [earningscost for earningscost in df32_grouped['earnings_cost_ratio']]\nearningsgrowth32kk = [earnings for earnings in df32_grouped['earnings_growth_y6_y10']]\nweighted_growth32k = [worthit for worthit in df32_grouped['weighted_growth_to_tuition']]"}, {'identified': 'mean_learned_control_states', 'updated_code': 'mean_learned_control_states = ACCanalysis.Valid.mean()\nsms.DescrStatsW(ACCanalysis.Valid).tconfint_mean()'}, {'identified': 'boundary_histogram', 'updated_code': 'def estimate_anottation_correct(img, coords, line_width=2, threshold=0.8):\n    """Make histograms of boundaries to estimate annotation error\n    (if boundaries are blank, high chance of dumb walk failure)"""\n    total = 0\n    cont = 0\n    xleft, ytop, xright, ybottom = [int(c) for c in coords]\n    boundary_histogram = img[ytop:ybottom, xleft:xleft + line_width - 1]\n    rightside = img[ytop:ybottom, xright - line_width:xright]\n    topside = img[ytop:ytop + line_width - 1, xleft:xright]\n    bottomside = img[ybottom - line_width:ybottom, xleft:xright]\n    total = boundary_histogram.sum() + rightside.sum()\n    cont = boundary_histogram.size + rightside.size\n    \'for y in range(ytop, ybottom):\\n        for x in range(xleft, xright):\\n            total += img[y, xleft:xleft + line_width].sum()  # Left side\\n            total += img[y, xright - line_width:xright].sum()  # Right side\\n            total += img[ytop:ytop + line_width, x].sum()  # Top side\\n            total += img[ybottom - line_width:ybottom, x].sum()  # Bottom side\\n            cont +=1\\n    \'\n    percent_black = total / cont\n    return int(percent_black)'}, {'identified': 'tutorial_steps', 'updated_code': "tutorial_steps = []\ntutorial_steps.append('A')\ntutorial_steps.append('d')\ntutorial_steps.append('d')\nprint(tutorial_steps)"}, {'identified': "'X_test_time_series' or 'X_test_text_generation'", 'updated_code': 'train_test_split = int(np.ceil(2 * len(y) / float(3)))\nX_train = X[:train_test_split, :]\ny_train = y[:train_test_split]\nX_test_time_series = X[train_test_split:, :]\ny_test = y[train_test_split:]\nX_train = np.asarray(np.reshape(X_train, (X_train.shape[0], window_size, 1)))\nX_test_time_series = np.asarray(np.reshape(X_test_time_series, (X_test_time_series.shape[0], window_size, 1)))'}, {'identified': 'updated_weights_and_bias', 'updated_code': "def test_sgd(x_train, t_train, x_valid, t_valid, w, b):\n    Lts = []\n    Lvs = []\n    epsilon = 0.01\n    while (len(Lts) < 2 or abs(Lts[-1] - Lts[-2]) > epsilon) and len(Lts) < 25:\n        updated_weights_and_bias, w, b = sgd_iter(x_train, t_train, w, b)\n        logp = []\n        for x, t in zip(x_valid, t_valid):\n            _, _, logp_valid = logprob(x, w, b)\n            logp.append(logp_valid[t])\n        L_valid = np.array(logp).mean()\n        Lts.append(updated_weights_and_bias)\n        Lvs.append(L_valid)\n    x = np.arange(len(Lts))\n    plt.plot(x, Lts, label='L_train')\n    plt.plot(x, Lvs, label='L_validation')\n    plt.legend()\n    plt.show()\n    return (w, b)\nnp.random.seed(1243)\nw = np.zeros((28 * 28, 10))\nb = np.zeros(10)\nw, b = test_sgd(x_train, t_train, x_valid, t_valid, w, b)"}, {'identified': 'second_pc_arrow', 'updated_code': "plt.figure(figsize=(15, 5))\nplt.subplot(131)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *vec[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\nsecond_pc_arrow = plt.arrow(0, 0, *vec[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\nplt.subplot(132)\nplt.scatter(L[y == 0, 0], L[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(L[y == 1, 0], L[y == 1, 1], color='blue', alpha=0.5)\nplt.grid(True)\nplt.subplot(133)\nplt.scatter(S[y == 0, 0], S[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(S[y == 1, 0], S[y == 1, 1], color='blue', alpha=0.5)\nplt.grid(True)\nplt.show()"}, {'identified': 'principal_components', 'updated_code': "sparse_data = my_spca.transform(X)\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.5)\npca11 = plt.arrow(0, 0, *principal_components[:, 0] * val[0], head_width=0.05, head_length=0.05, color='Green', label='First PC')\npca12 = plt.arrow(0, 0, *principal_components[:, 1] * val[1], head_width=0.05, head_length=0.05, color='magenta', label='Second PC')\nplt.grid(True)\nnew_pc_cen = sparse_data - sparse_data.mean(0, keepdims=True)\ncov = new_pc_cen.T @ new_pc_cen / (new_pc_cen.shape[0] - 1)\nval, principal_components = np.linalg.eigh(cov)\nplt.subplot(122)\nplt.scatter(new_pc[y == 0, 0], new_pc[y == 0, 1], color='red', alpha=0.5)\nplt.scatter(new_pc[y == 1, 0], new_pc[y == 1, 1], color='blue', alpha=0.5)\npca21 = plt.arrow(0, 0, *principal_components[:, 0] * val[0], head_width=0.005, head_length=0.005, color='Green', label='First PC')\npca22 = plt.arrow(0, 0, *principal_components[:, 1] * val[1], head_width=0.005, head_length=0.005, color='magenta', label='Second PC')\nplt.grid(True)\nplt.show()"}, {'identified': 'input_chars_definition', 'updated_code': 'start_inds = [100, 200, 300, 400, 500, 600, 700]\nf = open(\'text_gen_output/RNN_large_textdata_output.txt\', \'w\')\nmodel.load_weights(\'model_weights/best_RNN_large_textdata_weights.hdf5\')\nfor s in start_inds:\n    start_index = s\n    input_chars = text[start_index:start_index + window_size]\n    predict_input = predict_next_chars(model, input_chars, num_to_predict=100)\n    line = \'-------------------\' + \'\\n\'\n    print(line)\n    f.write(line)\n    input_chars_definition = \'input chars = \' + \'\\n\' + input_chars + \'"\' + \'\\n\'\n    print(input_chars_definition)\n    f.write(input_chars_definition)\n    predict_line = \'predicted chars = \' + \'\\n\' + predict_input + \'"\' + \'\\n\'\n    print(predict_line)\n    f.write(predict_line)\nf.close()'}, {'identified': 'encoded_io_pairs', 'updated_code': 'window_size = 100\nstep_size = 5\nX, encoded_io_pairs = encode_io_pairs(text, window_size, step_size)'}, {'identified': 'scaled_train_data', 'updated_code': "def inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]\n\ndef scale(train, test):\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler = scaler.fit(train)\n    train = train.reshape(train.shape[0], train.shape[1])\n    scaled_train_data = scaler.transform(train)\n    test = test.reshape(test.shape[0], test.shape[1])\n    test_scaled = scaler.transform(test)\n    return (scaler, scaled_train_data, test_scaled)\n\ndef invert_scale(scaler, X, value):\n    new_row = [x for x in X] + [value]\n    array = numpy.array(new_row)\n    array = array.reshape(1, len(array))\n    inverted = scaler.inverse_transform(array)\n    return inverted[0, -1]\n\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n    X, y = (train[:, 0:-1], train[:, -1])\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n        model.reset_states()\n    return model"}, {'identified': 'customer_segment_data', 'updated_code': 'import eda\nreload(eda)\nlog_data2 = log_data.loc[[x for x in log_data.index if x not in d_unique_idx.keys()]]\ncustomer_segment_data = log_data.loc[[x[0] for x in d_unique_idx.iteritems() if x[1] > 1]]\nax = eda.features_boxplot(log_data2, customer_segment_data, customer_segment_data.index)'}, {'identified': 'training_data_variables', 'updated_code': 'training_data_variables = data_train.names\ncolors = data_train.colors'}, {'identified': 'region_of_interest_vertices', 'updated_code': 'def lane_detection_ppline(image, k_size=3, vertex_ratio_h=0.45, vertex_ratio_v=0.6, low_thresh=50, high_thresh=200, L2gradient=False, rho=2, theta=1 * np.pi / 180.0, min_votes=15, min_line_len=40, max_line_gap=20, angle=3 * np.pi / 16, angle_thresh=np.pi / 16, debug=False):\n    """\n    Takes an image and parameters and applies the lane detection pipeline.\n    Returns an image combining the original and the extended lines detected\n    by the algorithm.\n     - debug: Whether or not to display the images after each step of the process, for\n     debugging or tuning purposes.\n    """\n    max_y, max_x = image.shape[:2]\n    region_of_interest_vertices = np.array([[(0, max_y), (round(max_x * vertex_ratio_h), round(max_y * vertex_ratio_v)), (round(max_x * (1 - vertex_ratio_h)), round(max_y * vertex_ratio_v)), (max_x, max_y)]])\n    if debug:\n        plt.subplot(4, 2, 1)\n        plt.imshow(image)\n    gray = grayscale(image)\n    if debug:\n        plt.subplot(4, 2, 2)\n        plt.imshow(gray, cmap=\'gray\')\n    blur_gray = gaussian_blur(gray, k_size)\n    if debug:\n        plt.subplot(4, 2, 3)\n        plt.imshow(blur_gray, cmap=\'gray\')\n    edges = canny(blur_gray, low_thresh, high_thresh, L2gradient=L2gradient)\n    if debug:\n        plt.subplot(4, 2, 4)\n        plt.imshow(edges)\n    masked_edges = region_of_interest(edges, region_of_interest_vertices)\n    if debug:\n        plt.subplot(4, 2, 5)\n        plt.imshow(masked_edges)\n    line_img, lines = hough_lines(masked_edges, rho, theta, min_votes, min_line_len, max_line_gap)\n    if debug:\n        plt.subplot(4, 2, 6)\n        plt.imshow(line_img)\n    try:\n        combined = extend_lines(image, lines, angle=angle, angle_thresh=angle_thresh)\n        if debug:\n            plt.subplot(4, 2, 7)\n            plt.imshow(combined)\n    except IndexError:\n        print(\'Error. Try relaxing your angle parameters a litte.\')\n    return combined'}, {'identified': 'control_state_mean', 'updated_code': "control_state_mean = posttest.groupby(['subjID'])['Q2_SceneSkyPresence'].mean()\nSkyPresenceSEM = pd.Series.std(control_state_mean) / n\nColorScheme = posttest.groupby(['subjID'])['Q2_SceneColorScheme'].mean()\nColorSchemeSEM = pd.Series.std(ColorScheme) / n\nTreeFreq = posttest.groupby(['subjID'])['Q2_SceneTreeFrequency'].mean()\nTreeFreqSEM = pd.Series.std(TreeFreq) / n\nImageType = posttest.groupby(['subjID'])['Q2_ImageType'].mean()\nImageTypeSEM = pd.Series.std(ImageType) / n\nFeatureType = posttest.groupby(['subjID'])['Q2_FeatureType'].mean()\nFeatureTypeSEM = pd.Series.std(FeatureType) / n\nLightType = posttest.groupby(['subjID'])['Q2_LightType'].mean()\nLightTypeSEM = pd.Series.std(LightType) / n"}, {'identified': 'region_codes', 'updated_code': "region_codes = [x for x in range(1, 9)]\ny_axis1 = df0_10k_grouped['earnings_cost_ratio']\ny_axis2 = df10_18k_grouped['earnings_cost_ratio']\ny_axis3 = df18_32_grouped['earnings_cost_ratio']\ny_axis4 = df32_grouped['earnings_cost_ratio']\nlessThan10k = plt.plot(region_codes, y_axis1, 'go--', linewidth=2, markersize=8, color='blue', label='lessThan10k')\nTenKto18k = plt.plot(region_codes, y_axis2, 'go--', linewidth=2, markersize=8, color='green', label='10Kto18k')\nEighteenTo32k = plt.plot(region_codes, y_axis3, 'go--', linewidth=2, markersize=8, color='yellow', label='18kTo32k')\ngreaterThan32k = plt.plot(region_codes, y_axis4, 'go--', linewidth=2, markersize=8, color='red', label='>32k')\nplt.title('Tiered Earnings-Cost ratio per region ($)')\nplt.xlabel('Region')\nplt.ylabel('Tiered Earnings-cost Ratio')\nplt.grid()\nplt.legend(title=['Tiered Tuitions'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"}, {'identified': 'text_corpus', 'updated_code': "text_corpus = open('datasets/holmes.txt').read().lower()\nprint('our original text has ' + str(len(text_corpus)) + ' characters')"}, {'identified': 'validation_batch_size_value', 'updated_code': "DEBUG = False\nJPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\nimage_dir = '../data/images'\ntest_images_dir = '../data/test_images'\nstored_images_resized = '../data/images_resized'\nstored_bottlenecks = '../data/bottlenecks'\ntmp_dir = '/tmp'\nbottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\nimages_resized_dir = os.path.join(tmp_dir, 'images_resized')\nsummaries_dir = os.path.join(tmp_dir, 'retrain_logs')\nmodel_dir = os.path.join(tmp_dir, 'inception')\ninception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\noutput_graph_orig = 'output_graph_orig.pb'\noutput_graph = 'output_graph.pb'\noutput_labels = 'output_labels.txt'\narchitecture = 'inception_v3'\nfinal_tensor_name = 'final_result'\nhow_many_training_steps = 500\nlearning_rate = 0.01\ntesting_percentage = 10\nvalidation_percentage = 10\neval_step_interval = 10\ntrain_batch_size = 100\ntest_batch_size = -1\nvalidation_batch_size_value = 100\nprint_misclassified_test_images = False\nflip_left_right = False\nrandom_crop = 0\nrandom_scale = 0\nrandom_brightness = 0\nforce_inception_download = False\nFLAGS = type('FlagsObject', (object,), {'architecture': architecture, 'model_dir': model_dir, 'intermediate_store_frequency': 0, 'summaries_dir': summaries_dir, 'learning_rate': learning_rate, 'image_dir': images_resized_dir, 'testing_percentage': testing_percentage, 'validation_percentage': validation_percentage, 'random_scale': random_scale, 'random_crop': random_crop, 'flip_left_right': flip_left_right, 'random_brightness': random_brightness, 'bottleneck_dir': bottleneck_dir, 'final_tensor_name': final_tensor_name, 'how_many_training_steps': how_many_training_steps, 'train_batch_size': train_batch_size, 'test_batch_size': test_batch_size, 'eval_step_interval': eval_step_interval, 'validation_batch_size': validation_batch_size_value, 'print_misclassified_test_images': print_misclassified_test_images, 'output_graph': output_graph, 'output_labels': output_labels})\nretrain.FLAGS = FLAGS"}, {'identified': 'chlamydia_incidence_rate', 'updated_code': 'p_asymp = 0.51\nsc = 0.47\natt_symp = 14.4\np_true_pos = 0.971\nchlamydia_incidence_rate = 0.00314'}, {'identified': 'conv2_layer_output', 'updated_code': "height = 28\nwidth = 28\nchannels = 1\nn_inputs = height * width\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = 'SAME'\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 2\nconv2_pad = 'SAME'\npool3_fmaps = conv2_fmaps\nn_fc1 = 64\nn_outputs = 10\nreset_graph()\nwith tf.name_scope('inputs'):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n    y = tf.placeholder(tf.int32, shape=[None], name='y')\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name='conv1')\nconv2_layer_output = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name='conv2')\nwith tf.name_scope('pool3'):\n    pool3 = tf.nn.max_pool(conv2_layer_output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\nwith tf.name_scope('fc1'):\n    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name='fc1')\nwith tf.name_scope('output'):\n    logits = tf.layers.dense(fc1, n_outputs, name='output')\n    Y_proba = tf.nn.softmax(logits, name='Y_proba')\nwith tf.name_scope('train'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\nwith tf.name_scope('init_and_save'):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"}, {'identified': 'financial_summary_pivot_table', 'updated_code': "financial_summary_pivot_table = df.pivot_table(index=[df.index.year, df.index.quarter], aggfunc=(np.mean, np.sum, min, max)).rename_axis(['year', 'quarter'])\nfinancial_summary_pivot_table.style.applymap(color_negative_red).apply(highlight_max).apply(highlight_min)"}, {'identified': 'num_epochs', 'updated_code': "from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nBATCH_SIZE = 32\nnum_epochs = 8\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0005, verbose=1)\naugment_data = False\nif not augment_data:\n    print('Training... without data augmentation')\n    history = scratch_model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), epochs=num_epochs, batch_size=BATCH_SIZE, callbacks=[checkpointer], verbose=1)\nelse:\n    print('Training... WITH data augmentation')\n    history = scratch_model.fit_generator(datagen_train.flow(train_tensors, train_targets, batch_size=BATCH_SIZE), steps_per_epoch=train_tensors.shape[0] // BATCH_SIZE, epochs=num_epochs, verbose=2, callbacks=[checkpointer], validation_data=datagen_valid.flow(valid_tensors, valid_targets, batch_size=BATCH_SIZE), validation_steps=valid_tensors.shape[0] // BATCH_SIZE)\nprint('Done training')\nshow_history_graph(history)"}, {'identified': 'scatter_plot', 'updated_code': "ncomponents = 2\npca = decomp.PCA(n_components=ncomponents)\nraw_pca = pca.fit(np.array(x_train.iloc[:, first_egemaps_feature:]).astype(np.float))\nx_pca = raw_pca.transform(x_train.iloc[:, first_egemaps_feature:])\ngroups = x_train['pos']\nshort_pos = {'nouns': 'nn', 'verbs': 'vrb', 'other': 'oth', 'function_words': 'fct', 'adjectives': 'adj'}\ngroup_values = [short_pos[i] for i in groups.unique()]\nposIDs = groups.factorize()[0]\nlabels = np.array(x_train['word'])\nfig = plt.figure()\nscatter_plot = fig.add_subplot(111)\nscatter_plot.set_xlabel('PC 1')\nscatter_plot.set_ylabel('PC 2')\ncolormap = plt.get_cmap('gnuplot', len(group_values))\npc1 = x_pca[:, 0]\npc2 = x_pca[:, 1]\np = scatter_plot.scatter(pc1, pc2, c=posIDs, cmap=colormap, s=10)\ncb = plt.colorbar(p)\nloc = np.arange(0, max(posIDs), max(posIDs) / float(len(group_values)))\ncb.set_ticks(loc)\ncb.set_ticklabels(group_values)\nfor label, x, y in zip(labels, pc1, pc2):\n    if y == np.amax(y) or x == np.amax(x):\n        plt.annotate(label, fontsize=2, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nif pos_filter == None:\n    axins = zoomed_inset_axes(scatter_plot, 2.5, loc=2)\n    x1, x2, y1, y2 = (-3.1, 2.5, 6.3, 9.8)\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n    axins.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(scatter_plot, axins, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\n    axins2 = zoomed_inset_axes(scatter_plot, 2.5, loc=4)\n    x1, x2, y1, y2 = (-4, 0.5, -8.5, -6.5)\n    axins2.set_xlim(x1, x2)\n    axins2.set_ylim(y1, y2)\n    axins2.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(scatter_plot, axins2, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nfilename = corpus + '_full_zoom.pdf'\nif pos_filter != None and len(pos_filter) == 1:\n    filename = corpus + '_pca_' + pos_filter[0][1] + '_' + pos_filter[0][2] + '.pdf'\nplt.savefig(filename, bbox_inches='tight')"}, {'identified': 'sign_language_words', 'updated_code': "sign_language_words = ['FISH', 'BOOK', 'VEGETABLE', 'FUTURE', 'JOHN']\nimport timeit"}, {'identified': 'batch_data', 'updated_code': 'batch_size = 64\nnum_unrollings = 10\n\nclass BatchGenerator(object):\n\n    def __init__(self, text, batch_size, num_unrollings):\n        self._text = text\n        self._text_size = len(text)\n        self._batch_size = batch_size\n        self._num_unrollings = num_unrollings\n        segment = self._text_size // batch_size\n        self._cursor = [offset * segment for offset in range(batch_size)]\n        self._last_batch = self._next_batch()\n\n    def _next_batch(self):\n        """Generate a single batch from the current cursor position in the data."""\n        batch_data = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n        for b in range(self._batch_size):\n            batch_data[b, char2id(self._text[self._cursor[b]])] = 1.0\n            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n        return batch_data\n\n    def next(self):\n        """Generate the next array of batches from the data. The array consists of\n    the last batch of the previous array, followed by num_unrollings new ones.\n    """\n        batches = [self._last_batch]\n        for step in range(self._num_unrollings):\n            batches.append(self._next_batch())\n        self._last_batch = batches[-1]\n        return batches\n\ndef characters(probabilities):\n    """Turn a 1-hot encoding or a probability distribution over the possible\n  characters back into its (most likely) character representation."""\n    return [id2char(c) for c in np.argmax(probabilities, 1)]\n\ndef batches2string(batches):\n    """Convert a sequence of batches back into their (most likely) string\n  representation."""\n    s = [\'\'] * batches[0].shape[0]\n    for b in batches:\n        s = [\'\'.join(x) for x in zip(s, characters(b))]\n    return s\ntrain_batches = BatchGenerator(train_text, batch_size, num_unrollings)\nvalid_batches = BatchGenerator(valid_text, 1, 1)\nprint(batches2string(train_batches.next()))\nprint(batches2string(train_batches.next()))\nprint(batches2string(valid_batches.next()))\nprint(batches2string(valid_batches.next()))'}, {'identified': 'top_arrival_airports_with_names', 'updated_code': 'def get_df_cols(csvfilename, cols, separator):\n    """\n  Method to get a dataframe from a csv file with specified columns\n  \n  @csvfilename : the name of the file to convert in dataframe\n  @cols        : list of string giving columns name to keep\n  @separator   : character used to delimit fields in the csv file\n  \n  @return      : a dataframe\n  """\n    dataframe = pd.read_csv(BOOKINGS, error_bad_lines=False, encoding=\'UTF8\', sep=separator, usecols=cols)\n    return dataframe\n\ndef get_name(IATA_code):\n    """\n    Function to return the name of the airport linked to IATA_code\n    \n    @IATA_code : String object which is a IATA_code\n   \n    @return    : String object which is the name of the airport\n  """\n    try:\n        result = GEO_O.get(IATA_code.replace(\' \', \'\'), \'name\')\n    except KeyError as e:\n        result = \'NOT FOUND IATA CODE\'\n    return result\n\ndef get_airports_arrival_sorted(dataframe):\n    """\n  Method to print the get arrivals airports in 2013 from searches file\n  \n  @dataframe : the dataframe containing the data\n  \n  @return    : a new dataframe\n  """\n    top_arrival_airports_with_names = dataframe.groupby([\'arr_port\']).sum()\n    top_arrival_airports_with_names = top_arrival_airports_with_names.sort_values(by=[\'pax\'], ascending=False)\n    return top_arrival_airports_with_names\n\ndef add_airports_name(dataframe):\n    """\n  Method to add a column in a dataframe containing the full name of airports\n  thanks to the IATA CODE\n  \n  @dataframe : the dataframe to modify\n  \n  @return    : the dataframe modified\n  """\n    dataframe = dataframe.reset_index()\n    dataframe[\'airport_name\'] = dataframe[\'arr_port\'].apply(lambda x: get_name(x))\n    return dataframe\n\ndef print_top_n_arrival_airport(dataframe, n):\n    """\n  Method to print the top n of arrival airports in 2013\n  \n  @dataframe : the preformatted dataframe by columns containing the data\n  @n         : the number of airports to show\n  """\n    top_arrival_airports_with_names = get_airports_arrival_sorted(dataframe)\n    top_arrival_airports_with_names = add_airports_name(top_arrival_airports_with_names)\n    print(top_arrival_airports_with_names.head(n))'}, {'identified': 'execution_role_arn', 'updated_code': "import boto3\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\nfrom sagemaker import get_execution_role\n\nexecution_role_arn = get_execution_role()\ntraining_image = get_image_uri(boto3.Session().region_name, 'xgboost')\ns3_input_train = 's3://{}/{}/train'.format(YOUR_BUCKET_NAME, prefix)\ns3_input_validation = 's3://{}/{}/validate/'.format(YOUR_BUCKET_NAME, prefix)\ntraining_job_definition = {'AlgorithmSpecification': {'TrainingImage': training_image, 'TrainingInputMode': 'File'}, 'InputDataConfig': [{'ChannelName': 'train', 'CompressionType': 'None', 'ContentType': 'csv', 'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3Uri': s3_input_train}}}, {'ChannelName': 'validation', 'CompressionType': 'None', 'ContentType': 'csv', 'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3Uri': s3_input_validation}}}], 'OutputDataConfig': {'S3OutputPath': 's3://{}/{}/output'.format(YOUR_BUCKET_NAME, prefix)}, 'ResourceConfig': {'InstanceCount': 1, 'InstanceType': 'ml.c5.4xlarge', 'VolumeSizeInGB': 20}, 'RoleArn': execution_role_arn, 'StaticHyperParameters': {'eval_metric': 'rmse', 'objective': 'reg:linear', 'rate_drop': '0.3', 'tweedie_variance_power': '1.4'}, 'StoppingCondition': {'MaxRuntimeInSeconds': 43200}}"}, {'identified': 'keypoint_detection_params', 'updated_code': "keypoint_detection_params = {'boxsize': 368}\nkeypoint_detection_params['stride'] = 8\nparam = {}\nparam['scale_search'] = [0.5, 1, 1.5, 2]\nmultiplier = [x * keypoint_detection_params['boxsize'] * 1.0 / oriImg.shape[0] for x in param['scale_search']]"}, {'identified': 'scaled_act_val_all', 'updated_code': 'act_train_svm = preprocessing.scale(act_train)\nscaled_act_val_all = preprocessing.scale(act_val_all)\nact_test_svm = preprocessing.scale(act_test)'}, {'identified': 'grayscale_image', 'updated_code': 'def process_image(image):\n    """ Filter color """\n    color_select = np.copy(image)\n    rgb_threshold = [200, 150, 95]\n    thresholds = (image[:, :, 0] < rgb_threshold[0]) | (image[:, :, 1] < rgb_threshold[1]) | (image[:, :, 2] < rgb_threshold[2])\n    color_select[thresholds] = [0, 0, 0]\n    grayscale_image = grayscale(color_select)\n    blurred = gaussian_blur(grayscale_image, 3)\n    edges = canny(blurred, 50, 150)\n    xsize = image.shape[1]\n    ysize = image.shape[0]\n    vertices = np.array([[(0, ysize), (xsize / 2, ysize / 1.71), (xsize / 2, ysize / 1.71), (xsize, ysize)]], dtype=np.int32)\n    regioned = region_of_interest(edges, vertices)\n    hough = hough_lines(regioned, 1, np.pi / 180, 35, 35, 20)\n    result = weighted_img(hough, image)\n    return result'}, {'identified': 'monthly_pm25_measurements_df', 'updated_code': "monthly_pm25_measurements_df = pd.read_csv('D:\\\\Annies_Dissertation\\\\Analysis\\\\Regression\\\\Validation\\\\Monthly_PM25_LSOA_Validation.csv', parse_dates=['time'])"}, {'identified': 'friction_force', 'updated_code': 'def blockMotion(t, blockPositions, vBlock, i, blockNum, kp, kc, mass, F0, v0, vf):\n    """\n    Returns the differential equation that models the motion of the blocks\n    \n    Arguments:  t - time\n                blockPositions - the positions of the blocks\n                vBlock - the velocity of the block\n                i - the index of the current block\n                blockNum - the number of blocks\n                kp - spring constant of leaf springs\n                kc - spring constant of springs between blocks\n                mass - mass of individual block\n                F0 - the static friction force\n                v0 - initial velocity of top plate\n                vf - the friction coefficient\n                \n    Returned: The differential equation modeling the motion of the individual blocks\n    \n    Examples:\n    \n    >>> blockMotion (0, (0, 1, 2, 3, 4), 0, 2, 5, 0, 0, 1, 0, 1, 20)\n    array([ 0.,  0.])\n    \n    """\n    xi = blockPositions[i] - i\n    vi = vBlock\n    if i == 0:\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc * (xiP - xi) + kp * (v0 * t - xi)\n    elif i == blockNum - 1:\n        xiM = blockPositions[i - 1] - (i - 1)\n        springForce = kc * (xiM - xi) + kp * (v0 * t - xi)\n    else:\n        xiM = blockPositions[i - 1] - (i - 1)\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc * (xiP + xiM - 2 * xi) + kp * (v0 * t - xi)\n    friction_force = friction(vi, vf, F0)\n    if abs(springForce) <= abs(friction_force):\n        dv = -vi\n        vi = 0\n        dx = vi\n    else:\n        totalForce = (springForce + friction_force) / mass\n        dx = vi\n        dv = totalForce\n    return np.array([dx, dv], float)'}, {'identified': 'initialization_value', 'updated_code': 'w = np.zeros(3)\ninitialization_value = 0'}, {'identified': 'covariance_diagonal', 'updated_code': 'import math\nfrom matplotlib import cm, pyplot as plt, mlab\n\ndef visualize(word, model):\n    """ visualize the input model for a particular word """\n    covariance_diagonal = np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])\n    figures = []\n    for parm_idx in range(len(model.means_[0])):\n        xmin = int(min(model.means_[:, parm_idx]) - max(covariance_diagonal[:, parm_idx]))\n        xmax = int(max(model.means_[:, parm_idx]) + max(covariance_diagonal[:, parm_idx]))\n        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)\n        colours = cm.rainbow(np.linspace(0, 1, model.n_components))\n        for i, (ax, colour) in enumerate(zip(axs, colours)):\n            x = np.linspace(xmin, xmax, 100)\n            mu = model.means_[i, parm_idx]\n            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])\n            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)\n            ax.set_title(\'{} feature {} hidden state #{}\'.format(word, parm_idx, i))\n            ax.grid(True)\n        figures.append(plt)\n    for p in figures:\n        p.show()\nvisualize(my_testword, model)'}, {'identified': 'estimated_prevalence_stratification', 'updated_code': "plt.figure(figsize=(8, 6))\nfor j in xrange(len(n3_test)):\n    wav = 0\n    wav_pos = 0\n    plt.plot([0, 0.09], [2 * (j + 1), 2 * (j + 1)], '0.8')\n    for i in xrange(len(n3_test[j])):\n        cov = -log(1 - n3_test[j][i])\n        adpc = -log(1 - n3_diag[j][i])\n        [incsol, scrsol] = fsolve(lambda x: [test_diag_fun(x)[0] - cov, test_diag_fun(x)[1] - adpc], [0.09, 0.25])\n        estimated_prevalence_stratification = 1 - U_fun(incsol * p_asymp, sc + scrsol * p_true_pos, incsol * (1 - p_asymp), scrsol * p_true_pos + att_symp * p_true_pos)\n        plt.plot(estimated_prevalence_stratification, 2 * (j + 1), 'ob', markerfacecolor='None', markersize=20 * sqrt(n3_props[j][i]))\n        wav = wav + n3_props[j][i] * estimated_prevalence_stratification\n        wav_pos = wav_pos + n3_props[j][i] * adpc / cov\n    plt.plot(wav, 2 * (j + 1), 'ob')\n    plt.text(0.082, 2 * (j + 1), factors[j], verticalalignment='center')\ncov = -log(1 - 0.346)\nadpc = -log(1 - 0.02)\n[incsol, scrsol] = fsolve(lambda x: [test_diag_fun(x)[0] - cov, test_diag_fun(x)[1] - adpc], [0.09, 0.25])\nestimated_prevalence_stratification = 1 - U_fun(incsol * p_asymp, sc + scrsol * p_true_pos, incsol * (1 - p_asymp), scrsol * p_true_pos + att_symp * p_true_pos)\nplt.plot([estimated_prevalence_stratification, estimated_prevalence_stratification], [0, 100])\ncur_axes = plt.gca()\ncur_axes.axes.get_yaxis().set_ticks([])\nplt.xlim([0, 0.08])\nplt.ylim([0, 26])\nplt.xlabel('Estimated Prevalence')\nplt.ylabel('Stratification')"}, {'identified': 'chlamydia_dynamic_system', 'updated_code': "import sympy as sym\nfrom sympy import *\nA, U, S = symbols('A U S')\nalpha_UA, alpha_AU, alpha_US, alpha_SU = symbols('alpha_UA alpha_AU alpha_US alpha_SU')\nchlamydia_dynamic_system = [alpha_UA * U - alpha_AU * A, alpha_AU * A + alpha_SU * S - (alpha_UA + alpha_US) * U, alpha_US * U - alpha_SU * S, A + U + S - 1]\nsol_dyn = solve(chlamydia_dynamic_system, A, U, S)\ndyn_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A] + sol_dyn[S])\nU_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[U])\nA_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[A])\nS_fun = lambdify((alpha_UA, alpha_AU, alpha_US, alpha_SU), sol_dyn[S])\nsol_dyn"}, {'identified': 'num_epochs', 'updated_code': "from keras.callbacks import ModelCheckpoint\nnum_epochs = ...\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)\nmodel.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), epochs=num_epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"}, {'identified': 'binary_matrix', 'updated_code': "box = (x < 400) & (x >= 300) & (y >= 300) & (y <= 400)\nbinary_matrix[box] = 0\nline1 = y >= 350 - 0.5 * (x - 300)\nline2 = y <= 350 + 1.0 * (x - 300)\nline3 = y <= 400 - 2.0 * (x - 350)\nbinary_matrix[line1 & line2 & line3 & box] = 1\nplt.imshow(binary_matrix, interpolation='bilinear')"}, {'identified': 'column_info_format', 'updated_code': "for key in profile_dict.keys():\n    column_number = column_number + 1\n    column_info_format = ' ({}/{})'\n    print('\\n', (' ' + key + ' ').center(report_width, header_spacing_char))\n    sub_dictionary = profile_dict[key]\n    for dictionary in sub_dictionary:\n        keys = list(dictionary.keys())\n        attribute = keys[0]\n        value = dictionary[attribute]\n        if 'percent' in attribute:\n            formatted_value = '{0:.2%}'.format(value)\n        else:\n            formatted_value = str(value)\n        print(attribute.ljust(just_width, attribute_spacing_char), formatted_value.rjust(just_width, attribute_spacing_char))"}, {'identified': 'max_pooled_image', 'updated_code': "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\nmax_pool = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\nwith tf.Session() as sess:\n    max_pooled_image = sess.run(max_pool, feed_dict={X: dataset})\nplt.imshow(max_pooled_image[0].astype(np.uint8))\nplt.show()"}, {'identified': 'num_data_points', 'updated_code': "import numpy as np\nfrom scipy.special import expit\nnum_data_points = 100\nxs = np.linspace(-3, 3, num_data_points)\nReLu = np.maximum(xs, 0)\nd_ReLu = np.concatenate((np.zeros(int(num_data_points / 2)), np.ones(int(num_data_points / 2))))\ntanh = np.tanh(xs)\nd_tanh = 1 - tanh ** 2\nsig = expit(xs)\nd_sig = sig * (1 - sig)\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 3, 1)\nplt.plot(xs, ReLu, label='ReLu')\nplt.plot(xs, d_ReLu, label='d_Relu')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('ReLu(x) Plot')\nplt.ylim(-1.1, 1.1)\nplt.legend()\nplt.subplot(1, 3, 2)\nplt.plot(xs, tanh, label='tanh')\nplt.plot(xs, d_tanh, label='d_tanh')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('tanh(x) Plot')\nplt.ylim(-1.1, 1.1)\nplt.legend()\nplt.subplot(1, 3, 3)\nplt.plot(xs, sig, label='sigmoid')\nplt.plot(xs, d_sig, label='d_sigmoid')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sigmoid(x) Plot')\nplt.ylim(-1.1, 1.1)\nplt.legend()\nplt.suptitle('Activation functions')\nplt.show()"}, {'identified': 'variable_to_factor_message', 'updated_code': "def calc_max_sum_variable_to_factor_msg(variable, factor):\n    neighbour_msg_prod = get_neighbour_messages(variable, factor)\n    if len(neighbour_msg_prod) > 0:\n        message = np.sum(np.array(neighbour_msg_prod), axis=0)\n    else:\n        message = np.zeros(variable.num_states)\n    message += np.log(variable.observed_state)\n    return message\n\ndef variable_send_ms_msg(self, factor):\n    assert isinstance(factor, Factor), 'Variable can only send messages to factor!'\n    assert can_send_message(self, factor), 'Cannot send message!'\n    variable_to_factor_message = calc_max_sum_variable_to_factor_msg(self, factor)\n    factor.receive_msg(self, variable_to_factor_message)\n    self.pending.discard(factor)\nVariable.send_ms_msg = variable_send_ms_msg"}, {'identified': 'naive_predictor_f_score', 'updated_code': "TP = np.sum(income)\nFP = income.count() - TP\nTN = 0\nFN = 0\naccuracy = 1.0 * (TP + TN) / (TP + TN + FP + FN)\nrecall = 1.0 * TP / (TP + FN)\nprecision = 1.0 * TP / (TP + FP)\nnaive_predictor_f_score = (1.0 + 0.5 * 0.5) * (precision * recall) / (0.5 * 0.5 * precision + recall)\nprint('Naive Predictor: [Accuracy score: {}, F-score: {}]'.format(accuracy, naive_predictor_f_score))"}, {'identified': 'colormap', 'updated_code': "ncomponents = 2\npca = decomp.PCA(n_components=ncomponents)\nraw_pca = pca.fit(np.array(x_train.iloc[:, first_egemaps_feature:]).astype(np.float))\nx_pca = raw_pca.transform(x_train.iloc[:, first_egemaps_feature:])\ngroups = x_train['pos']\nshort_pos = {'nouns': 'nn', 'verbs': 'vrb', 'other': 'oth', 'function_words': 'fct', 'adjectives': 'adj'}\ngroup_values = [short_pos[i] for i in groups.unique()]\nposIDs = groups.factorize()[0]\nlabels = np.array(x_train['word'])\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.set_xlabel('PC 1')\nax.set_ylabel('PC 2')\ncolormap = plt.get_cmap('gnuplot', len(group_values))\npc1 = x_pca[:, 0]\npc2 = x_pca[:, 1]\np = ax.scatter(pc1, pc2, c=posIDs, cmap=colormap, s=10)\ncb = plt.colorbar(p)\nloc = np.arange(0, max(posIDs), max(posIDs) / float(len(group_values)))\ncb.set_ticks(loc)\ncb.set_ticklabels(group_values)\nfor label, x, y in zip(labels, pc1, pc2):\n    if y == np.amax(y) or x == np.amax(x):\n        plt.annotate(label, fontsize=2, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nif pos_filter == None:\n    axins = zoomed_inset_axes(ax, 2.5, loc=2)\n    x1, x2, y1, y2 = (-3.1, 2.5, 6.3, 9.8)\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n    axins.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\n    axins2 = zoomed_inset_axes(ax, 2.5, loc=4)\n    x1, x2, y1, y2 = (-4, 0.5, -8.5, -6.5)\n    axins2.set_xlim(x1, x2)\n    axins2.set_ylim(y1, y2)\n    axins2.scatter(pc1, pc2, c=posIDs, cmap=colormap)\n    plt.yticks(visible=False)\n    plt.xticks(visible=False)\n    mark_inset(ax, axins2, loc1=2, loc2=4, fc='none', ec='0.5')\n    for label, x, y in zip(labels, pc1, pc2):\n        if y == np.amax(y) or x == np.amax(x):\n            plt.annotate(label, fontsize=8, xy=(x, y), xytext=(0, 1), textcoords='offset points', ha='right', va='bottom')\nfilename = corpus + '_full_zoom.pdf'\nif pos_filter != None and len(pos_filter) == 1:\n    filename = corpus + '_pca_' + pos_filter[0][1] + '_' + pos_filter[0][2] + '.pdf'\nplt.savefig(filename, bbox_inches='tight')"}, {'identified': 'subplot_ax', 'updated_code': "X, Annotations = marconi['Walk1']\ncamera = 0\nframe = 220\nfig = plt.figure(figsize=(16, 8))\nsubplot_ax = fig.add_subplot(111)\nim = X[camera, frame]\nsubplot_ax.imshow(im)\nAnnotations_for_cam = Annotations[camera]\nAnnot_on_frame_cam = Annotations_for_cam[frame]\nCOLORS = ['red', 'yellow']\nfor i, annot in enumerate(Annot_on_frame_cam):\n    if annot is not None:\n        (tl, br), joints = annot\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n        subplot_ax.plot(head_x, head_y, color=COLORS[i])\n        for jx, jy, visible in joints:\n            plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\nplt.axis('off')\nplt.show()"}, {'identified': 'mean_cross_entropy_loss', 'updated_code': "height = 28\nwidth = 28\nchannels = 1\nn_inputs = height * width\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = 'SAME'\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 2\nconv2_pad = 'SAME'\npool3_fmaps = conv2_fmaps\nn_fc1 = 64\nn_outputs = 10\nreset_graph()\nwith tf.name_scope('inputs'):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n    y = tf.placeholder(tf.int32, shape=[None], name='y')\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name='conv1')\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name='conv2')\nwith tf.name_scope('pool3'):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\nwith tf.name_scope('fc1'):\n    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name='fc1')\nwith tf.name_scope('output'):\n    logits = tf.layers.dense(fc1, n_outputs, name='output')\n    Y_proba = tf.nn.softmax(logits, name='Y_proba')\nwith tf.name_scope('train'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    mean_cross_entropy_loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(mean_cross_entropy_loss)\nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\nwith tf.name_scope('init_and_save'):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"}, {'identified': 'segment_data', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\nindex = 0\nfor segment in segments:\n    segment_data = data.loc[data[\'RiverSeg\'] == segment]\n    segment_data = segment_data[segment_data["lam\'"] > 0]\n    segment_data = segment_data.sort_values(\'Year\')\n    x = segment_data[\'Year\']\n    y = segment_data["N10\'"]\n    y.name = segment\n    plt.plot(x, y, colors[index])\n    plt.legend(loc=(1.05, 0.2))\n    index += 1\nplt.title("Hill N10\', " + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel("N10\'")\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + "_Hill_N10\'.png", bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'inception_model_dir', 'updated_code': "import sys\nimport tarfile\nfrom six.moves import urllib\nTF_MODELS_URL = 'http://download.tensorflow.org/models'\nINCEPTION_V3_URL = TF_MODELS_URL + '/inception_v3_2016_08_28.tar.gz'\ninception_model_dir = os.path.join('datasets', 'inception')\nINCEPTION_V3_CHECKPOINT_PATH = os.path.join(inception_model_dir, 'inception_v3.ckpt')\n\ndef download_progress(count, block_size, total_size):\n    percent = count * block_size * 100 // total_size\n    sys.stdout.write('\\rDownloading: {}%'.format(percent))\n    sys.stdout.flush()\n\ndef fetch_pretrained_inception_v3(url=INCEPTION_V3_URL, path=inception_model_dir):\n    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\n        return\n    os.makedirs(path, exist_ok=True)\n    tgz_path = os.path.join(path, 'inception_v3.tgz')\n    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n    inception_tgz = tarfile.open(tgz_path)\n    inception_tgz.extractall(path=path)\n    inception_tgz.close()\n    os.remove(tgz_path)"}, {'identified': 'hidden_layer_figure', 'updated_code': "t_min_max = (vsig.timestamps[0], vsig.timestamps[-1])\nlayer = '2'\no_or_s = 'output'\nval_arrays = np.load(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, o_or_s])) + '.npy')\nn_generations, _, n_neurons = val_arrays.shape\nncols = 2\nnrows = n_neurons // ncols\nfig, hidden_layer_figure = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 20))\nfor g in range(n_generations):\n    for i in range(n_neurons):\n        ax = hidden_layer_figure[i // ncols, i % ncols]\n        ax.cla()\n        y_pred_colors = val_arrays[g, :, i]\n        ax.plot(vsig.timestamps, vsig.mixed_signal, color='grey', alpha=0.3)\n        ax.scatter(vsig.timestamps, x_val[0, :, 0], vsig.timestamps[vsig.window_size - 1:], marker='o', c=y_pred_colors, cmap=plt.get_cmap('coolwarm'), vmin=-1, vmax=1)\n        ax.set_title('neuron = {}'.format(i + 1))\n        ax.set_xlim(t_min_max)\n        ax.grid(True)\n    plt.tight_layout()\n    plt.suptitle('hidden layer = {}, ({}), generation = {}'.format(layer, o_or_s, g + 1))\n    plt.savefig(os.path.join(vsig.out_dir, '_'.join(['valid_hidden_layer', layer, o_or_s, 'gen', str(g + 1)]) + '.png'))\nplt.show()"}, {'identified': 'regression_result', 'updated_code': 'regression_result = None'}, {'identified': 'frame_index', 'updated_code': "X, Annotations = marconi['Soccer2']\ncamera = 0\nframe_index = 220\nfig = plt.figure(figsize=(16, 8))\nax = fig.add_subplot(111)\nim = X[camera, frame_index]\nax.imshow(im)\nAnnotations_for_cam = Annotations[camera]\nAnnot_on_frame_cam = Annotations_for_cam[frame_index]\nCOLORS = ['red', 'yellow']\nfor i, annot in enumerate(Annot_on_frame_cam):\n    if annot is not None:\n        (tl, br), joints = annot\n        head_x, head_y = utils.tl_br_to_plt_plot(tl[1], tl[0], br[1], br[0])\n        ax.plot(head_x, head_y, color=COLORS[i])\n        for jx, jy, visible in joints:\n            plt.scatter(jx, jy, color=COLORS[i], alpha=1 if visible == 1 else 0.4)\nplt.axis('off')\nplt.show()"}, {'identified': 'inverted_prediction', 'updated_code': 'def invert_scale(scaler, X, value):\n    new_row = [x for x in X] + [value]\n    array = numpy.array(new_row)\n    array = array.reshape(1, len(array))\n    inverted_prediction = scaler.inverse_transform(array)\n    return inverted_prediction[0, -1]'}, {'identified': 'test_set_accuracy', 'updated_code': "my_labels = [16, 33, 11, 38, 35, 17]\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.import_meta_graph('./lenet.meta')\n    saver.restore(sess, './lenet')\n    my_images_normalized = [normalize(rgb2gray(img)) for img in my_images]\n    test_set_accuracy = evaluate(my_images_normalized, my_labels)\n    print('Test Set Accuracy = {:.3f}'.format(test_set_accuracy))"}, {'identified': 'flattened_output', 'updated_code': "def LeNet6(x, n_classes):\n    mu = 0\n    sigma = 0.1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean=mu, stddev=sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean=mu, stddev=sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n    flattened_output = flatten(conv2)\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 252), mean=mu, stddev=sigma))\n    fc1_b = tf.Variable(tf.zeros(252))\n    fc1 = tf.matmul(flattened_output, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n    fc1 = tf.nn.dropout(fc1, keep_prob)\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(252, 168), mean=mu, stddev=sigma))\n    fc2_b = tf.Variable(tf.zeros(168))\n    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n    fc2 = tf.nn.dropout(fc2, keep_prob)\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(168, 84), mean=mu, stddev=sigma))\n    fc3_b = tf.Variable(tf.zeros(84))\n    fc3 = tf.matmul(fc2, fc3_W) + fc3_b\n    fc3 = tf.nn.relu(fc3)\n    fc3 = tf.nn.dropout(fc3, keep_prob)\n    fc4_W = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean=mu, stddev=sigma))\n    fc4_b = tf.Variable(tf.zeros(n_classes))\n    logits = tf.matmul(fc3, fc4_W) + fc4_b\n    return logits"}, {'identified': 'airport_columns', 'updated_code': "airport_columns = ['arr_port', 'pax']\ndataframe = get_df_cols(BOOKINGS, airport_columns, '^')\nprint_top_n_arrival_airport(dataframe, 10)"}, {'identified': 'refined_track_durations', 'updated_code': "X_test_refined = pd.DataFrame([])\nr_precisions = []\npbar = tqdm(data_test.groupby(['playlist_pid']))\nfor pid, df in pbar:\n    labels = y_test.loc[df.index]\n    from IPython.core.debugger import set_trace\n    set_trace()\n    targets = dataset.loc[labels.index].track_duration_ms\n    positive_targets = dataset.loc[labels[labels == 1].index].index\n    refined_track_durations = dataset.loc[X_test[X_test.playlist_pid != pid].index].track_duration_ms\n    new_df = df.drop('track_duration_ms', axis=1)\n    new_test = refined_track_durations.append(targets)\n    new_df = pd.concat([new_df.head(1)] * len(new_test))\n    test_playlist_tracks = pd.concat([new_df.reset_index(drop=True), new_test.reset_index(drop=True)], axis=1).set_index(new_test.index)\n    test_playlist_tracks = (test_playlist_tracks - test_playlist_tracks.mean()) / (test_playlist_tracks.std() + 1e-08)\n    X_test_refined = X_test_refined.append(test_playlist_tracks)\n    y_prob = pd.DataFrame(lr_clf.predict_proba(test_playlist_tracks), index=test_playlist_tracks.index)\n    y_prob = y_prob.sort_values(by=[1], ascending=False)\n    if len(positive_targets) > 0:\n        r_precisions.append(r_precision(positive_targets, y_prob.index))\n    pbar.set_description('{}'.format(np.mean(r_precisions)))"}, {'identified': 'county_names', 'updated_code': "county_names = df_county_data['County Name']\nhouse_size = df_county_data['Household Size']\nx_axis = np.arange(len(house_size))\ntick_locations = [value for value in x_axis]\nplt.bar(x_axis, house_size, color='r', align='center')\nplt.title('County Household Size')\nplt.xlabel('Counties')\nplt.ylabel('Household Size')\nplt.text(140, 2.5, 'Note:\\nHousehold size for all counties in NJ, NY, & PA.')\nplt.savefig('Images/County_Household_Size.png', bbox_inches='tight')\nplt.show()"}, {'identified': 'false_positive_cases', 'updated_code': 'def roc_auc(labels, predictions, thresholds, get_fpr_tpr=True):\n    tpr = []\n    fpr = []\n    for th in thresholds:\n        tp_cases = tf.where(tf.greater_equal(predictions, th) & tf.equal(labels, 1))\n        tp = tf.size(tp_cases)\n        tn_cases = tf.where(tf.less(predictions, th) & tf.equal(labels, 0))\n        tn = tf.size(tn_cases)\n        false_positive_cases = tf.where(tf.greater_equal(predictions, th) & tf.equal(labels, 0))\n        fp = tf.size(false_positive_cases)\n        fn_cases = tf.where(tf.less(predictions, th) & tf.equal(labels, 1))\n        fn = tf.size(fn_cases)\n        tpr_th = tp / (tp + fn)\n        fpr_th = fp / (fp + tn)\n        tpr.append(tpr_th)\n        fpr.append(fpr_th)\n    auc_score = 0\n    for i in range(0, len(thresholds) - 1):\n        height_step = tf.abs(fpr[i + 1] - fpr[i])\n        b1 = tpr[i]\n        b2 = tpr[i + 1]\n        step_area = height_step * (b1 + b2) / 2\n        auc_score += step_area\n    return (auc_score, fpr, tpr)'}, {'identified': 'modflow_model', 'updated_code': "modflow_model = flopy.modflow.Modflow(modelname=modelname, exe_name=mfexe, model_ws=modelpth)\ndis = flopy.modflow.ModflowDis(modflow_model, nlay, nrow, ncol, delr=delr, delc=delc, top=botm[0, :, :], botm=botm[1:, :, :], perlen=1, nstp=1, steady=True)\nbas = flopy.modflow.ModflowBas(modflow_model, ibound=ibound, strt=strt)\nlpf = flopy.modflow.ModflowLpf(modflow_model, hk=0.0001, laytyp=4)\noc = flopy.modflow.ModflowOc(modflow_model, stress_period_data={(0, 0): ['print budget', 'print head', 'save head', 'save budget']})\nsms = flopy.modflow.ModflowSms(modflow_model, nonlinmeth=1, linmeth=1, numtrack=50, btol=1.1, breduc=0.7, reslim=0.0, theta=0.85, akappa=0.0001, gamma=0.0, amomentum=0.1, iacl=2, norder=0, level=5, north=7, iredsys=0, rrctol=0.0, idroptol=1, epsrn=1e-05, mxiter=500, hclose=0.001, hiclose=0.001, iter1=50)\nmodflow_model.write_input()\ntry:\n    os.remove(os.path.join(model_ws, '{0}.hds'.format(modelname)))\nexcept:\n    pass\nmodflow_model.run_model()"}, {'identified': 'cropped_image_shape', 'updated_code': 'def prepare_image_with_tensorflow(image, target_width=299, target_height=299, max_zoom=0.2):\n    """Zooms and crops the image randomly for data augmentation."""\n    image_shape = tf.cast(tf.shape(image), tf.float32)\n    height = image_shape[0]\n    width = image_shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = tf.cond(crop_vertically, lambda: width, lambda: height * target_image_ratio)\n    crop_height = tf.cond(crop_vertically, lambda: width / target_image_ratio, lambda: height)\n    resize_factor = tf.random_uniform(shape=[], minval=1.0, maxval=1.0 + max_zoom)\n    crop_width = tf.cast(crop_width / resize_factor, tf.int32)\n    crop_height = tf.cast(crop_height / resize_factor, tf.int32)\n    cropped_image_shape = tf.stack([crop_height, crop_width, 3])\n    image = tf.random_crop(image, cropped_image_shape)\n    image = tf.image.random_flip_left_right(image)\n    image_batch = tf.expand_dims(image, 0)\n    image_batch = tf.image.resize_bilinear(image_batch, [target_height, target_width])\n    image = image_batch[0] / 255\n    return image'}, {'identified': 'material_variable_definitions', 'updated_code': "L_AB_m, L_BD_m = sy.symbols('L_{AB}[m], L_{BD}[m]')\nL_AD_m = L_AB_m + L_BD_m\nmaterial_variable_definitions = {L_AB_m: 3, L_BD_m: 3}"}, {'identified': 'exact_inference_bbn', 'updated_code': 'import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport warnings\nfrom pybbn.generator.bbngenerator import generate_singly_bbn, generate_multi_bbn, convert_for_exact_inference\nfrom pybbn.generator.bbngenerator import convert_for_drawing\nfrom pybbn.pptc.inferencecontroller import InferenceController\nnp.random.seed(37)\ng, p = generate_multi_bbn(5, max_iter=5)\nexact_inference_bbn = convert_for_exact_inference(g, p)\nnx_multi_bbn = convert_for_drawing(exact_inference_bbn)\ng, p = generate_singly_bbn(5, max_iter=10)\ns_bbn = convert_for_exact_inference(g, p)\nnx_singly_bbn = convert_for_drawing(s_bbn)'}, {'identified': 'mean_X_coordinate', 'updated_code': 'stickwidth = 4\nfor i in range(17):\n    for n in range(len(subset)):\n        index = subset[n][np.array(limbSeq[i]) - 1]\n        if -1 in index:\n            continue\n        cur_canvas = canvas.copy()\n        Y = candidate[index.astype(int), 0]\n        X = candidate[index.astype(int), 1]\n        mean_X_coordinate = np.mean(X)\n        mY = np.mean(Y)\n        length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n        angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n        polygon = cv.ellipse2Poly((int(mY), int(mean_X_coordinate)), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n        cv.fillConvexPoly(cur_canvas, polygon, colors[i])\n        canvas = cv.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\nplt.imshow(canvas[:, :, [2, 1, 0]])\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(12, 12)'}, {'identified': 'average_height_difference', 'updated_code': 'klay = 2.01\nsteph = 1.91\naverage_height_difference = 2.06\nmin_height_difference = ...'}, {'identified': 'visualization_type', 'updated_code': "visualize_dataset = True\nfeatures_to_exclude = ['x', 'y', 'z']\ndeal_with_nulls = 'mean'\nnunique_range = [2, 10]\npalette = 'Dark2'\nidentity_type = 'hist'\nvisualization_type = 'scatter'"}, {'identified': 'fitted_parameters', 'updated_code': 'def expectedFit(x, a, b):\n    """\n    Returns the expected fit for the histogram\n    \n    Arguments: x - the x value in the equation\n               a - the first fit parameter\n               b - the second fit parameter\n               \n    Returned: The expected fit function\n    """\n    return a * np.exp(-b * x)\noccurenceRegion = bigOccurences[7:14]\nmagnitudeRegion = bigMagnitudes[7:14]\nparameters, fitted_parameters = curve_fit(expectedFit, magnitudeRegion, occurenceRegion)\nfitX = magnitudeRegion\naFit = parameters[0]\nbFit = parameters[1]\nfitY = aFit * np.exp(-bFit * magnitudeRegion)\nprint(\'A = \', aFit, \'b = \', bFit)'}, {'identified': 'cluster_distributions', 'updated_code': "cluster_means = []\nnp.random.seed(45)\nwith pm.Model() as model:\n    pi = pm.Dirichlet('pi', np.ones(K))\n    cluster_distributions = []\n    mu = []\n    sigma_sq = []\n    cov = []\n    for i in range(K):\n        temp_mean = np.random.randint(low=20, high=230, size=D)\n        mu.append(pm.Normal('mu%i' % i, temp_mean, 20, shape=D))\n        sigma_sq.append(pm.InverseGamma('sigma_sq%i' % i, 1, 1, shape=D))\n        cov.append(tt.nlinalg.alloc_diag(sigma_sq[i]))\n        cluster_distributions.append(pm.MvNormal.dist(mu=mu[i], cov=cov[i]))\n        cluster_means.append(temp_mean)\n    xobs = pm.Mixture('x_obs', pi, cluster_distributions, observed=X_shared)"}, {'identified': 'crossed_feature_column', 'updated_code': "def get_wide_deep():\n    is_male, mother_age, plurality, gestation_weeks = [tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False', 'Unknown']), tf.feature_column.numeric_column('mother_age'), tf.feature_column.categorical_column_with_vocabulary_list('plurality', ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)', 'Multiple(2+)']), tf.feature_column.numeric_column('gestation_weeks')]\n    age_buckets = tf.feature_column.bucketized_column(mother_age, boundaries=np.arange(15, 45, 1).tolist())\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, boundaries=np.arange(17, 47, 1).tolist())\n    wide = [is_male, plurality, age_buckets, gestation_buckets]\n    crossed_feature_column = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n    embed = tf.feature_column.embedding_column(crossed_feature_column, 3)\n    deep = [mother_age, gestation_weeks, embed]\n    return (wide, deep)"}, {'identified': 'experiment_config', 'updated_code': "PROJECT_DIR = '../../'\nuse_toy_data = False\nLOG_DIR = 'logs'\nif use_toy_data:\n    batch_size = 8\n    embedding_dim = 5\n    cell_size = 32\n    experiment_config = 6\nelse:\n    batch_size = 64\n    embedding_dim = 20\n    cell_size = 128\n    experiment_config = 33\nuse_attention = True\nuse_bidirectional_encoder = True\nis_time_major = True"}, {'identified': 'data_filepath', 'updated_code': 'rebin = 0\ntruncate = {0: 0.0336577}\ndata_filepath = \'/home/pyne-user/Dropbox/UCB/Research/ETAs/88Inch/Data/Experiments/PHS/33MeVTa_29-31Mar17/Unfold/BeamOnly/HEPROW/Inputs/\'\ngROOT.ProcessLine(\'HistogramWriter writer;\')\nfor detNum, detName in detNames.iteritems():\n    gROOT.ProcessLine(\'PulseHeightSpectrum{0} = (TH1D*)ops.truncateHist(phs{0}[1],{1},30)\'.format(detNum, truncate[detNum]))\n    gROOT.ProcessLine(\'PulseHeightSpectrum{0}->Rebin({1})\'.format(detNum, rebin))\n    gROOT.ProcessLine(\'TH1* dataHist{0} = ops.rebinStatistically(PulseHeightSpectrum{0},100);\'.format(detNum))\n    gROOT.ProcessLine(\'writer.PhToHEPROW(PulseHeightSpectrum{0},"{1}{2}_phs_03")\'.format(detNum, data_filepath, detName))\n    gROOT.ProcessLine(\'writer.PhToHEPROW(dataHist{0},"{1}{2}_stat_100_phs_03")\'.format(detNum, data_filepath, detName))'}, {'identified': 'data_scaler', 'updated_code': 'from sklearn.preprocessing import MinMaxScaler\ndata_scaler = MinMaxScaler(feature_range=(-1, 1))\ndata_scaler'}, {'identified': 'word_embeddings', 'updated_code': "batch_size = 128\nembedding_size = 128\nskip_window = 1\nnum_skips = 2\nvalid_size = 16\nvalid_window = 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64\ngraph = tf.Graph()\nwith graph.as_default(), tf.device('/cpu:0'):\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n    embed = tf.nn.embedding_lookup(word_embeddings, train_dataset)\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n    norm = tf.sqrt(tf.reduce_sum(tf.square(word_embeddings), 1, keep_dims=True))\n    normalized_embeddings = word_embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"}, {'identified': 'selected_dataset', 'updated_code': "fig = plt.figure(figsize=(16, 5))\nax = fig.add_subplot(121)\nax.axis('off')\nay = fig.add_subplot(122)\nay.axis('off')\nP1 = Ps[0]\nP2 = Ps[1]\nselected_dataset = Ks[0]\nK2 = Ks[1]\nRt1 = Rts[0]\nRt2 = Rts[1]\nannot1 = Annotations[0][0]\nannot2 = Annotations[1][0]\nindv_left = annot1[0]\nindv_right = annot2[1]\nlefthand_left = indv_left[1][0]\nlefthand_right = indv_right[1][0]\nax.imshow(X[0, 0])\nax.scatter(lefthand_left[0], lefthand_left[1], color='red')\nay.imshow(X[1, 0])\nay.scatter(lefthand_right[0], lefthand_right[1], color='red')\nplt.show()"}, {'identified': 'annotations', 'updated_code': 'annotations = create_annotations(lista)'}, {'identified': 'review_score_rmse', 'updated_code': 'from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nreview_score_rmse = sqrt(mean_squared_error(y_test, y_test_predict))\nreview_score_rmse'}, {'identified': 'start_datetime', 'updated_code': "def build_index(non_empty_days, from_time, to_time):\n    date_ranges = []\n    for date in non_empty_days:\n        yyyy, mm, dd = date.split('-')\n        from_hour, from_min = from_time.split(':')\n        to_hour, to_min = to_time.split(':')\n        start_datetime = datetime(int(yyyy), int(mm), int(dd), int(from_hour), int(from_min), 0)\n        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour), int(to_min), 0)\n        date_ranges.append(pd.DataFrame({'OrganizedDateTime': pd.date_range(start_datetime, t2, freq='1Min').values}))\n    agg = pd.concat(date_ranges, axis=0)\n    agg.index = agg['OrganizedDateTime']\n    return agg"}, {'identified': 'lowercase_uri_df', 'updated_code': "track_artist_lower_df = tracks_df['track_artist_uri'].apply(lambda x: x.lower())\ntrack_album_lower_df = tracks_df['track_album_uri'].apply(lambda x: x.lower())\nmerged_track_uri_lower_df = merged['track_uri'].apply(lambda x: x.lower())\n\ndef get_translation(uri_type, uri):\n    track = False\n    if uri_type == 'track_artist_uri':\n        lowercase_uri_df = track_artist_lower_df\n        col = 'track_artist_name'\n    elif uri_type == 'track_album_uri':\n        lowercase_uri_df = track_album_lower_df\n        col = 'track_album_name'\n    elif uri_type == 'track_uri':\n        lowercase_uri_df = merged_track_uri_lower_df\n        col = 'track_name'\n        track = True\n    for i in range(len(tracks_df)):\n        if lowercase_uri_df[i] == uri:\n            if track:\n                return merged.iloc[i][col]\n            return tracks_df.iloc[i][col]\n            break"}, {'identified': 'biodiversity_index', 'updated_code': 'colors = [\'b\', \'g\', \'r\', \'k\', \'c\', \'m\', \'y\']\nindex = 0\nfor segment in segments:\n    segDF = data.loc[data[\'RiverSeg\'] == segment]\n    segDF = segDF[segDF["lam\'"] > 0]\n    segDF = segDF.sort_values(\'Year\')\n    x = segDF[\'Year\']\n    biodiversity_index = segDF["N21\'"]\n    biodiversity_index.name = segment\n    plt.plot(x, biodiversity_index, colors[index])\n    plt.legend(loc=(1.05, 0.2))\n    index += 1\nplt.title("Hill N21\', " + river + \' River Segments\')\nplt.xlabel(\'Year\')\nplt.ylabel("N21\'")\nplt.xticks(np.arange(min(x), max(x) + 1, 1.0))\nplt.xticks(rotation=90)\nplt.savefig(output + \'\\\\\' + river + "_Hill_N21\'.png", bbox_inches=\'tight\', dpi=300, size=(2000, 2000))\nplt.show()'}, {'identified': 'hotel_reservations_df', 'updated_code': "unique_hotels_names = hotel_reservations_df['Hotel Name'].unique()\nunique_checkins = hotel_reservations_df['Checkin Date'].unique()\nunique_discount_code = [1, 2, 3, 4]\nimport itertools\nimport sys\ncombs = []\nfor x in unique_hotels_names:\n    for y in unique_checkins:\n        for z in unique_discount_code:\n            combs.append([x, y, z, sys.maxsize])\nnew_df = DataFrame.from_records(combs, columns=['Hotel Name', 'Checkin Date', 'Discount Code', 'Discount Price'])\nhotel_reservations_df = hotel_reservations_df.append(new_df)"}, {'identified': 'svm_parameter_search', 'updated_code': "tuned_parameters = [{'kernel': ['rbf'], 'nu': [0.0001, 0.001, 0.01, 0.1, 0.5]}, {'kernel': ['poly'], 'degree': np.arange(1, 4), 'nu': [0.0001, 0.001, 0.01, 0.1, 0.5], 'max_iter': [10000]}]\nsvm_parameter_search = ParameterSearch(svm.OneClassSVM, tuned_parameters, act_train_svm, act_train_all, pred_f_tr.flatten(), scorer_roc_probas_svm, n_iter=5, verbosity=11, n_jobs=-1, subsample_train=0.01, subsample_test=0.001)\nsvm_parameter_search.fit()"}, {'identified': 'training_start_time', 'updated_code': 'from sklearn.metrics import fbeta_score, accuracy_score\nbeta = 0.5\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test):\n    """\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    """\n    results = {}\n    training_start_time = time()\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time()\n    results[\'train_time\'] = end - training_start_time\n    training_start_time = time()\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    end = time()\n    results[\'pred_time\'] = end - training_start_time\n    results[\'acc_train\'] = accuracy_score(y_train[:300], predictions_train)\n    results[\'acc_test\'] = accuracy_score(y_test, predictions_test)\n    results[\'f_train\'] = fbeta_score(y_train[:300], predictions_train, beta)\n    results[\'f_test\'] = fbeta_score(y_test, predictions_test, beta)\n    print(\'{} trained on {} samples.\'.format(learner.__class__.__name__, sample_size))\n    return results'}, {'identified': 'measurement_resolution', 'updated_code': 'measurement_resolution, _, _, f = window_bounds(((x_start, 5000), (y_start, 5000)), orig_aff)\na, b, _, d, e, _, _, _, _ = tuple(orig_aff)\nnew_aff = rasterio.Affine(a, b, measurement_resolution, d, e, f)'}, {'identified': 'total_abundance', 'updated_code': 'for i in range(len(data)):\n    total_abundance = 0.0\n    for x in range(len(data.iloc[i]))[4:last]:\n        if data.iloc[i][x] > 0:\n            total_abundance += data.iloc[i][x]\n    array = data.iloc[i][4:last]\n    num = 0.0\n    for y in array:\n        num += y * (y - 1)\n    lam = num / (total_abundance * (total_abundance - 1))\n    data.loc[i, "lam\'"] = lam'}, {'identified': 'block_motion_force', 'updated_code': 'def blockMotionVaryingMasses(t, blockPositions, vBlock, i, blockNum, kp, kc, mass, F0, v0, vf):\n    """\n    Returns the differential equation that models the motion of the blocks\n    \n    Arguments:  t - time\n                blockPositions - the positions of the blocks\n                vBlock - the velocity of the block\n                i - the index of the current block\n                blockNum - the number of blocks\n                kp - spring constant of leaf springs\n                kc - spring constant of springs between blocks\n                mass - masses of individual blocks\n                F0 - the static friction force\n                v0 - initial velocity of top plate\n                vf - the friction coefficient\n                \n    Returned: The differential equation modeling the motion of the individual blocks\n    \n    Examples:\n    \n    >>> blockMotion (0, (0, 1, 2, 3, 4), 0, 2, 5, 0, 0, 1, 0, 1, 20)\n    array([ 0.,  0.])\n    \n    """\n    xi = blockPositions[i] - i\n    vi = vBlock\n    mi = mass[i]\n    if i == 0:\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc * (xiP - xi) + kp * (v0 * t - xi)\n    elif i == blockNum - 1:\n        xiM = blockPositions[i - 1] - (i - 1)\n        springForce = kc * (xiM - xi) + kp * (v0 * t - xi)\n    else:\n        xiM = blockPositions[i - 1] - (i - 1)\n        xiP = blockPositions[i + 1] - (i + 1)\n        springForce = kc * (xiP + xiM - 2 * xi) + kp * (v0 * t - xi)\n    frictionForce = friction(vi, vf, F0)\n    if abs(springForce) <= abs(frictionForce):\n        block_motion_force = -vi\n        vi = 0\n        dx = vi\n    else:\n        totalForce = (springForce + frictionForce) / mi\n        dx = vi\n        block_motion_force = totalForce\n    return np.array([dx, block_motion_force], float)'}, {'identified': 'theta', 'updated_code': 'k_size = 3\nvertex_ratio_h = 0.45\nvertex_ratio_v = 0.6\nlow_thresh = 50\nhigh_thresh = 200\nL2gradient = False\nrho = 2\ntheta = 1 * np.pi / 180.0\nmin_votes = 15\nmin_line_len = 40\nmax_line_gap = 20\nangle = 3 * np.pi / 16\nangle_threshold = np.pi / 16\n\ndef process_image(image):\n    result = lane_detection_ppline(image, k_size=k_size, low_thresh=low_thresh, high_thresh=high_thresh, L2gradient=L2gradient, rho=rho, theta=theta, min_votes=min_votes, min_line_len=min_line_len, max_line_gap=max_line_gap, angle=angle, angle_thresh=angle_threshold, debug=False)\n    return result'}]