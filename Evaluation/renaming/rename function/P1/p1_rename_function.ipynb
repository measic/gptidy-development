{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSimple prompt\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple prompt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from utils import print_check_gpt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "NUM_FILES = 213\n",
    "FOLDER_NAME = '../../determining_files_rename/random_samples_functions'\n",
    "GPT_SAVED_FILE_NAME = 'rename_function_gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files from folder random_cells \n",
    "random_cells = []\n",
    "\n",
    "# read in cells\n",
    "for i in range(NUM_FILES):\n",
    "    file_name = f'{FOLDER_NAME}/{i}.py'\n",
    "    with open(file_name, 'r') as f:\n",
    "        random_cells.append(f.read())\n",
    "\n",
    "# read in readmes\n",
    "with open(f'{FOLDER_NAME}/readmes.txt', 'r') as f:\n",
    "    readmes = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33722799999999997"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate cost\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "import utils\n",
    "\n",
    "def estimate_tokens():\n",
    "    in_tok = ''\n",
    "    out_tok = ''\n",
    "    for i, cell_src in enumerate(random_cells):\n",
    "        # estimate prompt\n",
    "        in_tok += \"Rename the function 'function_def' in the code delimited by triple backticks to a more meaningful name that reflects its usage and/or aligns with the project's purpose. Do not add, remove, or change anything else. Structure your response under the following headings: 'New function name' (only one new function name), 'Updated code' (the full code cell with the function renamed), and 'Explanation' (a 1-2 sentence explanation of the new function name).\"\n",
    "        in_tok += f'Project purpose:\\n{readmes[i]}\\n\\nCode:\\n```python\\n{cell_src}\\n```\"'\n",
    "        # estimate response\n",
    "        out_tok += cell_src\n",
    "    return in_tok, out_tok\n",
    "\n",
    "in_tok, out_tok = estimate_tokens()\n",
    "\n",
    "utils.gpt_35_turbo_token_dollar_cost(in_tok, out_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename using GPT\n",
    "# import openai\n",
    "# openai.api_key = my_key\n",
    "\n",
    "# # GPT\n",
    "# def rename(purpose, cell_src, name):\n",
    "#     while True:\n",
    "#         try:\n",
    "#             completion = openai.ChatCompletion.create(\n",
    "#                 model=\"gpt-3.5-turbo\",\n",
    "#                 temperature=0,\n",
    "#                 messages = [\n",
    "#                     {\"role\" : \"user\", \"content\" : f\"Rename the function '{name}' in the code delimited by triple backticks to a more meaningful name that reflects its usage and/or aligns with the project's purpose. Do not add, remove, or change anything else. Structure your response under the following headings: 'New function name' (only one new function name), 'Updated code' (the full code cell with the function renamed), and 'Explanation' (a 1-2 sentence explanation of the new function name).\"},\n",
    "#                     {\"role\" : \"user\", \"content\" : f\"Project purpose:\\n{purpose}\\n\\nCode:\\n```python\\n{cell_src}\\n```\"}\n",
    "#                 ]\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             if 'maximum context length' in str(e):\n",
    "#                 print('...Error.. too long...' + str(e))\n",
    "#                 return 'length', None\n",
    "#             else:\n",
    "#                 print('...Error.. trying again...' + str(e))\n",
    "#         else:\n",
    "#             break\n",
    "#     return completion.choices[0].finish_reason, completion.choices[0].message[\"content\"]\n",
    "\n",
    "# gpt_results = []\n",
    "# for i, cell_src in enumerate(random_cells):\n",
    "#     print(f'Processing file {i}')\n",
    "#     finish_reason, result = rename(readmes[i], cell_src, 'function_def')\n",
    "#     print(f'File {i} - {finish_reason}')\n",
    "#     gpt_results.append({'reason': finish_reason, 'result': result})\n",
    "\n",
    "# # save the results to a file\n",
    "# with open(GPT_SAVED_FILE_NAME, 'w') as f:\n",
    "#     f.write(str(gpt_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in gpt result from file\n",
    "with open(GPT_SAVED_FILE_NAME, 'r') as f:\n",
    "    gpt_results = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split the data into files\n",
    "gpt_new_names = []\n",
    "gpt_new_code = []\n",
    "gpt_explanation = []\n",
    "\n",
    "for i, result in enumerate(gpt_results):\n",
    "    if result['reason'] == 'stop':\n",
    "        # split the result\n",
    "        first_split = result['result'].split('New function name:')[1].split('Updated code:')\n",
    "        updated_name = first_split[0].strip()\n",
    "        second_split = first_split[1].split('Explanation:')\n",
    "        updated_code = second_split[0].strip()\n",
    "        explanation = second_split[1].strip()\n",
    "\n",
    "        # update name\n",
    "        if len(updated_name.split('`')) == 3:\n",
    "            updated_name = updated_name.split('`')[1]\n",
    "        \n",
    "        # update name\n",
    "        if updated_name.startswith('-'):\n",
    "            updated_name = updated_name.strip('-')\n",
    "        \n",
    "        updated_name = updated_name.strip()\n",
    "\n",
    "        # update explanation\n",
    "        if explanation.startswith('-'):\n",
    "            explanation = explanation.strip('-')\n",
    "        \n",
    "        explanation = explanation.strip()\n",
    "\n",
    "        # get the updated code\n",
    "        updated_code = updated_code.split('```')[1]\n",
    "        if updated_code.startswith('python'):\n",
    "            updated_code = updated_code[6:]\n",
    "        updated_code = updated_code.strip('\\n')\n",
    "        \n",
    "        # store\n",
    "        gpt_new_names.append(updated_name)\n",
    "        gpt_new_code.append(updated_code)\n",
    "        gpt_explanation.append(explanation)\n",
    "    else:\n",
    "        # if we error we assume nothing\n",
    "        gpt_new_names.append(None)\n",
    "        gpt_new_code.append(None)\n",
    "        gpt_explanation.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write gpt new names to a file\n",
    "with open('gpt_new_names.txt', 'w') as f:\n",
    "    for name in gpt_new_names:\n",
    "        f.write(f'{name}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "0 pass\n",
      "Processing file 1\n",
      "1 pass\n",
      "Processing file 2\n",
      "2 pass\n",
      "Processing file 3\n",
      "3 pass\n",
      "Processing file 4\n",
      "4 pass\n",
      "Processing file 5\n",
      "5 pass\n",
      "Processing file 6\n",
      "6 pass\n",
      "Processing file 7\n",
      "7 pass\n",
      "Processing file 8\n",
      "8 pass\n",
      "Processing file 9\n",
      "9 pass\n",
      "Processing file 10\n",
      "10 pass\n",
      "Processing file 11\n",
      "11 pass\n",
      "Processing file 12\n",
      "12 pass\n",
      "Processing file 13\n",
      "13 pass\n",
      "Processing file 14\n",
      "14 pass\n",
      "Processing file 15\n",
      "15 second fail\n",
      "Processing file 16\n",
      "16 pass\n",
      "Processing file 17\n",
      "17 pass\n",
      "Processing file 18\n",
      "18 pass\n",
      "Processing file 19\n",
      "19 pass\n",
      "Processing file 20\n",
      "20 pass\n",
      "Processing file 21\n",
      "21 pass\n",
      "Processing file 22\n",
      "22 pass\n",
      "Processing file 23\n",
      "23 second fail\n",
      "Processing file 24\n",
      "24 pass\n",
      "Processing file 25\n",
      "25 pass\n",
      "Processing file 26\n",
      "26 pass\n",
      "Processing file 27\n",
      "27 pass\n",
      "Processing file 28\n",
      "28 pass\n",
      "Processing file 29\n",
      "29 pass\n",
      "Processing file 30\n",
      "30 pass\n",
      "Processing file 31\n",
      "31 pass\n",
      "Processing file 32\n",
      "32 pass\n",
      "Processing file 33\n",
      "33 pass\n",
      "Processing file 34\n",
      "34 pass\n",
      "Processing file 35\n",
      "35 pass\n",
      "Processing file 36\n",
      "36 pass\n",
      "Processing file 37\n",
      "37 pass\n",
      "Processing file 38\n",
      "38 second fail\n",
      "Processing file 39\n",
      "39 pass\n",
      "Processing file 40\n",
      "40 pass\n",
      "Processing file 41\n",
      "41 second fail\n",
      "Processing file 42\n",
      "42 pass\n",
      "Processing file 43\n",
      "43 pass\n",
      "Processing file 44\n",
      "44 pass\n",
      "Processing file 45\n",
      "45 pass\n",
      "Processing file 46\n",
      "46 pass\n",
      "Processing file 47\n",
      "47 pass\n",
      "Processing file 48\n",
      "48 pass\n",
      "Processing file 49\n",
      "49 pass\n",
      "Processing file 50\n",
      "50 pass\n",
      "Processing file 51\n",
      "51 pass\n",
      "Processing file 52\n",
      "52 pass\n",
      "Processing file 53\n",
      "53 pass\n",
      "Processing file 54\n",
      "54 pass\n",
      "Processing file 55\n",
      "55 pass\n",
      "Processing file 56\n",
      "56 pass\n",
      "Processing file 57\n",
      "57 pass\n",
      "Processing file 58\n",
      "58 pass\n",
      "Processing file 59\n",
      "59 pass\n",
      "Processing file 60\n",
      "60 pass\n",
      "Processing file 61\n",
      "61 pass\n",
      "Processing file 62\n",
      "62 pass\n",
      "Processing file 63\n",
      "63 pass\n",
      "Processing file 64\n",
      "64 second fail\n",
      "Processing file 65\n",
      "65 pass\n",
      "Processing file 66\n",
      "66 pass\n",
      "Processing file 67\n",
      "67 pass\n",
      "Processing file 68\n",
      "68 pass\n",
      "Processing file 69\n",
      "69 second fail\n",
      "Processing file 70\n",
      "70 pass\n",
      "Processing file 71\n",
      "71 second fail\n",
      "Processing file 72\n",
      "72 pass\n",
      "Processing file 73\n",
      "73 pass\n",
      "Processing file 74\n",
      "74 pass\n",
      "Processing file 75\n",
      "75 pass\n",
      "Processing file 76\n",
      "76 pass\n",
      "Processing file 77\n",
      "77 pass\n",
      "Processing file 78\n",
      "78 pass\n",
      "Processing file 79\n",
      "79 pass\n",
      "Processing file 80\n",
      "80 second fail\n",
      "Processing file 81\n",
      "81 pass\n",
      "Processing file 82\n",
      "82 pass\n",
      "Processing file 83\n",
      "83 pass\n",
      "Processing file 84\n",
      "84 pass\n",
      "Processing file 85\n",
      "85 pass\n",
      "Processing file 86\n",
      "86 second fail\n",
      "Processing file 87\n",
      "87 pass\n",
      "Processing file 88\n",
      "88 pass\n",
      "Processing file 89\n",
      "89 pass\n",
      "Processing file 90\n",
      "90 pass\n",
      "Processing file 91\n",
      "91 pass\n",
      "Processing file 92\n",
      "92 second fail\n",
      "Processing file 93\n",
      "93 second fail\n",
      "Processing file 94\n",
      "94 second fail\n",
      "Processing file 95\n",
      "95 pass\n",
      "Processing file 96\n",
      "96 second fail\n",
      "Processing file 97\n",
      "97 pass\n",
      "Processing file 98\n",
      "98 pass\n",
      "Processing file 99\n",
      "99 pass\n",
      "Processing file 100\n",
      "100 second fail\n",
      "Processing file 101\n",
      "101 second fail\n",
      "Processing file 102\n",
      "102 pass\n",
      "Processing file 103\n",
      "103 pass\n",
      "Processing file 104\n",
      "104 pass\n",
      "Processing file 105\n",
      "105 pass\n",
      "Processing file 106\n",
      "106 pass\n",
      "Processing file 107\n",
      "107 pass\n",
      "Processing file 108\n",
      "108 pass\n",
      "Processing file 109\n",
      "109 second fail\n",
      "Processing file 110\n",
      "110 pass\n",
      "Processing file 111\n",
      "111 pass\n",
      "Processing file 112\n",
      "112 pass\n",
      "Processing file 113\n",
      "113 pass\n",
      "Processing file 114\n",
      "114 pass\n",
      "Processing file 115\n",
      "115 pass\n",
      "Processing file 116\n",
      "116 pass\n",
      "Processing file 117\n",
      "117 pass\n",
      "Processing file 118\n",
      "118 pass\n",
      "Processing file 119\n",
      "119 pass\n",
      "Processing file 120\n",
      "120 second fail\n",
      "Processing file 121\n",
      "121 second fail\n",
      "Processing file 122\n",
      "122 pass\n",
      "Processing file 123\n",
      "123 pass\n",
      "Processing file 124\n",
      "124 pass\n",
      "Processing file 125\n",
      "125 pass\n",
      "Processing file 126\n",
      "126 pass\n",
      "Processing file 127\n",
      "127 pass\n",
      "Processing file 128\n",
      "128 pass\n",
      "Processing file 129\n",
      "129 pass\n",
      "Processing file 130\n",
      "130 pass\n",
      "Processing file 131\n",
      "131 pass\n",
      "Processing file 132\n",
      "132 second fail\n",
      "Processing file 133\n",
      "133 pass\n",
      "Processing file 134\n",
      "134 pass\n",
      "Processing file 135\n",
      "135 pass\n",
      "Processing file 136\n",
      "136 pass\n",
      "Processing file 137\n",
      "137 second fail\n",
      "Processing file 138\n",
      "138 pass\n",
      "Processing file 139\n",
      "139 pass\n",
      "Processing file 140\n",
      "140 pass\n",
      "Processing file 141\n",
      "141 pass\n",
      "Processing file 142\n",
      "142 pass\n",
      "Processing file 143\n",
      "143 pass\n",
      "Processing file 144\n",
      "144 pass\n",
      "Processing file 145\n",
      "145 pass\n",
      "Processing file 146\n",
      "146 pass\n",
      "Processing file 147\n",
      "147 pass\n",
      "Processing file 148\n",
      "148 pass\n",
      "Processing file 149\n",
      "149 pass\n",
      "Processing file 150\n",
      "150 pass\n",
      "Processing file 151\n",
      "151 pass\n",
      "Processing file 152\n",
      "152 pass\n",
      "Processing file 153\n",
      "153 pass\n",
      "Processing file 154\n",
      "154 pass\n",
      "Processing file 155\n",
      "155 pass\n",
      "Processing file 156\n",
      "156 pass\n",
      "Processing file 157\n",
      "157 pass\n",
      "Processing file 158\n",
      "158 pass\n",
      "Processing file 159\n",
      "159 second fail\n",
      "Processing file 160\n",
      "160 pass\n",
      "Processing file 161\n",
      "161 pass\n",
      "Processing file 162\n",
      "162 pass\n",
      "Processing file 163\n",
      "163 pass\n",
      "Processing file 164\n",
      "164 pass\n",
      "Processing file 165\n",
      "165 pass\n",
      "Processing file 166\n",
      "166 pass\n",
      "Processing file 167\n",
      "167 pass\n",
      "Processing file 168\n",
      "168 pass\n",
      "Processing file 169\n",
      "169 second fail\n",
      "Processing file 170\n",
      "170 pass\n",
      "Processing file 171\n",
      "171 pass\n",
      "Processing file 172\n",
      "172 pass\n",
      "Processing file 173\n",
      "173 pass\n",
      "Processing file 174\n",
      "174 pass\n",
      "Processing file 175\n",
      "175 pass\n",
      "Processing file 176\n",
      "176 pass\n",
      "Processing file 177\n",
      "177 pass\n",
      "Processing file 178\n",
      "178 pass\n",
      "Processing file 179\n",
      "179 pass\n",
      "Processing file 180\n",
      "180 pass\n",
      "Processing file 181\n",
      "181 pass\n",
      "Processing file 182\n",
      "182 pass\n",
      "Processing file 183\n",
      "183 pass\n",
      "Processing file 184\n",
      "184 pass\n",
      "Processing file 185\n",
      "185 pass\n",
      "Processing file 186\n",
      "186 pass\n",
      "Processing file 187\n",
      "187 pass\n",
      "Processing file 188\n",
      "188 pass\n",
      "Processing file 189\n",
      "189 pass\n",
      "Processing file 190\n",
      "190 pass\n",
      "Processing file 191\n",
      "191 second fail\n",
      "Processing file 192\n",
      "192 pass\n",
      "Processing file 193\n",
      "193 pass\n",
      "Processing file 194\n",
      "194 pass\n",
      "Processing file 195\n",
      "195 pass\n",
      "Processing file 196\n",
      "196 pass\n",
      "Processing file 197\n",
      "197 pass\n",
      "Processing file 198\n",
      "198 pass\n",
      "Processing file 199\n",
      "199 pass\n",
      "Processing file 200\n",
      "200 pass\n",
      "Processing file 201\n",
      "201 pass\n",
      "Processing file 202\n",
      "202 pass\n",
      "Processing file 203\n",
      "203 pass\n",
      "Processing file 204\n",
      "204 pass\n",
      "Processing file 205\n",
      "205 pass\n",
      "Processing file 206\n",
      "206 pass\n",
      "Processing file 207\n",
      "207 pass\n",
      "Processing file 208\n",
      "208 pass\n",
      "Processing file 209\n",
      "209 pass\n",
      "Processing file 210\n",
      "210 pass\n",
      "Processing file 211\n",
      "211 pass\n",
      "Processing file 212\n",
      "212 pass\n",
      "Pass count: 190, 89.2018779342723%\n",
      "Fail count: 23, 10.7981220657277%\n",
      "Average length of failed files: 2292.608695652174\n",
      "Average length of passed files: 1296.0684210526315\n"
     ]
    }
   ],
   "source": [
    "# Count the number of times the variable name is/isn't successfully changed\n",
    "import sys\n",
    "sys.path.append('../../determining_files_rename')\n",
    "from ast_determine_usable_items import compare_code\n",
    "\n",
    "pass_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "failed_ids = []\n",
    "length_failed = 0\n",
    "length_passed = 0\n",
    "\n",
    "for i in range(NUM_FILES):\n",
    "    print(\"Processing file\", i)\n",
    "    if gpt_new_names[i] is None or gpt_new_code[i] is None or gpt_explanation[i] is None:\n",
    "        fail_count += 1\n",
    "        failed_ids.append(i)\n",
    "        length_failed += len(random_cells[i])\n",
    "        print(i, \"first fail\")\n",
    "    elif compare_code(random_cells[i], gpt_new_code[i], 'function_def', gpt_new_names[i]):\n",
    "        pass_count += 1\n",
    "        print(i, \"pass\")\n",
    "        length_passed += len(random_cells[i])\n",
    "    else:\n",
    "        fail_count += 1\n",
    "        print(i, \"second fail\")\n",
    "        length_failed += len(random_cells[i])\n",
    "        failed_ids.append(i)\n",
    "\n",
    "print(f'Pass count: {pass_count}, {pass_count / (pass_count + fail_count) * 100}%')\n",
    "print(f'Fail count: {fail_count}, {fail_count / (pass_count + fail_count) * 100}%')\n",
    "\n",
    "print(f'Average length of failed files: {length_failed / fail_count}')\n",
    "print(f'Average length of passed files: {length_passed / pass_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 23,\n",
       " 38,\n",
       " 41,\n",
       " 64,\n",
       " 69,\n",
       " 71,\n",
       " 80,\n",
       " 86,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 96,\n",
       " 100,\n",
       " 101,\n",
       " 109,\n",
       " 120,\n",
       " 121,\n",
       " 132,\n",
       " 137,\n",
       " 159,\n",
       " 169,\n",
       " 191]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compute_gradients'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_new_names[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class two_layer_nn(tf.keras.Model):\n",
      "\n",
      "    def __init__(self, output_size=2, loss_type='cross-entropy'):\n",
      "        super(two_layer_nn, self).__init__()\n",
      "        \" Define here the layers used during the forward-pass \\n            of the neural network.     \\n            Args:\\n                output_size: int (default=2). \\n                loss_type: string, 'cross-entropy' or 'regression' (default='cross-entropy')\\n        \"\n",
      "        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n",
      "        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n",
      "        self.dense_out = tf.layers.Dense(output_size, activation=None)\n",
      "        self.loss_type = loss_type\n",
      "\n",
      "    def predict(self, input_data):\n",
      "        \"\"\" Runs a forward-pass through the network.     \n",
      "            Args:\n",
      "                input_data: 2D tensor of shape (n_samples, n_features).   \n",
      "            Returns:\n",
      "                logits: unnormalized predictions.\n",
      "        \"\"\"\n",
      "        layer_1 = self.dense_1(input_data)\n",
      "        layer_2 = self.dense_2(layer_1)\n",
      "        logits = self.dense_out(layer_2)\n",
      "        return logits\n",
      "\n",
      "    def loss_fn(self, input_data, target):\n",
      "        \"\"\" Defines the loss function used during \n",
      "            training.         \n",
      "        \"\"\"\n",
      "        preds = self.predict(input_data)\n",
      "        if self.loss_type == 'cross-entropy':\n",
      "            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n",
      "        else:\n",
      "            loss = tf.losses.mean_squared_error(target, preds)\n",
      "        return loss\n",
      "\n",
      "    def function_def(self, input_data, target):\n",
      "        \"\"\" Dynamically computes the gradients of the loss value\n",
      "            with respect to the parameters of the model, in each\n",
      "            forward pass.\n",
      "        \"\"\"\n",
      "        with tfe.GradientTape() as tape:\n",
      "            loss = self.loss_fn(input_data, target)\n",
      "        return tape.gradient(loss, self.variables)\n",
      "\n",
      "    def fit(self, input_data, target, optimizer, num_epochs=500, verbose=50, track_accuracy=True):\n",
      "        \"\"\" Function to train the model, using the selected optimizer and\n",
      "            for the desired number of epochs. It also stores the accuracy\n",
      "            of the model after each epoch.\n",
      "        \"\"\"\n",
      "        if track_accuracy:\n",
      "            self.hist_accuracy = []\n",
      "            accuracy = tfe.metrics.Accuracy()\n",
      "        for i in range(num_epochs):\n",
      "            grads = self.grads_fn(input_data, target)\n",
      "            optimizer.apply_gradients(zip(grads, self.variables))\n",
      "            if track_accuracy:\n",
      "                logits = self.predict(X)\n",
      "                preds = tf.argmax(logits, axis=1)\n",
      "                accuracy(preds, target)\n",
      "                self.hist_accuracy.append(accuracy.result())\n",
      "                accuracy.init_variables()\n"
     ]
    }
   ],
   "source": [
    "print(random_cells[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class two_layer_nn(tf.keras.Model):\n",
      "\n",
      "    def __init__(self, output_size=2, loss_type='cross-entropy'):\n",
      "        super(two_layer_nn, self).__init__()\n",
      "        \" Define here the layers used during the forward-pass \\n            of the neural network.     \\n            Args:\\n                output_size: int (default=2). \\n                loss_type: string, 'cross-entropy' or 'regression' (default='cross-entropy')\\n        \"\n",
      "        self.dense_1 = tf.layers.Dense(20, activation=tf.nn.relu)\n",
      "        self.dense_2 = tf.layers.Dense(10, activation=tf.nn.relu)\n",
      "        self.dense_out = tf.layers.Dense(output_size, activation=None)\n",
      "        self.loss_type = loss_type\n",
      "\n",
      "    def predict(self, input_data):\n",
      "        \"\"\" Runs a forward-pass through the network.     \n",
      "            Args:\n",
      "                input_data: 2D tensor of shape (n_samples, n_features).   \n",
      "            Returns:\n",
      "                logits: unnormalized predictions.\n",
      "        \"\"\"\n",
      "        layer_1 = self.dense_1(input_data)\n",
      "        layer_2 = self.dense_2(layer_1)\n",
      "        logits = self.dense_out(layer_2)\n",
      "        return logits\n",
      "\n",
      "    def loss_fn(self, input_data, target):\n",
      "        \"\"\" Defines the loss function used during \n",
      "            training.         \n",
      "        \"\"\"\n",
      "        preds = self.predict(input_data)\n",
      "        if self.loss_type == 'cross-entropy':\n",
      "            loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\n",
      "        else:\n",
      "            loss = tf.losses.mean_squared_error(target, preds)\n",
      "        return loss\n",
      "\n",
      "    def compute_gradients(self, input_data, target):\n",
      "        \"\"\" Computes the gradients of the loss value\n",
      "            with respect to the parameters of the model, in each\n",
      "            forward pass.\n",
      "        \"\"\"\n",
      "        with tfe.GradientTape() as tape:\n",
      "            loss = self.loss_fn(input_data, target)\n",
      "        return tape.gradient(loss, self.variables)\n",
      "\n",
      "    def fit(self, input_data, target, optimizer, num_epochs=500, verbose=50, track_accuracy=True):\n",
      "        \"\"\" Function to train the model, using the selected optimizer and\n",
      "            for the desired number of epochs. It also stores the accuracy\n",
      "            of the model after each epoch.\n",
      "        \"\"\"\n",
      "        if track_accuracy:\n",
      "            self.hist_accuracy = []\n",
      "            accuracy = tfe.metrics.Accuracy()\n",
      "        for i in range(num_epochs):\n",
      "            grads = self.compute_gradients(input_data, target)\n",
      "            optimizer.apply_gradients(zip(grads, self.variables))\n",
      "            if track_accuracy:\n",
      "                logits = self.predict(X)\n",
      "                preds = tf.argmax(logits, axis=1)\n",
      "                accuracy(preds, target)\n",
      "                self.hist_accuracy.append(accuracy.result())\n",
      "                accuracy.init_variables()\n"
     ]
    }
   ],
   "source": [
    "print(gpt_new_code[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_code(random_cells[23], gpt_new_code[23], 'grads_fn', gpt_new_names[23])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
