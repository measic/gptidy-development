('https://raw.githubusercontent.com/madalinabuzau/tensorflow-eager-tutorials/f319dbdea756dbf50e68adb05f14597909a4d4c7/02_using_metrics_in_eager_mode.ipynb', '## Simple tutorials on deep learning using TensorFlow Eager\n\nThis repo aims to help people who would like to start getting hands-on experience with deep learning using the TensorFlow Eager mode. TensorFlow Eager mode lets you build neural networks as easy as you would do with Numpy, with the huge advantage that it provides automatic differentiation (no more handwritten backprop. YAAAY!). It can ran also on GPUs making the neural networks training significantly faster.\n\nI will try to make the tutorials accessible for everyone, thus I will try to work on problems that do not require a GPU to work on.\n\n**TensorFlow Version used in the tutorials - 1.7**\n\n### List of tutorials available:\n#### Getting started\n---\n* **01. Build a simple neural network** - This tutorial shows you how to build and train a one-hidden layer neural network using the Eager mode of TensorFlow, on a synthetically generated dataset.\n<img src="tutorials_graphics/01_flowchart.png" style="display:block; margin-left: auto; margin-right: auto; width: 80%;"/>\n\n* **02. Using metrics in Eager mode** - This tutorial shows you how to use metrics\nthat are compatible with Eager mode, for three types of machine learning problems (multi-classification, imbalanced dataset and regression).\n<img src="tutorials_graphics/02_flowchart.png" style="display:block; margin-left: auto; margin-right: auto; width: 80%;"/>\n#### Convolutional neural networks\n----\n* **03. Build a CNN for emotion recognition** - This tutorial shows you how to build a CNN from scratch using the TensorFlow Eager API and the FER2013 dataset. At the end of the tutorial you will be able to test the network on yourself using a webcam. Very fun exercise!\n<img src="tutorials_graphics/03_flowchart.png" style="display:block; margin-left: auto; margin-right: auto; width: 80%;"/>\n\n#### Recurrent neural networks\n----\n* **04. Build a dynamic RNN for sequence classification** - Learn how to work with variable sequence input data. This tutorial shows you how to build a dynamic RNN using the TensorFlow Eager API and the Stanford Large Movie Review Dataset.\n<img src="tutorials_graphics/04_flowchart.png" style="display:block; margin-left: auto; margin-right: auto; width: 80%;"/>\n* **05. Build a RNN for time series regression** - Learn how to build a RNN for timeseries forecasting.\n<img src="tutorials_graphics/05_flowchart.png" style="display:block; margin-left: auto; margin-right: auto; width: 80%;"/>\n\n\n\nRequests for tutorials:\n----\n* If you have any requests for a specific tutorial please let me know.\n\nImprovement advice:\n----\n* Please let me know if you have any suggestions to improve these tutorials. The aim is to help you getting a good grasp of this framework but I am also looking to improve my programming skills so any feedback will be really appreciated :)!')
('https://raw.githubusercontent.com/jpbarto/ml_lifecycle_lab/cc2c1a47f290cd415de6e5c8d930dee99ea18e57/03%20Hyperparameter%20tuning.ipynb', '# Machine learning lifecycle lab\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jpbarto/ml_lifecycle_lab/master)\n\nA collection of notebooks for walking through the typical ML lifecycle from data cleaning through to model hosting using Amazon SageMaker.\n\nA typical ML lifecycle will look something like...\n 1. Identify a business problem or question which ML can answer\n 1. Identify the data sources available to describe the problem space\n 1. Acquire and cleanse the data or a sample of the data\n 1. Engineer a feature set from the data or data sample so that everything has meaning and relevance\n 1. Apply this cleansing and feature engineering logic to the full data set\n 1. Spot check multiple ML algorithms against a sample of the feature set to assess which algorithm is likely to give the best result\n 1. Select one or more algorithms and perform hyperparameter optimization to determine the best configuration parameters, use a sample of the feature set\n 1. Train a model using the best performing algorithm and hyperparameters on the full training feature set\n 1. Test the model on a control or test feature set to produce a baseline for performance\n 1. Deploy the model for consumption by the business (Lambda, mobile device, container, etc)\n   1. Consider how future observations will be engineered in preparation for inference\n 1. Monitor the model for context drift\n \nFor this collection of labs we will start by defining a business problem and then work through the process through to model deployment.  \n\n \n Table of contents\n ---\n\n 1. [Feature engineering](./01 Feature engineering.ipynb)\n This notebook walks through acquiring the data, cleaning it and then engineering a base feature set which can then be prepared for ML training.\n 1. [ML algorithm spot check](./02 Algorithm spot check.ipynb)\n This notebook walks through transforming the cleansed data to assess the performance of many ML algorithms.\n 1. [Hyperparameter optimization](./03 Hyperparameter tuning.ipynb)\n This notebook walks through performing HPO on an algorithm and a subset of the feature set before performing a full scale training job.\n 1. [Training your model](./04 Training.ipynb)\n This notebook walks through performing a full scale training job of your model.\n 1. [Hosting and usage](./05 Host and infer.ipynb)\n This notebook walks through how to host a trained model and use it to make predictions.\n \n ## Further reading\n - [What’s your ML test score? A rubric for ML production systems](https://ai.google/research/pubs/pub45742)\n - [Automation of data profiling](https://github.com/pandas-profiling/pandas-profiling)\n - [Automated data profiling example](http://nbviewer.jupyter.org/github/JosPolfliet/pandas-profiling/blob/master/examples/meteorites.ipynb)\n - [Automated data profiling for Spark](https://github.com/julioasotodv/spark-df-profiling)\n - [Machine Learning: The High Interest Credit Card of Technical Debt](https://ai.google/research/pubs/pub43146)\n\n---\n\n ## Resources\n - [Forecasting time series with dynamic deep learning on AWS](https://aws.amazon.com/blogs/machine-learning/forecasting-time-series-with-dynamic-deep-learning-on-aws/)')
('https://raw.githubusercontent.com/odebeir/info-h-500-501/e42c5ef51dd2553e03effbc124ec3a35ac1383fe/05-Morphomathematics/01-Operators.ipynb', 'Introduction to image processing \n================================\n\n[![Build Status](https://travis-ci.org/odebeir/info-h-500-501.png?branch=master)](https://travis-ci.org/odebeir/info-h-500-501)\n\nOutline\n-------\n\nTo browse the content, [start here](http://nbviewer.ipython.org/github/odebeir/info-h-500-501/blob/master/Index.ipynb).\n\nTOC should be updated from .ipynb\n\nHow to read the content\n-----------------------\n\nThere are two ways to read *Introduction to image processing*:\n\n* The *recommended* way to read the book is to download and run the IPython notebooks interactively. You can do this by cloning the GitHub repository, installing the package and its dependencies, and running the notebooks interactively. Instructions for doing this are provided below in the **Installation** section.\n\n* The *easiest* way to read the book is to view the static notebooks online using [nbviewer](http://nbviewer.ipython.org/). You should [start here](http://nbviewer.ipython.org/github/odebeir/info-h-500-501/blob/master/Index.ipynb).\n\nIf you\'re new to using IPython or the IPython Notebook, you can find more information at the [IPython website](http://www.ipython.org/), [IPython Notebook website](http://ipython.org/notebook), and the [IPython Notebook example gallery](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks).\n\nInstallation\n------------\n\nIf you\'re going to read the book interactively (recommended), you\'ll need to clone this repository, install some dependencies, and launch the IPython Notebook. For example, the following commands should work for Linux and Mac OS X users:\n\n    anaconda distribution should do the job\n    conda install scikit-image \n\nFinally, launch the IPython Notebook to get started (be sure that you\'re in the ``Introduction to image processing`` directory when you run this command):\n\n    ipython notebook --pylab inline Index.ipynb\n\nThat\'s it!\n\nAcknowledgements\n----------------\n\nULB\n\ninspired by Greg Caporaso\'s Introduction to <a href="https://github.com/gregcaporaso/An-Introduction-To-Applied-Bioinformatics">Applied Bioinformatics</a>\n\nget style from [CFDPython](https://github.com/barbagroup/CFDPython)\n\nLicense\n-------\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>\n\n<br />')
('https://raw.githubusercontent.com/188xuhe/handson-ml/8f07ff0d3082d89e8d81474674ee436fa3bcf6a2/13_convolutional_neural_networks.ipynb', "Machine Learning Notebooks\n==========================\n\nThis project aims at teaching you the fundamentals of Machine Learning in\npython. It contains the example code and solutions to the exercises in my O'Reilly book [Hands-on Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do):\n\n[![book](http://akamaicovers.oreilly.com/images/0636920052289/cat.gif)](http://shop.oreilly.com/product/0636920052289.do)\n\nSimply open the [Jupyter](http://jupyter.org/) notebooks you are interested in:\n\n* Using [jupyter.org's notebook viewer](http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb)\n    * note: [github.com's notebook viewer](https://github.com/ageron/handson-ml/blob/master/index.ipynb) also works but it is slower and the math formulas are not displayed correctly,\n* or by cloning this repository and running Jupyter locally. This option lets you play around with the code. In this case, follow the installation instructions below.\n\n# Installation\n\nFirst, you will need to install [git](https://git-scm.com/), if you don't have it already.\n\nNext, clone this repository by opening a terminal and typing the following commands:\n\n    $ cd $HOME  # or any other development directory you prefer\n    $ git clone https://github.com/ageron/handson-ml.git\n    $ cd handson-ml\n\nIf you want to go through chapter 16 on Reinforcement Learning, you will need to [install OpenAI gym](https://gym.openai.com/docs) and its dependencies for Atari simulations.\n\nIf you are familiar with Python and you know how to install Python libraries, go ahead and install the libraries listed in `requirements.txt` and jump to the [Starting Jupyter](#starting-jupyter) section. If you need detailed instructions, please read on.\n\n## Python & Required Libraries\nOf course, you obviously need Python. Python 2 is already preinstalled on most systems nowadays, and sometimes even Python 3. You can check which version(s) you have by typing the following commands:\n\n    $ python --version   # for Python 2\n    $ python3 --version  # for Python 3\n\nAny Python 3 version should be fine, preferably ≥3.5. If you don't have Python 3, I recommend installing it (Python ≥2.6 should work, but it is deprecated so Python 3 is preferable). To do so, you have several options: on Windows or MacOSX, you can just download it from [python.org](https://www.python.org/downloads/). On MacOSX, you can alternatively use [MacPorts](https://www.macports.org/) or [Homebrew](https://brew.sh/). On Linux, unless you know what you are doing, you should use your system's packaging system. For example, on Debian or Ubuntu, type:\n\n    $ sudo apt-get update\n    $ sudo apt-get install python3\n\nAnother option is to download and install [Anaconda](https://www.continuum.io/downloads). This is a package that includes both Python and many scientific libraries. You should prefer the Python 3 version.\n\nIf you choose to use Anaconda, read the next section, or else jump to the [Using pip](#using-pip) section.\n\n## Using Anaconda\nWhen using Anaconda, you can optionally create an isolated Python environment dedicated to this project. This is recommended as it makes it possible to have a different environment for each project (e.g. one for this project), with potentially different libraries and library versions:\n\n    $ conda create -n mlbook python=3.5 anaconda\n    $ source activate mlbook\n\nThis creates a fresh Python 3.5 environment called `mlbook` (you can change the name if you want to), and it activates it. This environment contains all the scientific libraries that come with Anaconda. This includes all the libraries we will need (NumPy, Matplotlib, Pandas, Jupyter and a few others), except for TensorFlow, so let's install it:\n\n    $ conda install -n mlbook -c conda-forge tensorflow=1.0.0\n\nThis installs TensorFlow 1.0.0 in the `mlbook` environment (fetching it from the `conda-forge` repository). If you chose not to create an `mlbook` environment, then just remove the `-n mlbook` option.\n\nNext, you can optionally install Jupyter extensions. These are useful to have nice tables of contents in the notebooks, but they are not required.\n\n    $ conda install -n mlbook -c conda-forge jupyter_contrib_nbextensions\n\nYou are all set! Next, jump to the [Starting Jupyter](#starting-jupyter) section.\n\n## Using pip \nIf you are not using Anaconda, you need to install several scientific Python libraries that are necessary for this project, in particular NumPy, Matplotlib, Pandas, Jupyter and TensorFlow (and a few others). For this, you can either use Python's integrated packaging system, pip, or you may prefer to use your system's own packaging system (if available, e.g. on Linux, or on MacOSX when using MacPorts or Homebrew). The advantage of using pip is that it is easy to create multiple isolated Python environments with different libraries and different library versions (e.g. one environment for each project). The advantage of using your system's packaging system is that there is less risk of having conflicts between your Python libraries and your system's other packages. Since I have many projects with different library requirements, I prefer to use pip with isolated environments.\n\nThese are the commands you need to type in a terminal if you want to use pip to install the required libraries. Note: in all the following commands, if you chose to use Python 2 rather than Python 3, you must replace `pip3` with `pip`, and `python3` with `python`.\n\nFirst you need to make sure you have the latest version of pip installed:\n\n    $ pip3 install --user --upgrade pip\n\nThe `--user` option will install the latest version of pip only for the current user. If you prefer to install it system wide (i.e. for all users), you must have administrator rights (e.g. use `sudo pip3` instead of `pip3` on Linux), and you should remove the `--user` option. The same is true of the command below that uses the `--user` option.\n\nNext, you can optionally create an isolated environment. This is recommended as it makes it possible to have a different environment for each project (e.g. one for this project), with potentially very different libraries, and different versions:\n\n    $ pip3 install --user --upgrade virtualenv\n    $ virtualenv -p `which python3` env\n\nThis creates a new directory called `env` in the current directory, containing an isolated Python environment based on Python 3. If you installed multiple versions of Python 3 on your system, you can replace `` `which python3` `` with the path to the Python executable you prefer to use.\n\nNow you must activate this environment. You will need to run this command every time you want to use this environment.\n\n    $ source ./env/bin/activate\n\nNext, use pip to install the required python packages. If you are not using virtualenv, you should add the `--user` option (alternatively you could install the libraries system-wide, but this will probably require administrator rights, e.g. using `sudo pip3` instead of `pip3` on Linux).\n\n    $ pip3 install --upgrade -r requirements.txt\n\nGreat! You're all set, you just need to start Jupyter now.\n\n## Starting Jupyter\nIf you want to use the Jupyter extensions (optional, they are mainly useful to have nice tables of contents), you first need to install them:\n\n    $ jupyter contrib nbextension install --user\n\nThen you can activate an extension, such as the Table of Contents (2) extension:\n\n    $ jupyter nbextension enable toc2/main\n\nOkay! You can now start Jupyter, simply type:\n\n    $ jupyter notebook\n\nThis should open up your browser, and you should see Jupyter's tree view, with the contents of the current directory. If your browser does not open automatically, visit [localhost:8888](http://localhost:8888/tree). Click on `index.ipynb` to get started!\n\nNote: you can also visit [http://localhost:8888/nbextensions](http://localhost:8888/nbextensions) to activate and configure Jupyter extensions.\n\nCongrats! You are ready to learn Machine Learning, hands on!")
('https://raw.githubusercontent.com/gopala-kr/qiskit-tutorial/cd3e3a8a817d8a8edaac2960a63309050e51ebc5/1_introduction/running_on_IBM_DSX.ipynb', None)
('https://raw.githubusercontent.com/CFerraren/PyBank/7e4811e564e8fb049978d8123bba30651a74a590/2-EDA.ipynb', '## Bank Financial Record Analysis\n\n### Background\n\nIn this project, I created multiple python notebooks; first is intended for data-preparation and second for further data analysis and the rest is to check for data normality, stationarity and to predict and forecast our revenue target.  \n\nIn this end-to-end analysis,  I learned to import modules to read csv to pandas dataframe, format datatypes, combined mutli-dataframes using the concatenation and groupby method, merge of dataframes using the full-outer merge, adding of columns and at the same dropping of columns, visualized to understand the data, check and handle missing values, check for duplicates, munged data for time-series analysis and visualized to discover **Trend**, **Seasonal**, and **Cyclical** patterns, create machine-learning model to **predict revenue using Simple Uni-Variate Regression Analysis**. I also used Tableau for data blending, and to create pretty visualization and interactive dashboard.\n\nThis challenge encompasses real-world situation where my Python and data science skills can come in handy. \n\n\n![Revenue](data/image/revenue-per-lead.jpg)\n\n\nTools used to analyze data and create visualization and interactive dashboard:\n\n* Python Pandas\n\n* Matplotlib and Seaborn\n\n* Tableau for data blending and Visualization\n\n* Sckit-Learn for data-prep in Machine Learning.\n\n* Keras for Machine Learning model')
('https://raw.githubusercontent.com/tjwei/NDHU_AM1130/e22932b9571fc0ed7435e1c1468fece50adcbb8a/2018_Week07_Paint.ipynb', '# NDHU AM1130 軟體實作與計算實驗 2018\n東華應數軟體實作 2018\n\n* [簡介](Intro.md): 簡介\n* 前四週 Python 程式語言 [Python_Basic/](Python_Basic/)\n    * [控制流程語法] https://www.youtube.com/watch?v=rdzbJFXsM90&feature=youtu.be\n    * [函數](Python_Basic/Function/)\n    * [使用 List](Python_Basic/List.ipynb):  \n    * [module and class](Python_Basic/Module_and_Class.ipynb): \n    * 登記 cscircles 的 guru 為 tjw \n* Week05: 上機測驗\n* Week06-08: Numpy\n    * Pillow, numpy 畫圖 (參考 https://github.com/tjwei/numpy_tutorial/tree/master/numpy )\n    * numpy more (參考 https://github.com/tjwei/numpy_tutorial/tree/master/numpy )\n* Week09-11: 機器學習簡介參考 https://github.com/tjwei/HackNTU_Data_2017/tree/master/Week02 \n* Week12: Facebook API (參考 https://github.com/tjwei/HackNTU_Data_2017/tree/master/Week09 )\n* Micro:bit:\n    * http://python.microbit.org/v/1\n    * http://microbit-micropython.readthedocs.io/en/latest/index.html\n    * https://codewith.mu/\n    * https://github.com/carlosperate/awesome-microbit\n    * radio: http://microbit-micropython.readthedocs.io/en/latest/tutorials/radio.html\n    \n\n如果 ipynb 檔案無法線上看到, 可以使用\n\n\nhttps://nbviewer.jupyter.org/github/tjwei/NDHUAM2017/tree/master/\n\nAzure notebook https://notebooks.azure.com/tjwei/libraries/NDHUAM2017\n\nGoogle CoLab https://colab.research.google.com/')
('https://raw.githubusercontent.com/hfoffani/tensorflow-dl/e1f2947e31b5d94c0c834ffecbc4714529463f54/4_convolutions.ipynb', '# Deep Learning Examples in Tensorflow.\n\n\nThis repo contains a handful of Deep Learning examples written in Python Jupyter notebooks.\n\n\n### Directory\n\n - `1_notmnist.ipynb`\n    Processes MNIST data.\n - `2_fullyconnected.ipynb`\n    Uses a fully connected layer for prediction.\n - `3_regularization.ipynb`\n    Adds regularization.\n - `4_convolutions.ipynb`\n    Implements Convoluational Network.\n - `5_word2vec.ipynb`\n    Implement Word2Vec.\n - `6_lstm.ipynb`\n    Basic LSTM network.\n - `62a_lstm.ipynb`\n    An LSTM network.\n - `62b_lstm.ipynb`\n    An enhanced LSTM.\n - `63_lstm.ipynb`\n    Another LSTM.\n - `6_translate.ipynb`\n    A NN translator.\n\nThere are other notebooks as well.\n\n\n### Installation.\n\nFor Mac follow the following instructions.\n\nUpdate XCode and install `brew` and `pyenv`\n\n    xcode-select --install\n    brew\n\n    brew install pyenv\n    brew install pyenv-virtualenv\n\n\nDue to Matplotlib you will need to install a "Framework" Python.\n\n    git clone https://github.com/s1341/pyenv-alias.git ~/.pyenv/plugins/pyenv-alias\n\n    VERSION_ALIAS="framework_352" PYTHON_CONFIGURE_OPTS="--enable-framework CC=clang CFLAGS=-I$(brew --prefix openssl)/include LDFLAGS=-L$(brew --prefix openssl)/lib" pyenv install 3.5.2\n\n    pyenv virtualenv framework_352 tf\n\n\nModify your `.bash_profile` for Jupyter. In `~/.bash_profile` add the following lines:\n\n    # needed for Jupyter\n    export LC_ALL=en_US.UTF-8\n\nNow you are able to Install all the Python packages.\n\n    pip install -r requirements.txt\n    pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py3-none-any.whl')
('https://raw.githubusercontent.com/arvindshekar07/MachineLearning/eaa3d6d72181d9318017338de3c16f20d93f44aa/Assignement1/Ass1.ipynb', '# MachineLearning\nThis a machinelearning assignmnent project \n\nThe topics covered through this assignmnent are')
('https://raw.githubusercontent.com/rambandaru/MAE6286/f7e8401d7e542287ee148549908f45ce0be98190/Assignments/Assign_2.ipynb', 'This my repository for Numerical Techniques class\n\nAuthor - Ram Bandaru')
('https://raw.githubusercontent.com/tkawchak/KaggleResearch/b6bb9063f390db291ac653869aca5f3ebfcb072f/AutomatedModelBuilding/DataProcessing.ipynb', '# Kaggle Research\n\nThis repository contains some code and experiments that I am using to explore automated model building using some kaggle datasets and competitions.\n\nThe [AutomatedModelBuilding](https://github.com/tkawchak/KaggleResearch/tree/master/AutomatedModelBuilding) Folder contains more information on the Automated Model Builder.')
('https://raw.githubusercontent.com/ZaraYar/Analyzing-NYC-High-School-Data/88d2fa7974800142157e068fe7a97b7f345bcfeb/Basics.ipynb', "# Analyzing-NYC-High-School-Data\nI am going to explore relationships between SAT scores and demographic factors in New York City public schools. For a brief bit of background, the SAT, or Scholastic Aptitude Test, is a test that high school seniors in the U.S. take every year. The SAT has three sections, each of which is worth a maximum of 800 points. Colleges use the SAT to determine which students to admit. High average SAT scores are usually indicative of a good school. \nNew York City has published data on student SAT scores by high school, along with additional demographic data sets. We combine the following data sets into a single, clean pandas dataframe:\n\n- SAT scores by school - SAT scores for each high school in New York City\n- School attendance - Attendance information for each school in New York City\n- Class size - Information on class size for each school\n- AP test results - Advanced Placement (AP) exam results for each high school (passing an optional AP exam in a particular subject can earn a student college credit in that subject)\n- Graduation outcomes - The percentage of students who graduated, and other outcome information\n- Demographics - Demographic information for each school\n- School survey - Surveys of parents, teachers, and students at each school\n\nNew York City has a significant immigrant population and is very diverse, so comparing demographic factors such as race, income, and gender with SAT scores is a good way to determine whether the SAT is a fair test. For example, if certain racial groups consistently perform better on the SAT, we would have some evidence that the SAT is unfair.\n\nBefore I move into coding, I'll need to do some background research. A thorough understanding of the data will help me avoid costly mistakes, such as thinking that a column represents something other than what it does. Background research will also give me a better understanding of how to combine and analyze the data.\n\nIn this case, I'll want to research:\n\n- New York City\n- The SAT\n- Schools in New York City\n\nOur data\n\nI can learn a few different things from these resources. For example:\n\n- Only high school students take the SAT, so we'll want to focus on high schools.\n- New York City is made up of five boroughs, which are essentially distinct regions.\n- New York City schools fall within several different school districts, each of which can contains dozens of schools.\n\nOur data sets include several different types of schools. I'll need to clean them so that we can focus on high schools only.\n\nEach school in New York City has a unique code called a DBN, or district borough number.\n\nAggregating data by district will allow me to use the district mapping data to plot district-by-district differences.")
('https://raw.githubusercontent.com/tukichen/Bayesian_AB_testing/90ea9fb3c60374b53b7ffc05c87d74214bc6e38a/Bayesian_AB_testing.ipynb', '# Bayesian A/B testing\n\nThis notebook presents step by step instruction how to build a Bayesian A/B Test Calculator with visualization of results using R.\n* The Shiny web app under construction is https://qiaolinchen.shinyapps.io/ab_test/. \n* Another way to use is to run on R console: \n\ninstall.packages("shiny");  library(shiny); runGitHub("Bayesian_AB_testing", "tukichen")\n\n## Functions of this web app:\n### 1. Bayesian A/B test Calculator: \n- Perform a single A/B testing using input test data and prior parameters \n- Summarize the Bayes factor, point estimate of rate change with credible interval, probability of variant better than default, and a frequentist p-value.\n- Visualizae prior and posterior probabilities\n\n### 2. Compare performance of Bayesian and Frequentist A/B Tesing using Simulation\n* Simulate datasets for which true test diferences are known.\n- Summarize and visualize the conversion rates change over time\n- Visualize the trends of conversion rate change over time with CI (Bayesian credible interval or frequentist confidence interval) over time\n- Visualize the trends of the following quantities over time:  Bayes factor, p-value, Uplift probability of variant better than control \n\n### 3. Upload data and perform Bayesian A/B test: \n- Perform A/B test on datasets uploaded by users\n- Give the same summary and visualization as above in simulation part\n\n\n## Backgroups and Methods\n\nMost A/B test approaches are centered around frequentist hypothesis tests used to come up with a point estimate (probability of rejecting the null) of a hard-to-interpret value. Oftentimes, the statistician or data scientist laying down the groundwork for the A/B test will have to do a power test to determine sample size and then interface with a Product Manager or Marketing Exec in order to relay the results. This quickly gets messy in terms of interpretability. More importantly it is simply not as robust as A/B testing given informative priors and the ability to inspect an entire distribution over a parameter, not just a point estimate.\n\n### Bayesian A/B testing\nBayesian methods provide several benefits over frequentist methods in the context of A/B tests - namely in interpretability. Instead of p-values you get direct probabilities on whether A is better than B (and by how much). Instead of point estimates your posterior distributions are parametrized random variables which can be summarized any number of ways. Bayesian tests are also immune to ‘peeking’ and are thus valid whenever a test is stopped.\n\n### Methods\n\nUnlike a frequentist method, in a Bayesian approach you first encapsulate your prior beliefs mathematically. This involves choosing a distribution over which you believe your parameter might lie. As you expose groups to different tests, you collect the data and combine it with the prior to get the posterior distribution over the parameter(s) in question. Mathematically, you are looking for P(parameter | data) which is a combination of the prior and posterior (the math, while relatively straightforward, is outside of the scope of this brief intro).')
('https://raw.githubusercontent.com/kangwonlee/test-pytest-xdist-py36/986ce05294635d317b4c26caf0a362dd42816085/Ch05_Stress.in.Beams/ex05.004.numpy_sympy.BendingStress.W200.section_cantilever_m.ipynb', '# 재료역학 SciPy 사례집<br>Mechanics of Materials in SciPy\n\n주 참고문헌 : Pytel 외 저, 이주성 외 역, 재료역학 2판, 한티미디어, 2013.<br>\nMain Text: Pytel et. al, Mechanics of Materials, 2nd Ed, 2013.\n\n### 설치 항목 Software to install\n\n#### 파이썬 프로그래밍 언어 Python Programming Language\n##### [Anaconda 5.x Python 3.6.x version](https://www.continuum.io/downloads) <br>\n* 명령행에서 실행해야 할 경우, 경로에 추가하도록 설정 To run from a command line, configure to add python to the PATH\n* 경로 이름에 한글을 사용할 수 없음 Use ASCII characters for path name\n* 윈도우즈의 경우, 설치 후 [가상 환경 생성, 전환 2회 이상](https://graspthegist.com/post/learn-conda-1/) 확인 추천<br>For Windows, after install, please check [creating a virtual environment](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) and [switching between environment](https://conda.io/docs/user-guide/tasks/manage-environments.html#activating-an-environment) more than twice.\n\n#### 깃 버전 관리 소프트웨어 Git Version Control Software\n##### CUI\n아래 소프트웨어를 설치 Please install following software\n* [Git for Windows](https://git-scm.com/download/win)\n\n간단한 추천 사항 A few simple recommendations\n* 명령행에서 사용 Command line interface<br>\n* 실습실 환경에서는 credential manager 설치 삼가<br>Credential manager may not be a best choice for a computer lab\n* 아래 IDE 가운데 Microsoft Visual Studio Code 선택시 다음과 같이 생각해 볼 수 있음<br>\nTo use Microsoft Visual Studio Code among IDEs below, an administrator may consider followings<br>\n\n| 항목<br>Key | 값<br>Value |\n|:-------:|:-------:|\n| git 기본 편집기 <br> Default editor for Git | MS VS Code |\n| 경로 환경 조정 <br> Adjusting Path environment | Use Git from the Windows Command Prompt |\n| 터미널 에뮬레이터 <br>Terminal emulator | Use Windows\' default console window |\n| 자격 인증 관리자 활성화  <br> Enable Git Credential Manager | 전산 실습 환경의 경우, 선택 해제<br>If computer lab, unselect |\n\n이후 MS vscode 에서도 한두가지 추가 설정을 실시하면 내장 터미널로 `git-bash` 를 사용할 수 있음<br>\nWith a few additional configurations in MS vscode, one could use `git-bash` as the integrated terminal.\n\n#### GUI\n아래 가운데 택일\xa0Choose one from below \n* [SourceTree](https://www.sourcetreeapp.com/download/) \n* [Github Desktop](https://desktop.github.com/)\n\n#### 통합 개발 환경 Integrated Development Environment\n아래 가운데 택일\xa0Choose one from below \n##### Spyder\n* Anaconda 5.x 와 함께 설치됨.<br>Anaconda 5.x includes spyder.\n##### [PyCharm Community](https://www.jetbrains.com/pycharm/download/)\n* PyCharm 을 실행시키기 위해 [Java Development Kit](http://www.oracle.com/technetwork/java/javase/downloads/index.html) 를 설치해야 할 수 있음 (2016 09)<br>\nPyCharm may need [Java Development Kit](http://www.oracle.com/technetwork/java/javase/downloads/index.html) to run.\n##### [Microsoft Visual Studio Code](https://code.visualstudio.com/download)\n* Anaconda 설치 후 설치 선택 가능<br>\nInstallation [button](https://blogs.msdn.microsoft.com/pythonengineering/2018/02/15/visual-studio-code-is-now-shipping-with-anaconda/) available at the end of Anaconda installation<br>\n* [다운로드](https://code.visualstudio.com/download) 받아서 설치도 가능<br>\nPossible to [download](https://code.visualstudio.com/download) and install from the website<br>\n[Setup Overview](https://code.visualstudio.com/docs/setup/setup-overview) / \n[Python Configuration Instruction](https://code.visualstudio.com/docs/python/python-tutorial)\n* Windows 에서 `git-bash`를 내장 터미널로 사용하려면 다음과 같은 추가 설정이 필요함.\\[[참고](https://code.visualstudio.com/docs/editor/integrated-terminal)\\]<br>\nIn Windows, to use `git-bash` as the integrated terminal, following additional settings would eable it.\\[[Ref](https://code.visualstudio.com/docs/editor/integrated-terminal)\\]\n\n1. File > Preferences > Settings[참고 Ref](https://code.visualstudio.com/docs/getstarted/settings)\\]\n2. \n```json\n"terminal.integrated.shell.windows": **path to bash.exe here**\n```\n\n#### 설치 동영상<br>Installation video\n[![설치 동영상 Installation video](https://i.ytimg.com/vi/NAQn1jQws3Q/hqdefault.jpg)](https://www.youtube.com/embed/videoseries?list=PLA6B0Lmr9oJOuvxMPNjDcnAfmqw907Bqy)\n\n### `jupyter` 노트북 실행시키는 법<br>How to start a `jupyter` notebook\n이 저장소 는 주로 [`jupyter` 노트북](http://blog.ncsoft.com/?p=21870)으로 만들어져 있음.<br>\nThis repository is mostly written in [`jupyter` notebook](http://arogozhnikov.github.io/2016/09/10/jupyter-features.html).<br>\n`jupyter` 노트북은 웹브라우저를 통해 프로그램 코드를 수정 실행하고 LaTex 수식을 포함한 문서 작성이 가능함.<br>\nThrough a web browser, `jupyter` notebook enables editing & running program codes and writing documents including LaTex equations.<br>\n\n* 적당한 folder 를 만듦 (예를 들어 User/Documents/SolMech/) <br> Make an appropriate folder (e.g., User/Documents/SolMech/)\n* Git 또는 SourceTree 를 이용하여 위 folder 아래 이 원격 저장소를 `git clone` <br> Using the Git or SourceTree, `git clone` this repository under the folder\n* `cmd` 또는 `git bash` 실행한 후 `clone` 된 지역 저장소 folder로 이동 (예를 들어 `cd User/Documents/Solmech/`) <br> Start `cmd` or `git bash` and change working folder to the cloned folder (e.g. `cd User/Documents/Solmech/`)\n* `cmd` 또는 `git bash` 에서 각각 `cd` 또는 `pwd` 로 `clone` 된 folder 인지 확인 <br>Check location using `cd` or `pwd` in `cmd` or `git bash`\n* `jupyter notebook` 실행 <br>Run `jupyter notebook`\n\n#### 실행 동영상<br>Instruction video\n[![실행 동영상 Instruction video](https://i.ytimg.com/vi/W6ynqGKJFSs/hqdefault.jpg)](https://www.youtube.com/embed/videoseries?list=PLA6B0Lmr9oJO9HeSC74wqxECtwpUPJfdm)')
('https://raw.githubusercontent.com/Areej32/project-1/775b283e8606e4942ae7937f9522cd5f74c6c4f1/Clean%20Data/Models.ipynb', '# Project 1 :- NJ High School Educational Success Measures\n\nGroup Repo: https://github.com/Areej32/project.git\n\n### Team Members:\n\n•\tPavan\n\n•\tTerry\n\n•\tAreej Edaibat https://www.linkedin.com/in/areej-edaibat-6276a2155/\n\n•\tIsaac\n\n•\tJustin\n\n### Project Description/Outline:\nHow do various different factors affect high school graduation rates in New Jersey, New York, and Pennsylvania. \n\n### Research Questions to Answer:\n1.\tDoes the poverty rate correlate to graduation rate?\n2.\tDoes household size correlate to graduation rate?\n3.\tDoes the unemployment rate correlate to graduation rate?\n4. \tDoes the median income correlate to graduation rate?\n5.\tDoes the language spoken at home affect graduation rates?\n\n\n### Data Sets to be Used: \n\nCensus API\n\nVarious NJDOE Data Sets available at: https://www.state.nj.us/education/data/\n\nhttps://www.opendatanetwork.com/\n\n### Rough Breakdown of Tasks:\nOne person will need to clean the data so that it is only the high schools’ graduation rate and test scores.Each person will select one question to answer. We will all use the cleaned data.\n\nWe will compile our code and work on the presentation together.')
('https://raw.githubusercontent.com/roei60/Data-Science-Hotels/9bed0b5800e31b47da4455f604eb8349f093f0a5/Clustering.ipynb', '# Hotels-Data-Science\nClassification and clustering data of hotels reservation to produce conclusions and decsions\n\n# Spark using Docker\n\n1. install `cifs` using:\n\n```\nsudo apt-get install cifs-utils\n```\n\n2. mount a remote notebook using:\n\n```\nsudo mount -t cifs -o username<your_username>,password=<your_password>,uid=<your_user_id>,gid=<your_group_id> //<host_ip>/<your_path> /mnt/<your_directory>\n```\n\n3. install `Docker` using\n\n```\nsudo apt update\n```\n\n```\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\n```\n\n```\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n```\n\n```\nsudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"\n```\n\n```\nsudo apt-get update\nsudo apt-get install docker-ce\n```\n\n4. run spark container using:\n\n```\ndocker run --name pyspark --user root -p 8888:8888 -d -e NB_USER=<your_user> -e CHOWN_HOME=yes -e CHOWN_HOME_OPTS=\'-R\' -e GRANT_SUDO=yes -v /mnt/<your_path>:/home/<your_user>/work -w /home/$NB_USER jupyter/pyspark-notebook\n```')
('https://raw.githubusercontent.com/CyrilWendl/SIE-Project/83b30da1318489d7a9d121d3d8a89a17d184a9af/Code/decision_tree.ipynb', '# SIE Project 2017\nFolder for various codes for the SIE project at EPFL in Fall Semester 2017.\n\n## Files\n### `Code/`\nVisualizations\n- `decision_tree.ipynb`: Decision Trees and Random Forest on randomly generated labelled data\n- `density_tree.ipynb`: Density Trees on randomly generated labelled data\n- `MNIST.ipynb`: Trainig of a CNN on the MNIST dataset, retrieval of the FC layer activation weights, Density Forest\n\n### `Code/density_tree`\n\nPackage for implementation of Decision Trees, Density Forests and Random Forests\n- `create_data.py`: functions for generating labelled and unlabelled data\n- `decision_tree.py`: data structure for decision tree nodes\n- `decision_tree_create.py`: functions for generating decision trees\n- `decision_tree_traverse.py`: functions for traversing a decision tree to predict labels\n- `density_tree.py`: data struture for density tree nodes\n- `density_tree_create.py`: functions for generating density trees\n- `density_tree_traverse.py`: functions for descending density trees and retreiving their Gaussian parameters\n- `density_forest.py`: functions for creating density forests\n- `helper.py`: helper functions\n- `plots.py`: functions for plotting the data\n- `random_forests`: functions for creating random forests\n\n## Supervisors:\n- Prof. Devis Tuia, University of Wageningen\n- Diego Marcos González, University of Wageningen\n- Prof. François Golay, EPFL\n\nCyril Wendl, 2017')
('https://raw.githubusercontent.com/GoSAUN/Hartmann/3c889cf9f5153338aea2216bc4f1feb916ef5018/Codes/Graficas%20%20Hartmann.ipynb', '# Hartmann')
('https://raw.githubusercontent.com/willijm92/willijm92-repo/fcf40463770dfdb8d9027452f3ed3a94b6a47ee1/Continuum_blog_post/Jupyter_notebook_files/blog_post_notebook.ipynb', '# willijm92-repo\n\nVarious projects of mine for work, school, and fun.')
('https://raw.githubusercontent.com/tralpha/spotify-project/41b730709baac06d4fa5706c3664662b6175921a/DataLoader.ipynb', '# The Million Playlist Dataset\n\n## To DO:\n1. Review and think of the Average Track Duration plot\n2. Do EDA on more features, one Question at the time? Should we include 10 more plots, _in addition_ to John\'s histograms previously calculated?\n3. Start working on final report, in a *nice* and well documented Jupyter\n4. Test the classifier performance based on re-calculating the total number of unique tracks, playlist duration, and other global playlist features.\n5. \n\n\n## Dataset Description:\nThe Million Playlist Dataset contains 1,000,000 playlists created by\nusers on the Spotify platform.  It can be used by researchers interested\nin exploring how to improve the music listening experience.\n\n## What\'s in the Million Playlist Dataset\nThe MPD contains a million user-generated playlists. These playlists\nwere created during the period of January 2010 through October 2017.\nEach playlist in the MPD contains a playlist title, the track list\n(including track metadata) editing information (last edit time, \nnumber of playlist edits) and other miscellaneous information \nabout the playlist. See the **Detailed\nDescription** section for more details.\n\n\n## License\nUsage of the Million Playlist Dataset is subject to these \n[license terms](https://recsys-challenge.spotify.com/license)\n\n## Citing the Million Playlist Dataset\n\nCitation information for the dataset can be found at\n[recsys-challenge.spotify.com/dataset](https://recsys-challenge.spotify.com/dataset)\n\n\n## Getting the dataset\nThe dataset is available at [recsys-challenge.spotify.com/dataset](https://recsys-challenge.spotify.com/dataset)\n\n## Verifying your dataset\nYou can validate the dataset by checking the md5 hashes of the data.  From the top level directory of the MPD:\n   \n    % md5sum -c md5sums\n  \nThis should print out OK for each of the 1,000 slice files in the dataset.\n\nYou can also compute a number of statistics for the dataset as follows:\n\n    % python src/stats.py data\n  \nThe output of this program should match what is in \'stats.txt\'. Depending on how \nfast your computer is, stats.py can take 30 minutes or more to run.\n\n## Detailed description\nThe Million Playlist Dataset consists of 1,000 slice files. These files have the naming convention of:\n\nmpd.slice._STARTING\\_PLAYLIST\\_ID\\_-\\_ENDING\\_PLAYLIST\\_ID_.json\n\nFor example, the first 1,000 playlists in the MPD are in a file called \n`mpd.slice.0-999.json` and the last 1,000 playlists are in a file called\n`mpd.slice.999000-999999.json`.\n\nEach slice file is a JSON dictionary with two fields:\n*info* and *playlists*.\n\n### `info` Field\nThe info field is a dictionary that contains general information about the particular slice:\n\n   * **slice** - the range of slices that in in this particular file - such as 0-999\n   * ***version*** -  - the current version of the MPD (which should be v1)\n   * ***generated_on*** - a timestamp indicating when the slice was generated.\n\n### `playlists` field \nThis is an array that typically contains 1,000 playlists. Each playlist is a dictionary that contains the following fields:\n\n\n* ***pid*** - integer - playlist id - the MPD ID of this playlist. This is an integer between 0 and 999,999.\n* ***name*** - string - the name of the playlist \n* ***description*** - optional string - if present, the description given to the playlist.  Note that user-provided playlist descrptions are a relatively new feature of Spotify, so most playlists do not have descriptions.\n* ***modified_at*** - seconds - timestamp (in seconds since the epoch) when this playlist was last updated. Times are rounded to midnight GMT of the date when the playlist was last updated.\n* ***num_artists*** - the total number of unique artists for the tracks in the playlist.\n* ***num_albums*** - the number of unique albums for the tracks in the playlist\n* ***num_tracks*** - the number of tracks in the playlist\n* ***num_followers*** - the number of followers this playlist had at the time the MPD was created. (Note that the follower count does not including the playlist creator)\n* ***num_edits*** - the number of separate editing sessions. Tracks added in a two hour window are considered to be added in a single editing session.\n* ***duration_ms*** - the total duration of all the tracks in the playlist (in milliseconds)\n* ***collaborative*** -  boolean - if true, the playlist is a collaborative playlist. Multiple users may contribute tracks to a collaborative playlist.\n* ***tracks*** - an array of information about each track in the playlist. Each element in the array is a dictionary with the following fields:\n   * ***track_name*** - the name of the track\n   * ***track_uri*** - the Spotify URI of the track\n   * ***album_name*** - the name of the track\'s album\n   * ***album_uri*** - the Spotify URI of the album\n   * ***artist_name*** - the name of the track\'s primary artist\n   * ***artist_uri*** - the Spotify URI of track\'s primary artist\n   * ***duration_ms*** - the duration of the track in milliseconds\n   * ***pos*** - the position of the track in the playlist (zero-based)\n\nHere\'s an example of a typical playlist entry:\n  \n        {\n            "name": "musical",\n            "collaborative": "false",\n            "pid": 5,\n            "modified_at": 1493424000,\n            "num_albums": 7,\n            "num_tracks": 12,\n            "num_followers": 1,\n            "num_edits": 2,\n            "duration_ms": 2657366,\n            "num_artists": 6,\n            "tracks": [\n                {\n                    "pos": 0,\n                    "artist_name": "Degiheugi",\n                    "track_uri": "spotify:track:7vqa3sDmtEaVJ2gcvxtRID",\n                    "artist_uri": "spotify:artist:3V2paBXEoZIAhfZRJmo2jL",\n                    "track_name": "Finalement",\n                    "album_uri": "spotify:album:2KrRMJ9z7Xjoz1Az4O6UML",\n                    "duration_ms": 166264,\n                    "album_name": "Dancing Chords and Fireflies"\n                },\n                {\n                    "pos": 1,\n                    "artist_name": "Degiheugi",\n                    "track_uri": "spotify:track:23EOmJivOZ88WJPUbIPjh6",\n                    "artist_uri": "spotify:artist:3V2paBXEoZIAhfZRJmo2jL",\n                    "track_name": "Betty",\n                    "album_uri": "spotify:album:3lUSlvjUoHNA8IkNTqURqd",\n                    "duration_ms": 235534,\n                    "album_name": "Endless Smile"\n                },\n                {\n                    "pos": 2,\n                    "artist_name": "Degiheugi",\n                    "track_uri": "spotify:track:1vaffTCJxkyqeJY7zF9a55",\n                    "artist_uri": "spotify:artist:3V2paBXEoZIAhfZRJmo2jL",\n                    "track_name": "Some Beat in My Head",\n                    "album_uri": "spotify:album:2KrRMJ9z7Xjoz1Az4O6UML",\n                    "duration_ms": 268050,\n                    "album_name": "Dancing Chords and Fireflies"\n                },\n                // 8 tracks omitted\n                {\n                    "pos": 11,\n                    "artist_name": "Mo\' Horizons",\n                    "track_uri": "spotify:track:7iwx00eBzeSSSy6xfESyWN",\n                    "artist_uri": "spotify:artist:3tuX54dqgS8LsGUvNzgrpP",\n                    "track_name": "Fever 99\\u00b0",\n                    "album_uri": "spotify:album:2Fg1t2tyOSGWkVYHlFfXVf",\n                    "duration_ms": 364320,\n                    "album_name": "Come Touch The Sun"\n                }\n            ],\n\n        }\n\n\n## Tools\nThere are some tools that you can use with the dataset.\n### stats.py\nThis python program will iterate through the entire MPD and display summary information about the contents of the MPD.\n\nUsage:\n\n    % python src/stats.py data\n    \n### show.py\nThis python program will show playlists given their ID.\n\nShow playlist with PID 10:\n\n    % python src/show.py 10\n    \nShow the first 500 and the last 500 playlists:\n\n    % python src/show.py 0-500 999500-1000000\n    \nShow the raw json for 3 playlists:\n\n    % python src/show.py --raw 10 20 30\n        \nShow 1000 playlists without the track details:\n\n    % python src/show.py --compact 300-1300 \n    \nShow all playlists:\n\n    % python src/show.py --compact 0-1000000\n\n## How was the dataset built\nThe Million Playist Dataset is created by sampling playlists from the billions of playlists that Spotify users have created over the years.  Playlists that meet the following criteria are selected at random:\n\n * Created by a user that resides in the United States and is at least 13 years old\n * Was a public playlist at the time the MPD was generated\n * Contains at least 5 tracks\n * Contains no more than 250 tracks\n * Contains at least 3 unique artists\n * Contains at least 2 unique albums\n * Has no local tracks (local tracks are non-Spotify tracks that a user has on their local device)\n * Has at least one follower (not including the creator)\n * Was created after January 1, 2010 and before December 1, 2017\n * Does not have an offensive title\n * Does not have an adult-oriented title if the playlist was created by a user under 18 years of age\n\nAdditionally, some playlists have been modified as follows:\n\n * Potentially offensive playlist descriptions are removed\n * Tracks added on or after November 1, 2017 are removed\n\nPlaylists are sampled randomly, for the most part, but with some dithering to disguise the true distribution of playlists within Spotify. [Paper tracks](https://en.wikipedia.org/wiki/Fictitious_entry) may be added to some playlists to help us identify improper/unlicensed use of the dataset.\n\n## Overall demographics of users contributing to the MPD\n\n### Gender\n * Male: 45%\n * Female: 54%\n * Unspecified: 0.5%\n * Nonbinary: 0.5%\n\n### Age\n * 18-24:  43%\n * 25-34:  31%\n * 35-44:   9%\n * 45-54:   4%\n * 55+:     3%\n * Other:  10%\n\n### Country\n * US: 100%\n\n\n## Who built the dataset\nThe million playlist dataset was built by the following researchers @ Spotify:\n\n* Cedric De Boom\n* Ching-Wei Chen\n* Jean Garcia-Gathright\n* Paul Lamere\n* James McInerney\n* Vidhya Murali\n* Hugh Rawlinson\n* Sravana Reddy\n* Romain Yon')
('https://raw.githubusercontent.com/qzlvyh/dlab-berkeley-python-fundamentals/7c19e2ea79267ca9f01937053841b5f6a6c19466/Day_1/05_Strings.ipynb', "[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/dlab-berkeley/python-fundamentals)\n\n# D-Lab Python Fundamentals Series\n\nTo sign up for the Python series and other workshops, please visit the D-Lab's [website](http://dlab.berkeley.edu/training).\n\nThe D-Lab introductory series consists of four parts:\n\nPart 1:\n\n* Running Python\n* Jupyter notebooks\n* Variables assignment\n* Types conversion\n* Strings\n* Built-ins\n\nPart 2:\n\n* Lists\n* Loops\n* Conditionals\n* Functions\n* Scope\n\nPart 3:\n\n* Dictionaries\n* Files\n* Libraries\n* Errors\n* Comprehensions\n\nPart 4:\n\n* Python in Application\n\nParts 1-3 focus on learning the basics of programming in Python. This includes variables, data types, conditionals, functions, scope, debugging, and style. All of the materials aim to use examples from the social sciences and humanities in order to better relate to our target audience. In support of this, Part 4 is a day of application. Learners will work with a real-world text data set of UN documents, extracting targeted information and generating tabular data, ultimately writing to a .csv file suitable for subsequent statistical analysis. Everything needed to complete Part 4 is covered in parts 1-3.\n\n### Credits:\n\nMuch of these materials were adapted from those produced by [Software Carpentry](http://software-carpentry.org/). Thank you!")
('https://raw.githubusercontent.com/annafeit/optimizing-the-french-keyboard/48848751b14b5a390a59e8f45642668bc26cccda/Define%20scenario%20and%20optimize.ipynb', None)
('https://raw.githubusercontent.com/tychovdo/Char-RNN-Understanding/80146b831b1dbfded1663ff574a7cd23150a3ad4/DiagnosticClassifier.ipynb', "# Understanding Character-level RNN-LMs in PyTorch\n\nThis repository contains a PyTorch implementation for Recurrent Neural Network Language Models (RNN-LMs). Visualization of hidden RNN layers (see [this](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blog post by Andrej Karpathy). Analysis using Diagnostic Classifier ([\nVisualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure](https://arxiv.org/abs/1711.10203))\n\n### Example Usage\n\nTrain a simple model\n\n```\npython main.py data/tiny-shakespeare/train.txt data/tiny-shakespeare/test.txt models/simple.model --cuda\n```\n\nExample analysis can be found in the iPython Notebooks\n\n### Diagnostic Classifiers\n\nDiagnostic classifiers can be used to verify hypotheses about information in the hidden representations of a recurrent networks.\nThe figures below show how how a diagnostic classifier verifies that the network captures the position in line and whether characters are part of a comment.\n\n![Example analysis 1](https://raw.githubusercontent.com/tychovdo/char-rnn-visualization/master/plots/ex1.png)\n![Example analysis 2](https://raw.githubusercontent.com/tychovdo/char-rnn-visualization/master/plots/ex2.png)\n\n### Most Responsible Neuron\n\nDiagnostic classifiers can also used to automatically find neurons encoded to perform specific subtasks.\nThe figure below shows a neuron that is active inside quotation marks.\n\n![Most responsible neuron](https://raw.githubusercontent.com/tychovdo/char-rnn-visualization/master/plots/ex4.png)\n\n### Requirements\n\n- `python=3.6.3`\n- `pytorch=0.2.0`\n\n### Acknowledgements\n\n- [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Karpathy, et. al (2015)\n- [Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure](https://arxiv.org/abs/1711.10203) by Dieuwke Hupkes, et al. (2016)\n- [Example PyTorch character-level RNN](https://github.com/spro/char-rnn.pytorch) by Sean Robertson")
('https://raw.githubusercontent.com/qwretz/fpraktikum/3efc7f5815a5138de4d56fdf9cd04e31a0b8ba3f/Dynamic%20Light%20Scattering/Struktur.ipynb', '# fpraktikum')
('https://raw.githubusercontent.com/SlaybaughLab/88_Data/f7e1f56b01c3aa789563f68188de49d1000cbe40/Experiments/PHS/33MeVTa_29-31Mar17/33MeVTa_BeamOnly.ipynb', '# 88-Inch Experimental Data\n\nSimulated and experimental data from a series of experiments in support of the ETA validation conducted Febuary-April, 2017. The repo contains foil activation and PHS measurements made in support of neutron spectrum unfolding for different beams and ETA configurations. The raw PHS .data and .root files are not included in the repo due to their size.\n\nTo manipulate this data you need to:\n\n1) Install Root (https://root.cern.ch/downloading-root). Use version 6.04/18.  \n2) Get have access to the tools developed by the BANG group (contact Bethany Goldblum or Josh Brown)\n3) Clone and compile the nsd-geant4 and nsd-rootscripts repositories\n\nNote: .dat and .root raw data files are missing due to their size.  Contact James Bevins these files.')
('https://raw.githubusercontent.com/van-dang/FEniCS-Colab/7dab5b4c678e75a7cb1fb8f5ae1ccf51507a189d/FDM_FEM_1D.ipynb', None)
('https://raw.githubusercontent.com/aMartin6/Final_Project/683aa8f33b6422a0060efa88a129eab7823896f2/Final_Project.ipynb', '# Final_Project\n\nThis project models earthquake behavior using the one-dimensional block spring model proposed by Burridge and Knopoff. The main purpose of the numerical simulations is to compare the output of this model to statistics gathered from real earthquakes. This model is also used analyze the frequency and magnitude of earthquakes over time and help understand the nature of self-organized criticality. \n\nCases studied:\n* 25 blocks with varied initial positions\n* 100 blocks with varied initial positions\n* 100 blocks with varied initial positions and masses\n\nCases yet to try:\n* Varying spring constants \n* Different friction functions\n\nThis project was written by Adrian Martin.  \n  \nResources:  \n* Project Jupyter: https://jupyter.org/\n* Numpy: http://www.numpy.org/\n* Matplotlib: https://matplotlib.org/')
('https://raw.githubusercontent.com/manhcuogntin4/handwritting-ocr/37d95e5990cb80a398c5c5772a8d99f41141b5cc/GapClassifier-BiRNN.ipynb', '# Handwriting OCR\nOCR software for recognising handwritten characters (personal project)\n\nUsing Python 3.5, OpenCV 3.1, Numpy, ...')
('https://raw.githubusercontent.com/Merinorus/adaisawesome/f2f77fd95f0555a6a36cced9bac5d878ab94b173/Homework/03%20-%20Interactive%20Viz/HW3_Interactive_Viz.ipynb', '#adaisawesome\n# We should write a shortdescription here !')
('https://raw.githubusercontent.com/WilliamJCole/IS352_Week_7/6e7e778e8ba6ef818d22e6c2b66454d3330a66ab/IS362%20Week%207%20Assignment.ipynb', None)
('https://raw.githubusercontent.com/GuilhermeFreire/Keystrokes/d57c9385f894f32db16e230b824bea1d1350ef52/Keystroke%20Analysis.ipynb', '# Keystrokes')
('https://raw.githubusercontent.com/cbelyea/LRMF-Biodiversity-BAP/206bcaa0be16463caf00839141f68629d6950a9d/Large_River_Monitoring_Forum_Biodiversity_Indices_Analysis.ipynb', '\ufeff\n# Table of contents\n1.\t[Purpose](#purpose)\n2.\t[Inputs](#inputs)\n3.\t[Outputs](#outputs)\n4.\t[Constraints](#constraints)\n5.\t[Dependencies](#dependencies)\n    * [5.1 Code Language](#codelanguage)\n        * [5.1.1 package requirements](#packagerequirements)\n6.\t[Code](#code)\n7.\t[Tests](#tests)\n8.\t[Provenance](#provenance)\n9.\t[Citations](#citations)\n---\n\n# Large River Monitoring Forum Biogeographic Analysis Package – Biodiversity Indices\n\n## Purpose <a name="purpose"></a>\n\nThe Large River Monitoring Forum (LRMF) is a USGS led initiative to bring together researchers from across the nation involved in monitoring species and populations of fishes in large river systems.  The forum works collaboratively to understand best practices in monitoring and to study the trends in fish populations through space in time in response to environmental drivers.  \nThe initial work of the forum focused on comparisons of methods and documented in five large river systems (Ward et al. 2017).  The analyses presented here build on that work by taking the data compiled in that effort (Counihan et al. 2016) and allowing the user to explore trends in biodiversity by river reach through time.  Specifically, this package summarizes the data for five commonly used biodiversity indices: \n*\tMargalef Species Riches Index (Margalef 1958)\n*\tShannon-Wiener Indices (Shannon 1948; )\n*\tSimpson (Simpson 1949)\n*\tHill Numbers (Hill 1973)\n*\tPielou’s Species Evenness (Pielou 1966)\n\n## Inputs <a name="inputs"></a>\nThe bases of the analyses are the fish assemblage data Large River Monitoring Forum Fish Assemblage Database 2016 (https://www.sciencebase.gov/catalog/item/575ee33fe4b04f417c2b05ca).  That dataset includes data for the Colorado, Columbia, Mississippi, Illinois and Tallapoosa Rivers.  For each river the fish assemblage data collected during the monitoring efforts was compiled and summarized as the mean number of individuals of that species in that reach for a given year.  \n\nThe native format of the LRMF assemblage data is ESRI Geodatabase (.gdb), which contains polyline feature classes depicting river stretches and tables containing the species assemblage data.  As such, the following intermediate steps were taken to get the assemblage data into csv format for use as input for the scripting in the attached i python notebook, for each river table.\n\n*  Open ESRI\'s ArcMap\n*  Add each river table from the LRMF geodatabase\n*  Open each table by right-clicking on the table name in the ArcMap Table of Contents and choosing "Open"\n*  From "Table Options", choose "Export"\n*  Save as text/csv file (i.e. "Colorado_biodiv.csv")\n\n\nThe analysis package was developed with the following environment(s):\n\n*  Python 3.6 with math and os modules\n*  Anaconda 4.3.1, with Jupyter Notebook\n*  Pandas 0.19.2\n*  Matplotlib 2.0.0\n*  Numpy 1.11.3\n\n## Outputs \nThe output of the analyses are variants of five biodiversity indices used by biologist to characterize species assemblages (Clarke and Gorley 2006). The presentation of these indices is not an endorsement of the use of these indice but rather to present options to output indices that are commonly used. For more information on these indices please thoroughly read the references provided below (e.g., Washington 1984).  For each of the indices tables are generated showing the values for each river reach and each year.  Those tables are then used to graph the trends in diversity through time for each reach.  \n\n*  Margalef Species Richness Index \n*  Shannon-Wiener Indexes, Log(e), Log(2), Log(10)\n*  Simpsons:\n    *  Lambda\n    *  1-Lambda\n    *  Lambda\'\n    *  1-Lambda\'\n* Hill Numbers:\n    *  N1\n    *  N2\n    *  N-Inf\n    *  N10\n    *  N10\'\n    *  N21\n    *  N21\'\n*  Pielou\'s Species Evenness Index\n\n\n\n## Constraints \nThese data were compiled for a set of analyses related to the fish assemblages on portions of the five large rivers. The data represent a subset of the monitoring data that are collected for each of the rivers and in some cases a subset of the gear used in the monitoring protocols, so it is important that any re-analyses with this data be done with a clear understanding of the criteria used in compiling this data. Electrofishing sampling was done for the four of the rivers (Colorado, Illinois, Mississippi, and Tallapoosa) and gillnetting was done on the Columbia River.\n\n## Dependencies \nThese analyses are currently reliant on a local copy of the LRMF Fish Assemblage dataset (Counihan et al 2016).\n\nCode execution dependencies\n*  Code Language\n    <br>This code is written in Python 3.x. with the math module.\n    *  Python package requirements \n    <br>The required packages for proper code execution include:\n        *  Anaconda 4.3.1, with Jupyter Notebook \n        *  Pandas 0.19.2\n        *  Matplotlib 2.0.\n        *  Numpy 1.11.3\n\nInput dependencies\n\n    CSV formatting\n    Inputs for the script are text files exported from LRMF river feature datasets (https://www.sciencebase.gov/catalog/item/575ee33fe4b04f417c2b05ca), and have the following column ordering: ObjectID, River, RiverSeg, Year, sp1, sp2...spN. The following assumptions are therefore made, regarding input formatting:\n\n      - Existence of the field "River", with river name.\n      - Existence of the field "RiverSeg" with river segment names.\n      - Existence of the field "Year" with year for which observations were made.\n      - Species survey data begins at the fifth column and continues to end of table.\n\n\n\n\n## Tests \nIn order to verify results from this script, a csv file of fish species assemblage data for the Columbia River was used as input for the script and as input into PRIMER version 7 (Clarke & Gorley 2015), a software package for calculating biodiveristy indices for species assemblage data.  Upon completion, results from the script and Primer 7 matched.\n\n\n## Provenance \n\nThis code was written to be consistent with the format of the 2016 Fish Assemblage database developed by the Large River Monitoring Forum (Counihan et al. 2016; https://www.sciencebase.gov/catalog/item/575ee33fe4b04f417c2b05ca).  If additional surveys are included either adding years, river reaches or whole rivers, the format will need to be consistent with the format of the original dataset. \n\n## Citations \nClarke, K. R., and R. N. Gorley. 2006. Primer v6: User Manual/Tutorial.  Primer-E Ltd. United Kindom. 190 p.\n\nClarke, K.R., Gorley, R.N., Somerfield, P.J. & Warwick, R.M. 2014. Change in Marine Communities: An Approach to Statistical Analysis and Interpretation, 3rd edition. PRIMER-E Ltd: Plymouth, UK. 262 pp\nClarke, K.R., Gorley, R.N.  2015.  Primer v7: User Manual / Tutorial.  PRIMER-E Ltd: Plymouth, UK. 300 pp\n\nCounihan, T.D., Waite, I.R., Casper, Andy, Ward, David, Sauer, Jennifer, Irwin, Elise, Chapman, Colin, Paukert, Craig, Ickes, Brian, Kosovich, John, and Bayer, J.M., 2016, Large River Monitoring Forum Fish Assemblage Database 2016: U.S. Geological Survey, https://doi.org/10.5066/F7CN723D.\n\nHill, M 1973. Diversity and evenness: a unifying notation and its consequences. Ecology 54: 427– 432.  https://doi.org/10.2307/1934352\n\nMargalef, R. 1958 Information theory in ecology. General Systems, 3 (1958), pp. 36-71\n\nPielou, E. C., 1966. The measurement of diversity in different types of biological collections. Journal of Theoretical Biology, 13, 131–44.  https://doi.org/10.1016/0022-5193(66)90013-0\n\nShannon, C.E. 1948. A mathematical theory of communication. Bell System Technical Journal, 27, 379–423.  https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n\nSimpson, E. 1949. Measurement of diversity. - Nature, Lond. 163: 688.  https://doi.org/10.1038/163688a0\n\nSpellerberg, I. F., and P. J. Fedor.  2003. A tribute to Claude Shannon (1916–2001) and a plea for more rigorous use of species richness, species diversity and the ‘Shannon–Wiener’ Index.  Global Ecology and Biodiversity. https://doi.org/10.1046/j.1466-822X.2003.00015.x\n\nWard, D., A. Casper, T. Counihan, J. Bayer, I. Waite, J. Kosovich, C. Chapman, E. Irwin, J. Saur, B. Ickes, A. McKerrow.  2017.  Long-term fish monitoring in large rivers: Utility of “benchmarking” across basins. Fisheries.  https://doi.org/10.1080/03632415.2017.1276330\n\nWashington, H.G., 1984. Diversity, biotic and similarity indices: a review with special relevance to aquatic ecosystems. Water research, 18(6), pp.653-694.  https://doi.org/10.1016/0043-1354(84)90164-7\n\n\n\n\n\nThis code was written to calculate a number of indices describing biodiversity calculated from results of fish surveys on the five major rivers studied by the Large River Monitoring Forum (LRMF).  Results of these calculations are then plotted by river segment and year, with png format images saved along with a csv file containing the input data with columns of biodiversity indices appended.\n\nSince this was written specifically to receive LRMF data as inputs, it was tailored accordingly.  Inputs for the script are text files exported from LRMF river feature datasets (https://www.sciencebase.gov/catalog/item/575ee33fe4b04f417c2b05ca), and have the following column ordering: ObjectID, River, RiverSeg, Year, sp1, sp2...spN.  The following assumptions are therefore made, regarding input formatting:\n\n    1.  Existence of the field "River", with river name.\n    2.  Existence of the field "RiverSeg" with river segment names.\n    3.  Existence of the field "Year" with year for which observations were made.\n    4.  While order of 1-3 is inconsequential, species survey data begins at the fifth column and continues to end of table.\n    \nThis was developed with the following environment(s):\n\n    1.  Python 3.6.7 with math module\n    2.  Anaconda 1.8.4, with Jupyter Notebook\n    3.  Pandas 0.23.4\n    4.  Matplotlib 3.0.2\n    5.  Numpy 1.15.4\n    \nThe following Biodiversity Indices are calculated and plotted:\n\n    1.  Margalef Species Richness Index\n    2.  Shannon-Wiener Indexes, Log(e), Log(2), Log(10)\n    3.  Simpsons:\n        a.  Lambda\n        b.  1-Lambda\n        c.  Lambda\'\n        d.  1-Lambda\'\n    4. Hill Numbers:\n        a.  N1\n        b.  N2\n        c.  N-Inf\n        d.  N10\n        e.  N10\'\n        f.  N21\n        g.  N21\'\n    5.  Pielou\'s Species Evenness Index\n=======\n# LRMF-Biodiversity-BAP\nCalculates and plots biodiveristy indices for fish population survey data collected by the Large River Monitoring Forum.  Plots and input data with appended indices are saved in .png and .csv format.  Indices calculated are: Margalef Species Richness Index (d); Shannon-Wiener Log(e) Log(2) and Log(10) indices (H\');  Simpson\'s Diversity Indices (Lambda, 1-Lambda, Lambda\' and 1-Lambda\'); Hill Numbers N1, N2, N-Inf, N10, N10\', N21, N21\'; Pielou\'s Evenness Index.\n>>>>>>> 140c25491bf620f47e5d1153030a3aa2304f5113')
('https://raw.githubusercontent.com/Moony2D/CME-LMU/38a41c742359d054ccd2802aee0fe7835adfa352/Lecture3_SLE/Lecture3_SLE.ipynb', None)
('https://raw.githubusercontent.com/ank91/Mixture_Gaussian_CM_of_Planets/e8686eed38dc39f1016a9d791bb55ad57d216252/List-Misture_G.ipynb', '# Mixture_Gaussian_CM_of_Planets\nGaussian Mixture Model for planetary systems CM vs. mass of plantes\n\nIt is an study of the planetary system classification from exoplant population data.   \n\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fjbautistas/Mixture_Gaussian_CM_of_Planets/Clasification.ipynb)')
('https://raw.githubusercontent.com/simonvdberg/POC/38adce6a6d1c897ef32c79b5d554c353da68037a/LocCat.ipynb', None)
('https://raw.githubusercontent.com/schen57/AI-ML-DL-Playground/a9c9c24b0a659c3b669b05908330903322fd0705/Local%20Python/AI-ML-DL%20Algorithms/LSTM%20Neural%20Networks/Time%20Series%20Forecasting%20with%20the%20Long%20Short-Term%20Memory%20Network%20in%20Python.ipynb', '# AI-ML-DL-Playground\nExplore and experiment in order to learn about AI/ML/DL\n\n\n\nLearning Paths to complete mastery of AI/ML/DL and ultimately building a pure AI/ML/DL application\nThis is a tall goal but in the plan to become familiar with the end to end build, I have some basic ideas of how to get there and what the necessary steps I should take. This repository serves as a documentation of the journey to get there.\n\nRoughly, I plan to tackle the following key components to setting up the ML pipeline through the following:\n1. Data Extraction ( Some sort of web-scraping or set up a database of collecting data )\n2. Data ETL ( Cleaning and merging the data in need and prepare them for the later analytics )\n3. Data Exploratory Analysis (In this component, data needs to be further transformed and whipped into the right shape, Data visualization is also an important sub-category. Some sort of dashboarding could be built as a result to better represent the underlying data)\n4. Feature Engineering (Based on the understanding of the data from EDA step, key variables need to be extracted and prepared for later model building)\n5. Model Building (Running various machine learning techniques and build models to achieve certain accuracy. Models could also include Deep Learning algorithms etc.)\n6. App Building (App building in itself is a separate category but the purpose of putting this here is to help evolve the model built into an end-user friendly, consumable product. Models, data refresh, model refresh all need to be baked into production )\n\n#### These are my current thoughts as of now. Will come and update if there are more thoughts around this')
('https://raw.githubusercontent.com/chipk215/ncaa_basketball/89b04f9f9f3746e5ff6907c0c026fc3be486ae4b/Models/Tournament_Model_LR-Delta_No_Opp_Stats.ipynb', None)
('https://raw.githubusercontent.com/jpcolino/IPython_notebooks/aeeefb1c25f90c1e4b3211cda4c03982c979a47b/Multiprocessing%20MonteCarlo%20Simulation.ipynb', '****\n# IPython_notebooks\n<p style="text-align: right"><i>Jesus Perez Colino</i></p>\n****\nSet of iPython (Jupyter) notebooks about Python, Reactive Programming, Statistics, Finance, Data Bases, Optimization, Science in general, Machine Learning, Time-Series... and more\n\nMy favourites?\n\n- **Algorithmic Problems in Python**: *(Computer Science)* It contains more than 50 algorithmic problems about generators, list comprehension, map/reduce/filter problems, decorators, regular expressions... This is still a *work-in-progress*, but feel free to have a look, and enjoy.\n\n- **Doing Science with Python**: *(Python programming)* this one is huge, almost a book... and it is still \'work-in-progress\' but it contains not only the basics about data-containers, or basic algorithms, but also advance hash-functions and functional programming... but more specifically, or advanced in Python, maybe you find intriguing the notebooks about **Decorators, Closures and Wrappers** or the one about **Iterables, Generetors and Yield**, or related with **High Performance Computation (HPC)** check **HPC Cython vs. Multiprocessing** out. Hope you like. \n\n- **Doing Statistics with Python**: *(Statisitics)* Another huge notebook, in fact, I wrote it thinking in a free eBook, but it is quite general in terms of statistics.  \n  - If you want to go deeper in **Statistics Techniques for Big Data**, maybe you should have a look into the **PCA for Big Data** or into **Regression Techniques for Big Data** notebooks, where I am using **PySpark** and **Apache Spark**. \n  - If you are working with **Scikit-learn**, and you would be interested in knowing how to store in MongoDB your input/output or model results, have a look to the code of **MongoDB, Numpy and Scikit-learn** notebook. \n\n- **Two-Factor Model**: *(Stochastic Modelling)* this one is more technical, from the mathematical point of view, but quite interesting if you are working in *commodities modelling, derivatives pricing*, or *risk-management*. This one is the simplest possible version, that made the equivalence between Black-76 and the Two-Factor Model for forward contracts. I will update it soon, with a  more object-oriented version. Stay tune. If you want to have the full object-oriented version, with *Robust Calibration* of the SDE using *Semidefinite Programming* and unit-testing, contact me.\n\n- ... and if you are interested in **Financial Engineering**, you will find interesting the notebooks about   \n  - **European Option Class in Python**, \n  - **American Options pricing using LSMC**, \n  - **Asian Options pricing using MC with Control Variate**, \n  - **JumpDiffusion Option pricing**,\n  - **Spread Options pricing**, \n  - **Market-Risk and VaR**, \n  - or about **Swing options**... pls, be my guest, and serve yourself. Have fun.\n\n\n\n\n****\n<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">  <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a> <span style="font: 60% Arial,sans-serif; color:#0783B6;"> <br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</span></a>.\n\n****\n\n\n\n<a href="https://de.linkedin.com/pub/jesus-perez-colino/3/80a/9b5" style="text-decoration:none;"><span style="font: 60% Arial,sans-serif; color:#0783B6;"><img src="https://static.licdn.com/scds/common/u/img/webpromo/btn_in_20x15.png" width="20" height="15" alt="View Jesus Perez Colino\'s LinkedIn profile" style="vertical-align:middle;" border="0">&nbsp;View Jesus Perez Colino\'s profile</span></a>\n\n******')
('https://raw.githubusercontent.com/ColinDuc/BosonTeam/3c8eee5240381fa1fb4141765e8ac3a1dd376f57/New_TEST.ipynb', '# BosonTeam\nReadMe du projet\nIl va falloir le tenir à jour !')
('https://raw.githubusercontent.com/Keyrat06/Gaussian_Processes/3fc17f804193f8e464684acf84fe607fa8573600/P-Set/Gaussian%20PSet.ipynb', None)
('https://raw.githubusercontent.com/dataforcast/OC_Datascientist/6c015a638e18936bb9dbed86d72028ae7a87b6e3/P7/P7_Keras.ipynb', '# Set of projects \n<hr>\n\n\n<br>\nI\'am a software engineer consultant with a mathematical background.\n</br>\n\n<br>\nI started learning Data Sciences by 2014, using customers CRM databases to deliver insights serving sales and marketing services.\n\n<br>\nThen I decided to go indeep in Data Sciences skills to be able to deliver more value from Data Sciences technologies.\n\n<br>\nThe professional projects presented here are the result of my "deep learning" of Data Sciences.<br>\n\n<a href="URL">http://bit.ly/Linkedin_FBANGUI</a>\n<hr>\n## <h1><font color=\'blus\'>ADANET Evaluation</font></h1>\n<br>\nAdaNet is a TensorFlow framework for fast and flexible AutoML with learning guarantees.\nAdaNet implements an adaptive algorithm and learns neural architecture from neural Subnetworks.\n<br>\n\nSee project description on : \n<a href="URL">https://github.com/dataforcast/OC_Datascientist/tree/master/P8/README.mkd</a>\n\n\n<hr>\n## <h1><font color=\'blus\'>Images classification</font></h1>\n<br>\nBenchmarks issued from use of classifiers based on machine learning and use of classifiers based on artificial neuron networks are exposed.\n<br>\n\nSee project description on : \n<a href="URL">https://github.com/dataforcast/OC_Datascientist/tree/master/P7/README.mkd</a>\n<hr>\n\n## <h1><font color=\'blus\'>TAG engine for StackOverFlow platform</font></h1>\n<br>\nResults issued from use of NLP algorithm, supervised and unsupervized Machine Learning algorihtms are exposed.\n<br>\n\nA set of TAGs are suggested following the process of a detailed problem description post on StackOverFlow platform.\n<br>\n<br>\nSee project description on : \n\n<a href="URL">https://github.com/dataforcast/OC_Datascientist/tree/master/P6/Soutenance_P6_v3/README.mkd</a>\n\n<hr>\n\n## <h1><font color=\'blus\'>Market segmentation</font></h1>\n<br>\nResults of the use of generative Machine Learing algorithms along with supervized Machine Learning algorithms are experimented in order to reveal and interpret market segments from a e-commerce web database.\n<br>\nSee project description on : \n<br>\n\n<a href="URL">https://github.com/dataforcast/OC_Datascientist/blob/master/P5/p5_soutenance_F-BANGUI_V2/README.mkd</a>\n\n<br>\n\n\n<hr>\n\n## <h1><font color=\'blus\'>Flights delays estimator</font></h1>\n\nResults of the use of linear Machine Learning estimators are presented in order to estimate flight delays over TRANSTATS USA government database for year 2016.\n\n<br>\nSee project description on : \n<br>\n\n<a href="URL">https://github.com/dataforcast/OC_Datascientist/blob/master/P4/soutenance/README.md</a>\n\n<hr>\n\n## <h1><font color=\'blus\'>Moovies recommendation engine</font></h1>\n<br>\nThis is a simple engine based on textual data available from IMDB. Collaborative dimension of such engine \nis not taken into account.\n<br>\n\nA set of unsupervized machine learning algorithms are experimented and benchmarked.\n<br>\n\nSee project description : <a href="URL">https://github.com/dataforcast/OC_Datascientist/blob/master/P3/README.mkd</a>\n<hr>\n\n## <h1><font color=\'blus\'>Food recipe generator</font></h1>\n<br>\nExploratory analysis leads to propose a scoring algorithm in order to evaluate a recipe sanity. \n<br>\n\nSee project description : <a href="URL">https://github.com/dataforcast/OC_Datascientist/blob/master/P2/README.mkd</a>')
('https://raw.githubusercontent.com/edrias/seniorprojecteeg-master-fork/321f7d738884150da19fe88a7f0046fb83849c28/PCA/Emmanuil-PCA.ipynb', '# EEG Project\r\n* Team Members and Forked Repos\r\n  * Shateesh Bhugwansing - [shateeshb/seniorprojecteeg](https://github.com/shateeshb/seniorprojecteeg)  \r\n  * Tarekul Islam - [tarekul/seniorprojecteeg](https://github.com/tarekul/seniorprojecteeg)\r\n  * Junchen Liu - [JunchenLiu123/seniorprojecteeg](https://github.com/JunchenLiu123/seniorprojecteeg)\r\n  * Emmanuil Simkhayev - [edrias/seniorprojecteeg-master-fork](https://github.com/edrias/seniorprojecteeg-master-fork)\r\n  \r\n## Description\r\nThis project will explore using machine learning with EEG data. \r\n\r\nOur goal as data scientists is to help discover characteristics of EEG that help identify where in the brain semantically congruent stimuli are recognized. We want learn where and how semantic content is stored in the brain. Thus, we aim to discover the parts of the brain that work together and investigate the signal outputs of the brain given, semantic congruent stimuli(distractor/target). \r\n\r\n \r\n ## Deliverables\r\nDeliverables for each team member can be found here: https://github.com/edrias/seniorprojecteeg/tree/master/SeniorDesignII_WeeklyUpdates  \r\n  \r\nThe final presentation slides can be found here: \r\nhttps://docs.google.com/presentation/d/165lQ7J_GXI0EB2awjn87S2F8iiJhBtGyLjD5UHTwZVo/edit?usp=sharing\r')
('https://raw.githubusercontent.com/guneykan/montecarlo/90243d161635838957c0ff2071bf70853199ccf7/PS3/PS3.ipynb', '# montecarlo\nPhys 492 \n\nFiles and reports for PHYS 492.')
('https://raw.githubusercontent.com/iharnoor/PhylogeneticNetworks/7dad5384a16f67dad577ef9ac39d33236ae98899/Phylogenetics.ipynb', '# Research-in-Julia\nCreating a level 1 Phylogenetic Tree\n\n## Built By \n* [Naman Kanwar](https://github.com/Naman26) \n* [Harnoor Singh](https://github.com/iharnoor) \n\n## Test cases\n1,2,3,4,5\n(n+1) choose 2 = 6 choose 2 = 15\n\n5 repeated:\n* SN(1,1) = 1\n* SN(2,2) = 2\n* SN(3,3) = 3\n* SN(4,4) = 4\n* SN(5,5) = 5\n\n5 choose 2 = 10 => no repeated\n* SN(1,2) = {1, 2}\n* SN(1,3) = {1, 3, 2, 4}\n* SN(1,4) = {1, 2, 3, 4}\n* SN(1,5) = {1, 5, 2, 3, 4}\n* SN(2,3) = {1, 2, 3, 4}\n* SN(2,4) = {1, 2, 3, 4}\n* SN(2,5) = {1, 2, 3, 4, 5}\n* SN(3,4) = {1, 2, 3, 4}\n* SN(3,5) = {1, 2, 3, 4, 5}\n* SN(4,5) = {1, 2, 3, 4, 5}')
('https://raw.githubusercontent.com/cabb99/IntroCompBio/fd654c4e42f9a01e0009de77d83d07c7830cdf16/PreProcessing.ipynb', '# IntroCompBio\nTo run please install\npython2\npip (or anaconda in windows)\n\n#If not anaconda\npip install pandas numpy xlrw scipy scikit-learn')
('https://raw.githubusercontent.com/wakemeup-xyz/TianChi_IJCAI-17_sales_prediction_code/c632e76e964ca03e09c902f3347551551bd7b5ec/Prediction.ipynb', None)
('https://raw.githubusercontent.com/Petelr/ml2019/32205d6ee9e0fe053dfcf60e4f853ddba4a7cb56/Proj2_Linear%20Classification.ipynb', '# ml2019\nThis is a repo for my Machine Learning Class')
('https://raw.githubusercontent.com/kgcornell/Airbnb-Price-and-Occupancy-Rate-Prediction-in-Seattle/f86b226605514d6ece3b604e8ba46c811dddb53c/Project.ipynb', '# Airbnb Price and Occupancy Rate Prediction in Seattle\n### Jialin Liu (jl3455), Kerou Gao (kg486), Kartikay Gupta (kg477)\n\nAirbnb is a popular online hospitality service that connects people, who are willing to rent a part of their housing space, with people who are on the lookout for short term lodging. Review score of a lisiting is a critical metric that guides a tenant\'s decision making process. Moreover, review scores are used by Airbnb for quality control and policy making purposes. \n\nKeeping in mind the practicality of the review score, the aim of this project is to build a robust algorithm that can accurately predict review scores. We\'ll use data from "Inside Airbnb" to develop and train our model and will identify key features, via a sensitivity analysis, that significantly influence the review scores. The results from our study will generate great insights that can be used by Airbnb listing owners to ensure customer satisfaction which in turn would increase a property\'s occupancy rate.')
('https://raw.githubusercontent.com/kent-huynh/ucb_project_1/e1ba36d3d928bc7552d2793bc3177a5b3d84eea6/Python%20Analytics%20-%20Best%20Region%20for%20Undergrad%20Degrees.ipynb', '# Project_1\nUCB - Python Project (More Info to Come)\n\nPart I Introduction: (Zeynep)\n    a.Introduce topic:\n        1.Introduce Question\n        2.Scope 2100 -> 1500\n        3.Introduce Paramter Ratios\n        4.Intended Audiance\n    b.Mean Graph - MultiBar - Tuition Cost and Earnings 6 Years\n    c.Earnings/Tuition Graph - Line Graph w/ the mean Earnings/Cost ratio for all regions\n\nPart II Binning & Quartile: (Derrick)\n    a.Show Bins\n    b.Explain Quartiles\n    c.Offer Example of Why This is Important - ex. Boston has many Ivy League schools, Region 5 has 3x the number of schools of other regions\n\nPart III Analysis: (Kent)\n    a. Show screenshot of some of our dataframes\n    b. Show screenshots of some of our code \n    c. Graph 1: Multiline Earnings/Cost Ratio for Each Region\n    d. Graph 2: MultiBar Chart Earnings Growth Ratio for Each Region \n    e. Graph 3: Multiline "Worth It\' Ratio for Each Region\n    d. Table of Best Region by Cost Tier\n    \nPart IV What Our Data Didn\'t Accout For: (Derrick)\n    a. We didn\'t look at degrees offered\n    b. We dropped 2 regions and schools that were missing data\n    c. We didn\'t add a weight for the number of schools per region, regions differ in number\n    ')
('https://raw.githubusercontent.com/askdjango/networkons-201801/6a5719dde416fa91dd40ef8dacb2242255ac5f79/Python%20Day%203.ipynb', None)
('https://raw.githubusercontent.com/pradau/python_for_psychologists/857fd00925376ba8d8c78f729d5a8ec3dd7b25cf/Python_for_Psychologists.ipynb', '# python_for_psychologists\nOct. 24 2018\nThis Jupyter notebook follows the outline of "Matlab for Psychologists" by [Antonia Hamilton](http://www.antoniahamilton.com/matlab.html), with entirely new translation of the code into Python. The pdf provides more context for the notebook.')
('https://raw.githubusercontent.com/dogeplusplus/AIND-RecurrentNeuralNetworks/58ed81e43c6bec03c29bd01b19177ac7490b457a/RNN_project.ipynb', "# Recurrent Neural Networks course project: time series prediction and text generation\n\n### Amazon Web Services\n\nInstead of training your model on a local CPU (or GPU), you could use Amazon Web Services to launch an EC2 GPU instance.  Please refer to the Udacity instructions in your classroom for setting up a GPU instance for this project.  [link for AIND students](https://classroom.udacity.com/nanodegrees/nd889/parts/16cf5df5-73f0-4afa-93a9-de5974257236/modules/53b2a19e-4e29-4ae7-aaf2-33d195dbdeba/lessons/2df3b94c-4f09-476a-8397-e8841b147f84/project)\n\n\n## Rubric items\n\n#### Files Submitted\n\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Submission Files      |  RNN_project.ipynb, my_answers.py --> both the completed notebook  RNN_project.ipynb as well as all completed python functions requested in the main notebook RNN_project.ipynb (TODO items) should be copied into this python script and submitted for grading.\t\t|\n\n#### Step 1:  Implement a function to window time series\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Window time series data. |  The submission returns the proper windowed version of input time series of proper dimension listed in the notebook.  |\n\n\n#### Step 2: Create a simple RNN model using keras to perform regression\n\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Build an RNN model to perform regression. |  The submission constructs an RNN model in keras with LSTM module of dimension defined in the notebook.        |\n\n\n#### Step 3: Clean up a large text corpus\n\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Find and remove all non-english or punctuation characters from input text data.  The submission removes all non-english / non-punctuation characters.  |\n\n\n#### Step 4: Implement a function to window a large text corpus\n\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Implement a function to window input text data| The submission returns the proper windowed version of input text of proper dimension listed in the notebook.  |\n\n\n#### Step 5: Create a simple RNN model using keras to perform multiclass classification\n\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Build an RNN model to perform multiclass classification. |  The submission constructs an RNN model in keras with LSTM module of dimension defined in the notebook.        |\n\n\n#### Step 6: Generate text using a fully trained RNN model and a variety of input sequences\n| Criteria       \t\t|     Meets Specifications\t        \t\t\t            | \n|:---------------------:|:---------------------------------------------------------:| \n| Generate text using a trained RNN classifier.   | The submission presents examples of generated text from a trained RNN module.  The majority of this generated text should consist of real english words. |\n\n## Submission\nBefore submitting your solution to a reviewer, you are required to submit your project to Udacity's Project Assistant, which will provide some initial feedback.  \n\nThe setup is simple.  If you have not installed the client tool already, then you may do so with the command `pip install udacity-pa`.  \n\nTo submit your code to the project assistant, run `udacity submit` from within the top-level directory of this project.  You will be prompted for a username and password.  If you login using google or facebook, visit [this link](https://project-assistant.udacity.com/auth_tokens/jwt_login) for alternate login instructions.\n\nThis process will create a zipfile in your top-level directory named rnn-<id>.zip.  This is the file that you should submit to the Udacity reviews system.")
('https://raw.githubusercontent.com/annalisasheehan/masters/5bcaa18aebed6b8e2d44644082c29fd16aeedbb7/RasterstatsFromXArray.ipynb', '## General Details\nThis repository contains code used to for Annalisa Sheehan, University of Southampton, \nMSc in Applied GIS and Remote Sensing thesis on: \n### Satellite-derived Particulate Matter pollution measurements and their relationship with hospital admissions in the Wessex region\n\nFor more information please contact: ans1g15@soton.ac.uk or annalisa.sheehan@hotmail.com\n\nIf you use this code please cite my thesis: \nSheehan, A.N. (2016) Satellite-derived Particule Matter pollution measurements and their relationship with hospital admissions in the Wessex region, MSc Thesis, University of Southampton. \n\nThis code is written for personal use and the data used within the code is not publically available. \nHowever, to run the code in the environment in which it was written use the environment.yml file and run the following code in the command line:\n```conda env create -f environment.yml```\n\n## Overview of the Code\nThe code in this repository reprojects MAIAC AOT images from WGS84 datum into the British National Grid and converts AOT measurements into PM2.5 estimates. Additionally, the code analyses the PM2.5 data to investigate how PM2.5 is changing spatially and temporally over Wessex and the impact on hospital admissions for circulatory and respiratory diseases.\n\nThe different jupyter notebooks correspond to different sections of my thesis. \n- Section 3.3 - Getting_MAIAC_into_dask_daily_data.ipynb\n- Section 3.4.1 - Reprojecting_MAIAC_data.ipynb\n- Section 3.4.1/3.5.1/3.5.2/3.5.4 - MAIAC_Analysis.ipynb \n- Section 3.4.2 Rasterstats_from_Xarray.ipynb\n- Section 3.4.4 Lamb_Weather_Types.ipynb\n- Section 3.5.2 Linear_regression.ipynb\n- Section 3.5.3 Regression_analysis.ipynb\n- Section 3.5.3 Regression_MSOA.ipynb\n- Section 3.5.3 Regression_coastal.ipynb\n- Section 3.5.3 Stratified_regression.ipynb\n- Section 3.5.3 Weighted_least_square_regression.ipynb\n- Section 3.6 AERONET_validation.ipynb\n- Section 3.6 AURN_Validation.ipynb')
('https://raw.githubusercontent.com/gshind/NYCTraffic/0ed983a7e150d2e1d343860416d173504be5f02a/RegionsBarCode.ipynb', '# NYCTraffic\nModeling Traffic Patterns in NYC')
('https://raw.githubusercontent.com/remyclem/CartPole-RL/d312d1bb3f2242711892b6d0e411134892f32798/Reinforcement_CartPole.ipynb', 'For some parts of the neural network agents, the code was largely inspired by the code in:\n\nhttps://github.com/ageron/handson-ml')
('https://raw.githubusercontent.com/mojia/rocket/8d695305340e0596f3b059777638880da3b51e17/RocketRegression.ipynb', None)
('https://raw.githubusercontent.com/gardiac2002/monkies/2e148e7d0d71d7951378dfd04754f2bd4f3ae5b1/Sessions%20Analysis.ipynb', None)
('https://raw.githubusercontent.com/DrSoap/SSY226_Energy_Consumption/91f003071fbcf902f389ceaf45e31fb23cb7998a/Setup.ipynb', None)
('https://raw.githubusercontent.com/djaballah/Stack-Overflow-2018-Developer-Survey-Analysis/88e71f67a36d9a81851fb6f1ca0fd09206e5aeb1/Simple Eda.ipynb', "# Stack-Overflow-2018-Developer-Survey-Analysis\n\n> This project is part of the week 1 of the 0 to Data Science Hackathon 2018 \n\nIn this project, I'll do some data exploration and visualization on the Stack-Overflow-2018-Developer-Survey found on \nthis [link](https://www.kaggle.com/stackoverflow/stack-overflow-2018-developer-survey)")
('https://raw.githubusercontent.com/jessehahm/SlopeAreaAnalysis/f4ca7851f0c215b40dcb256fe607f16581a2b5c5/SlopeAreaAnalysis.ipynb', None)
('https://raw.githubusercontent.com/angadkalra/QuantumComputingProject/f4a1eeca68d9a7d1e76316a53f929578d9ee0ac3/StructBalance/DataExperiments.ipynb', None)
('https://raw.githubusercontent.com/JaeDukSeo/Daily-Neural-Network-Practice-2/42b191fcae6064b44d0a79e2b5f3abc3573ae701/Understanding_Concepts/PCA_Collection/b%20kernel%20pca.ipynb', '[![PHP Censor](http://ci.php-censor.info/build-status/image/2?branch=master&label=PHPCensor&style=flat-square)](http://ci.php-censor.info/build-status/view/2?branch=master)\n[![Travis CI](https://img.shields.io/travis/php-censor/php-censor/master.svg?label=TravisCI&style=flat-square)](https://travis-ci.org/php-censor/php-censor?branch=master)\n[![SensioLabs Insight](https://img.shields.io/sensiolabs/i/26f28bee-a861-45b2-bc18-ed2ac7defd22.svg?label=Insight&style=flat-square)](https://insight.sensiolabs.com/projects/26f28bee-a861-45b2-bc18-ed2ac7defd22)\n[![Codecov](https://img.shields.io/codecov/c/github/php-censor/php-censor.svg?label=Codecov&style=flat-square)](https://codecov.io/gh/php-censor/php-censor)\n[![Latest Version](https://img.shields.io/packagist/v/php-censor/php-censor.svg?label=Version&style=flat-square)](https://packagist.org/packages/php-censor/php-censor)\n[![Total downloads](https://img.shields.io/packagist/dt/php-censor/php-censor.svg?label=Downloads&style=flat-square)](https://packagist.org/packages/php-censor/php-censor)\n[![License](https://img.shields.io/packagist/l/php-censor/php-censor.svg?label=License&style=flat-square)](https://packagist.org/packages/php-censor/php-censor)\n   \n   \n<p align="center">\n    <img width="800" height="auto" src="/image/image.png" alt="PHP Censor" />\n</p>\n   \n   \n**Daily-Neural-Network-Practice-2** This is my repo for daily practice of coding as well as machine learning (Season 2)\nThe cover image is from this [website](https://www.pexels.com/photo/selective-focus-photography-of-sparkler-955792/). \n\n**Twitter**: [@JaeDukSeo](https://twitter.com/JaeDukSeo?lang=en).\n\n[![Dashboard](docs/screenshots/dashboard.png)](docs/screenshots/dashboard.png)\n\n* [Updating](#updating)\n* [Configuring project](#configuring-project)\n* [Migrations](#migrations)\n* [Tests](#tests)\n* [Documentation](#documentation)\n* [License](#license)\n\n## System requirements\n\n* Python 3 or 2\n* Tensorflow or numpy\n* scikit learn\n\n## Features\n\n\n* Set up and tear down database tests for [PostgreSQL](docs/en/plugins/pgsql.md), [MySQL](docs/en/plugins/mysql.md) or \n[SQLite](docs/en/plugins/sqlite.md);\n\n* Install [Composer](docs/en/plugins/composer.md) dependencies;\n\n* Run tests for PHPUnit, Atoum, Behat, Codeception and PHPSpec;\n\n* Check code via Lint, PHPParallelLint, Pdepend, PHPCodeSniffer, PHPCpd, PHPCsFixer, PHPDocblockChecker, PHPLoc, \nPHPMessDetect, PHPTalLint and TechnicalDept;\n\n* Run through any combination of the other [supported plugins](docs/en/README.md#plugins), including Campfire, \nCleanBuild, CopyBuild, Deployer, Env, Git, Grunt, Gulp, PackageBuild, Phar, Phing, Shell and Wipe;\n\n* Send notifications on Email, XMPP, Slack, IRC, Flowdock, HipChat and \n[Telegram](https://github.com/LEXASOFT/PHP-Censor-Telegram-Plugin);\n\n* Use your LDAP-server for authentication;\n\n## Installing\n\n* If you don\'t have python installed please install it. \n\n* [Add a virtual host to your web server](docs/en/virtual_host.md), pointing to the `public` directory within your new\n\n* [Set up the PHP Censor Worker](docs/en/workers/worker.md) (Need configured Queue) or \n[a cron-job](docs/en/workers/cron.md) to run PHP Censor builds;\n\n## Installing via Docker\n\nIf you want to install PHP Censor as Docker container, you can use \n[php-censor/docker-php-censor](https://github.com/php-censor/docker-php-censor) project.\n\n## Updating\n* I am going to make thsi repo my main repo\n\n## Configuring project\n* Add project without any project config (Runs "zero-config" plugins, including: Composer, TechnicalDept, PHPLoc, \nPHPCpd, PHPCodeSniffer, PHPMessDetect, PHPDocblockChecker, PHPParallelLint, PHPUnit and Codeception);\n\n* Similar to [Travis CI](https://travis-ci.org), to support PHP Censor in your project, you simply need to add a \n`.php-censor.yml` (`phpci.yml`/`.phpci.yml` for backward compatibility with PHPCI) file to the root of your repository;\n\n* Add project config in PHP Censor project page (And it will cancel file config from project repository);\n\nThe project config should look something like this:\nMore details about [configuring project](docs/en/configuring_project.md).\n\n\nFor Phar plugin tests set \'phar.readonly\' setting to Off (0) in `php.ini` config. Otherwise tests will be skipped.  \n\ndatabase name in `phpunit.xml` config constants). If connection failed tests will be skipped. and more')
('https://raw.githubusercontent.com/andreydung/metriclearning/e37393e717afb85c0df0af17ac877491928aacf0/ViewingAngles.ipynb', None)
('https://raw.githubusercontent.com/julienroy13/COMP767-assignments/978c037c795a863350f67c775720d545804b9d1c/Week2-MDP/dynamicProgramming.ipynb', '# COMP767-assignments\nAssignments for Reinforcement Learning course (COMP767) at McGill')
('https://raw.githubusercontent.com/BouchardLab/cv_paper_plots/b81bb73c75f40a08da48d358f7f3343417c14bb3/accuracy_slope/accuracy_slope.ipynb', '# cv_paper_plots\nCode for create plots for cv paper.')
('https://raw.githubusercontent.com/jbirky/photonCounting/b4d5acb49c6e11ef0f34d88903fc4843ed04e9cc/analysis.ipynb', '# photonCounting\nPhysics 164 Lab 1')
('https://raw.githubusercontent.com/r-hopper/W207-Summer-2017-Final/e43d7214742948a65b01f6bdb67f8b1e204a3184/analysis/hopper_miller_zhang_final0814.ipynb', '# W207-Summer-2017-Final\nMIDS W207 Applied Machine Learning Final Project\n\n\n1) Download the training and test datasets from the Shelter Animal Outcomes competition on Kaggle: [Animal Shelter Outcomes](https://www.kaggle.com/c/shelter-animal-outcomes/data)')
('https://raw.githubusercontent.com/AgremE/Hand_Signs_Recognizer/9416b82f855f4aa0e75bfdc4cdf08861363f246b/asl_recognizer.ipynb', '# Artificial Intelligence Engineer Nanodegree\n## Probabilistic Models\n## Project: Sign Language Recognition System\n\n### Install\n\nThis project requires **Python 3** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [SciPy](https://www.scipy.org/)\n- [scikit-learn](http://scikit-learn.org/0.17/install.html)\n- [pandas](http://pandas.pydata.org/)\n- [matplotlib](http://matplotlib.org/)\n- [jupyter](http://ipython.org/notebook.html)\n- [hmmlearn](http://hmmlearn.readthedocs.io/en/latest/)\n\nIt is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python and load the environment included in the "Your conda env for AI ND" lesson.\n\n### Code\n\nA template notebook is provided as `asl_recognizer.ipynb`. The notebook is a combination tutorial and submission document.  Some of the codebase and some of your implementation will be external to the notebook. For submission, complete the **Submission** sections of each part.  This will include running your implementations in code notebook cells, answering analysis questions, and passing provided unit tests provided in the codebase and called out in the notebook. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `asl_recognizer/` (that contains this README) and run one of the following command:\n\n`jupyter notebook asl_recognizer.ipynb`\n\nThis will open the Jupyter Notebook software and notebook in your browser. Follow the instructions in the notebook for completing the project.\n\n\n### Additional Information\n##### Provided Raw Data\n\nThe data in the `asl_recognizer/data/` directory was derived from \nthe [RWTH-BOSTON-104 Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database-rwth-boston-104.php). \nThe handpositions (`hand_condensed.csv`) are pulled directly from \nthe database [boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml](boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml). The three markers are:\n\n*   0  speaker\'s left hand\n*   1  speaker\'s right hand\n*   2  speaker\'s nose\n*   X and Y values of the video frame increase left to right and top to bottom.\n\nTake a look at the sample [ASL recognizer video](http://www-i6.informatik.rwth-aachen.de/~dreuw/download/021.avi)\nto see how the hand locations are tracked.\n\nThe videos are sentences with translations provided in the database.  \nFor purposes of this project, the sentences have been pre-segmented into words \nbased on slow motion examination of the files.  \nThese segments are provided in the `train_words.csv` and `test_words.csv` files\nin the form of start and end frames (inclusive).\n\nThe videos in the corpus include recordings from three different ASL speakers.\nThe mappings for the three speakers to video are included in the `speaker.csv` \nfile.  In addition, min and max hand location statistics have been gathered and are\nprovided in the `speakerstats.csv` file.  This may be helpful in standardizing the \nhand position values since taller speakers have a wider range of motion.')
('https://raw.githubusercontent.com/fanxiao001/ift6135-assignment/17e598f072c62f32c84d0156c9c7d4db1f3db5a2/assignment4/ift6135_hwk4.ipynb', '# ift6135-assignment')
('https://raw.githubusercontent.com/mdeff/dlaudio/0b214ac787ba8763cc660ae0c1bac9079e0e1e58/audio_features.ipynb', '# Master thesis: Structured Auto-Encoder with application to Music Genre Recognition\n\n[Michaël Defferrard](https://deff.ch).\nSupervized by [Xavier Bresson](https://www.ntu.edu.sg/home/xbresson),\n[Johan Paratte](https://www.linkedin.com/in/johan-paratte-a2070039),\n[Pierre Vandergheynst](https://people.epfl.ch/pierre.vandergheynst).\n\n> In this work, we present a technique that learns discriminative audio\n> features for Music Information Retrieval (MIR). The novelty of the proposed\n> technique is to design auto-encoders that make use of data structures to\n> learn enhanced sparse data representations. The data structure is borrowed\n> from the Manifold Learning field, that is data are supposed to be sampled\n> from smooth manifolds, which are here represented by graphs of proximities of\n> the input data. As a consequence, the proposed auto-encoders finds sparse\n> data representations that are quite robust w.r.t. perturbations. The model is\n> formulated as a non-convex optimization problem. However, it can be\n> decomposed into iterative sub-optimization problems that are convex and for\n> which well-posed iterative schemes are provided in the context of the Fast\n> Iterative Shrinkage-Thresholding (FISTA) framework. Our numerical experiments\n> show two main results. Firstly, our graph-based auto-encoders improve the\n> classification accuracy by 2% over the auto-encoders without graph structure\n> for the popular GTZAN music dataset. Secondly, our model is significantly\n> more robust as it is 8% more accurate than the standard model in the presence\n> of 10% of perturbations.\n\n## Content\n\nThis repository contains the code developed during my master thesis.\n\nRelated resources:\n* Report: <https://infoscience.epfl.ch/record/218019>\n* Slides: <https://deff.ch/dlaudio_slides.pdf>\n* Code: <https://github.com/mdeff/dlaudio>\n* Experimental results: <http://nbviewer.jupyter.org/github/mdeff/dlaudio_results>\n* Latex sources of the report: <https://github.com/mdeff/dlaudio_report>')
('https://raw.githubusercontent.com/fastforwardlabs/cuckoofilter/b0122e429f14a6416f00bfa794bd1ed19fcb864c/bench_marking_notebook.ipynb', '# cuckoofilter')
('https://raw.githubusercontent.com/hananiel/gcptraining/d21f17f513cf0317986c3230205908fb5f5d2154/blogs/babyweight/babyweight.ipynb', '# training-data-analyst \n\nLabs and demos for Google Cloud Platform courses (http://cloud.google.com/training).\n\n### Try out the code on Google Cloud Platform\n[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open/?git_repo=https://github.com/GoogleCloudPlatform/training-data-analyst.git)\n\n## Courses\n\nCode for the following courses is included in this repo:\n\n### Google Cloud Platform Big Data and Machine Learning Fundamentals\n\nhttps://cloud.google.com/training/courses/data-ml-fundamentals\n\n[GCP Big Data & Machine Learning Fundamentals](CPB100)\n\n\n### Data Engineering on Google Cloud Platform\n  \nhttps://cloud.google.com/training/courses/data-engineering\n\n1. [Serverless Data Analysis](courses/data_analysis)\n2. [Leveraging unstructured data](courses/unstructured)\n3. [Serverless Machine Learning](courses/machine_learning)\n4. [Resilient streaming systems](courses/streaming)\n\n### Machine Learning on Google Cloud Platform (& Advanced ML on GCP)\n\nhttps://www.coursera.org/learn/google-machine-learning\n\n1. [How Google Does ML](courses/machine_learning/deepdive/01_googleml)\n2. [Launching into ML](courses/machine_learning/deepdive/02_generalization)\n3. [Introduction to TensorFlow](courses/machine_learning/deepdive/03_tensorflow)\n4. [Feature Engineering](courses/machine_learning/deepdive/04_features)\n5. [Art and Science of ML](courses/machine_learning/deepdive/05_artandscience)\n6. Production ML models\n7. [End-to-end machine learning on Structured Data](courses/machine_learning/deepdive/07_structured)\n8. [Image Classification Models in TensorFlow](courses/machine_learning/deepdive/08_image)\n9. [Sequence Models for Time-Series and Text problems](courses/machine_learning/deepdive/09_sequence)\n10. [Recommendation Engines using TensorFlow](courses/machine_learning/deepdive/10_recommend)\n\n\n\n### Blog posts\n\nblogs/')
('https://raw.githubusercontent.com/amcm29/MachineLearning_Marathon/d8c6c55858b89f922364c0c7da92ee4fce122062/boston_model.ipynb', '# MachineLearning_Marathon\nMachine Learning Project with marathon race data')
('https://raw.githubusercontent.com/barrasa8/GA_DS_FinalProject/d0e875eae78d3a390d2680310837aaab9b9e979b/code/GA_FinalProject_TimeSeries_ActiveUsers.ipynb', '# GA_DS_FinalProject')
('https://raw.githubusercontent.com/kaiszybiak/RoboND-Rover-Project/0421efbbb610d9f15414a899accf50d49413cc39/code/Rover_Project_Test_Notebook.ipynb', '[//]: # (Image References)\n[image_0]: ./misc/rover_image.jpg\n[![Udacity - Robotics NanoDegree Program](https://s3-us-west-1.amazonaws.com/udacity-robotics/Extra+Images/RoboND_flag.png)](https://www.udacity.com/robotics)\n# Search and Sample Return Project\n\n\n![alt text][image_0] \n\nThis project is modeled after the [NASA sample return challenge](https://www.nasa.gov/directorates/spacetech/centennial_challenges/sample_return_robot/index.html) and it will give you first hand experience with the three essential elements of robotics, which are perception, decision making and actuation.  You will carry out this project in a simulator environment built with the Unity game engine.  \n\n## The Simulator\nThe first step is to download the simulator build that\'s appropriate for your operating system.  Here are the links for [Linux](https://s3-us-west-1.amazonaws.com/udacity-robotics/Rover+Unity+Sims/Linux_Roversim.zip), [Mac](\thttps://s3-us-west-1.amazonaws.com/udacity-robotics/Rover+Unity+Sims/Mac_Roversim.zip), or [Windows](https://s3-us-west-1.amazonaws.com/udacity-robotics/Rover+Unity+Sims/Windows_Roversim.zip).  \n\nYou can test out the simulator by opening it up and choosing "Training Mode".  Use the mouse or keyboard to navigate around the environment and see how it looks.\n\n## Dependencies\nYou\'ll need Python 3 and Jupyter Notebooks installed to do this project.  The best way to get setup with these if you are not already is to use Anaconda following along with the [RoboND-Python-Starterkit](https://github.com/ryan-keenan/RoboND-Python-Starterkit). \n\n\nHere is a great link for learning more about [Anaconda and Jupyter Notebooks](https://classroom.udacity.com/courses/ud1111)\n\n## Recording Data\nI\'ve saved some test data for you in the folder called `test_dataset`.  In that folder you\'ll find a csv file with the output data for steering, throttle position etc. and the pathnames to the images recorded in each run.  I\'ve also saved a few images in the folder called `calibration_images` to do some of the initial calibration steps with.  \n\nThe first step of this project is to record data on your own.  To do this, you should first create a new folder to store the image data in.  Then launch the simulator and choose "Training Mode" then hit "r".  Navigate to the directory you want to store data in, select it, and then drive around collecting data.  Hit "r" again to stop data collection.\n\n## Data Analysis\nIncluded in the IPython notebook called `Rover_Project_Test_Notebook.ipynb` are the functions from the lesson for performing the various steps of this project.  The notebook should function as is without need for modification at this point.  To see what\'s in the notebook and execute the code there, start the jupyter notebook server at the command line like this:\n\n```sh\njupyter notebook\n```\n\nThis command will bring up a browser window in the current directory where you can navigate to wherever `Rover_Project_Test_Notebook.ipynb` is and select it.  Run the cells in the notebook from top to bottom to see the various data analysis steps.  \n\nThe last two cells in the notebook are for running the analysis on a folder of test images to create a map of the simulator environment and write the output to a video.  These cells should run as-is and save a video called `test_mapping.mp4` to the `output` folder.  This should give you an idea of how to go about modifying the `process_image()` function to perform mapping on your data.  \n\n## Navigating Autonomously\nThe file called `drive_rover.py` is what you will use to navigate the environment in autonomous mode.  This script calls functions from within `perception.py` and `decision.py`.  The functions defined in the IPython notebook are all included in`perception.py` and it\'s your job to fill in the function called `perception_step()` with the appropriate processing steps and update the rover map. `decision.py` includes another function called `decision_step()`, which includes an example of a conditional statement you could use to navigate autonomously.  Here you should implement other conditionals to make driving decisions based on the rover\'s state and the results of the `perception_step()` analysis.\n\n`drive_rover.py` should work as is if you have all the required Python packages installed. Call it at the command line like this: \n\n```sh\npython drive_rover.py\n```  \n\nThen launch the simulator and choose "Autonomous Mode".  The rover should drive itself now!  It doesn\'t drive that well yet, but it\'s your job to make it better!  \n\n**Note: running the simulator with different choices of resolution and graphics quality may produce different results!  Make a note of your simulator settings in your writeup when you submit the project.**\n\n### Project Walkthrough\nIf you\'re struggling to get started on this project, or just want some help getting your code up to the minimum standards for a passing submission, we\'ve recorded a walkthrough of the basic implementation for you but **spoiler alert: this [Project Walkthrough Video](https://www.youtube.com/watch?v=oJA6QHDPdQw) contains a basic solution to the project!**.')
('https://raw.githubusercontent.com/gkbharathy/Py_ML_Book_Raschka/ef6c60f81e20a3717a18812f88d3cfd6d264a1bd/code/ch08/ch08.ipynb', '## Python Machine Learning (2nd Ed.) Code Repository\n\n[![Build Status](https://travis-ci.com/rasbt/python-machine-learning-book-2nd-edition.svg?token=zvSsJVLJFKzB2yqaeKN1&branch=master)](https://travis-ci.com/rasbt/python-machine-learning-book-2nd-edition)\n![Python 3.6](https://img.shields.io/badge/Python-3.6-blue.svg)\n![License](https://img.shields.io/badge/Code%20License-MIT-blue.svg)\n\n\n\n**Python Machine Learning, 2nd Ed.**  \n\npublished September 20th, 2017\n\nPaperback: 622 pages  \nPublisher: Packt Publishing  \nLanguage: English\n\nISBN-10: 1787125939  \nISBN-13: 978-1787125933  \nKindle ASIN: B0742K7HYF  \n\n[<img src="./images/cover_1.jpg" width="348">](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939)\n\n\n## Links\n\n- [Amazon Page](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939)\n- [Packt Page](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition)\n\n\n\n## Table of Contents and Code Notebooks\n\n**Helpful installation and setup instructions can be found in the [README.md file of Chapter 1](code/ch01/README.md)**\n\nTo access the code materials for a given chapter, simply click on the `open dir` links next to the chapter headlines to navigate to the chapter subdirectories located in the [code/](code/) subdirectory. You can also click on the `ipynb` links below to open and view the Jupyter notebook of each chapter directly on GitHub.\n\nIn addition, the [code/](code/) subdirectories also contain .py script files, which were created from the Jupyter Notebooks. However, I highly recommend working with the Jupyter notebook if possible in your computing environment. Not only do the Jupyter notebooks contain the images and section headings for easier navigation, but they also allow for a stepwise execution of individual code snippets, which -- in my opinion -- provide a better learning experience.\n\n**Please note that these are just the code examples accompanying the book, which I uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.**   \n\n\n1. Machine Learning - Giving Computers the Ability to Learn from Data [[open dir](./code/ch01)] [[ipynb](./code/ch01/ch01.ipynb)] \n2. Training Machine Learning Algorithms for Classification [[open dir](./code/ch02)] [[ipynb](./code/ch02/ch02.ipynb)] \n3. A Tour of Machine Learning Classifiers Using Scikit-Learn [[open dir](./code/ch03)] [[ipynb](./code/ch03/ch03.ipynb)] \n4. Building Good Training Sets – Data Pre-Processing [[open dir](./code/ch04)] [[ipynb](./code/ch04/ch04.ipynb)] \n5. Compressing Data via Dimensionality Reduction [[open dir](./code/ch05)] [[ipynb](./code/ch05/ch05.ipynb)] \n6. Learning Best Practices for Model Evaluation and Hyperparameter Optimization [[open dir](./code/ch06)] [[ipynb](./code/ch06/ch06.ipynb)]\n7. Combining Different Models for Ensemble Learning [[open dir](./code/ch07)] [[ipynb](./code/ch07/ch07.ipynb)]\n8. Applying Machine Learning to Sentiment Analysis [[open dir](./code/ch08)] [[ipynb](./code/ch08/ch08.ipynb)] \n9. Embedding a Machine Learning Model into a Web Application [[open dir](./code/ch09)] [[ipynb](./code/ch09/ch09.ipynb)] \n10. Predicting Continuous Target Variables with Regression Analysis [[open dir](./code/ch10)] [[ipynb](./code/ch10/ch10.ipynb)] \n11. Working with Unlabeled Data – Clustering Analysis [[open dir](./code/ch11)] [[ipynb](./code/ch11/ch11.ipynb)] \n12. Implementing a Multi-layer Artificial Neural Network from Scratch [[open dir](./code/ch12)] [[ipynb](./code/ch12/ch12.ipynb)] \n13. Parallelizing Neural Network Training with TensorFlow [[open dir](./code/ch13)] [[ipynb](./code/ch13/ch13.ipynb)] \n14. Going Deeper: The Mechanics of TensorFlow [[open dir](./code/ch14)] [[ipynb](./code/ch14/ch14.ipynb)] \n15. Classifying Images with Deep Convolutional Neural Networks [[open dir](./code/ch15)] [[ipynb](./code/ch15/ch15.ipynb)] \n16. Modeling Sequential Data Using Recurrent Neural Networks [[open dir](./code/ch16)] [[ipynb](./code/ch16/ch16.ipynb)] \n\n### What’s new in the second edition from the first edition?\n\n> Oh, there are so many things that we improved or added; where should I start!? The one issue on top of my priority list was to fix all the nasty typos that were introduced during the layout stage or my oversight. I really appreciated all the helpful feedback from readers in this manner! Furthermore, I addressed all the feedback about sections that may have been confusing or a bit unclear, reworded paragraphs, and added additional explanations. Also, special thanks go to the excellent editors of the second edition, who helped a lot along the way! \n\n> Also, the figures and plots became much prettier. While readers liked the graphic content a lot, some people criticized the PowerPoint-esque style and layout. Thus, I decided to overhaul every little figure with a hopefully more pleasing choice of fonts and colors. Also, the data plots look much nicer now, thanks to the matplotlib team who put a lot of work in matplotlib 2.0 and its new styling theme.\n\n> Beyond all these cosmetic fixes, new sections were added here and there. Among these is, for example, is a section on dealing with imbalanced datasets, which several readers were missing in the first edition and short section on Latent Dirichlet Allocation among others.\n\n> As time and the software world moved on after the first edition was released in September 2015, we decided to replace the introduction to deep learning via Theano. No worries, we didn\'t remove it but it got a substantial overhaul and is now based on TensorFlow, which has become a major player in my research toolbox since its open source release by Google in November 2015. \nAlong with the new introduction to deep learning using TensorFlow, the biggest additions to this new edition are three brand new chapters focussing on deep learning applications: A more detailed overview of the TensorFlow mechanics, an introduction to convolutional neural networks for image classification, and an introduction to recurrent neural networks for natural language processing. Of course, and in a similar vein as the rest of the book, these new chapters do not only provide readers with practical instructions and examples but also introduce the fundamental mathematics behind those concepts, which are an essential building block for understanding how deep learning works.\n\n[ [Excerpt from "Machine Learning can be useful in almost every problem domain:" An interview with Sebastian Raschka](https://www.packtpub.com/books/content/machine-learning-useful-every-problem-domain-interview-sebastian-raschka/) ]\n\n\n--- \n\n<br>\n<br>\n\nRaschka, Sebastian, and Vahid Mirjalili. *Python Machine Learning, 2nd Ed*. Packt Publishing, 2017.\n\n    @book{RaschkaMirjalili2017,  \n    address = {Birmingham, UK},  \n    author = {Raschka, Sebastian and Mirjalili, Vahid},  \n    edition = {2},  \n    isbn = {978-1787125933},  \n    keywords = {Clustering,Data Science,Deep Learning,  \n                Machine Learning,Neural Networks,Programming,  \n                Supervised Learning},  \n    publisher = {Packt Publishing},  \n    title = {{Python Machine Learning, 2nd Ed.}},  \n    year = {2017}  \n    }\n\n# Translations\n\n### German\n\n- ISBN-10: 3958457339\n- ISBN-13: 978-3958457331\n- [Amazon.de link](https://www.amazon.de/Machine-Learning-Python-Scikit-Learn-TensorFlow/dp/3958457339/ref=tmm_pap_swatch_0?_encoding=UTF8&qid=1513601461&sr=8-5)\n- [Publisher link](https://mitp.de/IT-WEB/Programmierung/Machine-Learning-mit-Python-oxid.html)\n\n![](images/cover-german.jpg)')
('https://raw.githubusercontent.com/seangtkelley/personal-health-tracking/afc2a7594485c4075f3dbd5917f221cd9d2b8643/college-summary.ipynb', '# personal-health-tracking')
('https://raw.githubusercontent.com/wigasper/viral-mirna-database/8793ce8c17ec8a5c93fd5bd6c4db9bc96104a913/community_enrichment_analysis.ipynb', "# viral-mirna-final-project\nFinal project for my BIOI 4870 (Database Search & Pattern Discovery) class.\n\n## File Descriptions\n**format_gene_ontology.py** - Converts the gene ontology in OBO format into a tab delimited format  \n**get_uniprot_xmls.py** - Uses the UniProt API to retrieve and store XML files for a list of needed UniProt IDs specified by [Qureshi et al.'s viral miRNA target data](http://crdd.osdd.net/servers/virmirna/)  \n**initialize_db.sql** - PostgreSQL code to set up the database  \n**prep_data.sh** - Shell script to remove unneeded columns and headers from raw data  \n**uniprot_data_extraction.py** - Extracts required data for the database from UniProt XMLs  ")
('https://raw.githubusercontent.com/nthiery/harmonic-modules/3feac84d4f41878350e48830b69a55285e0a0491/compute_character.ipynb', None)
('https://raw.githubusercontent.com/florisdf/hoe-zit-het/5194298625381fefb9d0a7f7c3222dd717f884e1/content/lessen/wiskunde/functies/grafiek/grafieken.ipynb', 'Source files voor [hoe zit het](https://hoezithet.nu/).')
('https://raw.githubusercontent.com/mgribov/coursera-uw-ml/a98a5034fbb5426bf8ff1cec7955b2e9ec26e1fb/course-4/2_kmeans-with-text-data_blank.ipynb', '# Machine Learning Specialization\n\n## Datasets\n### Course 1\n**amazon_baby**\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-1/amazon_baby.gl.zip\n\n**home_data**\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-1/home_data.gl.zip\n\n**image_test_data**\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-1/image_test_data.gl.zip\n\n**image_train_data**\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-1/image_train_data.gl.zip\n\n**people_wiki.gl**\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-1/people_wiki.gl.zip\n\n**song_data**\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-1/song_data.gl.zip\n\n<br/>\n#### Course 2\n* https://s3.amazonaws.com/static.dato.com/files/coursera/course-2/kc_house_data.gl.zip\n\n### References\n\nMore information on the Amazon data set may be found [here](http://jmcauley.ucsd.edu/data/amazon/) as well as in the following paper.\n\n```\nInferring networks of substitutable and complementary products\nJ. McAuley, R. Pandey, J. Leskovec\nKnowledge Discovery and Data Mining, 2015\n```')
('https://raw.githubusercontent.com/GoogleCloudPlatform/training-data-analyst/618c776a2f88936fc4606a6daf79413891b26340/courses/fast-and-lean-data-science/02_Dataset_playground.ipynb', '# training-data-analyst \n\nLabs and demos for Google Cloud Platform courses (http://cloud.google.com/training).\n\n## Contributing to this repo\n\n* Small edits are welcome! Please submit a Pull-Request. See also [CONTRIBUTING.md](./CONTRIBUTING.md)\n* For larger edits, please submit an issue, and we will create a branch for you. Then, get the code reviewed (in the branch) before submitting.\n\n## Organization of this repo\n\n### Try out the code on Google Cloud Platform\n[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open/?git_repo=https://github.com/GoogleCloudPlatform/training-data-analyst.git)\n\n## Courses\nCode for the following courses is included in this repo:\n\n### Google Cloud Platform Big Data and Machine Learning Fundamentals\n#### Course\nhttps://cloud.google.com/training/courses/data-ml-fundamentals\n\n#### Code\n1. [GCP Big Data & Machine Learning Fundamentals](CPB100)\n\n### Data Engineering on Google Cloud Platform\n#### Course\nhttps://cloud.google.com/training/courses/data-engineering\n\n#### Code\n1. [Leveraging unstructured data](courses/unstructured)\n2. [Serverless Data Analysis](courses/data_analysis)\n3. [Serverless Machine Learning](courses/machine_learning)\n4. [Resilient streaming systems](courses/streaming)\n\n\n### Machine Learning on Google Cloud Platform (& Advanced ML on GCP)\n#### Courses\n1. https://www.coursera.org/learn/google-machine-learning\n2. https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp\n\n#### Codes\n1. [How Google Does ML](courses/machine_learning/deepdive/01_googleml)\n2. [Launching into ML](courses/machine_learning/deepdive/02_generalization)\n3. [Introduction to TensorFlow](courses/machine_learning/deepdive/03_tensorflow)\n4. [Feature Engineering](courses/machine_learning/deepdive/04_features)\n5. [Art and Science of ML](courses/machine_learning/deepdive/05_artandscience)\n6. [End-to-end machine learning on Structured Data](courses/machine_learning/deepdive/06_structured)\n7. Production ML models\n8. [Image Classification Models in TensorFlow](courses/machine_learning/deepdive/08_image)\n9. [Sequence Models for Time-Series and Text problems](courses/machine_learning/deepdive/09_sequence)\n10. [Recommendation Engines using TensorFlow](courses/machine_learning/deepdive/10_recommend)\n\n\n\n### Blog posts\n\nblogs/')
('https://raw.githubusercontent.com/RafaelGoncalves8/breaking-captcha/acca35f1525ef29a01308701cb89fe071ca341b4/deliver/Break%20captcha.ipynb', '# breaking-captcha\nProject for course IA009C on machine learning at Unicamp. Breaking captchas with convolutional neural networks.')
('https://raw.githubusercontent.com/nanomishka/ods_ml/d90ed27b534268ce63638b5122390a50da8d1bdd/docker_files/check_docker.ipynb', "## [mlcourse.ai](https://mlcourse.ai), open Machine Learning course\n\n![ODS stickers](https://github.com/Yorko/mlcourse.ai/blob/master/img/ods_stickers.jpg)\n\n:ru: [Russian version](https://github.com/Yorko/mlcourse.ai/wiki/About-the-course-(in-Russian)) :ru: \n\n:exclamation: Current session launched on **October 1, 2018**. Fill in [this form](https://drive.google.com/open?id=1_pDNuVHwBxV5wuOcdaXoxBZneyAQcqfOl4V2qkqKbNQ) to participate, ou can still join :exclamation:\n\nMirrors (:uk:-only): [mlcourse.ai](https://mlcourse.ai) (main site), [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse) (same notebooks as Kernels)\n\n### Outline\nThis is the list of published articles on medium.com [:uk:](https://medium.com/open-machine-learning-course), habr.com [:ru:](https://habr.com/company/ods/blog/344044/), and jqr.com [:cn:](https://www.jqr.com). Icons are clickable. Also, links to Kaggle Kernels (in English) are given. This way one can reproduce everything without installing a single package.  \n1. Exploratory Data Analysis with Pandas [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68)  [:ru:](https://habrahabr.ru/company/ods/blog/322626/) [:cn:](https://www.jqr.com/article/000079), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas)\n2. Visual Data Analysis with Python [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-2-visual-data-analysis-in-python-846b989675cd)  [:ru:](https://habrahabr.ru/company/ods/blog/323210/) [:cn:](https://www.jqr.com/article/000086), Kaggle Kernels: [part1](https://www.kaggle.com/kashnitsky/topic-2-visual-data-analysis-in-python), [part2](https://www.kaggle.com/kashnitsky/topic-2-part-2-seaborn-and-plotly)\n3. Classification, Decision Trees and k Nearest Neighbors [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd) [:ru:](https://habrahabr.ru/company/ods/blog/322534/) [:cn:](https://www.jqr.com/article/000139), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-3-decision-trees-and-knn)\n4. Linear Classification and Regression [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) [:ru:](https://habrahabr.ru/company/ods/blog/323890/) [:cn:](https://www.jqr.com/article/000175), Kaggle Kernels: [part1](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols), [part2](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification), [part3](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-3-regularization), [part4](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), [part5](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-5-validation)\n5. Bagging and Random Forest [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7) [:ru:](https://habrahabr.ru/company/ods/blog/324402/) [:cn:](https://www.jqr.com/article/000241), Kaggle Kernels: [part1](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging), [part2](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-2-random-forest), [part3](https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-3-feature-importance)\n6. Feature Engineering and Feature Selection [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-6-feature-engineering-and-feature-selection-8b94f870706a) [:ru:](https://habrahabr.ru/company/ods/blog/325422/) [:cn:](https://www.jqr.com/article/000249), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection)\n7. Unsupervised Learning: Principal Component Analysis and Clustering [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-7-unsupervised-learning-pca-and-clustering-db7879568417) [:ru:](https://habrahabr.ru/company/ods/blog/325654/) [:cn:](https://www.jqr.com/article/000336), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering)\n8. Vowpal Wabbit: Learning with Gigabytes of Data [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-8-vowpal-wabbit-fast-learning-with-gigabytes-of-data-60f750086237) [:ru:](https://habrahabr.ru/company/ods/blog/326418/) [:cn:](https://www.jqr.com/article/000348), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-8-online-learning-and-vowpal-wabbit)\n9. Time Series Analysis with Python, part 1 [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3) [:ru:](https://habrahabr.ru/company/ods/blog/327242/) [:cn:](https://www.jqr.com/article/000450). Predicting future with Facebook Prophet, part 2 [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-part-3-predicting-the-future-with-facebook-prophet-3f3af145cdc), Kaggle Kernels: [part1](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python), [part2](https://www.kaggle.com/kashnitsky/topic-9-part-2-time-series-with-facebook-prophet)\n10. Gradient Boosting [:uk:](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-10-gradient-boosting-c751538131ac) [:ru:](https://habrahabr.ru/company/ods/blog/327250/), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-10-gradient-boosting)\n\n### Lectures\nVideolectures are uploaded to [this](https://bit.ly/2zY6Xe2) YouTube playlist.\n\nIntroduction, [video](https://youtu.be/QKTuw4PNOsU), [slides](https://bit.ly/2NuadRV)\n\n1. Exploratory data analysis with Pandas, [video](https://youtu.be/fwWCw_cE5aI). Discussion of the 1st demo assignment is [here](https://youtu.be/Twpn7ihVWjQ)\n\n\n\n### Assignments\n1. Exploratory Data Analysis of Olympic games with Pandas, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_fall2018/assignment1_pandas_olympic.ipynb?flush_cache=true). Deadline: **October 14, 20:59 CET**\n2. Exploratory Data Analysis of US flights,  [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_fall2018/assignment2_USA_flights.ipynb?flush_cache=true). Deadline: **October 21, 20:59 CET**\n\nThese are demo versions. Just for practice, they don't have an impact on rating.  \n1. Exploratory data analysis with Pandas, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment01_pandas_uci_adult.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-1-pandas-and-uci-adult-dataset)\n2. Analyzing cardiovascular disease data, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment02_analyzing_cardiovascular_desease_data.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-2-analyzing-cardiovascular-data)\n3. Decision trees with a toy task and the UCI Adult dataset, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment03_decision_trees.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-3-decision-trees)\n4. Linear Regression as an optimization problem, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment04_linreg_optimization.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-4-linear-regression-as-optimization)\n5. Logistic Regression and Random Forest in the credit scoring problem, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment05_logit_rf_credit_scoring.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-5-logit-and-rf-for-credit-scoring)\n6. Exploring OLS, Lasso and Random Forest in a regression task, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment06_regression_wine.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-6-linear-models-and-rf-for-regression)\n7. Unsupervised learning, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment07_unsupervised_learning.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-7-unupervised-learning)\n8. Implementing online regressor, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment08_implement_sgd_regressor.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-8-implementing-online-regressor)\n9. Time series analysis, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment09_time_series.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-9-time-series-analysis)\n10. Gradient boosting and flight delays, [nbviewer](https://mlcourse.ai/notebooks/blob/master/jupyter_english/assignments_demo/assignment10_flight_delays_kaggle.ipynb?flush_cache=true), [Kaggle Kernel](https://www.kaggle.com/kashnitsky/assignment-10-gradient-boosting-and-flight-delays)\n\n### Kaggle competitions\n1. Catch Me If You Can: Intruder Detection through Webpage Session Tracking. [Kaggle Inclass](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2)\n2. How good is your Medium article? [Kaggle Inclass](https://www.kaggle.com/c/how-good-is-your-medium-article/)\n\n### Rating\nThroughout the course we are maintaining a student [rating](https://drive.google.com/open?id=19AGEhUQUol6_kNLKSzBsjcGUU3qWy3BNUg8x8IFkO3Q). It takes into account credits scored in assignments and Kaggle competitions. Top students (according to the final rating) will be listed on a special Wiki page.\n\n### Community\nDiscussions between students are held in the **#mlcourse_ai** channel of the OpenDataScience Slack team. Fill in [this form](https://drive.google.com/open?id=1_pDNuVHwBxV5wuOcdaXoxBZneyAQcqfOl4V2qkqKbNQ) to get an invitation. The form will also ask you some personal questions, don't hesitate :wave:\n\n### More info\nGo to [mlcourse.ai](https://mlcourse.ai)\n\n*The course is free but you can support organizers by making a pledge on [Patreon](https://www.patreon.com/ods_mlcourse)*")
('https://raw.githubusercontent.com/davenquinn/Attitude/494803122abad56bc51e6a93c2fbc57b5477fea9/docs/example-notebooks/Plotting-Interactive.ipynb', '**Attitude** is for fitting the orientation of planes! Its core mission is to compute\nplanar fits and transform them (*along with meaningful error distributions*) into\na form that can be used for geology. It is designed to support the collection of structural\nmeasurements from remote sensing data.\n\n# Usage\n\nThe module accepts input in the form of a *n*-by-3 matrix with columns corresponding\nto X, Y, and Z coordinates. These data are commonly extracted from linear or polygonal\nfeatures on a digital elevation model.\n\nA planar fit can be constructed as such:\n\n```python\nfrom attitude import Orientation\nmeasurement = Orientation(array)\n```\n\nThe `Regression` object underlying this result can be accessed as `measurement.fit`.\n\n# Installation\n\nAttitude is packaged as a standard Python module. The latest version\nis available on PyPI and can be installed using PIP:\n```\npip install Attitude\n```\n\nIf you want to use the development version (which may be more up-to-date), you can instead run\n```\npip install git+git://github.com/davenquinn/Attitude.git\n```\n\n## Dependencies\n\nAttitude currently depends on several python modules:\n\n- Numpy\n- SciPy\n- matplotlib\n- mplstereonet\n\nOnly the first two are needed for the core functionality (fitting planes). The\nother two are for plotting (`matplotlib` and `mplstereonet`).\n\n# Future Plans\n\nThis module is in development, and there are several things I hope to enhance:\n\n- Python 3 support (it should be mostly fine, but is currently only tested on 2.7)\n- Shapely integration\n- Linking to a separate library to extract data from elevation models')
('https://raw.githubusercontent.com/rsumner31/pymc3/16f44293fd4c0d3ef4827bc3eca2b8fc5312e328/docs/source/notebooks/GP-Latent.ipynb', None)
('https://raw.githubusercontent.com/rsumner31/pymc3-23/98c9e2e947938751837717633a8f9879d93af484/docs/source/notebooks/dp_mix.ipynb', None)
('https://raw.githubusercontent.com/yoshihikosuzuki/BITS/d0eb5a3200a84fbd0ece1ebc3b1c3a347dc5b462/docs/usage.ipynb', 'Here I put some reusable (bioinformatics-related) codes as a python3 package.\n\n## How to install\n\n```bash\n$ git clone https://github.com/yoshihikosuzuki/BITS\n$ cd BITS\n$ python setup.py install\n```\n\n## How to use\n\nSee [Jupyter Notebook](https://nbviewer.jupyter.org/github/yoshihikosuzuki/BITS/blob/master/docs/usage.ipynb).')
('https://raw.githubusercontent.com/ColiLea/prosodyAOA/9d0686e539c80dcda85cfad8476632fd7ceb168e/egemap_prosody_feature_analysis.ipynb', '# Prosodic Features from Large Corpora of Child-Directed Speech as Predictors of the Age of Acquisition of Words\n\nThis repository contains code and data for the Ridge Regression analyses presented in the paper "Prosodic Features from Large Corpora of Child-Directed Speech as Predictors of the Age of Acquisition of Words (ArXiv preprint, 2017)"\n\n## Data\nThe data folder contains word type-level prosodic features (derived from the [Brent](http://childes.talkbank.org/access/Eng-NA/Brent.html) and [Providence](http://phonbank.talkbank.org/access/Eng-NA/Providence.html) \ncorpus, respectively) for a set of 600 target words (from the [wordbank](http://wordbank.stanford.edu/) project. \n\n## Code\nThe three jupyter notebooks contain python code for prosody feature analysis (egemap_prosody_feature_analysis.ipnb); language model-derived feature analysis (srilm_features_analysis.ipnb); and ridge regression models for predicting the age of acquisition of words using egemap prosody (experiment 1 in the paper) features and language model-derived features (experiment 3 in the paper) (python_regression.ipnb).\n\n\n<!-- This code requires [Torch7](http://torch.ch/) and [nngraph](http://github.com/torch/nngraph).  -->\n<!-- It is updated to use torch version around May 2016. Minimum preprocessing is needed to obtain a good accuracy, including lower-casing and tokenization. -->\n\n## Citation\n```\n@article{frermann:frank:2017,\n  author = {Lea Frermann and Michael C. Frank},\n  title = {Prosodic Features from Large Corpora of Child-Directed Speech as Predictors of the Age of Acquisition of Words},\n  year={2017},\n  journal = "arXiv preprint cs.CL/1709.09443"\n}\n\n```')
('https://raw.githubusercontent.com/modflowpy/flopy/184e716ff40c64506778bd15585254a11f92037f/examples/Notebooks/flopy3_Zaidel_example.ipynb', '<img src="https://raw.githubusercontent.com/modflowpy/flopy/master/examples/images/flopy3.png" alt="flopy3" style="width:50;height:20">\n\n### Version 3.3.0\n[![Build Status](https://travis-ci.org/modflowpy/flopy.svg?branch=master)](https://travis-ci.org/modflowpy/flopy)\n[![PyPI Version](https://img.shields.io/pypi/v/flopy.png)](https://pypi.python.org/pypi/flopy)\n[![Coverage Status](https://coveralls.io/repos/github/modflowpy/flopy/badge.svg?branch=master)](https://coveralls.io/github/modflowpy/flopy?branch=master)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/b23a5edd021b4aa19e947545ab49e577)](https://www.codacy.com/app/jdhughes-usgs/flopy?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=modflowpy/flopy&amp;utm_campaign=Badge_Grade)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/modflowpy/flopy.git/master)\n\nIntroduction\n-----------------------------------------------\n\nFloPy includes support for [MODFLOW 6](docs/mf6.md), MODFLOW-2005, MODFLOW-NWT, MODFLOW-USG, and MODFLOW-2000. Other supported MODFLOW-based models include MODPATH (version 6 and 7), MT3DMS, MT3D-USGS, and SEAWAT.\n\nFor general modeling issues, please consult a modeling forum, such as the [MODFLOW Users Group](https://groups.google.com/forum/#!forum/modflow).  Other MODFLOW resources are listed in the [MODFLOW Resources](https://github.com/modflowpy/flopy#modflow-resources) section.\n\n\nContributing\n------------------------------------------------\n\nBug reports, code contributions, or improvements to the documentation are welcome from the community. Prior to contributing, please read up on our guidelines for [contributing](CONTRIBUTING.md) and then check out one of our issues in the [hotlist: community-help](https://github.com/modflowpy/flopy/labels/hotlist%3A%20community%20help).\n\n\nDocumentation\n-----------------------------------------------\n\nFloPy code documentation is available at [http://modflowpy.github.io/flopydoc/](http://modflowpy.github.io/flopydoc/)\n\n\nGetting Started\n-----------------------------------------------\n\n### MODFLOW 6 Quick Start\n```python\nimport os\nimport flopy\nws = \'./mymodel\'\nname = \'mymodel\'\nsim = flopy.mf6.MFSimulation(sim_name=name, sim_ws=ws, exe_name=\'mf6\')\ntdis = flopy.mf6.ModflowTdis(sim)\nims = flopy.mf6.ModflowIms(sim)\ngwf = flopy.mf6.ModflowGwf(sim, modelname=name, save_flows=True)\ndis = flopy.mf6.ModflowGwfdis(gwf, nrow=10, ncol=10)\nic = flopy.mf6.ModflowGwfic(gwf)\nnpf = flopy.mf6.ModflowGwfnpf(gwf, save_specific_discharge=True)\nchd = flopy.mf6.ModflowGwfchd(gwf, stress_period_data=[[(0, 0, 0), 1.],\n                                                       [(0, 9, 9), 0.]])\nbudget_file = name + \'.bud\'\nhead_file = name + \'.hds\'\noc = flopy.mf6.ModflowGwfoc(gwf,\n                            budget_filerecord=budget_file,\n                            head_filerecord=head_file,\n                            saverecord=[(\'HEAD\', \'ALL\'), (\'BUDGET\', \'ALL\')])\nsim.write_simulation()\nsim.run_simulation()\nhead = flopy.utils.HeadFile(os.path.join(ws, head_file)).get_data()\nbud = flopy.utils.CellBudgetFile(os.path.join(ws, budget_file),\n                                 precision=\'double\')\nspdis = bud.get_data(text=\'DATA-SPDIS\')[0]\npmv = flopy.plot.PlotMapView(gwf)\npmv.plot_array(head)\npmv.plot_grid(colors=\'white\')\npmv.contour_array(head, levels=[.2, .4, .6, .8], linewidths=3.)\npmv.plot_specific_discharge(spdis, color=\'white\')\n```\n<img src="examples/images/quickstart.png" alt="plot" style="width:30;height:30">\n\n### [Frequently asked questions](docs/flopyFAQ.md)\n\n### [Tutorials](http://modflowpy.github.io/flopydoc/tutorials.html)\n\n### [Additional jupyter Notebook Examples](docs/notebook_examples.md)\n\n### [Python Script Examples](docs/script_examples.md)\n\n\nIf You Get Stuck\n-----------------------------------------------\n\nFloPy usage has been growing rapidly, and as the number of users has increased, so has the number of questions about how to use FloPy.  We ask our users to carefully consider the nature of their problem and seek help in the appropriate manner.\n\n### Questions\n\nFor questions related to how to do something with FloPy, we ask our users to submit the question to [Stack Overflow](https://stackoverflow.com) and assign the [flopy](https://stackoverflow.com/questions/tagged/flopy) tag.  Many of our recent questions have been related to MODFLOW or Python, and the Flopy developers cannot always respond to these inquiries.\n\n### Bugs\n\nIf you think you have discovered a bug in FloPy in which you feel that the program does not work as intended, then we ask you to submit a [Github issue](https://github.com/modflowpy/flopy/labels/bug).\n\n\nFloPy Supported Packages\n-----------------------------------------------\n\nA list of supported packages in FloPy is available in [docs/supported_packages.md](docs/supported_packages.md) on the github repo.\n\n\nFloPy Model Checks\n-----------------------------------------------\n\nA table of the supported and proposed model checks implemented in  FloPy is available in [docs/model_checks.md](docs/model_checks.md) on the github repo.\n\n\nFloPy Changes\n-----------------------------------------------\n\nA summary of changes in each FloPy version is available in [docs/version_changes.md](docs/version_changes.md) on the github repo.\n\n\nInstallation\n-----------------------------------------------\n\n**Python versions:**\n\nFloPy requires **Python** 3.5 (or higher).\n\n\n**Dependencies:**\n\nFloPy requires **NumPy** 1.9 (or higher).\n\n\n**For base and Anaconda Python distributions:**\n\nTo install FloPy type:\n\n    pip install flopy\n\nor\n\n\tconda install -c conda-forge flopy\n\nTo update FloPy type:\n\n    pip install flopy --upgrade\n\nor\n\n\tconda update -c conda-forge flopy\n\nTo uninstall FloPy type:\n\n    pip uninstall flopy\n\nor\n\n\tconda uninstall flopy\n\n\n**Installing from the git repository:**\n\n***Current Version of FloPy:***\n\nTo install the current version of FloPy from the git repository type:\n\n    pip install https://github.com/modflowpy/flopy/zipball/master\n\nTo update your version of FloPy with the current version from the git repository type:\n\n    pip install https://github.com/modflowpy/flopy/zipball/master --upgrade\n\n***Development version of FloPy:***\n\nTo install the latest development version of FloPy from the git repository type:\n\n    pip install https://github.com/modflowpy/flopy/zipball/develop\n\nTo update your version of FloPy with the latest development version from the git repository type:\n\n    pip install https://github.com/modflowpy/flopy/zipball/develop --upgrade\n\n\n\n***Optional Method Dependencies:***\n\nAdditional dependencies to use optional FloPy helper methods are listed below.\n\n| Method                                                                               | Python Package                                     |\n| ------------------------------------------------------------------------------------ | -------------------------------------------------- |\n| `.PlotMapView()` in `flopy.plot`                                                     | **matplotlib** >= 1.4                              |\n| `.PlotCrossSection()` in `flopy.plot`                                                | **matplotlib** >= 1.4                              |\n| `.plot()`                                                                            | **matplotlib** >= 1.4                              |\n| `.plot_shapefile()`                                                                  | **matplotlib** >= 1.4 and **Pyshp** >= 1.2         |\n| `.to_shapefile()`                                                                    | **Pyshp** >= 1.2                                   |\n| `.export(*.shp)`                                                                     | **Pyshp** >= 1.2                                   |\n| `.export(*.nc)`                                                                      | **netcdf4** >= 1.1 and **python-dateutil** >= 2.4  |\n| `.export(*.tif)`                                                                     | **rasterio**                                       |\n| `.export(*.asc)` in `flopy.utils.reference` `SpatialReference` class                 | **scipy.ndimage**                                  |\n| `.interpolate()` in `flopy.utils.reference` `SpatialReference` class                 | **scipy.interpolate**                              |\n| `.interpolate()` in `flopy.mf6.utils.reference` `StructuredSpatialReference` class   | **scipy.interpolate**                              |\n| `.get_dataframes()` in `flopy.utils.mflistfile` `ListBudget` class                   | **pandas** >= 0.15                                 |\n| `.get_dataframes()` in `flopy.utils.observationfile` `ObsFiles` class                | **pandas** >= 0.15                                 |\n| `.get_dataframes()` in `flopy.utils.sfroutputfile` `ModflowSfr2` class               | **pandas** >= 0.15                                 |\n| `.get_dataframes()` in `flopy.utils.util_list` `MfList` class                        | **pandas** >= 0.15                                 |\n| `.get_dataframes()` in `flopy.utils.zonebud` `ZoneBudget` class                      | **pandas** >= 0.15                                 |\n| `.pivot_keyarray()` in `flopy.mf6.utils.arrayutils` `AdvancedPackageUtil` class      | **pandas** >= 0.15                                 |\n| `._get_vertices()` in `flopy.mf6.utils.binaryfile_utils` `MFOutputRequester` class   | **pandas** >= 0.15                                 |\n| `.get_dataframe()` in `flopy.mf6.utils.mfobservation` `Observations` class           | **pandas** >= 0.15                                 |\n| `.df()` in `flopy.modflow.mfsfr2` `SfrFile` class                                    | **pandas** >= 0.15                                 |\n| `.time_coverage()` in `flopy.export.metadata` `acc` class - ***used if available***  | **pandas** >= 0.15                                 |\n| `.loadtxt()` in `flopy.utils.flopyio` - ***used if available***                      | **pandas** >= 0.15                                 |\n| `.generate_classes()` in `flopy.mf6.utils`                                           | [**pymake**](https://github.com/modflowpy/pymake)  |\n| `.intersect()` in `flopy.discretization.VertexGrid`                                  | **matplotlib** >= 1.4                              |\n| `GridIntersect()` in `flopy.utils.gridintersect`                                     | **shapely**                                        |\n| `GridIntersect().plot_polygon()` in `flopy.utils.gridintersect`                      | **shapely** and **descartes**                      |\n| `Raster()` in `flopy.utils.Raster`                                                   | **rasterio**, **affine**, and **scipy**            |\n| `Raster().sample_polygon()` in `flopy.utils.Raster`                                  | **shapely**                                        |\n| `Raster().crop()` in `flopy.utils.Raster`                                            | **shapely**                                        |\n\nHow to Cite\n-----------------------------------------------\n\n##### ***Citation for FloPy:***\n\n[Bakker, M., Post, V., Langevin, C. D., Hughes, J. D., White, J. T., Starn, J. J. and Fienen, M. N., 2016, Scripting MODFLOW Model Development Using Python and FloPy: Groundwater, v. 54, p. 733–739, doi:10.1111/gwat.12413.](http://dx.doi.org/10.1111/gwat.12413)\n\n##### ***Software/Code citation for FloPy:***\n\n[Bakker, M., Post, V., Langevin, C. D., Hughes, J. D., White, J. T., Leaf, A. T., Paulinski, S. R., Larsen, J. D., Toews, M. W., Morway, E. D., Bellino, J. C., Starn, J. J., and Fienen, M. N., 2019, FloPy v3.3.0: U.S. Geological Survey Software Release, 14 December 2019, http://dx.doi.org/10.5066/F7BK19FH](http://dx.doi.org/10.5066/F7BK19FH)\n\n\nMODFLOW Resources\n-----------------------------------------------\n\n+ [MODFLOW and Related Programs](http://water.usgs.gov/ogw/modflow/)\n+ [Online guide for MODFLOW-2000](http://water.usgs.gov/nrp/gwsoftware/modflow2000/Guide/index.html)\n+ [Online guide for MODFLOW-2005](http://water.usgs.gov/ogw/modflow/MODFLOW-2005-Guide/)\n+ [Online guide for MODFLOW-NWT](http://water.usgs.gov/ogw/modflow-nwt/MODFLOW-NWT-Guide/)\n\n\nDisclaimer\n----------\n\nThis software has been approved for release by the U.S. Geological Survey\n(USGS). Although the software has been subjected to rigorous review, the USGS\nreserves the right to update the software as needed pursuant to further analysis\nand review. No warranty, expressed or implied, is made by the USGS or the U.S.\nGovernment as to the functionality of the software and related material nor\nshall the fact of release constitute any such warranty. Furthermore, the\nsoftware is released on condition that neither the USGS nor the U.S. Government\nshall be held liable for any damages resulting from its authorized or\nunauthorized use.')
('https://raw.githubusercontent.com/bjodah/pyneqsys/5e89a86fcf1617a3132d095f9a80770ea855b3de/examples/chem_equil_ammonia.ipynb', None)
('https://raw.githubusercontent.com/opesci/devito/9a28c83faf034ead38249587001a1dc0748bfe53/examples/seismic/tutorials/02_rtm.ipynb', '# Devito: Fast Finite Difference Computation from Symbolic Specification\n\n![Build Status](https://travis-ci.org/opesci/devito.svg?branch=master)\n![Code Coverage](https://codecov.io/gh/opesci/devito/branch/master/graph/badge.svg)\n\n[Devito](http://www.devitoproject.org) is a software to\nimplement optimised finite difference (FD) computation from\nhigh-level symbolic problem definitions. Starting from symbolic\nequations defined in [SymPy](http://www.sympy.org/en/index.html),\nDevito employs automated code generation and just-in-time (JIT)\ncompilation to execute FD kernels on multiple computer platforms.\n\n## Get in touch\n\nIf you\'re using Devito, we would like to hear from you. Whether you\nare facing issues or just trying it out, join the\n[conversation](https://opesci-slackin.now.sh).\n\n## Quickstart\n\nThe recommended way to install Devito uses the Conda package manager\nfor installation of the necessary software dependencies. Please\ninstall either [Anaconda](https://www.continuum.io/downloads) or\n[Miniconda](https://conda.io/miniconda.html) using the instructions\nprovided at the download links. You will need the Python 3.6 version.\n\nTo install Devito, including examples, tests and tutorial notebooks,\nfollow these simple passes:\n\n```sh\ngit clone https://github.com/opesci/devito.git\ncd devito\nconda env create -f environment.yml\nsource activate devito\npip install -e .\n```\n\nAlternatively, you can also install and run Devito via\n[Docker](https://www.docker.com/):\n\n```sh\n# get the code\ngit clone https://github.com/opesci/devito.git\ncd devito\n\n# run the tests\ndocker-compose run devito /tests\n\n# start a jupyter notebook server on port 8888\ndocker-compose up devito\n\n# start a bash shell with devito\ndocker-compose run devito /bin/bash\n```\n\n## Examples\n\nAt the core of the Devito API are the so-called `Operator` objects, which\nallow the creation and execution of efficient FD kernels from SymPy\nexpressions. Examples of how to define operators are provided:\n\n* A set of introductory notebook tutorials introducing the basic\n  features of Devito operators can be found under\n  `examples/cfd`. These tutorials cover a range of well-known examples\n  from Computational Fluid Dynamics (CFD) and are based on the excellent\n  introductory blog ["CFD Python:12 steps to\n  Navier-Stokes"](http://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/)\n  by the Lorena A. Barba group. To run these, simply go into the tutorial\n  directory and run `jupyter notebook`.\n* A set of tutorial notebooks for seismic inversion examples is available in\n  `examples/seismic/tutorials`.\n* A set of tutorial notebooks concerning the Devito compiler can be found in\n  `examples/compiler`.\n* Devito with MPI can be explored in `examples/mpi`.\n* Example implementations of acoustic forward, adjoint, gradient and born\n  operators for use in full-waveform inversion (FWI) methods can be found in\n  `examples/seismic/acoustic`.\n* An advanced example of a Tilted Transverse Isotropy forward operator\n  for use in FWI can be found in `examples/seismic/tti`.\n* A benchmark script for the acoustic and TTI forward operators can be\n  found in `benchmarks/user/benchmark.py`.\n\n\n## Compilation\n\nDevito\'s JIT compiler engine supports multiple backends, with provided\npresets for the most common compiler toolchains. By default, Devito\nwill use the default GNU compiler `g++`, but other toolchains may be\nselected by setting the `DEVITO_ARCH` environment variable to one of\nthe following values:\n * `gcc` or `gnu` - Standard GNU compiler toolchain\n * `clang` or `osx` - Mac OSX compiler toolchain via `clang`\n * `intel` or `icpc` - Intel compiler toolchain via `icpc`\n\nThread parallel execution via OpenMP can also be enabled by setting\n`DEVITO_OPENMP=1`.\n\nFor the full list of available environment variables and their\npossible values, simply run:\n\n```py\nfrom devito import print_defaults\nprint_defaults()\n```\n\nOr with Docker, run:\n\n```sh\ndocker-compose run devito /print-defaults\n```\n\n## Performance optimizations\n\nDevito supports two classes of performance optimizations:\n * Flop-count optimizations - They aim to reduce the operation count of an FD\n   kernel. These include, for example, code motion, factorization, and\n   detection of cross-stencil redundancies. The flop-count optimizations\n   are performed by routines built on top of SymPy, implemented in the\n   Devito Symbolic Engine (DSE), a sub-module of Devito.\n * Loop optimizations - Examples include SIMD vectorization and parallelism\n   (via code annotations) and loop blocking. These are performed by the Devito\n   Loop Engine (DLE), another sub-module of Devito.\n\nFurther, [YASK](https://github.com/intel/yask) is being integrated as a Devito\nbackend, for optimized execution on Intel architectures.\n\nDevito supports automatic auto-tuning of block sizes when loop blocking is\nenabled. Enabling auto-tuning is simple: it can be done by passing the special\nflag `autotune=True` to an `Operator`. Auto-tuning parameters can be set\nthrough the special environment variable `DEVITO_AUTOTUNING`.\n\nFor more information on how to drive Devito for maximum run-time performance,\nsee [here](benchmarks/user/README.md) or ask the developers on the Slack\nchannel linked above.')
('https://raw.githubusercontent.com/KDD-OpenSource/wRaR/e16881ebb10ce97b82da5367f54d23568249ddaf/examples/test_run.ipynb', '# Cost Sensitive HiCS\n\nImplemented in python. \n// TODO: Real Readme')
('https://raw.githubusercontent.com/christinabejjani/controltransfer/238963bdf25015712fb599e3fdf2efd5ed9b863e/expt1/analysis/SSAnalysis.ipynb', '# Control Transfer Paper:\n\nThis behavioral study is the first to probe whether learned control states (e.g., high attentional focus) can be transferred across closely linked cues. Although cognitive control has been traditionally viewed in opposition to associative learning, recent studies have shown that people can learn to associate particular stimuli with specific attentional control states. In experiment one, we examined whether these learned stimulus-control associations could transfer across related stimuli. First, specific face and house stimuli repeatedly preceded the presentation of particular scene stimuli to create paired face/house-scene associates. Next, scenes acted as implicit probabilistic cues that mostly preceded either congruent or incongruent Stroop trials and were thus associated with either low or high control-demand. Finally, the face and house stimuli preceded Stroop stimuli but were not predictive of congruency, testing whether stimulus-control associations would transfer from scenes to their associated face/house stimuli. We found evidence of a transfer effect, and in experiment two, showed that transfer of control state associations depended on the initial associations linking the stimuli pairs. Generalizing attentional states across linked stimuli or contexts saves effort and makes dealing with volatile environmental demands easier. This form of transfer learning has previously been demonstrated for simple actions (stimulus-response associations) and for reward associations, but to the best of our knowledge, the present study provides the first ever evidence for the associative transfer of stimulus-control associations across related stimuli. This learning mechanism may form the basis of the human ability to generalize cognitive strategies over similar contexts.')
('https://raw.githubusercontent.com/jiangzl2016/prob_prog_final_project/ac33d5a364b51e8aed67fb8108800a7b627adb61/final-project/final-notebook.ipynb', "# Final Project Repository Template\n\nThis is the final project repository template for\n[Machine Learning with Probabilistic Programming](http://www.proditus.com/syllabus2018.html).\n\n## Duplicating your own copy of this repository\n\nPlease follow these\n[instructions](https://help.github.com/articles/duplicating-a-repository/)\nto make a copy of this repository and push it to your own GitHub account.\n\nMake sure to create a **new repository** on your own GitHub account before\nstarting this process.\n\n## Final Project Notebook\nWe have included a example of a Jupyter notebook under\n`/notebook-example/example.ipynb`. This shows how to use markdown along with\nLaTeX to create section headings and typeset math.\n\nYour final project notebook should go under\n`/final-project/final-notebook.ipynb`. This notebook will be your final report.\nWe must be able to run it in a reasonable amount of time. (If your project\ninvolves a massive dataset, please contact me.)\n\nYour final report should be 8 pages long. Since it is hard to translate between\na Jupyter notebook and page numbers, we've come up with the following metric:\n> the Markdown export of your notebook should be approximately 1500 words.\n\nTo compute this, save your Jupyter notebook as a Markdown file by going to\n```\nFile > Download as > Markdown (.md)\n```\nand then counting the words\n```\nwc -w final-notebook.md\n```\n\nSince this includes your code as well, we encourage you to develop separate\npython scripts and include these in your final notebook. My recommendation is\nthat you only do basic data loading, manipulation, and plotting within Jupyter;\ndo all of the heavy lifting in separate Python files. (Note our strict\nguidelines on coding style below.)\n\n### Structure\nYour notebook should follow the basic structure described in the project\nproposal template. Make sure to clearly indicate section headings and to\npresent a clear narrative structure. Every subsection of your report should\ncorrespond to a particular step of Box's loop. Feel free to include images; you\ncan embed them in markdown cells.\n\n## Development\nUse Python 3.6+. (I use Python 3.6.5).\n\nConfigure a virtual environment.\nFollow the documentation\n[here](https://docs.python.org/3.6/tutorial/venv.html).\n(I like to use [virtualenvwrapper](http://virtualenvwrapper.readthedocs.io/).)\n\nOnce you activate the virtual environment, use `pip` to install a variety of\npackages.\n```{bash}\n(venv)$ pip install -r requirements.txt\n```\n\nThis should install Edward, along with Jupyter and other useful libraries.\nYou should see a message at the end that resembles something like\n```\nSuccessfully installed appnope-0.1.0 bleach-1.5.0 ...\n```\n\n### Additional dependencies\nIf you introduce any new dependencies to your final project, you **MUST**\nupdate `requirements.txt` with pinned versioning.\n\nIf you plan to introduce any non-pip-installable (e.g. Stan) dependencies to\nyour final project, you **MUST** provide a `Dockerfile`. (Please contact me before you do so.)\n\n### Git stuff\nThere is a comprehensive `.gitignore` file in this repository. This should prevent you from committing any unnecessary files. Please edit it as needed and do not commit any large files to the repository. (Especially huge datasets.)\n\n### Code styling\nAny additional code you write must pass `flake8` linting. See this\n[blog post](https://medium.com/python-pandemonium/what-is-flake8-and-why-we-should-use-it-b89bd78073f2) for details.\n\nThe first thing we will do after cloning your repository is:\n```{bash}\n(venv)$ pytest --flake8\n```\n\nIf your repository fails any checks, we will **deduct 20%** from your final project grade.")
('https://raw.githubusercontent.com/josmanual/finde/086d9c5189a9bd0e887a07da1575948e8dc1c507/finde.ipynb', '# finde\nFinancial independence is possible by monitoring your income and expenses.\n*finde* is here to help you with such task.\n\n## Requisites\n\n- Jupyter\n- Plotly\n- Numpy\n- Pandas\n- xlrd\n\n## Run\n```\n$ jupyter-notebook finde.ipynb\n```')
('https://raw.githubusercontent.com/adrifmonte/finding-donors/d566efce652782ef5ad484f4d69c847ebd39d6ec/finding_donors.ipynb', '# Machine Learning Engineer Nanodegree\n# Supervised Learning\n## Project: Finding Donors for CharityML\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nWe recommend students install [Anaconda](https://www.continuum.io/downloads), a pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Code\n\nTemplate code is provided in the `finding_donors.ipynb` notebook file. You will also be required to use the included `visuals.py` Python file and the `census.csv` dataset file to complete your work. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in `visuals.py` is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `finding_donors/` (that contains this README) and run one of the following commands:\n\n```bash\nipython notebook finding_donors.ipynb\n```  \nor\n```bash\njupyter notebook finding_donors.ipynb\n```\n\nThis will open the iPython Notebook software and project file in your browser.\n\n### Data\n\nThe modified census dataset consists of approximately 32,000 data points, with each datapoint having 13 features. This dataset is a modified version of the dataset published in the paper *"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid",* by Ron Kohavi. You may find this paper [online](https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf), with the original dataset hosted on [UCI](https://archive.ics.uci.edu/ml/datasets/Census+Income).\n\n**Features**\n- `age`: Age\n- `workclass`: Working Class (Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked)\n- `education_level`: Level of Education (Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool)\n- `education-num`: Number of educational years completed\n- `marital-status`: Marital status (Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse)\n- `occupation`: Work Occupation (Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces)\n- `relationship`: Relationship Status (Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried)\n- `race`: Race (White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black)\n- `sex`: Sex (Female, Male)\n- `capital-gain`: Monetary Capital Gains\n- `capital-loss`: Monetary Capital Losses\n- `hours-per-week`: Average Hours Per Week Worked\n- `native-country`: Native Country (United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands)\n\n**Target Variable**\n- `income`: Income Class (<=50K, >50K)')
('https://raw.githubusercontent.com/AlexlwAstro/phd_work/a742ee32764a742bf726cbfad219cb87d7d3e03c/franec_stampe_plots.ipynb', '# phd_work\n\n# PhD work on stellar interior modelling')
('https://raw.githubusercontent.com/youfengx/deep-learning/294442d9a626cd9b60fa8192871a3d29081f65d3/gan_mnist/Intro_to_GANs_Exercises.ipynb', '# Deep Learning Nanodegree Foundation\n\nThis repository contains material related to Udacity\'s [Deep Learning Nanodegree Foundation](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101) program. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight intialization and batch normalization.\n\nThere are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by Udacity experts, but they are available here as well.\n\n## Table Of Contents\n\n### Tutorials\n\n* [Sentiment Analysis with Numpy](https://github.com/udacity/deep-learning/tree/master/sentiment-network): [Andrew Trask](http://iamtrask.github.io/) leads you through building a sentiment analysis model, predicting if some text is positive or negative.\n* [Intro to TensorFlow](https://github.com/udacity/deep-learning/tree/master/intro-to-tensorflow): Starting building neural networks with Tensorflow.\n* [Weight Intialization](https://github.com/udacity/deep-learning/tree/master/weight-initialization): Explore how initializing network weights affects performance.\n* [Autoencoders](https://github.com/udacity/deep-learning/tree/master/autoencoder): Build models for image compression and denoising, using feed-forward and convolution networks in TensorFlow.\n* [Transfer Learning (ConvNet)](https://github.com/udacity/deep-learning/tree/master/transfer-learning). In practice, most people don\'t train their own large networkd on huge datasets, but use pretrained networks such as VGGnet. Here you\'ll use VGGnet to classify images of flowers without training a network on the images themselves.\n* [Intro to Recurrent Networks (Character-wise RNN)](https://github.com/udacity/deep-learning/tree/master/intro-to-rnns): Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text.\n* [Embeddings (Word2Vec)](https://github.com/udacity/deep-learning/tree/master/embeddings): Implement the Word2Vec model to find semantic representations of words for use in natural language processing.\n* [Sentiment Analysis RNN](https://github.com/udacity/deep-learning/tree/master/sentiment-rnn): Implement a recurrent neural network that can predict if a text sample is positive or negative.\n* [Tensorboard](https://github.com/udacity/deep-learning/tree/master/tensorboard): Use TensorBoard to visualize the network graph, as well as how parameters change through training.\n* [Reinforcement Learning (Q-Learning)](https://github.com/udacity/deep-learning/tree/master/reinforcement): Implement a deep Q-learning network to play a simple game from OpenAI Gym.\n* [Sequence to sequence](https://github.com/udacity/deep-learning/tree/master/seq2seq): Implement a sequence-to-sequence recurrent network.\n* [Batch normalization](https://github.com/udacity/deep-learning/tree/master/batch-norm): Learn how to improve training rates and network stability with batch normalizations.\n* [Generative Adversatial Network on MNIST](https://github.com/udacity/deep-learning/tree/master/gan_mnist): Train a simple generative adversarial network on the MNIST dataset.\n* [Deep Convolutional GAN (DCGAN)](https://github.com/udacity/deep-learning/tree/master/dcgan-svhn): Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.\n* [Intro to TFLearn](https://github.com/udacity/deep-learning/tree/master/intro-to-tflearn): A couple introductions to a high-level library for building neural networks.\n\n### Projects\n\n* [Your First Neural Network](https://github.com/udacity/deep-learning/tree/master/first-neural-network): Implement a neural network in Numpy to predict bike rentals.\n* [Image classification](https://github.com/udacity/deep-learning/tree/master/image-classification): Build a convolutional neural network with TensorFlow to classify CIFAR-10 images.\n* [Text Generation](https://github.com/udacity/deep-learning/tree/master/tv-script-generation): Train a recurrent neural network on scripts from The Simpson\'s (copyright Fox) to generate new scripts.\n* [Machine Translation](https://github.com/udacity/deep-learning/tree/master/language-translation): Train a sequence to sequence network for English to French translation (on a simple dataset)\n* [Face Generation](https://github.com/udacity/deep-learning/tree/master/face_generation): Use a DCGAN on the CelebA dataset to generate images of novel and realistic human faces.\n\n\n## Dependencies\n\nEach directory has a `requirements.txt` describing the minimal dependencies required to run the notebooks in that directory.\n\n### pip\n\nTo install these dependencies with pip, you can issue `pip3 install -r requirements.txt`.\n\n### Conda Environments\n\nYou can find Conda environment files for the Deep Learning program in the `environments` folder. Note that environment files are platform dependent. Versions with `tensorflow-gpu` are labeled in the filename with "GPU".')
('https://raw.githubusercontent.com/cmdavis4/sf-elections/85eb642feb8c0d9e5c73d8cfed6cc543d5fa4551/geodatabase_from_source_data.ipynb', "# Analysis of San Francisco's November 2016 Election Results\n\nThis repository contains analysis I have conducted on publicly available data regarding the November 2016 elections in San Francisco, coupled with some demographic data taken from the 2010 Bay Area Census. Most of the analysis is contained [here](./election_analysis.md). The scripts for the data pre-processing and the creation of an ArcGIS geodatabase are contained in [this IPython notebook](./geodatabase_from_source_data.ipynb).\n\n### Data sources\n* **2010 Census block lines**: https://data.sfgov.org/Geographic-Locations-and-Boundaries/Census-2010-Blocks-for-San-Francisco/2uzy-uv2r\n* **2010 Bay Area Census demographic data by block**: http://www.bayareacensus.ca.gov/small/small.htm, specifically http://www.bayareacensus.ca.gov/small/2010_Pop_Block_County.xls (note: the shapefile for this data is broken, which is why I used the .xls file and manually spatially joined it)  \n* **San Francisco precinct lines**: http://sfgov.org/elections/district-citywide-maps, specifically http://sfgov.org/elections/sites/default/files/2012lines.zip  \n* **San Francisco November 2016 election results**: http://www.sfelections.org/results/20161108/#english_detail, specifically http://www.sfelections.org/results/20161108/data/20161206/20161206_sov.xlsx")
('https://raw.githubusercontent.com/h2oai/h2o-3/18a2d0cb16b31b48a1c8d55d6a959dd60373d1c0/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb', '# H2O\n\n[![Join the chat at https://gitter.im/h2oai/h2o-3](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/h2oai/h2o-3?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nH2O scales statistics, machine learning, and math over Big Data. \n\nH2O uses familiar interfaces like R, Python, Scala, the Flow notebook graphical interface, Excel, & JSON so that Big Data enthusiasts & experts can explore, munge, model, and score datasets using a range of algorithms including advanced ones like Deep Learning. H2O is extensible so that developers can add data transformations and model algorithms of their choice and access them through all of those clients.\n\nData collection is easy. Decision making is hard. H2O makes it fast and easy to derive insights from your data through faster and better predictive modeling. H2O allows online scoring and modeling in a single platform.\n\n* [Downloading H2O-3](#Downloading)\n* [Open Source Resources](#Resources)\n    * [Issue tracking](#IssueTracking) \n* [Using H2O-3 Code Artifacts (libraries)](#Artifacts)\n* [Building H2O-3](#Building)\n* [Launching H2O after Building](#Launching)\n* [Building H2O on Hadoop](#BuildingHadoop)\n* [Sparkling Water](#Sparkling)\n* [Documentation](#Documentation)\n* [Community](#Community) / [Advisors](#Advisors) / [Investors](#Investors)\n\n<a name="Downloading"></a>\n## 1. Downloading H2O-3\n\nWhile most of this README is written for developers who do their own builds, most H2O users just download and use a pre-built version.  If that\'s you, just follow these steps:\n\n1.  Point to <http://h2o.ai>\n2.  Click on Download\n3.  Scroll down to find the section for H2O-3\n4.  Click the version you want (generally the latest numbered release)\n\n<a name="Resources"></a>\n## 2. Open Source Resources\n\nMost people interact with three primary open source resources:  **GitHub** (which you\'ve already found), **JIRA** (for issue tracking), and **h2ostream** (a community discussion forum).\n\n<a name="IssueTracking"></a>\n### 2.1 Issue tracking\n\nYou can browse and create new issues in our open source **JIRA**:  <http://jira.h2o.ai>\n\n*  You can **browse** and search for **issues** without logging in to JIRA:\n    1.  Click the `Issues` menu\n    1.  Click `Search for issues`\n*  To **create** an **issue** (either a bug or a feature request), please create yourself an account first:\n    1.  Click the `Log In` button on the top right of the screen\n    1.  Click `Create an acccount` near the bottom of the login box\n    1.  Once you have created an account and logged in, use the `Create` button on the menu to create an issue\n    1.  Create H2O-3 issues in the [PUBDEV](https://0xdata.atlassian.net/projects/PUBDEV/issues) project\n\n> (Note: There is only one issue tracking system for the project.  GitHub issues are not enabled, you must use JIRA.)\n\n### 2.2 List of open source resources\n\n*  GitHub\n    * <https://github.com/h2oai/h2o-3>\n*  JIRA - file issues here ([PUBDEV](https://0xdata.atlassian.net/projects/PUBDEV/issues) contains issues for the current H2O-3 project)\n    * <http://jira.h2o.ai>\n*  h2ostream community forum - ask your questions here\n    * Web: <https://groups.google.com/d/forum/h2ostream>\n    * Mail to: [h2ostream@googlegroups.com](mailto:h2ostream@googlegroups.com)\n*  Documentation\n    * Bleeding edge nightly build page: <https://s3.amazonaws.com/h2o-release/h2o/master/latest.html>\n    * FAQ: <http://h2o.ai/product/faq/>\n*  Download (pre-built packages)\n    * <http://h2o.ai/download>\n*  Jenkins\n    * <http://test.h2o.ai>\n*  Website\n    * <http://h2o.ai>\n*  Follow us on Twitter, [@h2oai](https://twitter.com/h2oai)\n\n\n<a name="Artifacts"></a>\n## 3. Using H2O-3 Artifacts\n\nEvery nightly build publishes R, Python, Java, and Scala artifacts to a build-specific repository.  In particular, you can find Java artifacts in the maven/repo directory.\n\nHere is an example snippet of a gradle build file using h2o-3 as a dependency.  Replace x, y, z, and nnnn with valid numbers.\n\n```\n// h2o-3 dependency information\ndef h2oBranch = \'master\'\ndef h2oBuildNumber = \'nnnn\'\ndef h2oProjectVersion = "x.y.z.${h2oBuildNumber}"\n\nrepositories {\n  // h2o-3 dependencies\n  maven {\n    url "https://s3.amazonaws.com/h2o-release/h2o-3/${h2oBranch}/${h2oBuildNumber}/maven/repo/"\n  }\n}\n\ndependencies {\n  compile "ai.h2o:h2o-core:${h2oProjectVersion}"\n  compile "ai.h2o:h2o-algos:${h2oProjectVersion}"\n  compile "ai.h2o:h2o-web:${h2oProjectVersion}"\n  compile "ai.h2o:h2o-app:${h2oProjectVersion}"\n}\n```\n\nRefer to the latest H2O-3 bleeding edge [nightly build page](http://s3.amazonaws.com/h2o-release/h2o-3/master/latest.html) for information about installing nightly build artifacts.\n\nRefer to the [h2o-droplets GitHub repository](https://github.com/h2oai/h2o-droplets) for a working example of how to use Java artifacts with gradle.\n\n> Note: Stable H2O-3 artifacts are periodically published to Maven Central ([click here to search](http://search.maven.org/#search%7Cga%7C1%7Cai.h2o)) but may substantially lag behind H2O-3 Bleeding Edge nightly builds.\n\n-----\n\n<a name="Building"></a>\n## 4. Building H2O-3\n\nGetting started with H2O development requires [JDK 1.7](http://www.oracle.com/technetwork/java/javase/downloads/), [Node.js](https://nodejs.org/), and Gradle.  We use the Gradle wrapper (called `gradlew`) to ensure up-to-date local versions of Gradle and other dependencies are installed in your development directory.\n\n### 4.1. Building from the command line (Quick Start)\n\nTo build H2O from the repository, perform the following steps. \n\n\n#### Recipe 1: Clone fresh, build, skip tests, and run H2O\n\n```\n# Build H2O\ngit clone https://github.com/h2oai/h2o-3.git\ncd h2o-3\n./gradlew build -x test\n\n# Start H2O\njava -jar build/h2o.jar\n\n# Point browser to http://localhost:54321\n\n```\n\n#### Recipe 2: Clone fresh, build, and run tests (requires a working install of R)\n\n```\ngit clone https://github.com/h2oai/h2o-3.git\ncd h2o-3\n./gradlew syncSmalldata\n./gradlew syncRPackages\n./gradlew build\n```\n\n>**Notes**: \n>\n> - Running tests starts five test JVMs that form an H2O cluster and requires at least 8GB of RAM (preferably 16GB of RAM).\n> - Running `./gradlew syncRPackages` is supported on Windows, OS X, and Linux, and is strongly recommended but not required. `./gradlew syncRPackages` ensures a complete and consistent environment with pre-approved versions of the packages required for tests and builds. The packages can be installed manually, but we recommend setting an ENV variable and using `./gradlew syncRPackages`. To set the ENV variable, use the following format (where `${WORKSPACE} can be any path):\n>  \n>  ```\n> mkdir -p ${WORKSPACE}/Rlibrary\nexport R_LIBS_USER=${WORKSPACE}/Rlibrary\n```\n\n#### Recipe 3:  Pull, clean, build, and run tests\n\n```\ngit pull\n./gradlew syncSmalldata\n./gradlew syncRPackages\n./gradlew clean\n./gradlew build\n```\n\n#### Notes\n\n - We recommend using `./gradlew clean` after each `git pull`.\n\n- Skip tests by adding `-x test` at the end the gradle build command line.  Tests typically run for 7-10 minutes on a Macbook Pro laptop with 4 CPUs (8 hyperthreads) and 16 GB of RAM.\n\n- Syncing smalldata is not required after each pull, but if tests fail due to missing data files, then try `./gradlew syncSmalldata` as the first troubleshooting step.  Syncing smalldata downloads data files from AWS S3 to the smalldata directory in your workspace.  The sync is incremental.  Do not check in these files.  The smalldata directory is in .gitignore.  If you do not run any tests, you do not need the smalldata directory.\n- Running `./gradlew syncRPackages` is supported on Windows, OS X, and Linux, and is strongly recommended but not required. `./gradlew syncRPackages` ensures a complete and consistent environment with pre-approved versions of the packages required for tests and builds. The packages can be installed manually, but we recommend setting an ENV variable and using `./gradlew syncRPackages`. To set the ENV variable, use the following format (where `${WORKSPACE} can be any path):\n\n  ```\n  mkdir -p ${WORKSPACE}/Rlibrary\n  export R_LIBS_USER=${WORKSPACE}/Rlibrary\n  ```\n\n#### Recipe 4:  Just building the docs\n\n```\n./gradlew clean && ./gradlew build -x test && (export DO_FAST=1; ./gradlew dist)\nopen target/docs-website/h2o-docs/index.html\n```\n\n### 4.2. Setup on all Platforms\n\n> **Note**: The following instructions assume you have installed the latest version of [**Pip**](https://pip.pypa.io/en/latest/installing/#install-or-upgrade-pip), which is installed with the latest version of [**Python**](https://www.python.org/downloads/).  \n\n##### Install required Python packages (prepending with `sudo` if unsuccessful)\n\n    pip install grip\n    pip install tabulate\n    pip install wheel\n    pip install scikit-learn\n\nPython tests require:\n\n    pip install scikit-learn\n    pip install numpy\n    pip install scipy\n    pip install pandas\n    pip install statsmodels\n    pip install patsy\n    pip install future\n\n### 4.3. Setup on Windows\n\n##### Step 1: Download and install [WinPython](https://winpython.github.io). \n  From the command line, validate `python` is using the newly installed package by using `which python` (or `sudo which python`). [Update the Environment variable](https://github.com/winpython/winpython/wiki/Environment) with the WinPython path.\n  \n###### Step 2: Install required Python packages:\n\n    pip install grip\n    pip install tabulate\n    pip install wheel\n\n##### Step 3: Install JDK\n\nInstall [Java 1.7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html) and add the appropriate directory `C:\\Program Files\\Java\\jdk1.7.0_65\\bin` with java.exe to PATH in Environment Variables. To make sure the command prompt is detecting the correct Java version, run:\n\n    javac -version\n\nThe CLASSPATH variable also needs to be set to the lib subfolder of the JDK: \n\n    CLASSPATH=/<path>/<to>/<jdk>/lib\n\n##### Step 4. Install Node.js\n\nInstall [Node.js](http://nodejs.org/download/) and add the installed directory `C:\\Program Files\\nodejs`, which must include node.exe and npm.cmd to PATH if not already prepended. \n\n##### Step 5. Install R, the required packages, and Rtools:\n\nTo install these packages from within an R session, enter:\n\n    R> install.packages("RCurl")\n    R> install.packages("jsonlite")\n    R> install.packages("statmod")\n    R> install.packages(c("devtools", "roxygen2", "testthat"))\n\nInstall [R](http://www.r-project.org/) and add the preferred bin\\i386 or bin\\x64 directory to your PATH.\n\nNote: Acceptable versions of R are >= 2.13 && <= 3.0.0 && >= 3.1.1.\n\nTo manually install packages, download the releases of the following R packages: \n\n- [bitops](http://cran.r-project.org/package=bitops)\n- [devtools](http://cran.r-project.org/package=devtools)\n- [digest](http://cran.r-project.org/package=digest)\n- [Rcpp](http://cran.r-project.org/package=Rcpp)\n- [RCurl](http://cran.r-project.org/package=RCurl)\n- [jsonlite](http://cran.r-project.org/package=jsonlite)\n- [roxygen2](http://cran.r-project.org/package=roxygen2)\n- [statmod](http://cran.r-project.org/package=statmod)\n- [stringr](http://cran.r-project.org/package=stringr)\n- [testthat](http://cran.r-project.org/package=testthat).\n\n```\n    cd Downloads\n    R CMD INSTALL bitops_x.x-x.zip\n    R CMD INSTALL RCurl_x.xx-x.x.zip\n    R CMD INSTALL jsonlite_x.x.xx.zip\n    R CMD INSTALL statmod_x.x.xx.zip\n    R CMD INSTALL Rcpp_x.xx.x.zip\n    R CMD INSTALL digest_x.x.x.zip\n    R CMD INSTALL testthat_x.x.x.zip\n    R CMD INSTALL stringr_x.x.x.zip\n    R CMD INSTALL roxygen2_x.x.x.zip\n    R CMD INSTALL devtools_x.x.x.zip\n```\n\nFinally, install [Rtools](http://cran.r-project.org/bin/windows/Rtools/), which is a collection of command line tools to facilitate R development on Windows.\n>**NOTE**: During Rtools installation, do **not** install Cygwin.dll.\n\n##### Step 6. Install [Cygwin](https://cygwin.com/setup-x86_64.exe)\n**NOTE**: During installation of Cygwin, deselect the Python packages to avoid a conflict with the Python.org package. \n\n###### Step 6b. Validate Cygwin\nIf Cygwin is already installed, remove the Python packages or ensure that Native Python is before Cygwin in the PATH variable. \n\n##### Step 7. Update or validate the Windows PATH variable to include R, Java JDK, Cygwin. \n\n##### Step 8. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)\n\nIf you don\'t already have a Git client, please install one.  The default one can be found here http://git-scm.com/downloads.  Make sure that command prompt support is enabled before the installation.\n\nDownload and update h2o-3 source codes:\n\n    git clone https://github.com/h2oai/h2o-3\n\n##### Step 9. Run the top-level gradle build:\n\n    cd h2o-3\n    ./gradlew.bat build\n\n> If you encounter errors run again with `--stacktrace` for more instructions on missing dependencies.\n\n\n### 4.4. Setup on OS X\n\nIf you don\'t have [Homebrew](http://brew.sh/), we recommend installing it.  It makes package management for OS X easy.\n\n##### Step 1. Install JDK\n\nInstall [Java 1.7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html). To make sure the command prompt is detecting the correct Java version, run:\n\n    javac -version\n\n##### Step 2. Install Node.js:\n\nUsing Homebrew:\n\n    brew install node\n\nOtherwise, install from the [NodeJS website](http://nodejs.org/download/). \n\n##### Step 3. Install R and the required packages:\n\nInstall [R](http://www.r-project.org/) and add the bin directory to your PATH if not already included.\n\n<a name="InstallRPackagesInUnix"></a>\n\nInstall the following R packages: \n\n- [RCurl](http://cran.r-project.org/package=RCurl)\n- [jsonlite](http://cran.r-project.org/package=jsonlite)\n- [statmod](http://cran.r-project.org/package=statmod)\n- [devtools](http://cran.r-project.org/package=devtools)\n- [roxygen2](http://cran.r-project.org/package=roxygen2) \n- [testthat](http://cran.r-project.org/package=testthat).\n\n```   \n    cd Downloads\n    R CMD INSTALL bitops_x.x-x.tgz\n    R CMD INSTALL RCurl_x.xx-x.x.tgz\n    R CMD INSTALL jsonlite_x.x.xx.tgz\n    R CMD INSTALL statmod_x.x.xx.tgz\n    R CMD INSTALL Rcpp_x.xx.x.tgz\n    R CMD INSTALL digest_x.x.x.tgz\n    R CMD INSTALL testthat_x.x.x.tgz\n    R CMD INSTALL stringr_x.x.x.tgz\n    R CMD INSTALL roxygen2_x.x.x.tgz\n    R CMD INSTALL devtools_x.x.x.tgz\n```\nTo install these packages from within an R session:\n\n    R> install.packages("RCurl")\n    R> install.packages("jsonlite")\n    R> install.packages("statmod")\n    R> install.packages(c("devtools", "roxygen2", "testthat"))\n\n##### Step 4. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)\n\nOS X should already have Git installed. To download and update h2o-3 source codes:\n\n    git clone https://github.com/h2oai/h2o-3\n\n##### Step 5. Run the top-level gradle build:\n\n    cd h2o-3\n    ./gradlew build\n\n> If you encounter errors run again with `--stacktrace` for more instructions on missing dependencies.\n\n### 4.5. Setup on Ubuntu 14.04\n\n##### Step 1. Install Node.js\n\n    curl -sL https://deb.nodesource.com/setup_0.12 | sudo bash -\n    sudo apt-get install -y nodejs\n\n##### Step 2. Install JDK:\n\nInstall [Java 1.7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html). Installation instructions can be found here [JDK installation](http://askubuntu.com/questions/56104/how-can-i-install-sun-oracles-proprietary-java-jdk-6-7-8-or-jre). To make sure the command prompt is detecting the correct Java version, run:\n\n    javac -version\n\n##### Step 3. Install R and the required packages:\n\nInstallation instructions can be found here [R installation](http://cran.r-project.org).  Click “Download R for Linux”.  Click “ubuntu”.  Follow the given instructions.\n\nTo install the required packages, follow the [same instructions as for OS X above](#InstallRPackagesInUnix).\n\n>**Note**: If the process fails to install RStudio Server on Linux, run one of the following: \n>\n>`sudo apt-get install libcurl4-openssl-dev`\n>\n>or\n> \n>`sudo apt-get install libcurl4-gnutls-dev`\n\n##### Step 4. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)\n\nIf you don\'t already have a Git client:\n\n    sudo apt-get install git\n\nDownload and update h2o-3 source codes:\n\n    git clone https://github.com/h2oai/h2o-3\n\n##### Step 5. Run the top-level gradle build:\n\n    cd h2o-3\n    ./gradlew build\n\n> If you encounter errors, run again using `--stacktrace` for more instructions on missing dependencies.\n\n> Make sure that you are not running as root, since `bower` will reject such a run.\n\n### 4.6. Setup on Ubuntu 13.10\n\n##### Step 1. Install Node.js\n\n    curl -sL https://deb.nodesource.com/setup_0.12 | sudo bash -\n    sudo apt-get install -y nodejs\n   \n\n##### Steps 2-4. Follow steps 2-4 for Ubuntu 14.04\n\n### 4.7. Setting up your preferred IDE environment\n\nFor users of Intellij\'s IDEA, generate project files with:\n\n    ./gradlew idea\n\nFor users of Eclipse, generate project files with:\n\n    ./gradlew eclipse\n\n\n\n###4.7 Setup on CentOS 7\n\n```\ncd /opt\nsudo wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz"\n\nsudo tar xzf jdk-7u79-linux-x64.tar.gz\ncd jdk1.7.0_79\n\nsudo alternatives --install /usr/bin/java java /opt/jdk1.7.0_79/bin/java 2\n\nsudo alternatives --install /usr/bin/jar jar /opt/jdk1.7.0_79/bin/jar 2\nsudo alternatives --install /usr/bin/javac javac /opt/jdk1.7.0_79/bin/javac 2\nsudo alternatives --set jar /opt/jdk1.7.0_79/bin/jar\nsudo alternatives --set javac /opt/jdk1.7.0_79/bin/javac\n                                                                                                                                                                       \ncd /opt\n\nsudo wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm\nsudo rpm -ivh epel-release-7-5.noarch.rpm\n\nsudo echo "multilib_policy=best" >> /etc/yum.conf\nsudo yum -y update\n\nsudo yum -y install R R-devel git python-pip openssl-devel libxml2-devel libcurl-devel gcc gcc-c++ make openssl-devel kernel-devel texlive texinfo texlive-latex-fonts libX11-devel mesa-libGL-devel mesa-libGL nodejs npm python-devel numpy scipy python-pandas\n\nsudo pip install scikit-learn grip tabulate statsmodels wheel\n\nmkdir ~/Rlibrary\nexport JAVA_HOME=/opt/jdk1.7.0_79\nexport JRE_HOME=/opt/jdk1.7.0_79/jre\nexport PATH=$PATH:/opt/jdk1.7.0_79/bin:/opt/jdk1.7.0_79/jre/bin\nexport R_LIBS_USER=~/Rlibrary\n\n# install local R packages\nR -e \'install.packages(c("RCurl","jsonlite","statmod","devtools","roxygen2","testthat"), dependencies=TRUE, repos="http://cran.rstudio.com/")\'\n\ncd\ngit clone https://github.com/h2oai/h2o-3.git\ncd h2o-3\n\n# Build H2O\n./gradlew syncSmalldata\n./gradlew syncRPackages\n./gradlew build -x test\n\n```\n\n\n<a name="Launching"></a>\n## 5. Launching H2O after Building\n\n    java -jar build/h2o.jar\n\n\n<a name="BuildingHadoop"></a>\n## 6. Building H2O on Hadoop\n\nPre-built H2O-on-Hadoop zip files are available on the [download page](http://h2o.ai/download).  Each Hadoop distribution version has a separate zip file in h2o-3.\n\nTo build H2O with Hadoop support yourself, first install sphinx for python: `pip install sphinx`\nThen start the build by entering  the following from the top-level h2o-3 directory:\n\n    (export BUILD_HADOOP=1; ./gradlew build -x test)\n    ./gradlew dist\n\nThis will create a directory called \'target\' and generate zip files there.  Note that `BUILD_HADOOP` is the default behavior when the username is `jenkins` (refer to `settings.gradle`); otherwise you have to request it, as shown above.\n\n\n### Adding support for a new version of Hadoop\n\nIn the `h2o-hadoop` directory, each Hadoop version has a build directory for the driver and an assembly directory for the fatjar.\n\nYou need to:\n\n1.  Add a new driver directory and assembly directory (each with a `build.gradle` file) in `h2o-hadoop`\n2.  Add these new projects to `h2o-3/settings.gradle`\n3.  Add the new Hadoop version to `HADOOP_VERSIONS` in `make-dist.sh`\n4.  Add the new Hadoop version to the list in `h2o-dist/buildinfo.json`\n\n### Debugging HDFS\n\nThese are the required steps to debug HDFS in IDEA as a standalone H2O process.\n\nDebugging H2O on Hadoop as a `hadoop jar` hadoop mapreduce job is a difficult thing to do. However, what you can do relatively easily is tweak the gradle settings for the project so that H2OApp has HDFS as a dependency.  Here are the steps:\n\n1.  Make the following changes to gradle build files below\n    *  Change the `hadoop-client` version in `h2o-persist-hdfs` to the desired version     \n    *  Add `h2o-persist-hdfs` as a dependency to `h2o-app`\n1.  Close IDEA\n1.  `./gradlew cleanIdea`\n1.  `./gradlew idea`\n1.  Re-open IDEA\n1.  Run or debug H2OApp, and you will now be able to read from HDFS inside the IDE debugger\n\n`h2o-persist-hdfs` is normally only a dependency of the assembly modules, since those are not used by any downstream modules.  We want the final module to define its own version of HDFS if any is desired.\n\nNote this example is for MapR 4, which requires the additional `org.json` dependency to work properly.\n\n```\n$ git diff\ndiff --git a/h2o-app/build.gradle b/h2o-app/build.gradle\nindex af3b929..097af85 100644\n--- a/h2o-app/build.gradle\n+++ b/h2o-app/build.gradle\n@@ -8,5 +8,6 @@ dependencies {\n   compile project(":h2o-algos")\n   compile project(":h2o-core")\n   compile project(":h2o-genmodel")\n+  compile project(":h2o-persist-hdfs")\n }\n \ndiff --git a/h2o-persist-hdfs/build.gradle b/h2o-persist-hdfs/build.gradle\nindex 41b96b2..6368ea9 100644\n--- a/h2o-persist-hdfs/build.gradle\n+++ b/h2o-persist-hdfs/build.gradle\n@@ -2,5 +2,6 @@ description = "H2O Persist HDFS"\n \n dependencies {\n   compile project(":h2o-core")\n-  compile("org.apache.hadoop:hadoop-client:2.0.0-cdh4.3.0")\n+  compile("org.apache.hadoop:hadoop-client:2.4.1-mapr-1408")\n+  compile("org.json:org.json:chargebee-1.0")\n }\n```\n\n-----\n\n<a name="Sparkling"></a>\n## 7. Sparkling Water\n\nSparkling Water combines two open-source technologies: Apache Spark and H2O, our machine learning engine.  It makes H2O’s library of Advanced Algorithms, including Deep Learning, GLM, GBM, K-Means, and Distributed Random Forest, accessible from Spark workflows. Spark users can select the best features from either platform to meet their Machine Learning needs.  Users can combine Spark\'s RDD API and Spark MLLib with H2O’s machine learning algorithms, or use H2O independently of Spark for the model building process and post-process the results in Spark. \n\n**Sparkling Water Resources**:\n\n* [Download page for pre-built packages](http://h2o.ai/download/) (Scroll down for Sparkling Water)\n* [Sparkling Water GitHub repository](https://github.com/h2oai/sparkling-water)\n* [README](https://github.com/h2oai/sparkling-water/blob/master/README.md)\n* [Developer documentation](https://github.com/h2oai/sparkling-water/blob/master/DEVEL.md)\n\n<a name="Documentation"></a>\n## 8. Documentation\n\n### Generate REST API documentation \n\nTo generate the REST API documentation, use the following commands: \n\n    cd ~/h2o-3\n    cd py\n    python ./generate_rest_api_docs.py  # to generate Markdown only\n    python ./generate_rest_api_docs.py --generate_html  --github_user GITHUB_USER --github_password GITHUB_PASSWORD # to generate Markdown and HTML\n\nThe default location for the generated documentation is `build/docs/REST`. \n\nIf the build fails, try `gradlew clean`, then `git clean -f`. \n\n### Bleeding edge build documentation\n\nDocumentation for each bleeding edge nightly build is available on the [nightly build page](http://s3.amazonaws.com/h2o-release/h2o-3/master/latest.html).\n\n-----\n\n<a name="Community"></a>\n## 9. Community\n\nWe will breathe & sustain a vibrant community with the focus of taking a software engineering approach to data science and empowering everyone interested in data to be able to hack data using math and algorithms.\nJoin us on google groups at [h2ostream](https://groups.google.com/forum/#!forum/h2ostream) and feel free to file issues directly on our [JIRA](http://jira.h2o.ai). \n\nTeam & Committers\n\n```\nSriSatish Ambati\nCliff Click\nTom Kraljevic\nTomas Nykodym\nMichal Malohlava\nKevin Normoyle\nSpencer Aiello\nAnqi Fu\nNidhi Mehta\nArno Candel\nJosephine Wang\nAmy Wang\nMax Schloemer\nRay Peck\nPrithvi Prabhu\nBrandon Hill\nJeff Gambera\nAriel Rao\nViraj Parmar\nKendall Harris\nAnand Avati\nJessica Lanford\nAlex Tellez\nAllison Washburn\nAmy Wang\nErik Eckstrand\nNeeraja Madabhushi\nSebastian Vidrio\nBen Sabrin\nMatt Dowle\nMark Landry\nErin LeDell\nOleg Rogynskyy\nNick Martin\nNancy Jordan \nNishant Kalonia\nNadine Hussami\nJeff Cramer\nStacie Spreitzer\nVinod Iyengar\nCharlene Windom\nParag Sanghavi\n```\n\n<a name="Advisors"></a>\n## Advisors\n\nScientific Advisory Council\n\n```\nStephen Boyd\nRob Tibshirani\nTrevor Hastie\n```\n\nSystems, Data, FileSystems and Hadoop\n\n```\nDoug Lea\nChris Pouliot\nDhruba Borthakur\n```\n\n<a name="Investors"></a>\n## Investors\n\n```\nJishnu Bhattacharjee, Nexus Venture Partners\nAnand Babu Periasamy\nAnand Rajaraman\nAsh Bhardwaj\nRakesh Mathur\nMichael Marks\nEgbert Bierman\nRajesh Ambati\n```')
('https://raw.githubusercontent.com/roshcagra/Shakespearebot/a93e95d13aae8ad2b6f95257e9facec7601735b0/hmm-rhyme-and-meter.ipynb', None)
('https://raw.githubusercontent.com/aalto1/digitalepidemiology/d01f6d4622df04f8f67e1376a77b69a12ac274b9/influenza.ipynb', '# digitalepidemiology')
('https://raw.githubusercontent.com/yyu/jupyter_scratchbook/aa6f39b0cea47bdb6dfe6bb36143243fb490a618/ipyleaflet.ipynb', '## jupyter scratchbook\n\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/yyu/jupyter_scratchbook/master?urlpath=tree/ipyleaflet.ipynb)')
('https://raw.githubusercontent.com/LiChangNY/jarvis/4e5f0c24e4f73b97f1f47a7a3ad853e4f688aa87/jarvis/test.ipynb', '# jarvis (just another visualization tool)\nBuilt on top of d3, jarvis is a visualization library that can be rendered in a browser or Jupyter Notebook. Hence, people who don\'t know javascript or d3 can make some nice, interactive charts in mininal Python code.\n\nExamples here: https://lichangny.github.io/jarvis/. You can also download the [test.ipynb](https://github.com/LiChangNY/jarvis/blob/master/jarvis/test.ipynb) to see how to render charts in Jupyter Notebook.\n\n## Chart Types\nIf you want to visualize basic graphes (e.g., line charts, bar charts, and pie charts, etc.), there\'re many open-source libraries - To name a few, nvd3, plotly, and altair - that would fulfill your purpose and certainly each has its own pros and cons. The current plan for jarvis is to focus on more complex charts that are not used very commonly. For example:\n\n### Force-directed graph\n\n<img src="https://github.com/LiChangNY/jarvis/blob/master/img/force%20graph.png" width="600" height="450">\n\n### Sankey chart\n\n<img src="https://github.com/LiChangNY/jarvis/blob/master/img/sankey.png" width="600" height="450">\n\n### Radial chart\n\n<img src="https://github.com/LiChangNY/jarvis/blob/master/img/radial.png" width="600" height="550">\n\n### Geo-map\n\n<img src="https://github.com/LiChangNY/jarvis/blob/master/img/map.png" width="600" height="450">\n\n### A map with orthographic projection\n\n<img src="https://github.com/LiChangNY/jarvis/blob/master/img/globe.png" width="480" height="450">')
('https://raw.githubusercontent.com/vangj/py-bbn/37f6d0f82e0e6222eb48e90ebce61855c61a8e5b/jupyter/generate-bbn.ipynb', '# PyBBN\n\nPyBBN is Python library for Bayesian Belief Networks (BBNs) exact inference using the \n[junction tree algorithm](https://en.wikipedia.org/wiki/Junction_tree_algorithm) or Probability\nPropagation in Trees of Clusters. The implementation is taken directly from [C. Huang and A. Darwiche, "Inference in\nBelief Networks: A Procedural Guide," in International Journal of Approximate Reasoning, vol. 15,\npp. 225--263, 1999](http://pages.cs.wisc.edu/~dpage/ijar95.pdf). PyBBN also has approximate\ninference algorithm using [Gibbs sampling](http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf) for\nlinear Gaussian BBN models. The exact inference algorithm is for BBNs that have all variables\nthat are discrete, while the approximate inference algorithm is for BBNs that have all variables\nthat are continuous (and assume to take a multivariate Gaussian distribution). Additionally, there is\nthe ability to generate singly- and multi-connected graphs, which is taken from [JS Ide and FG Cozman, \n"Random Generation of Bayesian Network," in Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol 2507](https://pdfs.semanticscholar.org/5273/2fb57129443592024b0e7e46c2a1ec36639c.pdf).\n\n# Exact Inference Usage\n\nBelow is an example code to create a Bayesian Belief Network, transform it into a join tree, \nand then set observation evidence. The last line prints the marginal probabilities for each node.\n\n```python\nfrom pybbn.graph.dag import Bbn\nfrom pybbn.graph.edge import Edge, EdgeType\nfrom pybbn.graph.jointree import EvidenceBuilder\nfrom pybbn.graph.node import BbnNode\nfrom pybbn.graph.variable import Variable\nfrom pybbn.pptc.inferencecontroller import InferenceController\n\n# create the nodes\na = BbnNode(Variable(0, \'a\', [\'on\', \'off\']), [0.5, 0.5])\nb = BbnNode(Variable(1, \'b\', [\'on\', \'off\']), [0.5, 0.5, 0.4, 0.6])\nc = BbnNode(Variable(2, \'c\', [\'on\', \'off\']), [0.7, 0.3, 0.2, 0.8])\nd = BbnNode(Variable(3, \'d\', [\'on\', \'off\']), [0.9, 0.1, 0.5, 0.5])\ne = BbnNode(Variable(4, \'e\', [\'on\', \'off\']), [0.3, 0.7, 0.6, 0.4])\nf = BbnNode(Variable(5, \'f\', [\'on\', \'off\']), [0.01, 0.99, 0.01, 0.99, 0.01, 0.99, 0.99, 0.01])\ng = BbnNode(Variable(6, \'g\', [\'on\', \'off\']), [0.8, 0.2, 0.1, 0.9])\nh = BbnNode(Variable(7, \'h\', [\'on\', \'off\']), [0.05, 0.95, 0.95, 0.05, 0.95, 0.05, 0.95, 0.05])\n\n# create the network structure\nbbn = Bbn() \\\n    .add_node(a) \\\n    .add_node(b) \\\n    .add_node(c) \\\n    .add_node(d) \\\n    .add_node(e) \\\n    .add_node(f) \\\n    .add_node(g) \\\n    .add_node(h) \\\n    .add_edge(Edge(a, b, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(a, c, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(b, d, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(c, e, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(d, f, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(e, f, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(c, g, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(e, h, EdgeType.DIRECTED)) \\\n    .add_edge(Edge(g, h, EdgeType.DIRECTED))\n\n# convert the BBN to a join tree\njoin_tree = InferenceController.apply(bbn)\n\n# insert an observation evidence\nev = EvidenceBuilder() \\\n    .with_node(join_tree.get_bbn_node_by_name(\'a\')) \\\n    .with_evidence(\'on\', 1.0) \\\n    .build()\njoin_tree.set_observation(ev)\n\n# print the marginal probabilities\nfor node in join_tree.get_bbn_nodes():\n    potential = join_tree.get_bbn_potential(node)\n    print(node)\n    print(potential)\n```\n\n# Approximate Inference Usage\n\nBelow is an example to create a linear Gaussian BBN and perform inference.\n\n```python\nimport numpy as np\nfrom pybbn.lg.graph import Dag, Parameters, Bbn\n\n# create the directed acylic graph\ndag = Dag()\ndag.add_node(0)\ndag.add_node(1)\ndag.add_edge(0, 1)\n\n# create parameters\nmeans = np.array([0, 25])\ncov = np.array([\n    [1.09, 1.95],\n    [1.95, 4.52]\n])\nparams = Parameters(means, cov)\n\n# create the bayesian belief network\nbbn = Bbn(dag, params)\n\n# do the inference\nM, C = bbn.do_inference()\nprint(M)\n\n# set the evidence on node 0 to a value of 1\nbbn.set_evidence(0, 1)\nM, C = bbn.do_inference()\nprint(M)\nbbn.clear_evidences()\n\n# set the evidence on node 1 to a value of 20\nbbn.set_evidence(1, 20)\nM, C = bbn.do_inference()\nprint(M)\nbbn.clear_evidences()\n```\n\n# Building\n\nTo build, you will need Python 2.7 or 3.7. Managing environments through [Anaconda](https://www.anaconda.com/download/#linux) is highly recommended to be able to build this project (though not absolutely required if you know what you are doing). Assuming you have installed Anaconda, you may create an environment as follows (make sure you `cd` into the root of this project\'s location).\n\nTo create the environment, use one of the following commands.\n\n```bash\nconda env create -f environment-py27.yml\nconda env create -f environment-py37.yml\n```\n\nIf you want to use the environments with Jupyter, install the kernel.\n\n```bash\nconda activate pybbn27\npython -m ipykernel install --user --name pybbn27 --display-name "pybbn27"\n\nconda activate pybbn37\npython -m ipykernel install --user --name pybbn37 --display-name "pybbn37"\n```\n\nThen you may build the project as follows. (Note that in Python 3.6 you will get some warnings).\n\n```bash\nmake build\n```\n\nTo build the documents, go into the docs sub-directory and type in the following.\n\n```bash\nmake html\n```\n\n# Publishing\n\nTo publish, make sure you have `.pypirc` with the correct credentials stored. Then type in the following.\n\n```bash\n./build.sh -r testpypi -v 1.0.0 # for testing\n./build.sh -r pypi -v 1.0.0 # the real thing\n```\n\n# Installing\n\nUse pip to install the package as it has been published to [PyPi](https://pypi.python.org/pypi/pybbn).\n\n```bash\npip install pybbn\n```\n\n## GraphViz issue\n\nMake sure you [install GraphViz](https://graphviz.gitlab.io/download/) on your system.\n\n* CentOS: `yum install graphviz*`\n* Ubuntu: `sudo apt-get install libgraphviz-dev`\n* Mac OSX: `brew install graphviz`\n* Windows: use the [msi installer](https://graphviz.gitlab.io/_pages/Download/windows/graphviz-2.38.msi)\n\n## testpypi issue\n\nYou should **NOT** be doing this operation, but if you do want to install from `testpypi`, then add the `--extra-index-url` as follows.\n\n```bash\npip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ pybbn\n```\n\n# Other Python Bayesian Belief Network Inference Libraries\n\nHere is a list of other Python libraries for inference in Bayesian Belief Networks. \n\n| Library | Algorithm | Algorithm Type | License |\n| -------:| ---------:| -------------: | -------:|\n| [BayesPy](https://github.com/bayespy/bayespy)| variational message passing | approximate | MIT |\n| [pomegranate](https://github.com/jmschrei/pomegranate) | loopy belief | approximate | MIT |\n| [pgmpy](https://github.com/pgmpy/pgmpy) | multiple | approximate/exact | MIT |\n| [libpgm](https://github.com/CyberPoint/libpgm) | likelihood sampling | approximate | Proprietary |\n| [bayesnetinference](https://github.com/sonph/bayesnetinference) | variable elimination | exact | None |\n\nI found other [packages](https://pypi.python.org/pypi?%3Aaction=search&term=bayesian+network&submit=search) in PyPI too.\n\n# Citation\n\n```\n@misc{vang_2017, \ntitle={PyBBN}, \nurl={https://github.com/vangj/py-bbn/}, \njournal={GitHub},\nauthor={Vang, Jee}, \nyear={2017}, \nmonth={Jan}}\n```\n\n# Copyright Stuff\n\n```\nCopyright 2017 Jee Vang\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```')
('https://raw.githubusercontent.com/christopherhadley/kaggle-housing/d65966bb07d29afa9f158c9f52a4ab1f7ae62af8/kaggle-housing.ipynb', '# Kaggle competion for house prices\nWorkbook for the Kaggle 101 competition on house prices and regression.  My kernel is available on [Kaggle](https://www.kaggle.com/hadders/modelling-house-prices-regularisation).')
('https://raw.githubusercontent.com/jltobias/ArcGIS/af63a15e2c5c08def971c6f694dde99e6e291ae3/labs/create_data.ipynb', '# ArcGIS Python API\nDocumentation and samples for the ArcGIS Python API https://developers.arcgis.com/python/ \n\n[ArcGIS Python API](https://developers.arcgis.com/python/)\n\n[Join the conversation](https://geonet.esri.com/groups/arcgis-python-api/)\n\n## What\'s included\n\n* Documentation\n* Sample Notebooks\n* Guides\n\n## Requirements\n\n* [See ArcGIS Python API - Install and Set Up](https://developers.arcgis.com/python/guide/install-and-set-up/)\n\n## Resources\n\n* [ArcGIS for Developers](https://developers.arcgis.com/python/)\n\n## Issues\n\nFind a bug or want to request a new feature?  Please let us know by submitting an issue.  Thank you!\n\n## Contributing\n\nAnyone and everyone is welcome to contribute. Please see our [guidelines for contributing](https://github.com/esri/contributing).\n\n## Licensing\nCopyright 2016 Esri\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nA copy of the license is available in the repository\'s [license.txt](https://github.com/Esri/arcgis-python-api/blob/master/license.txt) file.')
('https://raw.githubusercontent.com/epfml/ML_course/c8bb9be4ad1acc4bc217eb6852c5cc37271b385d/labs/ex03/template/ex03.ipynb', '# EPFL Machine Learning Course CS-433\nMachine Learning Course, Fall 2019\n\nRepository for all lecture notes, labs and projects - resources, code templates and solutions.\n\nThe course website and syllabus is available here: https://www.epfl.ch/labs/mlo/machine-learning-cs-433/\n\nContact us if you have any questions, via the [moodle discussion forum](https://moodle.epfl.ch/course/view.php?id=14221), or email to the assistants or teachers, or feel free to create issues and pull requests here using the menu above.')
('https://raw.githubusercontent.com/andrewfhughes/CFD_1/ca780099c606a86735a9c6e6c2c423b90ed4a6b5/lessons/05_Step_4.ipynb', "##Welcome to CFD Python\n\nHello! Welcome to the **12 steps to Navier-Stokes**. This is a practical module that is used in the beginning of an interactive Computational Fluid Dynamics (CFD) course taught by [Prof. Lorena Barba](lorenabarba.com) between 2009 and 2013 at Boston University (Prof. Barba since moved to the George Washington University). The course assumes only basic programming knowledge (in any language) and of course some foundation in partial differential equations and fluid mechanics. The practical module was inspired by the ideas of Dr. Rio Yokota, who was a post-doc in Prof. Barba's lab, and has been refined by Prof. Barba and her students over several semesters teaching the course. The course is taught entirely using Python and students who don't know Python just learn as we work through the module.\n\n###Installing Python\n\nThe core of this mini-course is built around [Jupyter(formerly IPython) notebooks](https://jupyter-notebook.readthedocs.org/en/latest/notebook.html), an interactive computational environment that is run in a web-browser.\n\n\n####Anaconda\nWe *highly* recommend that you install the [Anaconda Python Distribution](http://docs.continuum.io/anaconda/install).\n\nYou can download and install Anaconda on Windows, OSX and Linux. To ensure that it's up to date, run (in a terminal)\n\n```Bash\nconda update conda\nconda update jupyter numpy sympy scipy matplotlib\n```\n\nIf you prefer Miniconda (a mini version of Anaconda), to install all the necessary libraries to follow this course, run (in a terminal)\n\n```Bash\nconda update conda\nconda install jupyter\nconda install numpy scipy sympy matplotlib\n```\n\n\n####Without Anaconda\nIf you already have Python installed, you can install the notebooks using pip\n\n```Bash\npip install jupyter\n```\n\nPlease also make sure that you have the necessary libraries installed by running\n\n```Bash\npip install numpy scipy sympy matplotlib\n```\n\n\n###Running a notebook server\n\nOnce jupyter is installed, just open up a terminal and then run \n\n```Bash\njupyter notebook\n```\n\nto start up a jupyter session in your browser!\n\nLessons\n-------\n\n* [Quick Python Intro](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/00_Quick_Python_Intro.ipynb)\n* [Step 1](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/01_Step_1.ipynb)\n* [Step 2](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/02_Step_2.ipynb)\n* [CFL Condition](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/03_CFL_Condition.ipynb)\n* [Step 3](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/04_Step_3.ipynb)\n* [Step 4](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/05_Step_4.ipynb)\n* [Array Operations with NumPy](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/06_Array_Operations_with_NumPy.ipynb)\n* [Step 5](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/07_Step_5.ipynb)\n* [Step 6](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/08_Step_6.ipynb)\n* [Step 7](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/09_Step_7.ipynb)\n* [Step 8](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/10_Step_8.ipynb)\n* [Defining Function in Python](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/11_Defining_Function_in_Python.ipynb)\n* [Step 9](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/12_Step_9.ipynb)\n* [Step 10](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/13_Step_10.ipynb)\n* [Step 11](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/15_Step_11.ipynb)\n* [Step 12](http://nbviewer.ipython.org/urls/github.com/barbagroup/CFDPython/blob/master/lessons/16_Step_12.ipynb)")
('https://raw.githubusercontent.com/olemke/linemixing-test/7613414b34ed352e4cd29818f0b886c7786cf8f5/linemixing.ipynb', None)
('https://raw.githubusercontent.com/developer-smartwebzone/data-visualization/6b47942d46a697ad4b092f8838bd7156596b1ce0/location-history/visualize-location-history.ipynb', None)
('https://raw.githubusercontent.com/nskh/grads/18307554da65d26ad5b14479ce850c846efd99f9/lqr.ipynb', '# grads\npaper on gradient accuracy in rl algorithms')
('https://raw.githubusercontent.com/dsqx71/Social_Media_Cascade_Prediction/d4b77e4cfc25a76ca79c60af6258737797fc9920/main.ipynb', "##Social network interaction prediction\n\nXu Dong\n\n###[Description & Data](http://tianchi.aliyun.com/competition/introduction.htm?spm=5176.100068.5678.1.fOqq0e&raceId=5)\n###Preprocessing\n    1. 删掉同时满足：发出时间相同 & 相似度很高 & 同一作者（不用直接判定而是增加一些能够表现出是无效微博的特征就行）\n    2. 对连续变量做放缩（有助于提升线性模型的能力）\n    3. 用正则表达式把符号特征提取出来\n    4. 英文变小写，并删掉中英文的停用词 \n    5. 为了让变量连续，把1月改为13月 \n###Base feature\n\tuid,pid,time,share,comment,zan raw_corpus,cleaned&segment,链接，'/@,@,#,【，《，['\n###User attribute\n    1. 用户总的点赞、分享、评论数量的统计量 \n    2. 有效微博数量 +  无效的微博数量 + 总微博数量 + 出现在训练集中的数量\n    3. 微博的长度的统计值\n    5. 发微博的平均周期（若只有一条微博，则设为7个月）\n    6. 周一到周日发出有效微博的数量/频率/频率的方差\n    7. 用户微博的平均主题分布\n    8. 用户出现在训练集的次数\n###Missing value processing\n\t1.许多std的缺失值暂时 -1 来替代（因为-1能够体现出这条样本是没有std的）\n\t2.用户没有出现在训练集中，所以有关share，comment，zan的统计量都设为0（两类测试集分开预测,所以这里设定的值不重要，因为在这一类训练集中不使用上述特征）\n###Text feature\n    1. 原始文本长度 + 清理后的文本长度\n    2. 特殊字符： [r'http[0-9a-zA-Z?:=._@%/\\-#&\\+|]+' ,r'//@',   r'@' ,  r'#' ,  r'【' ,r'《' ,r'\\[' ]\n    3. 提取 tf-idf \n    4. 人工抽取有区分度的词\n    5. 离最近一次有效微博的时间间隔（若没有，则设为7个月）\n    6. 最近n次有效微博的转发/评论/点赞 数量（若没有，则设为0，n待定）\n    7. 星期几\n    8. 与最近n次有效微博的文本相似度\n    9. 与其他用户的微博相似度\n        \n###Extract features using unsupervised learning models\n    1. Latent dirichlet allocation(LDA)\n    2. Opinion leader identification\n    3. sentiment analysis\n    4. Gaussian Mixture Model\n    5. collaborative filtering\n\n### Blending\n\t1.测试集分为两类：用户在训练集出现过+没有出现，两类分开预测，前者可以多用一些特殊的特征，比如share，comment，zan的统计量等，后者不能直接出现这些特征。两类模型分开训练\n\t（因为两者的特征重要度不同，前者一定很偏重训练集中的share，comment，zan的统计量，\n\t后者因为没有在训练集中出现，更加依赖文本特征，聚类特征等，这类特征间接使用了训练集中的share，comment，zan的统计量\n\t分为两对分别训练，保证了训练集和测试集是一致的） \n    2.分类，预测是否为0 //因为多数人的转发量为0,搞定这些就能获得很大一部分的得分\n    3.分类预测不为0的，回归再预测 //清理掉很有可能不被关注的微博，只对有可能的进行回归分析\n\n###Validation set design\n\n - 选择一定时间以后的数据（缺点：训练集数据会相对变少）\n - 在所有样本中随机选择一部分作为5-折交叉验证（缺点：没有考虑到时间序列之间的关系）\n - 所有用户选择一些时间靠后的样本作为验证集，如果某用户只有一条，并且在后两月，则放在验证集上（缺点：有些用户只有一条微博）\n - 使用bagging方法的oob（缺点：结果太悲观，不能反映真正的误差）\n - 前几个月一定放在训练集，后几个月随机抽取一部分作为训练集，另一部分为验证集（3折）\n - 加权结合上述方法\n\n###Observation\n\n    1 分类和回归的特征重要度几乎不同，可能说明\n    2 有些数据在训练集中完全没有出现的。比如：月份，用户\n    3 有许多用户没有出现在训练集中\n    4 没有考虑题目的意义：预测一条文博在7天内的互动\n    5 寻找影响优质内容的因素\n    6 发了微博之后新的微博会置顶把之前的顶下去，所以把7天之内出现其他微博的次数作为特征\n    7 微博之间的内容有连续性\n    8  应当考虑转发上升的速度和加速度 \n    9 用户之前没有发过任何微博作为特征\n    10 7天之内的微博内容绝对有影响\n    11 没有给图片信息，关注网络，平台的内容推广策略，这个些都会是强噪声\n    12 预测集中有24818个用户，但是1214个用户都不在测试集中的。\n    13增加comment，share，zan 高的权重\n\n\n###Reflection\n- 每次做特征的时候，一定要进行自动化检验，比如样本数量，缺失值比例等等，用python的装饰器搞定\n- 如果数据很大，通常先降采样，快速特征抽取代码。\n- 抽取不同的特征时，可以放在不同的函数中，做到低耦合，然后分配进程池，做并行计算。\n- 不要使用pandas做特征的管理，它比numpy的开销大，而且numba等加速工具不直接支持\n- 计算力不足时，可以不做grid search，而是随意指定几个典型的值，供之后的ensemble再加工\n- 多做profiling，避免浪费时间等待\n- 尽量向量化，否则请考虑 numpy 下的take，pandas 下的请使用irow ，icol\n- 如果要用线性模型，则dummy code 和合理的缺失值\n- 可以通过残差图，不同类别对应特征的分布图来决定离群点的\n- 对于离群点不用直接删掉，而是降低权重\n- 对于特征选择：可以选择训练集和预测集特征分布一致的特征，可以用这种特征分布的相似性来决定bagging抽取特征的概率（kl散度来衡量）")
('https://raw.githubusercontent.com/tonydisera/w207_6_sum19_g5_final_project/5d1880091d6100b994ef86705f075af25f28a47c/merge_pancancer_somatic_mutations/Logistic_regression.ipynb', '# w207_6_sum19_g5_final_project')
('https://raw.githubusercontent.com/WillieMaddox/MixSig/fc887f2214c472745c9fb26b11a167b4cc6a147c/mixsig.ipynb', '# MixSig\n￼\n[![Build Status](https://travis-ci.org/WillieMaddox/MixSig.svg?branch=master)](https://travis-ci.org/WillieMaddox/MixSig)\n[![Coverage Status](https://coveralls.io/repos/github/WillieMaddox/MixSig/badge.svg?branch=master)](https://coveralls.io/github/WillieMaddox/MixSig?branch=master)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/WillieMaddox/MixSig/blob/master/LICENSE)\n\nAn RNN project to study hidden layers by generating data in the form of mixed waves.\n\n\n# License\n\nMIT licensed. See the bundled [LICENSE](<https://github.com/WillieMaddox/MixSig/blob/master/LICENSE>) file for more details.')
('https://raw.githubusercontent.com/bjoernrost/ml-live/4b871767e8aa48332ba387232862c363f694177c/ml-live.ipynb', '# ml-live\nA hands-on intro presentation to machine learning')
('https://raw.githubusercontent.com/ChuckBowers/info370-group11/d0450963ee86d1cc35745c9c61d3d560ac78368c/ml_analysis.ipynb', "# info370-group11\n\n## Project Description\n\n- For our final project, we are trying to answer what effect, if any, the weather has on outgoing flights. More specifically we are interested in the weather in Seattle and its effect on flights leaving SeaTac. We believe this to be an important topic as thousands of people fly through Seattle everyday, and as all of us as residents know Seattle's weather can be both variable and rainy. If we were be able to inform either the populace of Seattle or the organization of SeaTac with this information, the hope would be that individuals would be more prepared for their journeys, and that SeaTac can react accordingly.\n\n- Flight delays can have costly consequences. It is estimated that in the U.S. alone, flight delays have a 40.7 billion dollar impact. Additionally, all of that time planes spend on the tarmac results in excess fuel being used and more emissions being released (Fleurquin). Flight delays also result in significant disruptions to aviation safety and the decreased traffic results in losses for the airlines (Gao, 68). Flight delays also cause passengers to prefer other airlines if they experience delays with a certain carrier (Tae-Hwee Lee, 277). Causes of flight delays can range from the unpreventable, severe weather (Gao, 68), to the preventable, crew mishaps and flight order rotation (Fleurquin).\n\n- The specific question we hope to answer with this data is, given a particular day at SeaTac International Airport, can we predict the average departure delay for all flights based upon the weather? Our _null hypothesis_ would be: there is no relationship between the average daily departure delay at SeaTac and the weather at SeaTac. Conversely, our _alternative hypothesis_ would be: there is a relationship between the average daily departure delay at SeaTac and the weather at SeaTac.\n\n- To estimate the average departure delay at SeaTac International Airport (SEA), two datasets 1) flight delay data at SEA and 2) weather data of SEA will be used. From [Kaggle](https://www.kaggle.com/fabiendaniel/predicting-flight-delays-tutorial/data), we acquired the 2015 flight delays and cancellations dataset. This dataset contains three tables including the list of airlines, airports and the actual flights. From [National Centers for Environmental Information (NCEI)](https://www.ncdc.noaa.gov/), we downloaded the weather data for the year of 2015 that is recorded by weather station at Seattle Tacoma International Airport (SEA).  \n\n- We intend to employ linear multivariate regression on modeling. It is unlikely to use a univariate regression unless there is a single weather factor that exerts an extremely significant effect on flight delays. On the side of machine learning, we expect to use cross-validation and grid search to evaluate the flight delay.  \n\n- There are a couple of audience groups for our project. For example, the Department of Transportation (DOT) and the airlines may issue prior delay notice and operate accordingly on the given estimated information. Moreover, There are also airport rating agencies around which would use the data for ratings on airport efficiency of responsiveness on weather conditions. However, the primary audience for our project is Seattle-based travelers. Since we are only evaluating flights departing from Seattle Tacoma International Airport, all travelers through SEA, especially frequent flyers based in the Greater Seattle Area will benefit from our project.  \n\n- Our resource serves all travelers who travel through Seattle Tacoma International Airport (SEA), especially those based in Seattle. Before travelers heading to the airport or before passengers with connecting flight landing, with the resource we provide, they may be able to have a rough prediction on how long their flights will be delayed. Travelers could also modify the schedule of subsequent activities.  \n\n## Technical\n\n- Our paper will be presented in an HTML page exported using ipynb. \n\n- The first dataset we found relevant to our question contains information on all US domestic flights in 2015 operated by 14 American airlines. The large size of the flights dataset (500MB+) could make it difficult to load and share, by filtering out the rows that have 'SEA' as the value for origin airport, we are able to reduce the size from ~5800,000 rows to ~110,000. A potential problem for our research question is our treatment of flight cancellation. In the dataset, cancelled flights have a 'CANCELLED' value of 1, and null for the actual departure/arrival time, so there is the need to differentiate between cancelled and normal flights in the calculations. The weather dataset includes daily weather information such as wind, precipitation and snowfall. Again, treatment of null values can be a challenge in this dataset. There is a large amount of null values in several columns. We will need to make trade-offs to drop unimportant columns from our analysis. In addition, airports rely on weather data available at present to make decisions about potential flight delays in the future, so a time-lagged variable might be better at predicting flight delays. \n\n- We will need to get familiar with more Pandas manipulations we are not currently used to doing for the project. Because we are dealing with two separate datasets with completely different format (each row in the flight dataset represent a flight while each row in weather dataset represent a day), we will need to group the two data frames together using specific types of join that makes sense, and transform the result into the desired format (average flight delay for a given day in minutes). \n\n- To conduct our analysis we need to first prepare our data by handling missing values (potentially mean of adjacent rows) and normalizing it to a common scale. Then we will create new features by combining data from existing columns. In order to not overfit our data and create noise, we will select features that are co-related and remove ones with low variance. To select our best model we need to find a balance between _bias_ (under fitting our data, making our model inflexible and so it doesn’t account for all our features) and _variance_ (overfitting  our data so that our model accounts for random errors and underlying distribution of our data). To validate our model and hyperparameters are a good fit to our data we’ll use cross-validation. This will give us a better understanding of how our algorithm is performing and allow us to use all of our data for training. By splitting out data into subsets, we’ll improve our validation accuracy. Using grid search and mean absolute error as a scoring metric, we’ll find the optimum parameters for our model.\n\n- As I mentioned above, the treatment of cancelled flights is certainly a technical challenge to our analysis approach. In terms of feature engineering, we need to do outside research to identify potential factors that we want to include, or exclude variables that introduce noise. Unlike linear relationships we dealt with in past assignments, there might be polynomial relationships we need to account for using polynomial transformations. We also need to figure out ways of scaling/offsetting the data to give us a better model performance. \n\n## Works Cited\n\n- Fleurquin, Pablo et al. “Systemic delay propagation in the US airport network” Scientific reports\n    vol. 3 (2013): 1159.\n\n- Gao, Mingang. “Models Responding to Large-Area Flight Delays in Aviation Production Engineering.”\n    NeuroImage, Academic Press, 2 June 2012, www.sciencedirect.com/science/article/pii/S2211381912000549.\n\n- Tae-Hwee Lee, Taylor. “Impact of Flight Departure Delay on Airline Choice Behavior.” NeuroImage,\n    Academic Press, 6 Feb. 2018, www.sciencedirect.com/science/article/pii/S2092521217300676.")
('https://raw.githubusercontent.com/phys201/nsdmd/4bfe205eb0a6ac0ea119c3382ce16ef835a783f3/model.ipynb', '# NestedSampling_DMD\nThis python package explores how the measured rotation curve of the Milky Way galaxy constraints the dark matter density distribution in our galaxy.  Of particular interest are the variations in the dark matter density at the location of the earth.\n\nFor: Data Analysis (PHYS 201), Spring 2017, Harvard University.\nCreate by: Daniya Seitova, Jun Yin\nLicense:GNU General Public License v3\n\n\n\nPackage Manual:\n\nmodel.ipynb : shows the contents where we import our package, load our test data, do some inference using our model, and show some plots.\ntutprial.ipynb: shows how to use our package. \ntestdmdcode.py : test the code . \nnsdmd/model.py : contains the modules for our model.\nio.py: contains functions to read data file. ')
('https://raw.githubusercontent.com/kurtrm/navigation_lights_ml/b26d3c8ed3d6cf6b3e333a0002780d852dba8e13/nav_lights_analysis.ipynb', "# Navigation Lights Classification\n\nThis repo will ideally produce an algorithm that will interpret a ship's navigation\nlights correctly. The images will be manually generated for now (no real images of ship navigation lights will be used).")
('https://raw.githubusercontent.com/dpploy/engy-4350/952f974315b61b9220a06f7012d11533215a82fc/notebooks/02b-cumulative-fission-product-yields.ipynb', '# Engy-4350 Nuclear Reactor Engineering\n![Website](https://img.shields.io/website/https/github.com/dpploy/engy-4350.svg)\n[![Repo-Size](https://img.shields.io/github/repo-size/dpploy/engy-4350.svg?label=size&style=flat)](https://github.com/dpploy/engy-4350)\n[![Azure Notebooks](https://notebooks.azure.com/launch.svg)](https://notebooks.azure.com/dealmeidavf/projects/engy-4350)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/dpploy/engy-4350/master)\n[![NBViewer](https://github.com/jupyter/design/blob/master/logos/Badges/nbviewer_badge.svg)](http://nbviewer.jupyter.org/github/dpploy/engy-4350/tree/master/notebooks)\n\n    University of Massachusetts Lowell, Spring 2019\n    Dept. of Chemical Engineering\n    Prof. Valmor F. de Almeida (valmor_dealmeida@uml.edu)\n    \n![](notebooks/images/point-reactor.png)![](notebooks/images/point-reactor-response-3.png) \n\nThe goal of this [course](https://github.com/dpploy/engy-4350) is to present to undergraduate students of nuclear chemical engineering the elements of nuclear energy conversion to (electrical) work. This is done primarily in large scale power reactors by using nuclear heat. Although there are direct ways to convert nuclear energy into electricity, the focus of this course is on the prevalent mode of energy transfer using nuclear heat.\n\nFeedback and collaboration to improve this course are welcome through GitHub `pull requests` and `issues` or direct email.\n\nThis course uses Jupyter Notebooks in Python programming language. The content can be accessed in\nthe following ways:\n+ Static HTML version of the notebooks will be displayed on the current browser if a\nnotebook file listed in the code repository is clicked on. This will not allow for rendering mathematical formulae. Alternatively you can render the notebooks on [NBViewer](http://nbviewer.jupyter.org/) by clicking on the `render|nbviewer` badge above.\n+ Click on the `Azure Notebooks|launch` above to use the Azure Notebook service; this is practical since you can use your UMass Lowell credentials to login into the site and use the GitHub upload option to clone the repository in your account.\n+ Click on the `launch/binder` badge above to launch a Jupyter Notebook server for the\ncourse notebooks. There will be a delay for the Binder cloud server to build a\nPython (Anaconda) programming environment for you. However once it is done, it will\nstart a Jupyter Notebook server on your web browser with all notebooks listed. Upon\nclicking on individual notebook files, you will access the live course notebooks.\n+ Use the green `download` button above on the right upper side of the page and download a ZIP archive to your local machine. Unzip the archive. Then use your own Jupyter Notebook server to navigate to the directory created by the unzip operation and upload the notebook files. In this case the files will not be updated and you will need to return to the repository for getting new files or updated versions of previously downloaded files.\n\nStudents will profit from either taking or self-studying a [companion course](https://github.com/dpploy/chen-3170) that explains many of the computational aspects of using Jupyter notebooks, Python language programming, and methods in computational engineering.\n\nThanks in advance for inputs to improve this course.\\\nRegards,\\\nValmor.')
('https://raw.githubusercontent.com/ChasNelson1990/python-1day-course/05c7d5f5c75a8314d83716ad049426fe0ba60d75/notebooks/06_python-loops.ipynb', '# Python for Beginners Course\n\nThis repository contains materials for a beginners python course (delivery dates/information abotu self-taught options below).\n\n## Aim\n\nThe overall aim of this course it to:\n\n1. introduce the basic concepts of programming in Python within the comfort of the Jupyter framework\n2. showcase some of the advanced functionality available in Python by demonstrating (and providing take-home resources) numpy, scipy, matplotlib, seaborn and pandas code\n\n## Philosophy\n\nOur general philosophy for this course is\n\n1. teach in small chunks starting by introducing theory, demonstrating an example, working through a simple case and then setting an exercise. Each exercise is then gone through as a group.\n2. teach through errors, error messages and documentation - so that trainees can debug their own codes after they leave the course\n3. create a safe environment for asking any and all questions.\n\n## Contributors\n\n* [Chas Nelson](https://github.com/ChasNelson1990)\n* [Mikolaj Kundegorski](https://github.com/mixmixmix)\n\n## Self-taught On-line Version (12+ hours)\n\nWe have designed this course in such a way that it should be easy to follow and work through on your own. Each notebook stands alone and should provide you with all the information needed to complete the tasks (blue boxes) and exercises (yellow boxes).\n\nIn order to aid working through the notebooks we have provided short videos for all tasks and exercises. These videos provide complete answers for every task and should be viewed after attempting each task or exercise.\n\nIn order to work through the notebooks please follow the instructions in `setup.pdf` for installing Python and Jupyter Lab on your computer, dowload this repository (green button at the top of the landing page), unzip the files and navigate to them from within Jupyter Lab.\n\nWe suggest you work through each notebook in turn, attempting at least the tasks on your first run-through. You can then use the exercises to revisit and revise topics when you go through the notebooks again in the future. As with all languages, practice makes perfect.\n\n## In Person Course Delivery Dates (1 day course)\n\n* **2019-04-05**: Programming for Biologists, Royal Society of Biology, Charles Darwin House, London, UK\n* **2019-10-18**: Programming for Biologists, Royal Society of Biology, 1 Naoroji Street, London, UK\n\n## Other Information\n\nThese materials also form the prerequisite knowledge for following course:\n\n* **2019-12-09**: IAFIG-RMS: Bioimage analysis with Python, Craik-Marshall Building, Cambridge, UK')
('https://raw.githubusercontent.com/axsh/nii-project-2016/50ed6c83d1e163dee1651c7b6daf68e14fc6c942/notebooks/103_jenkins_job_execution_script.ipynb', '<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.')
('https://raw.githubusercontent.com/unccv/neural_networks/3d1aee8ead3476816ea31d530f23c7ffe02f420a/notebooks/A%20Brief%20History%20of%20Neural%20Networks.ipynb', "# Neural Networks\n\n![](graphics/lander.gif)\n\n\n## About \nThis repo contains resources and code for a computer vision module on neural networks. \n\n\n### Note on Launching the Jupyter Notebooks\nTo properly view the images and animations, please launch your jupyter notebook from the root directory of this repository. \n\n## Lectures\n| Order |   Notebook/Slides  | Topics | Required Viewing Before | Additional Reading | Notes |\n| ----- | ------------------ | ------ | ----------------------- | ------------------ | ----- |\n| 1 | [A Brief History of Neural Networks](https://github.com/unccv/neural_networks/blob/master/notebooks/A%20Brief%20History%20of%20Neural%20Networks.ipynb) | Development of Perceptrons, MLPs, Backpropogation | - | - | - |\n| 2 | [Neural Nets Demystified Part 1](https://github.com/unccv/neural_networks/blob/master/notebooks/Neural%20Networks%20Demystified%20%5BPart%201%5D.ipynb) | Building a Simple Neural Networok, Forwardpropogation, Backpropogation| [Neural Networks Demystified Parts 1-4](https://www.youtube.com/watch?v=bxe2T-V8XRs) | [Who is afraid of non-convex loss functions?](https://www.youtube.com/watch?v=8zdo6cnCW2w) | Print [Backpropogation Notes](https://github.com/unccv/neural_networks/blob/master/backprop_guided_notes.pdf) before lecture. |\n| 3 | [Neural Nets Demystified Part 2](https://github.com/unccv/neural_networks/blob/master/notebooks/Neural%20Networks%20Demystified%20%5BPart%202%5D.ipynb) | Backpropogation, Training, Regularization, Overfitting| [Neural Networks Demystified Parts 4-7](https://www.youtube.com/watch?v=bxe2T-V8XRs) | [Regularization for Deep Learning](https://www.deeplearningbook.org/contents/regularization.html) | Print [Backpropogation Notes](https://github.com/unccv/neural_networks/blob/master/backprop_guided_notes.pdf) before lecture. |\n\n\n## Setup \n\nThe Python 3 [Anaconda Distribution](https://www.anaconda.com/download) is the easiest way to get going with the notebooks and code presented here. \n\n(Optional) You may want to create a virtual environment for this repository: \n\n~~~\nconda create -n cv python=3 \nsource activate cv\n~~~\n\nYou'll need to install the jupyter notebook to run the notebooks:\n\n~~~\nconda install jupyter\n\n# You may also want to install nb_conda (Enables some nice things like change virtual environments within the notebook)\nconda install nb_conda\n~~~\n\nThis repository requires the installation of a few extra packages, you can install them all at once with:\n~~~\npip install -r requirements.txt\n~~~\n\n(Optional) [jupyterthemes](https://github.com/dunovank/jupyter-themes) can be nice when presenting notebooks, as it offers some cleaner visual themes than the stock notebook, and makes it easy to adjust the default font size for code, markdown, etc. You can install with pip: \n\n~~~\npip install jupyterthemes\n~~~\n\nRecommend jupyter them for **presenting** these notebook (type into terminal before launching notebook):\n~~~\njt -t grade3 -cellw=90% -fs=20 -tfs=20 -ofs=20 -dfs=20\n~~~\n\nRecommend jupyter them for **viewing** these notebook (type into terminal before launching notebook):\n~~~\njt -t grade3 -cellw=90% -fs=14 -tfs=14 -ofs=14 -dfs=14\n~~~")
('https://raw.githubusercontent.com/IBM/powerai-transfer-learning/ec4a71e8c17e9b923a05ea0c17ea8a7d4e0b3c1f/notebooks/Classifying-House-And-Pool-Images.ipynb', '# Image recognition training with TensorFlow Inception and transfer learning\n\nTransfer learning is the process of taking a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and “fine-tuning” the model with your own dataset. The idea is that this pre-trained model will act as a feature extractor. You will remove the last layer of the network and replace it with your own classifier (depending on what your problem space is). You then freeze the weights of all the other layers and train the network normally (Freezing the layers means not changing the weights during gradient descent/optimization).\nFor this experiment we used Google\'s Inception-V3 pretrained model for Image Classification. This model consists of two parts:\n- Feature extraction part with a convolutional neural network.\n- Classification part with fully-connected and softmax layers.\n\nThe pre-trained Inception-v3 model achieves state-of-the-art accuracy for recognizing general objects with 1000 classes. The model extracts general features from input images in the first part and classifies them based on those features in the second part.\nWe will use this pre-trained model and re-train it it to classify houses with or without swimming pools.\n\n\n![](doc/source/images/architecture.png)\n\n## Included components\n\n* [IBM Power AI](https://www.ibm.com/ms-en/marketplace/deep-learning-platform): A software platform that includes the most popular machine learning frameworks with IBM Power Systems.\n* [IBM Power Systems](https://www-03.ibm.com/systems/power/): IBM Power Systems is IBM\'s Power Architecture-based server line, built with open technologies and designed for mission-critical applications.\n* [Nimbix Cloud Computing Platform](https://www.nimbix.net/): An HPC & Cloud Supercomputing platform enabling engineers, scientists & developers, to build, compute, analyze, and scale simulations in the cloud\n\n## Featured technologies\n\n* [Jupyter Notebooks](http://jupyter.org/): An open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text.\n* [Tensorflow](https://www.tensorflow.org/): An open source software library for numerical computation using data flow graphs.\n\n# Steps\n\nFollow these steps to setup and run this developer journey. The steps are\ndescribed in detail below.\n\n1. [Register for a Trial Nimbix Cloud Platform account](#1-register-for-a-trial-nimbix-cloud-platform-account)\n2. [Navigating the Nimbix UI](#2-navigating-the-nimbix-ui)\n3. [Deploy and run the PowerAI Notebooks application](#3-deploy-and-run-the-powerai-notebooks-application)\n4. [Access and start the Jupyter notebook](#4-access-and-start-the-jupyter-notebook)\n5. [Run the notebook](#5-run-the-notebook)\n6. [Analyze the results](#6-analyze-the-results)\n7. [Save and share](#7-save-and-share)\n8. [Shut down the PowerAI Notebooks job](#8-shut-down-the-powerai-notebooks-job)\n\n## 1. Register for a Trial Nimbix Cloud Platform account\n\nIBM has partnered with Nimbix to provide journey developers a trial\naccount that provides 10 hours of free processing time on the PowerAI\nplatform.\n\nThe registration process is as follows:\n\n* Go to the [IBM Cognitive Journey Demo Registration Page](https://www.nimbix.net/cognitive-journey)\nand submit the form to initiate the registration process.\n\n![](doc/source/images/nimbix-registration.png)\n\n* Wait for a confirmation email. Note that this process is not automated so\nit may take up to 24 hours to be reviewed and approved.\n* Once you receive the email, you will have 24 hours to complete the process\nby clicking on the provided link:\n\n![](doc/source/images/nimbix-confirmation-1.png)\n\n* The above link will take you to this page where you need to create and\nconfirm your account password:\n\n![](doc/source/images/nimbix-set-password.png)\n\n> *NOTE:* A "Promotional Code" is not required.\n\n* Wait for a confirmation email that will will provide instructions\nfor logging into Nimbix:\n\n![](doc/source/images/nimbix-registration-complete.png)\n\n* Click the [link](https://mc.jarvice.com?page=compute&team) to take you to\nthe Nimbix login page:\n\n![](doc/source/images/nimbix-login.png)\n\n## 2. Navigating the Nimbix UI\n\nThe Nimbix UI has two main controls located at the top of the panel.\n\n![](doc/source/images/nimbix-navigation.png)\n\n* Click on ``NIBMIX`` to display a drop-down list of available modes. Click\nagain to hide the list. In the example view above, we have selected\nthe ``Dashboard`` mode.\n\n* On the left-side, click on the ``collapsible`` icon to display a\ndrop-down list of views that are associated with the selected mode.\nClick on the icon again to hide the list. In the example above, we have\nselected the ``Current Jobs`` dashboard view.\n\n## 3. Deploy and run the PowerAI Notebooks application\n\nOnce signed into Nimbix, deploy the PowerAI Notebooks application.\n\n* Search for and select the ``PowerAI Notebooks`` application in the list\nof available apps in the ``Compute:All Applications`` view.\n\n![](doc/source/images/nimbix-search-page-demo.png)\n\n* From the ``PowerAI Notebooks`` application panel, click the\n``TensorFlow`` button:\n\n![](doc/source/images/tensor-flow-demo-launch.png)\n\n* From the ``TensorFlow`` configuration panel, accept all default values and\nclick the ``Submit`` button:\n\n![](doc/source/images/tensor-flow-demo-config.png)\n\n> *NOTE:* Ignore the $/hr charge listed. With your trial account you receive 10 hours of free processing time.\n\n* Once started, the following ``Dashboard`` panel will be displayed. When\nthe server ``Status`` turns to ``Processing``, the server is ready to be\naccessed.\n\n    Click on ``(click to show)`` to get the password.\n\n* Click ``Click here to connect`` to launch web access to the notebooks.\n\n![](doc/source/images/tensor-flow-demo-click.png)\n\n* Log-in using the user name ``nimbix`` and the previously supplied password.\n\n![](doc/source/images/tensor-flow-demo-login.png)\n\n## 4. Access and start the Jupyter notebook\n\nUse git clone to download the example notebook, dataset, and retraining library with a single command.\n\n* Get a new terminal window by clicking on the ```New``` pull-down and selecting ``Terminal``.\n\n![](doc/source/images/powerai-notebook-terminal.png)\n\n* Run the following command to clone the git repo:\n\n```commandline\ngit clone https://github.com/IBM/powerai-transfer-learning\n```\n\n![](doc/source/images/powerai-notebook-clone.png)\n\n* Once done, you can exit the terminal and return to the notebook browser. Use the ``Files`` tab and click on ``powerai-transfer-learning`` then ``notebooks`` and then ``Classifying-House-And-Pool-Images.ipynb`` to open the notebook.\n\n![](doc/source/images/powerai-notebook-open.png)\n\n## 5. Run the notebook\n\nWhen a notebook is executed, what is actually happening is that each code cell in\nthe notebook is executed, in order, from top to bottom.\n\nEach code cell is selectable and is preceded by a tag in the left margin. The tag\nformat is `In [x]:`. Depending on the state of the notebook, the `x` can be:\n\n* A blank, this indicates that the cell has never been executed.\n* A number, this number represents the relative order this code step was executed.\n* A `*`, this indicates that the cell is currently executing.\n\nThere are several ways to execute the code cells in your notebook:\n\n* One cell at a time.\n  * Select the cell, and then press the `Play` button in the toolbar.\n* Batch mode, in sequential order.\n  * From the `Cell` menu bar, there are several options available. For example, you\n    can `Run All` cells in your notebook, or you can `Run All Below`, that will\n    start executing from the first cell under the currently selected cell, and then\n    continue executing all cells that follow.\n\n![](doc/source/images/powerai-notebook-run.png)\n\n## 6. Analyze the results\n\n## 7. Save and share\n\n### How to save your work:\n\nBecause this notebook is running temporarily on a Nimbix\nCloud server, use the following options to save your work:\n\nUnder the `File` menu, there are options to:\n\n* `Download as...` will download the notebook to your local system.\n* `Print Preview` will allow you to print the current state of the\n  notebook.\n\n## 8. Shut down the PowerAI Notebooks job\n\nRemember to shutdown the server to free up resources on the Nimbix Cloud Platform. Also, remember that\nthe free trial registration only provides 10 hours of compute time.\n\n* From the Nimbix ``Dashboard:Current Jobs`` view, click on the\n``Shutdown`` button and confirm.\n\n# Troubleshooting\n\n[See DEBUGGING.md.](DEBUGGING.md)\n\n# License\n\n[Apache 2.0](LICENSE)')
('https://raw.githubusercontent.com/IvanBrasilico/ajna_docs/5a7255e8d063505240132ac362a456caf85b3247/notebooks/ContainerDetectionTraining1.ipynb', '[![Build Status](https://travis-ci.org/IvanBrasilico/ajna_docs.svg?branch=master)](https://travis-ci.org/IvanBrasilico/ajna_docs) \n\n\n# AJNA\n\nVisão computacional e aprendizado de máquina aplicados à vigilância e repressão aduaneira\n\n\n\n* [Documentação](http://ajna-mod.readthedocs.io/pt_BR/latest/)\n* [Módulos](#Módulos)\n  * [ajna_commons](#ajna_commons)\n  * [ajna_cov](#ajna_cov)\n  * [bhadrasana](#bhadrasana)\n  * [padma](#padma)\n  * [virasana](#virasana)\n  * [notebooks](#notebooks)\n* [Instalação](#instalação)\n* [Desenvolvimento](#desenvolvimento)\n* [Nota sobre TOX (Testes automatizados e Integração Contínua)](#tox)\n\n## Módulos:\n## ajna_commons\nBiblioteca com funções e classes utilizadas em várias aplicações do AJNA\n\n### Instalação automática\nEste módulo está marcado como requerido em TODAS as aplicações, portanto, no deploy, será automaticamente instalado pelo pip (requirements.txt) e/ou pelo setuptools(setup.py). \n\n### Instalação desenvolvedor\nRecomenda-se, caso queira-se um ambiente para desenvolvimento e edição, e geração de documentação clonar todos os repositórios conforme instruções dentro da pasta ajna_doc. Depois, dentro dos demais módulos, digitar as seguintes linhas:\n```\n$ pip uninstall ajna_commons\n$ ln -s ../ajna_commons/ajna_commons .\n```\n## ajna_cov\n\nInterface(s) para cadastramento de fontes de imagens, configuração de parâmetros, primeiros tratamentos, etc. Faz também a aquisição de imagens, vídeos e outros dados. Para ser instalado na(s) rede(s) em que ficam as imagens a serem adquiridas.\n\n## bhadrasana - sentinela\n\nControla o data_aq, permite cruzamento de dados e gerenciamento, manual ou automático, de parâmetros de risco. Disponibiliza seus dados em Restful API.\n\nAquisição de dados. Serviço com scripts que acessam sistemas fechados e dados públicos, estruturados e não estruturados, guardando em coleções/Bancos de Dados. Disponibiliza seus dados em Restful API.\n\nConstituído de uma aplicação web e wevbservice, e diversos workers controlados pelo Celery\n\n## padma - busca da verdade\n\nColeção de algoritmos de machine learning plugáveis e servidos em WebService. Basicamente é uma API que recebe dados e devolve predições.\n\nConstituído de uma aplicação web e wevbservice, e diversos workers controlados pelo Celery\n\n## virasana - heroi\n\nInterface para visualização e busca de imagens, recebimento de alertas e execução, manual ou automática, dos algoritmos do módulo ml_code nas imagens. Disponibiliza seus dados em Restful API.\n\nAquisição de imagens. Serviço com Scripts que acessam as fontes de imagens cadastradas, validam, pré-processam, fazem reconhecimento de caracteres, validam, monitoram mudanças, etc e copiam para um diretório único. Disponibiliza seus dados em Restful API.\n\nConstituído de uma aplicação web e wevbservice, e diversos workers controlados pelo Celery\n\n## notebooks\n\nRascunhos. Aproveitar a interatividade e praticidade do Jupyter Notebook para fazer e documentar as análises exploratórias de dados, treinamento e teste de algoritmos de aprendizado e projetar/validar scripts de data_aq\n\n\n### Instalação \nPara instalar os módulos bhadrasana, padma, virasana:\n\n```\n$git clone <nome do repositório>\n$cd <nome do módulo>\n$python3 -m venv <modulo>-venv\n$. <modulo>-venv/bin/activate\n$pip install .\n```\nEx:\n\n```\n$git clone https://github.com/IvanBrasilico/virasana.git\n$cd virasana\n$python3 -m venv virasana-venv\n$. virasana-venv/bin/activate\n(virasana-venv)$pip install .\n(virasana-venv)$python -m pytest (roda os testes automatizados)\n(virasana-venv)$./virasana/celery.sh (inicia os workers do serviço celery)\n(virasana-venv)$ python virasana/app.py (inicia o servidor web/api)\n```\n\n### Desenvolvimento\n\nClonar o módulo raiz ajna_doc\n\n```\n$git clone https://github.com/IvanBrasilico/ajna_docs.git\n$cd ajna_docs\n$python3 -m venv ajna-venv\n$. ajna-venv/bin/activate\n(ajna-venv)$pip install -e .[dev]\n$deactivate\n```\n\nA inclusão do parâmetro [dev] no comando pip terá o condão de instalar o que foi definido no [extras_require] no arquivo setup.py.\n\nRepetir os passos acima para os demais módulos, DENTRO do diretório ajna_docs\n\nA estrutura de diretórios ficará assim:\n\n<pre>/ajna_docs  \n ┬  \n ├ ajna_commons\n ├  bhadrasana\n ├  notebooks\n ├  padma\n └  virasana\n</pre>\n\nPara poder editar o ajna_commons num local único (pois será instalada uma cópia em cada venv criado), dentro de cada diretório/módulo, com o venv respectivo ativo, digite:\n\n```\n$ pip uninstall ajna_commons\n$ ln -s ../ajna_commons/ajna_commons .\n```\n\nPorque um venv para cada projeto???\n\nEmbora pareça uma complicação a mais um venv para cada projeto, é necessário pensar que cada um é uma aplicação independente, podendo inclusive rodar em máquinas separadas. Isolar cada uma, apenas com os pacotes requeridos, aumenta a segurança, diminui o tamanho total das aplicações e tempo de instalação e upgrades, e previne conflitos entre pacotes.\n\nO venv é considerado boa prática na comunidade python e permite isolamento total entre a aplicação e o host que a estiver rodando. \n\n### TOX\n#### Continuous integration, continuous deploy\n\nTODOS os módulos possuem configuração para o TOX. Esta configuração cria um ambiente virtual e roda todos os testes em python3.5 e python3.6. Adicionalmente, roda linters para checar adequação do código a padrões, procurar por erros e má qualidade, roda também linters de vulnerabilidades, dentre outros (ver arquivo tox.ini).\n\nO módulo ajna_docs tem documentação automatizada via Sphynx, também testada pelo tox (para gerar a documentação, rodar make html). A documentação de todos os módulos será centralizada no ajna_docs.\n\nAlém disso, está configurado nos repositórios um fluxo de CI/CD - os testes são rodados pelo Travis (linux) e Appveyor (windows) a cada push(conf nos arquivos .yaml). Se houver sucesso, o novo código é publicado automaticamente no heroku (conf nos arquivos Procfile).')
('https://raw.githubusercontent.com/s8rhdobi/fuzzingbook/3de737d330f30132565ac26fe85b37db299f8c49/notebooks/MutationFuzzer.ipynb', '# About this Book\n\n__Welcome to "Generating Software Tests"!__ \nSoftware has bugs, and catching bugs can involve lots of effort.  This book addresses this problem by _automating_ software testing, specifically by _generating tests automatically_.  Recent years have seen the development of novel techniques that lead to dramatic improvements in test generation and software testing.  They now are mature enough to be assembled in a book – even with executable code. \n\n\n```python\nfrom fuzzingbook_utils import YouTubeVideo\nYouTubeVideo("w4u5gCgPlmg")\n```\n\n\n\n\n\n        <iframe\n            width="640"\n            height="360"\n            src="https://www.youtube.com/embed/w4u5gCgPlmg"\n            frameborder="0"\n            allowfullscreen\n        ></iframe>\n        \n\n\n\n## A Textbook for Paper, Screen, and Keyboard\n\nYou can use this book in three ways:\n\n* You can __read chapters in your browser__.  Check out the list of chapters in the menu above, or start right away with the [introduction to testing](Intro_Testing.ipynb) or the [introduction to fuzzing](Fuzzer.ipynb).  All code is available for download.\n\n* You can __interact with chapters as Jupyter Notebooks__ (beta).  This allows you to edit and extend the code, experimenting _live in your browser._  Simply select "Resources $\\rightarrow$ Edit as Notebook" at the top of each chapter. <a href="https://mybinder.org/v2/gh/uds-se/fuzzingbook/master?filepath=docs/notebooks/Fuzzer.ipynb" target=_blank>Try interacting with the introduction to fuzzing.</a>\n\n* You can __present chapters as slides__.  This allows for presenting the material in lectures.  Just select "Resources $\\rightarrow$ View slides" at the top of each chapter. <a href="https://www.fuzzingbook.org/slides/Fuzzer.slides.html" target=_blank>Try viewing the slides for the introduction to fuzzing.</a>\n\n## Who this Book is for\n\nThis work is designed as a _textbook_ for a course in software testing; as _supplementary material_ in a software testing or software engineering course; and as a _resource for software developers_. We cover random fuzzing, mutation-based fuzzing, grammar-based test generation, symbolic testing, and much more, illustrating all techniques with code examples that you can try out yourself.\n\n## News\n\nThis book is _work in progress,_ with new chapters coming out every week.  To get notified when a new chapter comes out, <a href="https://twitter.com/FuzzingBook?ref_src=twsrc%5Etfw" data-show-count="false">follow us on Twitter</a>.\n\n<a class="twitter-timeline" data-width="500" data-chrome="noheader nofooter noborders transparent" data-link-color="#A93226" data-align="center" href="https://twitter.com/FuzzingBook?ref_src=twsrc%5Etfw">News from @FuzzingBook</a> \n\n\n## About the Authors\n\nThis book is written by _Andreas Zeller, Rahul Gopinath, Marcel Böhme, Gordon Fraser, and Christian Holler_.  All of us are long-standing experts in software testing and test generation; and we have written or contributed to some of the most important test generators and fuzzers on the planet.  As an example, if you are reading this in a Firefox, Chrome, or Edge Web browser, you can do so safely partly because of us, as _the very techniques listed in this book have found more than 2,600 bugs in their JavaScript interpreters so far._  We are happy to share our expertise and making it accessible to the public.\n\n## Frequently Asked Questions\n\n### Troubleshooting\n\n#### Why does it take so long to start an interactive notebook?\n\nThe interactive notebook uses the [mybinder.org](https://mybinder.org) service, which runs notebooks on their own servers.  Starting Jupyter through mybinder.org normally takes about 30 seconds, depending on your Internet connection. If, however, you are the first to invoke binder after a book update, binder recreates its environment, which will take a few minutes.  Reload the page occasionally.\n\n#### The interactive notebook does not work!\n\nmybinder.org imposes a [limit of 100 concurrent users for a repository](https://mybinder.readthedocs.io/en/latest/user-guidelines.html).  Also, as listed on the [mybinder.org status and reliability page](https://mybinder.readthedocs.io/en/latest/reliability.html),\n\n> As mybinder.org is a research pilot project, the main goal for the project is to understand usage patterns and workloads for future project evolution. While we strive for site reliability and availability, we want our users to understand the intent of this service is research and we offer no guarantees of its performance in mission critical uses.\n\nThere are alternatives to mybinder.org; see below.\n\n#### Do I have alternatives to the interactive notebook?\n\nIf mybinder.org does not work or match your needs, you have a number of alternatives:\n\n1. **Download the Python code** (using the menu at the top) and edit and run it in your favorite environment.  This is easy to do and does not require lots of resources.\n\n2. **Download the Jupyter Notebooks** (using the menu at the top) and open them in Jupyter.  Here\'s [how to install jupyter notebook on your machine](https://www.dataquest.io/blog/jupyter-notebook-tutorial/).\n\n3. **Run the notebook locally** in a Docker container. For more information, see [How to use the book with Docker](https://github.com/uds-se/fuzzingbook/blob/master/deploy/README.md).  \n\n4. If you want to use the book in a classroom, and depend on your users having access to the interactive notebooks, consider using or deploying a [JupyterHub](http://jupyter.org/hub) or [BinderHub](https://github.com/jupyterhub/binderhub) instance.\n\n#### Can I run the code on my Windows machine?\n\nWe try to keep the code as general as possible, but occasionally, when we interact with the operating system, we assume a Unix-like environment (because that is what Binder provides).  To run these examples on your own Windows machine, you can install a Linux VM or a [Docker environment](https://github.com/uds-se/fuzzingbook/blob/master/deploy/README.md).\n\n#### Can\'t you run your own dedicated cloud service?\n\nTechnically, yes; but this would cost money and effort, which we\'d rather spend on the book at this point.  If you\'d like to host a [JupyterHub](http://jupyter.org/hub) or [BinderHub](https://github.com/jupyterhub/binderhub) instance for the public, please _do so_ and let us know.\n\n### Content\n\n#### Which content will be coming up?\n\nThe contents of this book will include topics such as:\n\n1. Introduction to Testing\n1. Basic Fuzzing\n1. Coverage\n1. Mutation-Based Fuzzing\n1. Fuzzing with Grammars\n1. Efficient Grammar Fuzzing\n1. Grammar Coverage\n1. Configuration Fuzzing\n1. Parsing and Mutating Inputs\n1. Probabilistic Testing\n1. Reducing Failure-Inducing Inputs\n1. Fuzzing Function Calls\n1. Fuzzing User Interfaces\n1. Search-Based Testing\n1. Symbolic Testing\n1. Mining Grammars\n1. Fuzzing and Invariants\n1. Protection and Repair\n\nSee the table of contents in the menu above for those chapters that are already done.\n\n#### How do I cite your work?\n\nThanks for referring to our work!  Once the book is complete, you will be able to cite it in the traditional way.  In the meantime, just click on the "cite" button at the bottom of the Web page for each chapter to get a citation entry.\n\n#### Can you cite my paper?  And possibly write a chapter about it?\n\nWe\'re always happy to get suggestions!  If we missed an important reference, we will of course add it.  If you\'d like specific material to be covered, the best way is to _write a notebook_ yourself; see our [Guide for Authors](Guide_for_Authors.ipynb) for instructions on coding and writing.  We can then refer to it or even host it.\n\n### Teaching and Coursework\n\n#### Can I use your material in my course?\n\nOf course!  Just respect the [license](https://github.com/uds-se/fuzzingbook/blob/master/LICENSE.md) (including attribution and share alike).  If you want to use the material for commercial purposes, contact us.\n\n#### Can I extend or adapt your material?\n\nYes!  Again, please see the [license](https://github.com/uds-se/fuzzingbook/blob/master/LICENSE.md) for details.\n\n#### How can I run a course based on the book?\n\nWe have successfully used the material in various courses.  \n\n* Initially, we used the slides and code and did _live coding_ in lectures to illustrate how a technique works. \n\n* Now, the goal of the book is to be completely self-contained; that is, it should work without additional support.  Hence, we now give out completed chapters to students in a _flipped classroom_ setting, with the students working on the notebooks at their leisure.  We would meet in the classroom to discuss experiences with past notebooks and discuss future notebooks.\n\nWhen running a course, [do not rely on mybinder.org](#Troubleshooting) –\xa0it will not provide sufficient resources for a larger group of students.  Instead, [install and run your own hub.](#Do-I-have-alternatives-to-the-interactive-notebook?)\n\n#### How can I extend or adapt your slides?\n\nDownload the Jupyter Notebooks (using the menu at the top) and adapt the notebooks at your leisure (see above), including "Slide Type" settings.  Then,\n\n1. Download slides from Jupyter Notebook; or\n2. Use the RISE extension ([instructions](http://www.blog.pythonlibrary.org/2018/09/25/creating-presentations-with-jupyter-notebook/)) to present your slides right out of Jupyter notebook.\n\n#### Do you provide PDFs of your material?\n\nAt this point, we do not provide support for PDF versions.  We will be producing PDF and print versions once the book is complete.\n\n### Other Issues\n\n#### I have a question, comment, or a suggestion.  What do I do?\n\nYou can [tweet to @fuzzingbook on Twitter](https://twitter.com/fuzzingbook), allowing the community of readers to chime in.  For bugs you\'d like to get fixed, report an issue on the [development page](https://github.com/uds-se/fuzzingbook/issues).\n\n#### I have reported an issue two weeks ago.  When will it be addressed?\n\nWe prioritize issues as follows:\n\n1. Bugs in code published on fuzzingbook.org\n2. Bugs in text published on fuzzingbook.org\n3. Writing missing chapters\n4. Issues in yet unpublished code or text\n5. Issues related to development or construction\n6. Things marked as "beta"\n7. Everything else\n\n#### How can I solve problems myself?\n\nWe\'re glad you ask that.  The [development page](https://github.com/uds-se/fuzzingbook/) has all sources and some supplementary material.  Pull requests that fix issues are very welcome.\n\n#### How can I contribute?\n\nAgain, we\'re glad you\'re here!  We are happy to accept \n\n* **Code fixes and improvements.**  Please place any code under the MIT license such that we can easily include it.\n* **Additional text, chapters, and notebooks** on specialized topics.  We plan to set up a special folder for third-party contributions.\n\nSee our [Guide for Authors](Guide_for_Authors.ipynb) for instructions on coding and writing.')
('https://raw.githubusercontent.com/tepickering/HALcoll/d34d8ab4adf3ef86d600ed7f3585d752077ce7ff/notebooks/TF%20testing.ipynb', "# HALcoll\n\n![I'm sorry, Dave](https://cdn-images-1.medium.com/max/1320/1*c_faFfZdodL25p1RZ1fl4Q.jpeg)\n\nUsing ML and AI techniques to build open-loop lookup models for the MMTO")
('https://raw.githubusercontent.com/davidbrochart/pangeo-streamflow/e5ec99e06b3bd01018d4ee45b09e5eb2e445bd68/notebooks/amazonas_fit.ipynb', '# pangeo-streamflow\nA global streamflow model using the Pangeo platform: [![Binder](https://mybinder.org/badge_logo.svg)](http://binder.pangeo.io/v2/gh/davidbrochart/pangeo-streamflow/master)')
('https://raw.githubusercontent.com/kaphka/imi-master-thesis/e294bda8a5b76fbb69bfc5f0af856c2f3bc786bc/notebooks/data/hisdb.ipynb', None)
('https://raw.githubusercontent.com/cmaumet/nipype_tutorial/aba7bda10326689d721b5a99985df044852f50aa/notebooks/introduction_python.ipynb', '# Nipype Tutorial Notebooks\n\n[![CircleCi](https://img.shields.io/circleci/project/miykael/nipype_tutorial/master.svg?maxAge=2592000)](https://circleci.com/gh/miykael/nipype_tutorial/tree/master)\n[![GitHub issues](https://img.shields.io/github/issues/miykael/nipype_tutorial.svg)](https://github.com/miykael/nipype_tutorial/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg)](https://github.com/miykael/nipype_tutorial/pulls/)\n[![GitHub contributors](https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg)](https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/)\n[![GitHub Commits](https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg)](https://github.com/miykael/nipype_tutorial/commits/master)\n[![GitHub size](https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg)](https://github.com/miykael/nipype_tutorial/archive/master.zip)\n[![Docker Hub](https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000)](https://hub.docker.com/r/miykael/nipype_tutorial/)\n[![GitHub HitCount](http://hits.dwyl.io/miykael/nipype_tutorial.svg)](http://hits.dwyl.io/miykael/nipype_tutorial)\n\nThis is the Nipype Tutorial in Jupyter Notebook format. You can access the tutorial in two ways:\n\n1. [Nipype Tutorial Homepage](https://miykael.github.io/nipype_tutorial/): This website contains a static, read-only version of all the notebooks.\n2. [Nipype Tutorial Docker Image](https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html): This guide explains how to use Docker to run the notebooks interactively on your own computer. The nipype tutorial docker image is the best interactive way to learn Nipype.\n\n\n# Feedback, Help & Support\n\nIf you want to help with this tutorial or have any questions, feel free to fork the repo of the [Notebooks](https://github.com/miykael/nipype_tutorial) or interact with other contributors on the slack channel [brainhack.slack.com/messages/nipype/](https://brainhack.slack.com/messages/nipype/). If you have any questions or found a problem, open a new [issue on github](https://github.com/miykael/nipype_tutorial/issues).\n\n\n# Thanks and Acknowledgment\n\nA huge thanks to [Michael Waskom](https://github.com/mwaskom), [Oscar Esteban](https://github.com/oesteban), [Chris Gorgolewski](https://github.com/chrisfilo) and [Satrajit Ghosh](https://github.com/satra) for their input to this tutorial! And a huge thanks to [Dorota Jarecka](https://github.com/djarecka/) who updated this tutorial to Python 3 and is helping me with keeping this tutorial updated and running!')
('https://raw.githubusercontent.com/ekostat/ekostat_calculator/ce331e2c89c061ee27b00eff4af14ef6378ed216/notebooks/lv_notebook_Indicator-Copy1.ipynb', '# EKOSTAT_tool\nA tool for calculation of ecological status in Swedish water bodies for reporting. ')
('https://raw.githubusercontent.com/jclifton333/glucose/983f2018fecb3d59a847fd5a324ac5f71c171c52/notebooks/model-smoothed-fitted-q.ipynb', None)
('https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/df13ad1ad2bfc56ece95baf5250500ae7cc04624/notebooks/neural-machine-translation-with-transformer-and-tensorflow2.ipynb', None)
('https://raw.githubusercontent.com/smeingast/PNICER/c720d8b55b90bebe17e83b94f95048536172e08b/notebooks/pnicer.ipynb', 'PNICER is an astronomical software suite for estimating extinction for individual sources and creating extinction maps using unsupervised machine learning algorithms. If you want to know more about the technique, you are invited to study the published manuscript, which is currently available on Astro-ph. Please note that this is our first release and if you encounter problems, let us know so that we can fix issues asap.\n\n## Requirements\n\nPNICER is designed to have as few dependencies as possible and there is a good chance that you are already running Python with all necessary packages. PNICER requires *numpy*, *scipy*, *astropy*, *matplotlib*, and *scikit-learn*. All necessary packages will be installed or upgraded automatically with pip. Also, at the moment this package is not compatible with Windows operating systems due to parallel processing frameworks available in Python.\n\n\n## Installation\n\nTo install the package, download the latest release to your computer [here](https://github.com/smeingast/PNICER/releases/latest). Unpack the archive and install with pip\n\n```bash\npip install --user /path/to/PNICER/\n```\n\nwhere the last argument points to the directory of the saved and unpacked downloaded directory. All dependencies will be installed automatically.\n\n### Test\n\nTo test the installation, start up python (or ipython) and type\n\n```python\nfrom pnicer.tests import orion\norion()\n```\n\nwhich will go through all major PNICER methods. At the end you should see a plot window with an extinction map of Orion A created from 2MASS data:\n\n![Orion](https://raw.githubusercontent.com/smeingast/PNICER/master/pnicer/tests_resources/orion.png)\n\n## Introduction\n\n\nFor an introduction to the basic tools available in **PNICER**, please refer to the jupyter notebook provided with this package:\n\n[PNICER introduction notebook](https://github.com/smeingast/PNICER/blob/master/notebooks/pnicer.ipynb)\n\n\nIn the near future (April - May 2017) we plan to implement advanced extinction mapping tools and we will also soon provide the complete API of PNICER. If you have any questions, I am always happy to receive feedback (both positive and negative).')
('https://raw.githubusercontent.com/juditacs/morph-segmentation/cc2e1148b8aa6312f73c04cb9cf492ef5b1f14fc/notebooks/sandbox/seq2seq_attention.ipynb', "# Morphological segmentation\n\nMy final presentation is available as a Jupyter notebook [here](https://github.com/juditacs/morph-segmentation/blob/master/notebooks/final_presentation/presentation.ipynb).\n\nExperimenting with supervised morphological segmentation as a seq2seq problem.\n\nCurrently three supervised models are supported: seq2seq, sequence tagger using LSTM/GRU  and character-level CNN.\n\nNOTE: I use the GitHub issue tracker for major bugs and enhancement proposals.\nIt is available in my [main repository](https://github.com/juditacs/morph-segmentation/issues).\n\n## Setup\n\n    pip install -r requirements.txt\n    python setup.py install\n\n## Input data\n\nTre training scripts (`train.py`) expect the training input as either as its only positional argument or it reads it from standard input if no positional argument is provided.\nGzip files are supported.\nThe training data is expected to have one line per sample.\nThe input and the output sequences should be separated by TAB.\n\nExample:\n\n~~~\nautót\tautó t\nablakokat\tablak ok at\n~~~\n\nThe inference scripts (`inference.py`) also read from the standard input and expects one sample per line.\n\n## seq2seq\n\nThe previous (working) seq2seq implementation used `legacy_seq2seq`.\nThe new implementation is still under heavy development.\nUsage information will be added soon.\n\n\n### Old (deprecated) seq2seq instructions\n\nThe code is available on te `seq2seq_old` branch.\n\nThe seq2seq source code is located in the `morph_seg/seq2seq` directory.\nIt uses Tensorflow's `legacy_seq2seq`.\n\n#### Training your own model\n\n~~~\ncat training_data | python morph_seg/seq2seq/train.py --save-test-output test_output --save-model model_directory --cell-size 64 --result-file results.tsv\n~~~\n\nThis will train a seq2seq model with the default arguments listed in `train.py`:\n\n| argument | default | explanation |\n| ----- | ----- | ------ |\n| `save-test-output` | `None` | Save the model's output on the test set (randomly sampled) |\n| `save-model` | `None` |  Save the model and other stuff needed for inference. This should be an exisiting directory. |\n| `result-file` | `None` | Save the experiment's configuration and the result statistics. |\n| `cell-type` | `LSTM` | Use LSTM or GRU cells. |\n| `cell-size` | 16 | Number of LSTM/GRU cells to use. |\n| `layers` | 1 | Number of layers. |\n| `embedding-size` | 20 | Dimension of embedding. |\n| `early-stopping-threshold` | 0.001 | Stop training when val loss does not change more than this threshold for N steps. |\n| `early-stopping-patience` | 10 | Stop training if val loss does not change more than the threshold for N steps. |\n\nNote that the first three arguments' default is `None`.\nThis means that unless specified, they do not write to file.\nThey are not linked though, any one can be left out.\n\n#### Using your model for inference\n\n`train.py` saves everything needed for inference to the directory specified by the `save-model` argument.\nInference can be run like this:\n\n~~~\ncat test_data | python morph_seg/seq2seq/inference.py --model-dir your_saved_model\n~~~\n\nNote that longer samples than the maximum length in the training data will be trimmed from their beginning.\n\n## LSTM and CNN\n\nThis section needs updating.\n\nThe LSTM source code is located in the `morph_seg/sequence_tagger` directory.\nIt uses Keras's `LSTM`, `GRU` modules, and the usage is basically identical to the seq2seq model above.\n\n### Training your own model\n\n~~~\ncat training_data | python morph_seg/sequence_tagger/train.py --config config.yaml --architecture RNN\n~~~\n\nSee an example configuration at `config/sequence_tagger/toy.yaml`.\n\nThe majority of options is currently listed in the source code in `train.py`. Sorry :(\n\n### Using your model for inference\n\n~~~\ncat test_data | python morph_seg/sequence_tagger/inference.py --model-dir your_saved_model\n~~~")
('https://raw.githubusercontent.com/AISpace2/AISpace2/86cb4f99b486b00bfff316937a4cf268ba58c8fe/notebooks/search/search.ipynb', '# AISpace2\n\n## About\nAISpace2 is a set of notebooks and an extension for [Jupyter](http://jupyter.org), a web application that combines code, text, and visualizations into a single, rich document. These notebooks teach and demonstrate AI concepts by providing detailed explanations alongside Python code implementations, and the accompanying extension brings these concepts to life by providing interactive visualizations driven directly by the code you see.\n\nAISpace2 is the next version of [AISpace](http://aispace.org), and accompanies the book [_Artifical Intelligence 2E: Foundations of Computational Agents_](http://artint.info) by [David Poole](http://cs.ubc.ca/~poole/) and [Alan Mackworth](http://cs.ubc.ca/~mack/).\n\nWe encourage you to check out our [website](https://aispace2.github.io/AISpace2/) for more information, including installation instructions.')
('https://raw.githubusercontent.com/vuillaut/cta-lstchain/9c816c79d31ceed45db9fcdcfd504087d75d7b94/notebooks/tutorial_gammalearn.ipynb', "# cta-lstchain\n\nRepository for the high level analysis of the LST.    \nThe analysis is heavily based on [ctapipe](https://github.com/cta-observatory/ctapipe), adding custom code for mono reconstruction.\n\nmaster branch status: [![Build Status](https://travis-ci.org/cta-observatory/cta-lstchain.svg?branch=master)](https://travis-ci.org/cta-observatory/cta-lstchain)\n\n\n### Important message to lstchain users (May 4th 2019):\n*ctapipe* and *lstchain* are currently undergoing heavy and rapid changes.    \nThe core developer team is trying to stay up-to-date with the master version of *ctapipe* before reaching *ctapipe v0.7* release.\nYou might experience some issues if changes have been merged in *ctapipe* master before we could integrate these changes in *lstchain*. We are sorry for that. Do not hesitate to submit an issue or propose a patch through a pull request.\n\n- The basic functions of lstchain (reduction steps R0-->DL1 and DL1-->DL2) are unit tested and should be working as long as the build status is passing.    \n- However, the notebooks are not and might not be up-to-date before stable release. Do not rely on them for now.\n\n\n\n## Install\n\n> Old install procedure:\n> If you are a user and don't already have ctapipe installed:\n> ```\n> conda env create -f environment.yml\n> source activate cta\n> ```\n> This will create a conda environment called `cta` and install ctapipe with all dependencies.\n\n> Then you can install the `lstchain` in this environment with:\n> ```\n> python setup.py install\n> ```\n\nCurrent `lstchain` build uses `ctapipe` master version.   \nHere is how you should install:\n```\ngit clone https://github.com/cta-observatory/cta-lstchain.git\ncd cta-lstchain\nconda env create --name cta --file environment.yml\nconda activate cta\npip install https://github.com/cta-observatory/ctapipe/archive/master.tar.gz\npip install https://github.com/cta-sst-1m/protozfitsreader/archive/v1.4.2.tar.gz\npip install https://github.com/cta-observatory/ctapipe_io_lst/archive/master.tar.gz\npip install -e .\n```\n\n\n## Contributing\n\nAll contribution are welcomed.\n\nGuidelines are the same as [ctapipe's ones](https://cta-observatory.github.io/ctapipe/development/index.html)\nSee [here](https://cta-observatory.github.io/ctapipe/development/pullrequests.html) how to make a pull request to contribute.\n\n\n## Report issue / Ask a question\n\nUse GitHub Issues.")
('https://raw.githubusercontent.com/Bjarten/numpy-tutorial/2eaefb1c8c211c5eb1d7bd2823422d92ca04b69a/numpy-examples.ipynb', '# Numpy Tutorial\n\nThis repository contains Numpy examples. A more detailed README will be written when the project has come further along.')
('https://raw.githubusercontent.com/srjit/data-visualizations/f5f7a33a1a59164fb6d9ab3b8cc47d92982d48a2/nyse/src/NYSE-Data%20Analysis.ipynb', '# data-visualizations\nSimple Data VisuData/Modeling Visualizations from a number of publicly available datasets.')
('https://raw.githubusercontent.com/FelixandLiu/learning/73adf3a7392ceb067bf646fd084cd50fdfc9ef5b/optimal-road-trip/Computing%20the%20optimal%20road%20trip%20across%20the%20U.S..ipynb', '![Python 2.7](https://img.shields.io/badge/python-2.7-blue.svg)\n![Python 3.5](https://img.shields.io/badge/python-3.5-blue.svg)\n![License](https://img.shields.io/badge/license-MIT%20License-blue.svg)\n\n# Randy Olson\'s data analysis and machine learning projects\n\n© 2016 Randal S. Olson\n\nThis is a repository of teaching materials, code, and data for my data analysis and machine learning projects.\n\nEach repository will (usually) correspond to one of the blog posts on my [web site](http://www.randalolson.com/blog/).\n\nBe sure to check the documentation (usually in IPython Notebook format) in the directory you\'re interested in for the notes on the analysis, data usage terms, etc.\n\nIf you don\'t have the necessary software installed to run IPython Notebook, don\'t fret. You can use [nbviewer](http://nbviewer.ipython.org/) to view a notebook on the web.\n\nFor example, if you want to view the notebook in the `wheres-waldo-path-optimization` directory, copy the [full link](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/wheres-waldo-path-optimization/Where\'s%20Waldo%20path%20optimization.ipynb) to the notebook then paste it into [nbviewer](http://nbviewer.ipython.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/wheres-waldo-path-optimization/Where%27s%20Waldo%20path%20optimization.ipynb).\n\n## License\n\n### Instructional Material\n\nAll instructional material in this repository is made available under the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/). The following is a human-readable summary of (and not a substitute for) the [full legal text of the CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/legalcode).\n\nYou are free to:\n\n* **Share**—copy and redistribute the material in any medium or format\n* **Adapt**—remix, transform, and build upon the material\n\nfor any purpose, even commercially.\n\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\n\nUnder the following terms:\n\n* **Attribution**—You must give appropriate credit (mentioning that your work is derived from work that is © Randal S. Olson and, where practical, linking to http://www.randalolson.com/), provide a [link to the license](https://creativecommons.org/licenses/by/4.0/), and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n**No additional restrictions**—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.\n\n**Notices:**\n\n* You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\n* No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n### Software\n\nExcept where otherwise noted, the example programs and other software provided in this repository are made available under the [OSI](http://opensource.org/)-approved [MIT license](http://opensource.org/licenses/mit-license.html).\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.')
('https://raw.githubusercontent.com/milanchestojanovic/TestRepo/8a6a068d0f0252be6f9b44c3146db71daa6115e7/pileUpFilter/notebook/pileUpFilterMustRead.ipynb', '# TrackingCode\nA place to keep all the tracking code, tracking notebook, and instructions to use for cms HI group. There are a number of different major\nissues corresponding to different folders:\n\n- HIRun2015Ana, to make tracking performance plots, tracking efficiency correction factors for MB samples\n- pileUpFilters, everything you need for "pileUp filter" in run2 pp analysis \n\n<strong> See detail instructions in README.md under each folder </strong>\n\nTo get started:\n\n- do <pre><code> cd CMSSW_75X/src </pre></code>\n- do <pre><code> cmsenv </pre></code>\n- do <pre><code> git clone https://github.com/cmsHiTracking/TrackingCode.git </pre></code>\n- do <pre><code> scram b -j4 </pre></code>')
('https://raw.githubusercontent.com/brunoalvarez89/PrestamoBanco/478480aed4a9001d8a3d7ae0ea25dc4c5df592e5/prestamo_banco.ipynb', '# PrestamoBanco')
('https://raw.githubusercontent.com/informatik-mannheim/sysplace_demo_orderstatus/aca7c2036a2b57ab0cba5409f9be4a9d4896723a/python/TangibleTouch.ipynb', '\ufeff# sysplace_demo_orderstatus\nConnecting a Particle.io, a big touchscreen, an android app and a toy car utilizing Touchcodes.')
('https://raw.githubusercontent.com/Dev-Jahn/sofcon/0f95e88b868061a2baafe2a7e07740760ca3a876/recomm/vectorize_tag.ipynb', 'test')
('https://raw.githubusercontent.com/NeelChakraborty/ISI_Bank_Profitability/03df147bb1703c7e1ae859a8af275257ef19a131/regression.ipynb', None)
('https://raw.githubusercontent.com/r-jain/aima-python/cae3d019c24c50485dab216276ff364fadec9d33/rl.ipynb', '<div align="center">\n  <a href="http://aima.cs.berkeley.edu/"><img src="https://raw.githubusercontent.com/aimacode/aima-python/master/images/aima_logo.png"></a><br><br>\n</div>\n\n# `aima-python` [![Build Status](https://travis-ci.org/aimacode/aima-python.svg?branch=master)](https://travis-ci.org/aimacode/aima-python) [![Binder](http://mybinder.org/badge.svg)](http://mybinder.org/repo/aimacode/aima-python)\n\n\nPython code for the book *[Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu).* You can use this in conjunction with a course on AI, or for study on your own. We\'re looking for [solid contributors](https://github.com/aimacode/aima-python/blob/master/CONTRIBUTING.md) to help.\n\n\n\n## Structure of the Project\n\nWhen complete, this project will have Python implementations for all the pseudocode algorithms in the book, as well as tests and examples of use. For each major topic, such as `nlp` (natural language processing), we provide the following  files:\n\n- `nlp.py`: Implementations of all the pseudocode algorithms, and necessary support functions/classes/data.\n- `tests/test_nlp.py`: A lightweight test suite, using `assert` statements, designed for use with [`py.test`](http://pytest.org/latest/), but also usable on their own.\n- `nlp.ipynb`: A Jupyter (IPython) notebook that explains and gives examples of how to use the code.\n- `nlp_apps.ipynb`: A Jupyter notebook that gives example applications of the code.\n\n\n## Python 3.4 and up\n\nThis code requires Python 3.4 or later, and does not run in Python 2. You can [install Python](https://www.python.org/downloads) or use a browser-based Python interpreter such as [repl.it](https://repl.it/languages/python3).\nYou can run the code in an IDE, or from the command line with `python -i filename.py` where the `-i` option puts you in an interactive loop where you can run Python functions. See [jupyter.org](http://jupyter.org/) for instructions on setting up your own Jupyter notebook environment, or run the notebooks online with [try.jupiter.org](https://try.jupyter.org/).\n\n\n## Installation Guide\n\nTo download the repository:\n\n`git clone https://github.com/aimacode/aima-python.git`\n\nYou also need to fetch the datasets from the [`aima-data`](https://github.com/aimacode/aima-data) repository:\n\n```\ncd aima-python\ngit submodule init\ngit submodule update\n```\n\nWait for the datasets to download, it may take a while. Once they are downloaded, you need to install `pytest`, so that you can run the test suite:\n\n`pip install pytest`\n\nThen to run the tests:\n\n`py.test`\n\nAnd you are good to go!\n\n\n# Index of Algorithms\n\nHere is a table of algorithms, the figure, name of the algorithm in the book and in the repository, and the file where they are implemented in the repository. This chart was made for the third edition of the book and is being updated for the upcoming fourth edition. Empty implementations are a good place for contributors to look for an issue. The [aima-pseudocode](https://github.com/aimacode/aima-pseudocode) project describes all the algorithms from the book. An asterisk next to the file name denotes the algorithm is not fully implemented. Another great place for contributors to start is by adding tests and writing on the notebooks. You can see which algorithms have tests and notebook sections below. If the algorithm you want to work on is covered, don\'t worry! You can still add more tests and provide some examples of use in the notebook!\n\n| **Figure** | **Name (in 3<sup>rd</sup> edition)** | **Name (in repository)** | **File** | **Tests** | **Notebook**\n|:-------|:----------------------------------|:------------------------------|:--------------------------------|:-----|:---------|\n| 2      | Random-Vacuum-Agent               | `RandomVacuumAgent`           | [`agents.py`][agents]           | Done | Included |\n| 2      | Model-Based-Vacuum-Agent          | `ModelBasedVacuumAgent`       | [`agents.py`][agents]           | Done | Included |\n| 2.1    | Environment                       | `Environment`                 | [`agents.py`][agents]           | Done | Included |\n| 2.1    | Agent                             | `Agent`                       | [`agents.py`][agents]           | Done | Included |\n| 2.3    | Table-Driven-Vacuum-Agent         | `TableDrivenVacuumAgent`      | [`agents.py`][agents]           | Done | Included |\n| 2.7    | Table-Driven-Agent                | `TableDrivenAgent`            | [`agents.py`][agents]           | Done | Included |\n| 2.8    | Reflex-Vacuum-Agent               | `ReflexVacuumAgent`           | [`agents.py`][agents]           | Done | Included |\n| 2.10   | Simple-Reflex-Agent               | `SimpleReflexAgent`           | [`agents.py`][agents]           |      | Included |\n| 2.12   | Model-Based-Reflex-Agent          | `ReflexAgentWithState`        | [`agents.py`][agents]           |      | Included |\n| 3      | Problem                           | `Problem`                     | [`search.py`][search]           | Done | Included |\n| 3      | Node                              | `Node`                        | [`search.py`][search]           | Done | Included |\n| 3      | Queue                             | `Queue`                       | [`utils.py`][utils]             | Done | No Need  |\n| 3.1    | Simple-Problem-Solving-Agent      | `SimpleProblemSolvingAgent`   | [`search.py`][search]           |      | Included |\n| 3.2    | Romania                           | `romania`                     | [`search.py`][search]           | Done | Included |\n| 3.7    | Tree-Search                       | `tree_search`                 | [`search.py`][search]           | Done |          |\n| 3.7    | Graph-Search                      | `graph_search`                | [`search.py`][search]           | Done |          |\n| 3.11   | Breadth-First-Search              | `breadth_first_search`        | [`search.py`][search]           | Done | Included |\n| 3.14   | Uniform-Cost-Search               | `uniform_cost_search`         | [`search.py`][search]           | Done | Included |\n| 3.17   | Depth-Limited-Search              | `depth_limited_search`        | [`search.py`][search]           | Done |          |\n| 3.18   | Iterative-Deepening-Search        | `iterative_deepening_search`  | [`search.py`][search]           | Done |          |\n| 3.22   | Best-First-Search                 | `best_first_graph_search`     | [`search.py`][search]           | Done | Included |\n| 3.24   | A\\*-Search                        | `astar_search`                | [`search.py`][search]           | Done | Included |\n| 3.26   | Recursive-Best-First-Search       | `recursive_best_first_search` | [`search.py`][search]           | Done |          |\n| 4.2    | Hill-Climbing                     | `hill_climbing`               | [`search.py`][search]           | Done | Included |\n| 4.5    | Simulated-Annealing               | `simulated_annealing`         | [`search.py`][search]           | Done |          |\n| 4.8    | Genetic-Algorithm                 | `genetic_algorithm`           | [`search.py`][search]           | Done | Included |\n| 4.11   | And-Or-Graph-Search               | `and_or_graph_search`         | [`search.py`][search]           | Done |          |\n| 4.21   | Online-DFS-Agent                  | `online_dfs_agent`            | [`search.py`][search]           |      |          |\n| 4.24   | LRTA\\*-Agent                      | `LRTAStarAgent`               | [`search.py`][search]           | Done |          |\n| 5.3    | Minimax-Decision                  | `minimax_decision`            | [`games.py`][games]             | Done | Included |\n| 5.7    | Alpha-Beta-Search                 | `alphabeta_search`            | [`games.py`][games]             | Done | Included |\n| 6      | CSP                               | `CSP`                         | [`csp.py`][csp]                 | Done | Included |\n| 6.3    | AC-3                              | `AC3`                         | [`csp.py`][csp]                 | Done |          |\n| 6.5    | Backtracking-Search               | `backtracking_search`         | [`csp.py`][csp]                 | Done | Included |\n| 6.8    | Min-Conflicts                     | `min_conflicts`               | [`csp.py`][csp]                 | Done |          |\n| 6.11   | Tree-CSP-Solver                   | `tree_csp_solver`             | [`csp.py`][csp]                 | Done | Included |\n| 7      | KB                                | `KB`                          | [`logic.py`][logic]             | Done | Included |\n| 7.1    | KB-Agent                          | `KB_Agent`                    | [`logic.py`][logic]             | Done |          |\n| 7.7    | Propositional Logic Sentence      | `Expr`                        | [`utils.py`][utils]             | Done | Included |\n| 7.10   | TT-Entails                        | `tt_entails`                  | [`logic.py`][logic]             | Done |          |\n| 7.12   | PL-Resolution                     | `pl_resolution`               | [`logic.py`][logic]             | Done | Included |\n| 7.14   | Convert to CNF                    | `to_cnf`                      | [`logic.py`][logic]             | Done |          |\n| 7.15   | PL-FC-Entails?                    | `pl_fc_resolution`            | [`logic.py`][logic]             | Done |          |\n| 7.17   | DPLL-Satisfiable?                 | `dpll_satisfiable`            | [`logic.py`][logic]             | Done |          |\n| 7.18   | WalkSAT                           | `WalkSAT`                     | [`logic.py`][logic]             | Done |          |\n| 7.20   | Hybrid-Wumpus-Agent               | `HybridWumpusAgent`           |                                 |      |          |\n| 7.22   | SATPlan                           | `SAT_plan`                    | [`logic.py`][logic]             | Done |          |\n| 9      | Subst                             | `subst`                       | [`logic.py`][logic]             | Done |          |\n| 9.1    | Unify                             | `unify`                       | [`logic.py`][logic]             | Done | Included |\n| 9.3    | FOL-FC-Ask                        | `fol_fc_ask`                  | [`logic.py`][logic]             | Done |          |\n| 9.6    | FOL-BC-Ask                        | `fol_bc_ask`                  | [`logic.py`][logic]             | Done |          |\n| 9.8    | Append                            |                               |                                 |      |          |\n| 10.1   | Air-Cargo-problem                 | `air_cargo`                   | [`planning.py`][planning]       | Done |          |\n| 10.2   | Spare-Tire-Problem                | `spare_tire`                  | [`planning.py`][planning]       | Done |          |\n| 10.3   | Three-Block-Tower                 | `three_block_tower`           | [`planning.py`][planning]       | Done |          |\n| 10.7   | Cake-Problem                      | `have_cake_and_eat_cake_too`  | [`planning.py`][planning]       | Done |          |\n| 10.9   | Graphplan                         | `GraphPlan`                   | [`planning.py`][planning]       |      |          |\n| 10.13  | Partial-Order-Planner             |                               |                                 |      |          |\n| 11.1   | Job-Shop-Problem-With-Resources   | `job_shop_problem`            | [`planning.py`][planning]       | Done |          |\n| 11.5   | Hierarchical-Search               | `hierarchical_search`         | [`planning.py`][planning]       |      |          |\n| 11.8   | Angelic-Search                    |                               |                                 |      |          |\n| 11.10  | Doubles-tennis                    | `double_tennis_problem`       | [`planning.py`][planning]       |      |          |\n| 13     | Discrete Probability Distribution | `ProbDist`                    | [`probability.py`][probability] | Done | Included |\n| 13.1   | DT-Agent                          | `DTAgent`                     | [`probability.py`][probability] |      |          |\n| 14.9   | Enumeration-Ask                   | `enumeration_ask`             | [`probability.py`][probability] | Done | Included |\n| 14.11  | Elimination-Ask                   | `elimination_ask`             | [`probability.py`][probability] | Done | Included |\n| 14.13  | Prior-Sample                      | `prior_sample`                | [`probability.py`][probability] |      | Included |\n| 14.14  | Rejection-Sampling                | `rejection_sampling`          | [`probability.py`][probability] | Done | Included |\n| 14.15  | Likelihood-Weighting              | `likelihood_weighting`        | [`probability.py`][probability] | Done | Included |\n| 14.16  | Gibbs-Ask                         | `gibbs_ask`                   | [`probability.py`][probability] | Done | Included |\n| 15.4   | Forward-Backward                  | `forward_backward`            | [`probability.py`][probability] | Done |          |\n| 15.6   | Fixed-Lag-Smoothing               | `fixed_lag_smoothing`         | [`probability.py`][probability] | Done |          |\n| 15.17  | Particle-Filtering                | `particle_filtering`          | [`probability.py`][probability] | Done |          |\n| 16.9   | Information-Gathering-Agent       |                               |                                 |      |          |\n| 17.4   | Value-Iteration                   | `value_iteration`             | [`mdp.py`][mdp]                 | Done | Included |\n| 17.7   | Policy-Iteration                  | `policy_iteration`            | [`mdp.py`][mdp]                 | Done | Included |\n| 17.9   | POMDP-Value-Iteration             |                               |                                 |      |          |\n| 18.5   | Decision-Tree-Learning            | `DecisionTreeLearner`         | [`learning.py`][learning]       | Done | Included |\n| 18.8   | Cross-Validation                  | `cross_validation`            | [`learning.py`][learning]       |      |          |\n| 18.11  | Decision-List-Learning            | `DecisionListLearner`         | [`learning.py`][learning]\\*     |      |          |\n| 18.24  | Back-Prop-Learning                | `BackPropagationLearner`      | [`learning.py`][learning]       | Done | Included |\n| 18.34  | AdaBoost                          | `AdaBoost`                    | [`learning.py`][learning]       | Done | Included |\n| 19.2   | Current-Best-Learning             | `current_best_learning`       | [`knowledge.py`](knowledge.py)  | Done | Included |\n| 19.3   | Version-Space-Learning            | `version_space_learning`      | [`knowledge.py`](knowledge.py)  | Done | Included |\n| 19.8   | Minimal-Consistent-Det            | `minimal_consistent_det`      | [`knowledge.py`](knowledge.py)  | Done |          |\n| 19.12  | FOIL                              | `FOIL_container`              | [`knowledge.py`](knowledge.py)  | Done |          |\n| 21.2   | Passive-ADP-Agent                 | `PassiveADPAgent`             | [`rl.py`][rl]                   | Done | Included |\n| 21.4   | Passive-TD-Agent                  | `PassiveTDAgent`              | [`rl.py`][rl]                   | Done | Included |\n| 21.8   | Q-Learning-Agent                  | `QLearningAgent`              | [`rl.py`][rl]                   | Done | Included |\n| 22.1   | HITS                              | `HITS`                        | [`nlp.py`][nlp]                 | Done | Included |\n| 23     | Chart-Parse                       | `Chart`                       | [`nlp.py`][nlp]                 | Done | Included |\n| 23.5   | CYK-Parse                         | `CYK_parse`                   | [`nlp.py`][nlp]                 | Done | Included |\n| 25.9   | Monte-Carlo-Localization          | `monte_carlo_localization`    | [`probability.py`][probability] | Done |          |\n\n\n# Index of data structures\n\nHere is a table of the implemented data structures, the figure, name of the implementation in the repository, and the file where they are implemented.\n\n| **Figure** | **Name (in repository)** | **File** |\n|:-------|:--------------------------------|:--------------------------|\n| 3.2    | romania_map                     | [`search.py`][search]     |\n| 4.9    | vacumm_world                    | [`search.py`][search]     |\n| 4.23   | one_dim_state_space             | [`search.py`][search]     |\n| 6.1    | australia_map                   | [`search.py`][search]     |\n| 7.13   | wumpus_world_inference          | [`logic.py`][logic]       |\n| 7.16   | horn_clauses_KB                 | [`logic.py`][logic]       |\n| 17.1   | sequential_decision_environment | [`mdp.py`][mdp]           |\n| 18.2   | waiting_decision_tree           | [`learning.py`][learning] |\n\n\n# Acknowledgements\n\nMany thanks for contributions over the years. I got bug reports, corrected code, and other support from Darius Bacon, Phil Ruggera, Peng Shao, Amit Patil, Ted Nienstedt, Jim Martin, Ben Catanzariti, and others. Now that the project is on GitHub, you can see the [contributors](https://github.com/aimacode/aima-python/graphs/contributors) who are doing a great job of actively improving the project. Many thanks to all contributors, especially @darius, @SnShine, @reachtarunhere, @MrDupin, and @Chipe1.\n\n<!---Reference Links-->\n[agents]:../master/agents.py\n[csp]:../master/csp.py\n[games]:../master/games.py\n[grid]:../master/grid.py\n[knowledge]:../master/knowledge.py\n[learning]:../master/learning.py\n[logic]:../master/logic.py\n[mdp]:../master/mdp.py\n[nlp]:../master/nlp.py\n[planning]:../master/planning.py\n[probability]:../master/probability.py\n[rl]:../master/rl.py\n[search]:../master/search.py\n[utils]:../master/utils.py\n[text]:../master/text.py')
('https://raw.githubusercontent.com/jutanke/pak/8515a6eaa599ec7165c250c1b2dc1767da8b0fdf/samples/MARCOnI.ipynb', '# pak\n![milk](https://user-images.githubusercontent.com/831215/32673460-9057f8ac-c64f-11e7-97e0-672eef1fe75d.png)\nPersonal computer vision/deep learning dataset helper toolbox to make it less tedious to download and \nload common datasets. This software is not affiliated with any of the datasets but is instead just a thin helper box to ease \ninteracting with the data. Please respect the respective dataset author\'s licenses!\n\n## Install\nInstall the library using pip:\n```bash\npip install git+https://github.com/justayak/pak.git\n```\n\n### Requirements \n\n* python >=3.5\n* numpy\n* scipy\n* skimage\n* h5py\n* pppr (for evaluation)\n\n#### Install [pppr](https://github.com/justayak/pppr)\n```bash\npip install git+https://github.com/justayak/pppr.git\n```\n\n## [Evaluation](https://github.com/justayak/pak/tree/master/pak/evaluation)\n\nThis library offers some of the common evaluation strategies\n\n## Datasets\n\n### MOT16\n[Dataset](https://motchallenge.net/)[1] with 14 video sequences (7 train, 7 test) in unconstrained environments with both static and moving cameras.\nTracking + evaluation is done in image coordinates.\n[Sample code](https://github.com/justayak/pak/blob/master/samples/MOT16.ipynb)\n\n```python\nfrom pak.datasets.MOT import MOT16\nmot16 = MOT16(\'/place/to/store/the/data\')\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\n# Some videos might be too large to fit into your memory so you\n# can load them into a memory-mapped file for easier handling\nX, Y_det, Y_gt  = mot16.get_train("MOT16-02", memmapped=True)\nX, Y_det        = mot16.get_test("MOT16-01", memmapped=True)\n```\n\n![mot16](https://user-images.githubusercontent.com/831215/32783815-5336b2b4-c94d-11e7-8e8c-db4209e61450.png)\n\n#### License\n\n[Creative Commons Attribution-NonCommercial-ShareAlike 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/). \nThis means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license. If you are interested in commercial usage you can contact us for further options.\n\n### 2DMOT2015\n\n[Dataset](https://motchallenge.net/)[2] with video sequences in unconstrained environments using static and moving cameras.\n[Sample code](https://github.com/justayak/pak/blob/master/samples/MOT15_2D.ipynb)\n\n```python\nfrom pak.datasets.MOT import MOT152D\nmot15 = MOT152D(\'/place/to/store/the/data\')\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\nX, Y_det, Y_gt  = mot15.get_train("ADL-Rundle-6")\nX, Y_det        = mot15.get_test("ADL-Rundle-1")\n```\n\n![mot15](https://user-images.githubusercontent.com/831215/32783818-5407e69a-c94d-11e7-9569-f6942b2be857.png)\n\n#### License\n\n[Creative Commons Attribution-NonCommercial-ShareAlike 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/). \nThis means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license. If you are interested in commercial usage you can contact us for further options.\n\n### Market-1501\n[3]Person re-identitification dataset collected in front of a supermarket from six different cameras. The dataset\ncontains 1501 different identities that are captured by at least two cameras.\n[Sample code](https://github.com/justayak/pak/blob/master/samples/Market1501.ipynb)\n\n```python\nfrom pak.datasets.Market1501 import Market1501 \n\nmot15 = Market1501(\'/place/to/store/the/data\')\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\nX, Y = m1501.get_train()\n```\n\n![market1501](https://user-images.githubusercontent.com/831215/32785225-4afc5884-c951-11e7-95b1-542c11e7736e.png)\n\n### CUHK03\nDataset[4] with cropped images of persons from different angles and cameras. The dataset uses human annotated as well as\nautomatically annotated pictures.\nThe authors require users to explicitly download the data from their [website](https://docs.google.com/forms/d/e/1FAIpQLSfueNRWgRp3Hui2HdnqHGbpdLUgSn-W8QxpZF0flcjNnvLZ1w/viewform?formkey=dHRkMkFVSUFvbTJIRkRDLWRwZWpONnc6MA#gid=0) so it is not possible to automatically download it.\n[Sample code](https://github.com/justayak/pak/blob/master/samples/CUHK03.ipynb)\n\n```python\nfrom pak.datasets.CUHK03 import cuhk03\n\n# the images do not have the same size and have to be resized\nw, h = 60, 160\ncuhk03 = cuhk03(\'/place/where/the/downloaded/zip/is/stored\', target_w=w, target_h=h)\n\nX, Y = cuhk03.get_labeled()\n# X, Y = cuhk03.get_detected()\n```\n\n#### Licence \nThis dataset is ONLY released for academic use. Please do not further distribute the dataset (including the download link), or put any of the images on the public website, due to the university regulations and privacy policy in Hong Kong law. Please kindly cite our paper if you use our data in your research. Thanks and hope you will benefit from our dataset. \n\n\n### Leeds Sports Pose Extended Training Dataset\n[5][Dataset](http://sam.johnson.io/research/lspet.html)\n\n[Sample code](https://github.com/justayak/pak/blob/master/samples/LeedsSportsPoseExtended.ipynb)\n\n```python\nfrom pak.datasets.LSPE import LSPE\n\nlspe = LSPE(\'/place/to/store/the/data\')\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\nX, Y = lspe.get_raw()\n```\n\n![pak_lspe](https://user-images.githubusercontent.com/831215/32917435-77d235ac-cb1f-11e7-957e-8fcd8301e271.png)\n\n### DukeMTMC-reID\n[6][Dataset](https://drive.google.com/uc?id=0B0VOCNYh8HeRdnBPa2ZWaVBYSVk&export=download) that is similar to market-1501.\nHowever, the images are not cropped to have the same size!\n\n[Sample code](https://github.com/justayak/pak/blob/master/samples/DukeMTMC-reID.ipynb)\n\n```python\nfrom pak.datasets.DukeMTMC import DukeMTMC_reID\n\nduke = DukeMTMC_reID(\'/place/to/store/the/data\')\n# please download the dataset from the given url and put the zip file into the path\n\nX, Y = duke.get_test()\n```\n\n![duke](https://user-images.githubusercontent.com/831215/33133206-5fd89e88-cf9c-11e7-930c-65ef51ae061a.png)\n\n#### License\n\nATTRIBUTION PROTOCOL\n\nIf you use the DukeMTMC-reID data or any data derived from it, please cite the original work as follows:\n\n```\n@article{zheng2017unlabeled,\n  title={Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro},\n  author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},\n  journal={arXiv preprint arXiv:1701.07717},\n  year={2017}\n}\n```\n\nand include this license and attribution protocol within any derivative work.\n\n\nIf you use the DukeMTMC data or any data derived from it, please cite the original work as follows:\n\n```\n@inproceedings{ristani2016MTMC,\n  title =        {Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking},\n  author =       {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},\n  booktitle =    {European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking},\n  year =         {2016}\n}\n```\n\nand include this license and attribution protocol within any derivative work.\n\n\nThe DukeMTMC-reID dataset is derived from DukeMTMC dataset (http://vision.cs.duke.edu/DukeMTMC/).\n\nThe DukeMTMC-reID dataset is made available under the Open Data Commons Attribution License (https://opendatacommons.org/licenses/by/1.0/) and for academic use only.\n\nREADABLE SUMMARY OF Open Data Commons Attribution License \n\nYou are free:\n\n    To Share: To copy, distribute and use the database.\n    To Create: To produce works from the database.\n    To Adapt: To modify, transform and build upon the database.\n\nAs long as you:\n\n    Attribute: You must attribute any public use of the database, or works produced from the database, in the manner specified in the license. For any use or redistribution of the database, or works produced from it, you must make clear to others the license of the database and keep intact any notices on the original database.\n\n### Hand Dataset\n[7][Collection](http://www.robots.ox.ac.uk/~vgg/data/hands/) of hand images from various public images. \n[Sample code](https://github.com/justayak/pak/blob/master/samples/hand_dataset.ipynb)\n\n```python\nfrom pak.datasets.Hands import Hand\nhand = Hand(\'/place/to/store/the/data\')\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\nX_test, Y_test = hand.get_test()\nX_train, Y_train = hand.get_train()\nX_val, Y_val = hand.get_val()\n```\n\n![hand_dataset](https://user-images.githubusercontent.com/831215/33312758-13f0f7f4-d429-11e7-9561-3cebf8832548.png)\n\n\n### EgoHands Dataset\n[8][Dataset](http://vision.soic.indiana.edu/projects/egohands/) containing 48 Google Glass videos of interactions\nbetween two people.\n[Sample code](https://github.com/justayak/pak/blob/master/samples/EgoHands.ipynb)\n\n```python\nfrom pak.datasets.EgoHands import EgoHands\n\negohand = EgoHands(root)\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\n# the image data occupies ~15GB of RAM so if your computer\n# cannot handle such big data you should set memmapped=True\nX, Y = egohand.get_raw(memmapped=True)\n```\n\n![egohands](https://user-images.githubusercontent.com/831215/33382371-da8a84a0-d520-11e7-8e87-95c5aba8e814.png)\n\n### MARCOnI\n[MARker-less Motion Capture in Outdoor and Indoor Scenes](http://marconi.mpi-inf.mpg.de/#overview)[9] is a test data set for marker-less motcap methods.\n[Sample code](https://github.com/justayak/pak/blob/master/samples/MARCOnI.ipynb)\n\n```python\nfrom pak.datasets.MARCOnI import MARCOnI\n\nmarconi = MARCOnI(root)\n# if the library cannot find the data in the given directory it\n# will download it and place it there..\n\nX, CNN, Annotations = marconi[\'Soccer\']\n```\n\n![marconi_img](https://user-images.githubusercontent.com/831215/36093006-902b7ac8-0fe9-11e8-92a6-c5507f454fa1.png)\n\n\n# References\n\n[0] Milk-Icon: Icon made by Smashicons from www.flaticon.com\n\n[1] Milan, Anton, et al. "MOT16: A benchmark for multi-object tracking." arXiv preprint arXiv:1603.00831 (2016)\n\n[2] Leal-Taixé, Laura, et al. "Motchallenge 2015: Towards a benchmark for multi-target tracking." arXiv preprint arXiv:1504.01942 (2015).\n\n[3] Zheng, Liang, et al. "Scalable person re-identification: A benchmark." Proceedings of the IEEE International Conference on Computer Vision. 2015.\n\n[4] Li, Wei, et al. "Deepreid: Deep filter pairing neural network for person re-identification." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014.\n\n[5] Johnson, Sam, and Mark Everingham. "Learning effective human pose estimation from inaccurate annotation." Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011.\n\n[6] Ristani, Ergys, et al. "Performance measures and a data set for multi-target, multi-camera tracking." European Conference on Computer Vision. Springer International Publishing, 2016.\n\n[7] Mittal, Arpit, Andrew Zisserman, and Philip HS Torr. "Hand detection using multiple proposals." BMVC. 2011.\n\n[8] Bambach, Sven, et al. "Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions." Proceedings of the IEEE International Conference on Computer Vision. 2015.\n\n[9] Elhayek, Ahmed, et al. "Efficient ConvNet-based marker-less motion capture in general scenes with a low number of cameras." Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE, 2015.')
('https://raw.githubusercontent.com/warsi22/handdetection/26290cabdd589e02d2757c8a5a7708206b212bd2/samples/core/tutorials/keras/overfit_and_underfit.ipynb', "# TensorFlow Models\n\nThis repository contains a number of different models implemented in [TensorFlow](https://www.tensorflow.org):\n\nThe [official models](official) are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.\n\nThe [research models](https://github.com/tensorflow/models/tree/master/research) are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.\n\nThe [samples folder](samples) contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.\n\nThe [tutorials folder](tutorials) is a collection of models described in the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\nIf you want to contribute to models, be sure to review the [contribution guidelines](CONTRIBUTING.md).\n\n## License\n\n[Apache License 2.0](LICENSE)")
('https://raw.githubusercontent.com/meidata/sentiment_analysis/e2b2c7867e5914c38b882aa1e177ed569ea7153a/scrap-user-review.ipynb', '# sentiment_analysis')
('https://raw.githubusercontent.com/jackdesmarais/DabTransporterPaper/b404a2dda92057df2546b3ff30e568ede62a0487/scripts/Figure%201%20Library%20Construction.ipynb', None)
('https://raw.githubusercontent.com/jangeloni/challenge_amadeus/304529ed07b06c0aca900f5bbe4938aa7a087514/second_exercise.ipynb', '# challenge_amadeus\nAmadeus challenge in order to join their teams.\n\n\nFirst Exercise :\n  - Lack of computing capacity\n  - Just need the number of rows, one column is enough\n\n\nSecond exercise :\n  Issues and results :\n    - Cannot install properly Geobases or Neobase\n    - We can get airport name by using http://fr.whattheflight.com/aeroports/codes-iata-oaci/ (not programtically)\n    - After viewing Geobase structure and methods, the code to find city name or airport is at the end of the file.')
('https://raw.githubusercontent.com/paul-shannon/cyjs-jupyter/798b5c4b47cadcaff2321e927806151d55d8cc62/self-contained-notebooks/simple/waitOnBrowserResponse.ipynb', '# cyjs-jupyter\nA cytoscape.js jupyter widget, an nbextension')
('https://raw.githubusercontent.com/acesaif/ml_crash_course/a51ae18f8641a56765798f4e7e84d34e497d0d99/self_practice/trading/part_1/project_1/portfolio_optimization.ipynb', '# Machine Learning Crash Course\n### Course ongoing.\n\n### On `23/04/2018` completed this course.')
('https://raw.githubusercontent.com/ucbrise/flor-camp2018/8ff90121800a49c22f2d1f2e0f60909cdd8df8d7/src/tutorial.ipynb', '# flor-camp2018')
('https://raw.githubusercontent.com/maruf90/Google-s-TensorFlow/edaf3b342db4afa1c872da541fb0ac176a4e8ef9/tensorflow/examples/udacity/1_notmnist.ipynb', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_transp.png"><br><br>\n</div>\n-----------------\n\n|  **`Linux CPU`**   |  **`Linux GPU PIP`** | **`Mac OS CPU`** |  **`Android`** |\n|-------------------|----------------------|------------------|----------------|\n| [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-cpu)](https://ci.tensorflow.org/job/tensorflow-master-cpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-gpu_pip)](https://ci.tensorflow.org/job/tensorflow-master-gpu_pip) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-mac)](https://ci.tensorflow.org/job/tensorflow-master-mac) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-android)](https://ci.tensorflow.org/job/tensorflow-master-android) |\n\n**TensorFlow** is an open source software library for numerical computation using\ndata flow graphs.  Nodes in the graph represent mathematical operations, while\nthe graph edges represent the multidimensional data arrays (tensors) that flow\nbetween them.  This flexible architecture lets you deploy computation to one\nor more CPUs or GPUs in a desktop, server, or mobile device without rewriting\ncode.  TensorFlow also includes TensorBoard, a data visualization toolkit.\n\nTensorFlow was originally developed by researchers and engineers\nworking on the Google Brain team within Google\'s Machine Intelligence research\norganization for the purposes of conducting machine learning and deep neural\nnetworks research.  The system is general enough to be applicable in a wide\nvariety of other domains, as well.\n\n**If you\'d like to contribute to TensorFlow, be sure to review the [contribution\nguidelines](CONTRIBUTING.md).**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, but please see\n[Community](tensorflow/g3doc/resources/index.md#community) for general questions\nand discussion.**\n\n## Installation\n*See [Download and Setup](tensorflow/g3doc/get_started/os_setup.md) for instructions on how to install our release binaries or how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n* Linux CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/))\n* Linux GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/))\n* Mac CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/))\n* Mac GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/))\n* [Android](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-android/TF_BUILD_CONTAINER_TYPE=ANDROID,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=android-slave/lastSuccessfulBuild/artifact/bazel-out/local_linux/bin/tensorflow/examples/android/tensorflow_demo.apk) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-android/TF_BUILD_CONTAINER_TYPE=ANDROID,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=android-slave/))\n\n#### *Try your first TensorFlow program*\n```shell\n$ python\n```\n```python\n>>> import tensorflow as tf\n>>> hello = tf.constant(\'Hello, TensorFlow!\')\n>>> sess = tf.Session()\n>>> sess.run(hello)\nHello, TensorFlow!\n>>> a = tf.constant(10)\n>>> b = tf.constant(32)\n>>> sess.run(a+b)\n42\n>>>\n```\n\n##For more information\n\n* [TensorFlow website](http://tensorflow.org)\n* [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf)\n* [TensorFlow Model Zoo](https://github.com/tensorflow/models)\n* [TensorFlow MOOC on Udacity] (https://www.udacity.com/course/deep-learning--ud730)\n\nThe TensorFlow community has created amazing things with TensorFlow, please see the [resources section of tensorflow.org](https://www.tensorflow.org/versions/master/resources#community) for an incomplete list.')
('https://raw.githubusercontent.com/avlbanuba/tensorflow_pack/9cc871e81c04ed11829c3364546b4500742140eb/tensorflow/examples/udacity/5_word2vec.ipynb', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_transp.png"><br><br>\n</div>\n\n-----------------\n\n| **`Linux CPU`** | **`Linux GPU`** | **`Mac OS CPU`** | **`Windows CPU`** | **`Android`** |\n|-----------------|---------------------|------------------|-------------------|---------------|\n| [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-cpu)](https://ci.tensorflow.org/job/tensorflow-master-cpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-linux-gpu)](https://ci.tensorflow.org/job/tensorflow-master-linux-gpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-mac)](https://ci.tensorflow.org/job/tensorflow-master-mac) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-win-cmake-py)](https://ci.tensorflow.org/job/tensorflow-master-win-cmake-py) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-android)](https://ci.tensorflow.org/job/tensorflow-master-android) |\n\n**TensorFlow** is an open source software library for numerical computation using\ndata flow graphs.  Nodes in the graph represent mathematical operations, while\nthe graph edges represent the multidimensional data arrays (tensors) that flow\nbetween them.  This flexible architecture lets you deploy computation to one\nor more CPUs or GPUs in a desktop, server, or mobile device without rewriting\ncode.  TensorFlow also includes TensorBoard, a data visualization toolkit.\n\nTensorFlow was originally developed by researchers and engineers\nworking on the Google Brain team within Google\'s Machine Intelligence Research\norganization for the purposes of conducting machine learning and deep neural\nnetworks research.  The system is general enough to be applicable in a wide\nvariety of other domains, as well.\n\n**If you\'d like to contribute to TensorFlow, be sure to review the [contribution\nguidelines](CONTRIBUTING.md).**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, but please see\n[Community](https://www.tensorflow.org/community/) for general questions\nand discussion.**\n\n## Installation\n*See [Installing TensorFlow](https://www.tensorflow.org/install/) for instructions on how to install our release binaries or how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n* Linux CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.2.1-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.2.1-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/))\n* Linux GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.1-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.1-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.1-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/))\n* Mac CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.2.1-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.2.1-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/))\n* Mac GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.1-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.1-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/))\n* Windows CPU-only: [Python 3.5 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow-1.2.1-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows,PY=35/)) / [Python 3.6 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow-1.2.1-cp36-cp36m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows,PY=36/))\n* Windows GPU: [Python 3.5 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.2.1-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/)) / [Python 3.6 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/))\n* Android: [demo APK](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow_demo.apk), [native libs](http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/native/)\n([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-android/))\n\n#### *Try your first TensorFlow program*\n```shell\n$ python\n```\n```python\n>>> import tensorflow as tf\n>>> hello = tf.constant(\'Hello, TensorFlow!\')\n>>> sess = tf.Session()\n>>> sess.run(hello)\n\'Hello, TensorFlow!\'\n>>> a = tf.constant(10)\n>>> b = tf.constant(32)\n>>> sess.run(a+b)\n42\n>>>\n```\n\n## For more information\n\n* [TensorFlow website](https://www.tensorflow.org)\n* [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf)\n* [TensorFlow Model Zoo](https://github.com/tensorflow/models)\n* [TensorFlow MOOC on Udacity](https://www.udacity.com/course/deep-learning--ud730)\n\nThe TensorFlow community has created amazing things with TensorFlow, please see the [resources section of tensorflow.org](https://www.tensorflow.org/about/#community) for an incomplete list.')
('https://raw.githubusercontent.com/jinjagit/JupyterNotebooks/a1e4e4ac89bb2e0e65e3271f5b50fe0836eaa69e/test.ipynb', "# JupyterNotebooks\n\nLearning to use Jupyter notebooks:\n\nCurrent level of ability: Noob copy/paste exploration, with background of a bit of coding / environment setup experience.\n\nEven though I'm far from an experienced user, so far I am totally blown away by the usefullness, flexibility, and creative potential of these notebooks.\n\nNice article (and discussion in comments section) of limits and crtiticisms of Jupyter notebooks: http://opiateforthemass.es/articles/why-i-dont-like-jupyter-fka-ipython-notebook/")
('https://raw.githubusercontent.com/leftaroundabout/linearmap-family/195b0fbb1a3744ea74e2b13f6939df1eda8ff6d5/test/matrices.ipynb', "This project has two goals:\n\n### To give Haskell a competitive, _native_ linear algebra library\n\nThat is, a library that supports:\n\n1. Basic vector operations ✓\n2. Efficient linear maps ✓\n3. Solving linear equations\n  1. Inversion of finite-dimensional, linear isomorphisms (full-rank matrices) ✓\n  2. Least-squares solution to under/overdetermined problems (?)\n  3. Fast iterative methods (for large sparse systems, e.g. conjugate gradient) ✗\n4. Eigenvalue problems ✓\n5. Singular value decomposition ✗\n\nAt the moment, the only Haskell libraries that offer all of that appear to be\n[hmatrix](http://hackage.haskell.org/package/hmatrix) and [eigen](http://hackage.haskell.org/package/eigen),\nwhich use bindings to the [GSL](https://www.gnu.org/software/gsl/) (C)\nand [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) (C++) libraries.\n\n- Eigen is a great project that uses the C++ template system for both an elegant interface and nice optimisations.\n  However, this interface doesn't really translate that nicely over to Haskell. The C interface layer necessary\n  forgets much of the template niceties.\n- GSL is extremely comprehensive and well-suited for binding to other languages, in particular dynamic languages.\n  However, it's a bit of a messy _big collection of all kind of algorithms_, and not exactly the fastest library,\n  which adds to the general overhead of calling to external code with C-marshalled memory.\n\n### To get rid of those pesky matrices\n\nLinear algebra isn't really about matrices. Vectors aren't _really_ arrays of numbers.\n\nWhat LA is really about are _points in vector spaces_, and ultimately geometric relations between them.\nAnd many interesting spaces aren't even finite-dimensional.\n\nRegardless of whether matrices are used for the internal operations – and in fact, it's not so clear if\nthis is always a good idea! – matrices shouldn't dominate the API of a linear algebra library.\nHaskell has managed to bring a lot innovation to the world of programming languages.\nIt can nicely work with infinite, lazy data structures.\nHopefully it can also make a bit of a revolution in the field of linear algebra!")
('https://raw.githubusercontent.com/dragonfly90/mxnet_Realtime_Multi-Person_Pose_Estimation/c6a71507fd98e287250f9e375de758474f4b44eb/testModel.ipynb', "### Reimplementation of human keypoint detection in mxnet\n\n1. You can download mxnet model and parameters(coco and MPII) from google drive:\n\n   https://drive.google.com/drive/folders/0BzffphMuhDDMV0RZVGhtQWlmS1U\n\n   or check caffe_to_mxnet folder to download original caffe model and transfer it to mxnet model.\n   \n   install heatmap and pafmap cython:  cython/rebuild.sh\n   \n2. Test demo based on model of coco dataset: testModel.ipynb\n3. Test demo based on model of MPII dataset: testModel_mpi.ipynb\n4. Train with batch_size: TrainWeight.py \n5. Check if heat map, part affinity graph map, mask are generated correctly in training: test_generateLabel.ipynb\n6. Evaluation on coco validation dataset with transfered mxnet model: evaluation_coco.py\n\nThe result is as following, the mean average precision (AP) over 10 OKS threshold  on the first 2644 images in the val set is 0.550, which is 0.577 in original implementation.\n\n```bash\nAverage Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.550\nAverage Precision (AP) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.800\nAverage Precision (AP) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.610\nAverage Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.541\nAverage Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.576\nAverage Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 20 ] = 0.591\nAverage Recall (AR) @[ IoU=0.50 | area= all | maxDets= 20 ] = 0.812\nAverage Recall (AR) @[ IoU=0.75 | area= all | maxDets= 20 ] = 0.644\nAverage Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.549\nAverage Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.651\n```\n\n### Cite paper Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\n\n```\n@article{cao2016realtime,\n  title={Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n  author={Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\n  journal={arXiv preprint arXiv:1611.08050},\n  year={2016}\n  }\n```\n\noriginal caffe training https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose\n\n\n\n## TODO:\n- [x] Test demo\n- [x] Train demo\n- [x] Add image augmentation: rotation, flip\n- [x] Add weight vector\n- [x] Train all images\n- [x] Train from vgg model\n- [x] Evaluation code\n- [x] Generate heat map and part affinity graph map in C++\n- [ ] image read and augmentation in C++\n\n## Training with vgg warm up\n\npython TrainWeightOnVgg.py\n\nWe tested the code using two K80 GPUS on COCO dataset, with batch size set to 10 and learning rate set to 0.00004. and using vgg pretrained model on <data.mxnet.io> to initialize our parameters. After 20 epochs, we tested our model on COCO validation dataset(only 50 images) and we got only 0.048 as mAP, very low compared to original implementation. Please reach us if you have some ideas about this issue.  \n\n```bash\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.048\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.183\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.019\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.078\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.035\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.066\n Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.224\n Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.022\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.075\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.054\n\n```\n\nThe traning process is not so easy, I found this model even can't converge if all layers are initialized randomly, I guess one reason is that this model uses many convolution layers with a large kernel, whose big pad may introduce much noise, and another reason may be the fact that this model uses MSE as loss function, and maybe it's better to use sigmoid as the avtivation function of the last layer and use entropy loss function instead. \n\n\n## Other implementations \n\n[Original caffe training model](https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose)\n\n[Original data preparation and demo](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation)\n\n[Pytorch](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation)\n\n[keras](https://github.com/raymon-tian/keras_Realtime_Multi-Person_Pose_Estimation)")
('https://raw.githubusercontent.com/joanna-lewis/ct_surveillance/791378e310444440f5e57c33be5ea2ca5e44ed62/three_compartment_model/three_compartment_model.ipynb', "# Analysing chlamydia surveillance data for England.\n\nThis repository contains software for analysing chlamydia surveillance data, applied to data from England in 2012. Code consists of R scrips, STAN model files and Jupyter notebooks in the IPython language.\n\nFor the reader's convenience, the Jupyter notebooks have been downloaded as both LaTeX/PDF and html files, and the code and Figures can be viewed using a PDF reader or web browser. To run or edit the notebooks and generate figures, the necessary software to run IPython and Jupyter notebooks must be installed (see http://ipython.org/install.html and http://jupyter.org/). The material\n\n## Introducing the model\n\nFiles in the directory named **three_compartment_model** introduce the model and its properties.\n\n## Example: chlamydia in England, 2012\n\nFiles in the directory named **england** illustrate the use of the model, with data from England in 2012.\n\nAs well as the IPython notebook, the directory contains an R script which is used to derive prior distributions for the proportions of men and women in different age groups who are sexually active, using data from the National Study of Sexual Attitudes and Lifestyles, Natsal-3.\n\nThe subdirectory named 'stan' contains STAN model files and R scripts to run them and obtain samples for natural clearance rates of chlamydia infection in men and women, based on the work of Price _et al._ presented in _Stat Med_ **32**:1547-1560 (2013).\n\n## Local differences in chlamydia incidence, testing, diagnosis and prevalence\n\nFiles in the directory named **local_authorities.ipynb** use the model to investigate local differences in chlamydia testing and diagnosis rates, incidence and prevalence.\n\n## Data\n\nNatsal-3 data is available from the UK Data Archive:\nhttp://www.data-archive.ac.uk/\n\nChlamydia testing data was originally downloaded from:\nhttp://www.chlamydiascreening.nhs.uk/ps/data.asp\nChecked 9 February 2016\n\nEnglish indices of deprivation 2010 downloaded from:\nhttps://www.gov.uk/government/statistics/english-indices-of-deprivation-2010\nContains public sector information licensed under the Open Government Licence v3.0; http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/\nChecked 9 February 2016\n\nShape files and coding for English local authorities were downloaded from:\nhttps://geoportal.statistics.gov.uk/geoportal/catalog/content/filelist.page\nContains National Statistics data © Crown copyright and database right 2016")
('https://raw.githubusercontent.com/lucasputer/CienciaDeDatos/046d7f86033055c6584f1d7a8d5214ee3967e6ed/tp4/TP4.ipynb', None)
('https://raw.githubusercontent.com/zach401/acnportal/2649307788fa0abcde7357f6f39e4e60ade690e8/tutorials/lesson1_running_an_experiment.ipynb', "# ACN Portal\n\nThe ACN Portal is a suite research tools developed at Caltech to accelerate the pace of large-scale EV charging research.\nCheckout the documentation at https://acnportal.readthedocs.io/en/latest/.\n\n### ACN-Data\nThe ACN-Data Dataset is a collection of EV charging sessions collected at Caltech and NASA's Jet Propulsion Laboratory (JPL). This basic Python client simplifies the process of pulling data from his dataset via its public API.\n\n### ACN-Sim\nACN-Sim is a simulation environment for large-scale EV charging algorithms. It interfaces with ACN-Data to provide access to realistic test cases based on actual user behavior. \n\n### algorithms\nalgorithms is a package of common EV charging algorithms which can be used for comparison when evaluating new algorithms. \n\nThis package is intended to be populated by the community. If you have a promising EV charging algorithm, please implement it as a subclass of BasicAlgorithm and send a pull request. \n\n## Installation\nDownload or clone this repository. Navigate to its root directory. Install using pip. \n\n```bash\npip install .\n```")
('https://raw.githubusercontent.com/doppelgenkan/activity/1d54d40c601719bf41aa39f5b59ae66235fc795e/up-down_stairs/up-down_stairs.ipynb', None)
('https://raw.githubusercontent.com/sergegoussev/training_and_experiments/207536c72720f596f21365e465cd83783fea931a/walkthroughs/Visualizing_netwroks_in_igraph.ipynb', '# training_and_experiments\n\nTutorials, python walkthroughs, and examples of various topics. I put this together to make it easier to remember specific things I have done over the years and put them up. However examples are availible for anyone\n\n# Contents:\n\n## General Python\n* [Functions vs Object Oriented Programming](walkthroughs/Functions_vs_Object_Oriented_Programming.ipynb)\n* [How class (object) inheritance works in Python](walkthroughs/classes_and_inheritance.ipynb)\n* [Asynchronous (multi) processing in Python](walkthroughs/parallel_programming_tutorial.ipynb)\n\n## Network Analysis\n* [Creating visuals of graphs (using *igraph.plot()*)](walkthroughs/Visualizing_netwroks_in_igraph.ipynb)\n\n## Good external resources:\n* [Cheatsheet on pandas DataFrame processing (external)](https://www.dataquest.io/blog/large_files/pandas-cheat-sheet.pdf)')
('https://raw.githubusercontent.com/AUP-CS2091/class/3d6f9b6a2a0f557c23dc6f83e92851a60d2b4fa6/week1/lab01/lab01.ipynb', None)
('https://raw.githubusercontent.com/jroloughlin/first_test/16541cc3123d15b4910097433402f5df3c015c50/projects/boston_housing/boston_housing.ipynb', '# machine-learning\nContent for Udacity\'s Machine Learning curriculum, which includes projects and their descriptions.\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.\n\nThis is the text for git')
('https://raw.githubusercontent.com/KColburn/uSDC_NDp1/04c28791c577269fb38678f70341d310f6354e2e/P1.ipynb', '#**Finding Lane Lines on the Road** \n<img src="laneLines_thirdPass.jpg" width="480" alt="Combined Image" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means "Open-Source Computer Vision", which is a package that has many useful tools for analyzing images.  \n\n**Step 1:** Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if you\'re getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF="https://www.continuum.io/downloads" target="_blank">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type "help", "copyright", "credits" or "license" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`  \n`> conda install -c https://conda.anaconda.org/menpo opencv3`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the "moviepy" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF="https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python" target="_blank">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called "P1.ipynb".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  ')
('https://raw.githubusercontent.com/data-8/materials-x19/43ef9d8ef6dc15491250888ed41391442fce2b46/materials/x19/lab/1/lab01/lab01.ipynb', 'Data 8X Public Materials for 2019\n=======\n\nThis repo contains the publicly available materials that are used in the Data 8X\nFoundations of Data Science course offered on EdX in the self paced version.\n\nThis includes: labs and notebooks used in lecture.\n\nThe contents of this repository are licensed for reuse under [Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](http://creativecommons.org/licenses/by-nc/4.0/)')
('https://raw.githubusercontent.com/RodrigoVillatoro/machine_learning_projects/3a7fca2bdfd7789320b1e58f6f4170763c2950e6/creating_customer_segments/customer_segments.ipynb', '# machine-learning\nContent for Udacity\'s [Machine Learning Nanodegree curriculum](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009). It includes the solutions I\'ve worked out for the following projects: \n\n##### P1: Predicting Boston Housing Prices\n\nThe Boston housing market is highly competitive, and you want to be the best real estate agent in the area. To compete with your peers, you decide to leverage a few basic machine learning concepts to assist you and a client with finding the best selling price for their home. Luckily, you’ve come across the Boston Housing dataset which contains aggregated data on various features for houses in Greater Boston communities, including the median value of homes for each of those areas. Your task is to build an optimal model based on a statistical analysis with the tools available. This model will then used to estimate the best selling price for your client’s home.\n\n\n##### P2: Build a Student Intervention System\n\nA local school district has a goal to reach a 95% graduation rate by the end of the decade by identifying students who need intervention before they drop out of school. As a software engineer contacted by the school district, your task is to model the factors that predict how likely a student is to pass their high school final exam, by constructing an intervention system that leverages supervised learning techniques. The board of supervisors has asked that you find the most effective model that uses the least amount of computation costs to save on the budget. You will need to analyze the dataset on students\' performance and develop a model that will predict the likelihood that a given student will pass, quantifying whether an intervention is necessary.\n\n\n##### P3: Creating Customer Segments\n\nA wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week.Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries — losing the distributor more money than what was being saved. You’ve been hired by the wholesale distributor to find what types of customers they have to help them make better, more informed business decisions in the future. Your task is to use unsupervised learning techniques to see if any similarities exist between customers, and how to best segment customers into distinct categories.\n\n##### P4: Train a Smartcab to Drive\n\nIn the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents — known as smartcabs — to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to rely on smartcabs to get to where they need to go as safely and efficiently as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or efficient as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and efficiency can be achieved.\n\n##### P5: Capstone Project\n\nIn this capstone project, you will leverage what you’ve learned throughout the Nanodegree program to solve a problem of your choice by applying machine learning algorithms and techniques. You will first define the problem you want to solve and investigate potential solutions and performance metrics. Next, you will analyze the problem through visualizations and data exploration to have a better understanding of what algorithms and features are appropriate for solving it.\n\nYou will then implement your algorithms and metrics of choice, documenting the preprocessing, refinement, and postprocessing steps along the way. Afterwards, you will collect results about the performance of the models used, visualize significant quantities, and validate/justify these values. Finally, you will construct conclusions about your results, and discuss whether your implementation adequately solves the problem.\n\n---\n\nThe code is written in Python 2.7 and uses the following libraries: \n\n* ipython\n* matplotlib\n* pandas\n* pygame\n* numpy\n* scikit-learn\n* seaborn\n\n\n---\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.')
('https://raw.githubusercontent.com/llcclarke/machine-learning/7fe8ce40e2f0e8ee625c7655aa89567578ba0960/projects/customer_segments/customer_segments.ipynb', '# machine-learning\nContent for Udacity\'s Machine Learning curriculum, which includes projects and their descriptions.\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.')
('https://raw.githubusercontent.com/scuthbert/DataScience/0748b00dfd944d2f5adf1995acec36639e0ee6b9/hmwk/hmwk01/hmwk01.ipynb', '## Intro to Data Science\nHomework and practicum for CU Boulder CSCI3022 Fall 2017, taught by Chris Ketelsen.\n\nGo check out @chrisketelsen and @dblarremore.\n### NO CORRECTNESS GUARANTEED')
('https://raw.githubusercontent.com/abhiramkolli/datamaster/deee3aab4604793854c8a46ed4581d0f7951b82a/LoadToBigQuery.ipynb', '# datamaster\n#Export Script\nconda env export -n datamaster > environment.yml\n#Create Script\nconda env create -f environment.yml')
('https://raw.githubusercontent.com/RayleighKim/M06_Python_with_Google_Colab/9df04fb3a6714ea632e44dc8fa85bcda5a2ec067/00_Colab_Basics.ipynb', '# M06_Python_with_Google_Colab\nTutorial repository for lectures')
('https://raw.githubusercontent.com/maneesh-chouksey/cnn-dog-classifier/9725c0eff841c33980cf55bfdf535138343b9b9c/dog_app.ipynb', '[//]: # (Image References)\n\n[image1]: ./images/sample_dog_output.png "Sample Output"\n[image2]: ./images/vgg16_model.png "VGG-16 Model Keras Layers"\n[image3]: ./images/vgg16_model_draw.png "VGG16 Model Figure"\n\n\n## Project Overview\n\nWelcome to the Convolutional Neural Networks (CNN) project in the AI Nanodegree! In this project, you will learn how to build a pipeline that can be used within a web or mobile app to process real-world, user-supplied images.  Given an image of a dog, your algorithm will identify an estimate of the canine’s breed.  If supplied an image of a human, the code will identify the resembling dog breed.  \n\n![Sample Output][image1]\n\nAlong with exploring state-of-the-art CNN models for classification, you will make important design decisions about the user experience for your app.  Our goal is that by completing this lab, you understand the challenges involved in piecing together a series of models designed to perform various tasks in a data processing pipeline.  Each model has its strengths and weaknesses, and engineering a real-world application often involves solving many problems without a perfect answer.  Your imperfect solution will nonetheless create a fun user experience!\n\n## Project Instructions\n\n### Instructions\n\n1. Clone the repository and navigate to the downloaded folder.\n```\t\ngit clone https://github.com/udacity/dog-project.git\ncd dog-project\n```\n\n2. Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).  Unzip the folder and place it in the repo, at location `path/to/dog-project/dogImages`. \n\n3. Download the [human dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip).  Unzip the folder and place it in the repo, at location `path/to/dog-project/lfw`.  If you are using a Windows machine, you are encouraged to use [7zip](http://www.7-zip.org/) to extract the folder. \n\n4. Donwload the [VGG-16 bottleneck features](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG16Data.npz) for the dog dataset.  Place it in the repo, at location `path/to/dog-project/bottleneck_features`.\n\n5. (Optional) __If you plan to install TensorFlow with GPU support on your local machine__, follow [the guide](https://www.tensorflow.org/install/) to install the necessary NVIDIA software on your system.  If you are using an EC2 GPU instance, you can skip this step.\n\n6. (Optional) **If you are running the project on your local machine (and not using AWS)**, create (and activate) a new environment.\n\n\t- __Linux__ (to install with __GPU support__, change `requirements/dog-linux.yml` to `requirements/dog-linux-gpu.yml`): \n\t```\n\tconda env create -f requirements/dog-linux.yml\n\tsource activate dog-project\n\t```  \n\t- __Mac__ (to install with __GPU support__, change `requirements/dog-mac.yml` to `requirements/dog-mac-gpu.yml`): \n\t```\n\tconda env create -f requirements/dog-mac.yml\n\tsource activate dog-project\n\t```  \n\t**NOTE:** Some Mac users may need to install a different version of OpenCV\n\t```\n\tconda install --channel https://conda.anaconda.org/menpo opencv3\n\t```\n\t- __Windows__ (to install with __GPU support__, change `requirements/dog-windows.yml` to `requirements/dog-windows-gpu.yml`):  \n\t```\n\tconda env create -f requirements/dog-windows.yml\n\tactivate dog-project\n\t```\n\n7. (Optional) **If you are running the project on your local machine (and not using AWS)** and Step 6 throws errors, try this __alternative__ step to create your environment.\n\n\t- __Linux__ or __Mac__ (to install with __GPU support__, change `requirements/requirements.txt` to `requirements/requirements-gpu.txt`): \n\t```\n\tconda create --name dog-project python=3.5\n\tsource activate dog-project\n\tpip install -r requirements/requirements.txt\n\t```\n\t**NOTE:** Some Mac users may need to install a different version of OpenCV\n\t```\n\tconda install --channel https://conda.anaconda.org/menpo opencv3\n\t```\n\t- __Windows__ (to install with __GPU support__, change `requirements/requirements.txt` to `requirements/requirements-gpu.txt`):  \n\t```\n\tconda create --name dog-project python=3.5\n\tactivate dog-project\n\tpip install -r requirements/requirements.txt\n\t```\n\t\n8. (Optional) **If you are using AWS**, install Tensorflow.\n```\nsudo python3 -m pip install -r requirements/requirements-gpu.txt\n```\n\t\n9. Switch [Keras backend](https://keras.io/backend/) to TensorFlow.\n\t- __Linux__ or __Mac__: \n\t\t```\n\t\tKERAS_BACKEND=tensorflow python -c "from keras import backend"\n\t\t```\n\t- __Windows__: \n\t\t```\n\t\tset KERAS_BACKEND=tensorflow\n\t\tpython -c "from keras import backend"\n\t\t```\n\n10. (Optional) **If you are running the project on your local machine (and not using AWS)**, create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `dog-project` environment. \n```\npython -m ipykernel install --user --name dog-project --display-name "dog-project"\n```\n\n11. Open the notebook.\n```\njupyter notebook dog_app.ipynb\n```\n\n12. (Optional) **If you are running the project on your local machine (and not using AWS)**, before running code, change the kernel to match the dog-project environment by using the drop-down menu (**Kernel > Change kernel > dog-project**). Then, follow the instructions in the notebook.\n\n__NOTE:__ While some code has already been implemented to get you started, you will need to implement additional functionality to successfully answer all of the questions included in the notebook. __Unless requested, do not modify code that has already been included.__\n\n## Evaluation\n\nYour project will be reviewed by a Udacity reviewer against the CNN project [rubric](https://review.udacity.com/#!/rubrics/810/view).  Review this rubric thoroughly, and self-evaluate your project before submission.  All criteria found in the rubric must meet specifications for you to pass.\n\n## Project Submission\n\nWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload:\n- The `dog_app.ipynb` file with fully functional code, all code cells executed and displaying output, and all questions answered.\n- An HTML or PDF export of the project notebook with the name `report.html` or `report.pdf`.\n- Any additional images used for the project that were not supplied to you for the project. __Please do not include the project data sets in the `dogImages/` or `lfw/` folders.  Likewise, please do not include the `bottleneck_features/` folder.__\n\nAlternatively, your submission could consist of the GitHub link to your repository.')
('https://raw.githubusercontent.com/rajeevpatn/Machine-Learning/79d8ce7b7f7c803e76ae24619378ff93b14a0a30/machine_learning/ensemble_and_randomforest.ipynb', 'This repository contains machine learning projects and notebooks for the course of [CloudxLab](https://CloudxLab.com/)\n\nTo know more about us [click here](https://CloudxLab.com/)')
('https://raw.githubusercontent.com/danielc92/python_nb_data_profiling/1bf2142a1f27f1266bae203e14b01520f29616e7/notebook_data_profililng.ipynb', '# python_nb_data_profiling\nThis repository contains a `jupyter notebook` which is capable of profiling data and visualizing it using `python`.\n\n# Setup\nThis project is using `python3`.\n\nTo grab this repository:\n```sh\ngit clone https://github.com/danielc92/python_nb_data_profiling.git\n```\n\nInstall the following modules before getting started:\n- `pandas`\n- `numpy`\n- `seaborn`\n- `matplotlib`\n- `jupyter`\n\n**note**: you should have general understanding of how to import data using pandas to use this notebook.\n\n# Installation\n```sh\npip install pandas numpy seaborn matplotlib jupyter\n```\n\n# Usage\n1. First import your data using `pandas`, this can be in the form of a `.csv`, `.json`, `.sql`, `.txt` or any supported `pandas` format.\n2. Setup your export path in `1.4.0.2` section.\n3. Setup your report settings in `1.4.0.3` section.\n4. Setup your visualization settings in `1.4.0.4` section. note: visualization can be turned off completely, it is optional.\n5. Continue runnng the remaining cells, the notebook will now profile your dataset and visualize it if you have enabled the option.\n6. The notebook will export the profile results to both `.json` and `.csv`, and export your visualization to `.png`.\n\n\n# Contributors\n- Daniel Corcoran\n\n# Sources\n1. [seaborn colour palettes](https://seaborn.pydata.org/tutorial/color_palettes.html)\n2. [pandas website](https://pandas.pydata.org/)\n3. [seaborn website](https://seaborn.pydata.org/)')
('https://raw.githubusercontent.com/iyerhari5/P1-FindingLanes/f81c6ac12046c824eb616c1e5e0733ca210cabcd/P1.ipynb', '# **Finding Lane Lines on the Road** \n\n<img src="examples/laneLines_thirdPass.jpg" width="480" alt="Combined Image" />\n\nOverview\n---\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nThe goals / steps of this project are the following:\n* Make a computer vision based pipeline that finds lane lines on the road\n\n[//]: # (Image References)\n[image1]: ./examples/grayscale.jpg "Grayscale"\n\n---\n\n### Reflection\n\n### 1. Description of the pipeline\n\nThe pipeline consists of the following steps:\n\n1. Convert the image to gray scale\n2. Detect the edges using the canny edge detector\n3. Detect the lines in the edge image using Hough transform\n4. Combine the lines on the left and right sides to produce one single line for each side.\n5. Robust prediction of the lines for each frame\n\nIn order to draw a single line on the left and right lanes, a new function called draw_lines_roi() was created. Inside this function, all the lines detected from the hough transform are processed one by one. They are assigned to the left or the right lane based on the slope. In order to reject spurious lines and lines that are not in the expected direction, we use limits on the slope to reject unwatned lines. For each line that is accepted, we extrapolate the line to the edges of the ROI and store the end point coordinates. Once all the detected lines are processed like this,  the end points for the left and right lanes for that frame are computed by combining all the individual end points. We use the median instead of the mean here to make the averaging more robust to outlier points. Further robustness is built in by handling two conditions descried below:  \n\n1. If no detected line from the hough transform is accepted to be part of the left or right lane for a particular frame (or if no line was detected), we use the line from the previous frame as the predicted lane for the curent frame. This helps to handle frames that have difficult conditions to detect the lines (as in the challenge video). \n\n2. If the end points of the final line for each lane moves significantly from the previous frame, we use the line from the previous frame as the predicted line for the current frame. This helps to handle cases where the line "jumps" from one frame to other due to spurious detections.\n\n### 2. Potential shortcomings with the current pipeline\n\nThere are couple of potential shortcoming in the current pipeline. If there is a sudden substantial change in the location of the lanes (maybe due to the car swerving), the algorithm will not be able to jump to the new location due to the constraint on how much the line can move between frames. Another potential shortcoming is the ability of the feature detector to handle wide variation in lighting conditions.\n\n### 3. Possible improvements to the pipeline\n\nOne possible improvement to the pipeline would be to improve the feature detector to handle illumination changes. For example using adaptive thresholds for the feature detection based on information from the frame itself. Another potential improvement is to make the lane prediction more robust by weighting the information from the current frame and previous frames to smooth the lane locations as well as to help regain track even if there are suddeng jumps.')
('https://raw.githubusercontent.com/nkern/Astro_9/3db0f7a4728bcdea89ed01e368505fc752ec2750/lectures/02_IntroPython/IntroPython_II.ipynb', '### Course Repository for Astro 9\n\nSee the <a href="https://nkern.github.io/Astro_9/index.html">course webpage</a> for more details.')
('https://raw.githubusercontent.com/behrokhGitHub/Nanodegree-ml-projects/85dfd1dba5ccfd5435f7fb19a6274b2f95a6bfb7/projects/boston_housing/boston_housing.ipynb', '# machine-learning\nContent for Udacity\'s Machine Learning curriculum, which includes projects and their descriptions.\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.')
('https://raw.githubusercontent.com/jyoshimi/dst_stuff/1e9e1ee494c9b5474f1a031f6c4c89a1011b519d/ode_1d.ipynb', None)
('https://raw.githubusercontent.com/jingz8804/CarND-P01-Lane-Lines/b3d61e0ac63d71630c397ef1ea4d472798829054/P1.ipynb', '# **Finding Lane Lines on the Road** \n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\n<img src="examples/laneLines_thirdPass.jpg" width="480" alt="Combined Image" />\n\nOverview\n---\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means "Open-Source Computer Vision", which is a package that has many useful tools for analyzing images.  \n\nTo complete the project, two files will be submitted: a file containing project code and a file containing a brief write up explaining your solution. We have included template files to be used both for the [code](https://github.com/udacity/CarND-LaneLines-P1/blob/master/P1.ipynb) and the [writeup](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md).The code file is called P1.ipynb and the writeup template is writeup_template.md \n\nTo meet specifications in the project, take a look at the requirements in the [project rubric](https://review.udacity.com/#!/rubrics/322/view)\n\n\nCreating a Great Writeup\n---\nFor this project, a great writeup should provide a detailed response to the "Reflection" section of the [project rubric](https://review.udacity.com/#!/rubrics/322/view). There are three parts to the reflection:\n\n1. Describe the pipeline\n\n2. Identify any shortcomings\n\n3. Suggest possible improvements\n\nWe encourage using images in your writeup to demonstrate how your pipeline works.  \n\nAll that said, please be concise!  We\'re not looking for you to write a book here: just a brief description.\n\nYou\'re not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. Here is a link to a [writeup template file](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md). \n\n\nThe Project\n---\n\n## If you have already installed the [CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit/blob/master/README.md) you should be good to go!   If not, you should install the starter kit to get started on this project. ##\n\n**Step 1:** Set up the [CarND Term1 Starter Kit](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/83ec35ee-1e02-48a5-bdb7-d244bd47c2dc/lessons/8c82408b-a217-4d09-b81d-1bda4c6380ef/concepts/4f1870e0-3849-43e4-b670-12e6f2d4b7a7) if you haven\'t already.\n\n**Step 2:** Open the code in a Jupyter Notebook\n\nYou will complete the project code in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF="https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python" target="_blank">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an Ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, use terminal to navigate to your project directory and then run the following command at the terminal prompt (be sure you\'ve activated your Python 3 carnd-term1 environment as described in the [CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit/blob/master/README.md) installation instructions!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called "P1.ipynb".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n\n**Step 3:** Complete the project and submit both the Ipython notebook and the project writeup\n\n## How to write a README\nA well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing [this free course](https://www.udacity.com/course/writing-readmes--ud777).')
('https://raw.githubusercontent.com/laventura/AIND_dog_breed_classifier/cabcf3de693f3750aa59eab91a79064a5f292092/dog_app.ipynb', '[//]: # (Image References)\n\n[image1]: ./images/sample_dog_output.png "Sample Output"\n[image2]: ./images/vgg16_model.png "VGG-16 Model Keras Layers"\n[image3]: ./images/vgg16_model_draw.png "VGG16 Model Figure"\n\n\n## Project Overview\n\nWelcome to the Convolutional Neural Networks (CNN) project in the AI Nanodegree! In this project, you will learn how to build a pipeline that can be used within a web or mobile app to process real-world, user-supplied images.  Given an image of a dog, your algorithm will identify an estimate of the canine’s breed.  If supplied an image of a human, the code will identify the resembling dog breed.  \n\n![Sample Output][image1]\n\nAlong with exploring state-of-the-art CNN models for classification, you will make important design decisions about the user experience for your app.  Our goal is that by completing this lab, you understand the challenges involved in piecing together a series of models designed to perform various tasks in a data processing pipeline.  Each model has its strengths and weaknesses, and engineering a real-world application often involves solving many problems without a perfect answer.  Your imperfect solution will nonetheless create a fun user experience!\n\n## Project Instructions\n\n### Instructions\n\n1. Clone the repository and navigate to the downloaded folder.\n```\t\ngit clone https://github.com/udacity/dog-project.git\ncd dog-project\n```\n\n2. Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).  Unzip the folder and place it in the repo, at location `path/to/dog-project/dogImages`. \n\n3. Download the [human dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip).  Unzip the folder and place it in the repo, at location `path/to/dog-project/lfw`.  If you are using a Windows machine, you are encouraged to use [7zip](http://www.7-zip.org/) to extract the folder. \n\n4. Donwload the [VGG-16 bottleneck features](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG16Data.npz) for the dog dataset.  Place it in the repo, at location `path/to/dog-project/bottleneck_features`.\n\n5. (Optional) __If you plan to install TensorFlow with GPU support on your local machine__, follow [the guide](https://www.tensorflow.org/install/) to install the necessary NVIDIA software on your system.  If you are using an EC2 GPU instance, you can skip this step.\n\n6. (Optional) **If you are running the project on your local machine (and not using AWS)**, create (and activate) a new environment.\n\n\t- __Linux__ (to install with __GPU support__, change `requirements/dog-linux.yml` to `requirements/dog-linux-gpu.yml`): \n\t```\n\tconda env create -f requirements/dog-linux.yml\n\tsource activate dog-project\n\t```  \n\t- __Mac__ (to install with __GPU support__, change `requirements/dog-mac.yml` to `requirements/dog-mac-gpu.yml`): \n\t```\n\tconda env create -f requirements/dog-mac.yml\n\tsource activate dog-project\n\t```  \n\t- __Windows__ (to install with __GPU support__, change `requirements/dog-windows.yml` to `requirements/dog-windows-gpu.yml`):  \n\t```\n\tconda env create -f requirements/dog-windows.yml\n\tactivate dog-project\n\t```\n\t\n7. (Optional) **If you are running the project on your local machine (and not using AWS)** and Step 6 throws errors, try this __alternative__ step to create your environment.\n\n\t- __Linux__ or __Mac__ (to install with __GPU support__, change `requirements/requirements.txt` to `requirements/requirements-gpu.txt`): \n\t```\n\tconda create --name dog-project python=3.5\n\tsource activate dog-project\n\tpip install -r requirements/requirements.txt\n\t```  \n\t- __Windows__ (to install with __GPU support__, change `requirements/requirements.txt` to `requirements/requirements-gpu.txt`):  \n\t```\n\tconda create --name dog-project python=3.5\n\tactivate dog-project\n\tpip install -r requirements/requirements.txt\n\t```\n\t\n8. (Optional) **If you are using AWS**, install Tensorflow.\n```\nsudo python3 -m pip install -r requirements/requirements-gpu.txt\n```\n\t\n9. Switch [Keras backend](https://keras.io/backend/) to TensorFlow.\n\t- __Linux__ or __Mac__: \n\t\t```\n\t\tKERAS_BACKEND=tensorflow python -c "from keras import backend"\n\t\t```\n\t- __Windows__: \n\t\t```\n\t\tset KERAS_BACKEND=tensorflow\n\t\tpython -c "from keras import backend"\n\t\t```\n\n10. (Optional) **If you are running the project on your local machine (and not using AWS)**, create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `dog-project` environment. \n```\npython -m ipykernel install --user --name dog-project --display-name "dog-project"\n```\n\n11. Open the notebook.\n```\njupyter notebook dog_app.ipynb\n```\n\n12. (Optional) **If you are running the project on your local machine (and not using AWS)**, before running code, change the kernel to match the dog-project environment by using the drop-down menu (**Kernel > Change kernel > dog-project**). Then, follow the instructions in the notebook.\n\n__NOTE:__ While some code has already been implemented to get you started, you will need to implement additional functionality to successfully answer all of the questions included in the notebook. __Unless requested, do not modify code that has already been included.__\n\n## Evaluation\n\nYour project will be reviewed by a Udacity reviewer against the CNN project [rubric](https://review.udacity.com/#!/rubrics/810/view).  Review this rubric thoroughly, and self-evaluate your project before submission.  All criteria found in the rubric must meet specifications for you to pass.\n\n## Project Submission\n\nWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload:\n- The `dog_app.ipynb` file with fully functional code, all code cells executed and displaying output, and all questions answered.\n- An HTML or PDF export of the project notebook with the name `report.html` or `report.pdf`.\n- Any additional images used for the project that were not supplied to you for the project. __Please do not include the project data sets in the `dogImages/` or `lfw/` folders.  Likewise, please do not include the `bottleneck_features/` folder.__\n\nAlternatively, your submission could consist of the GitHub link to your repository.')
('https://raw.githubusercontent.com/hiddestokvis/CarND-LaneLines-P1/eb3765148973c89fb5f54c65bee9cb1eeff92ca8/P1.ipynb', '#**Finding Lane Lines on the Road** \n<img src="laneLines_thirdPass.jpg" width="480" alt="Combined Image" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means "Open-Source Computer Vision", which is a package that has many useful tools for analyzing images.  \n\n**Step 1:** Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if you\'re getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF="https://www.continuum.io/downloads" target="_blank">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type "help", "copyright", "credits" or "license" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`  \n`> conda install -c https://conda.anaconda.org/menpo opencv3`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the "moviepy" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF="https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python" target="_blank">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called "P1.ipynb".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  ')
('https://raw.githubusercontent.com/Olamyy/oldone/731c9b7b29f61f22823f40d45cb7b58af78afe93/plugins/ipynb/tests/pelican/content/with-metadata.ipynb', '')
('https://raw.githubusercontent.com/CyrilWendl/SIE-Master/cc813e89557ad780574218cea735a6bf681201be/Zurich/Zurich%20Dataset%20Error%20Detection.ipynb', "# Density Forest \nCode Repository of the EPFL SIE Master Project, Spring Semester 2018.\nThe goal of this project is to perform error detection and novelty detection in Convolutional Neural Networks (CNNs) using Density Forests. Applications to the MNIST dataset and a dataset for semantic segmentation of land cover classes in Zurich are visualized in  `Code/` and `Zurich/`.\n\n## Installation\nThe package can be simply installed from pip:\n\n```pip install density_forest```\n\n## 📈 Results\nDensity trees maximize Gaussianity at each split level. In 2D this might look as follows:\n\n![Simple 2D visualization](Figures/density_tree/gif/splits_visu.gif) \n\nA density forest is a collection of density trees each trained on a random subset of all data.\n\n![t-SNE of pre-softmax activations of Zurich dataset](Figures/Zurich/GIF/tsne_act.gif) \n\nThe above example shows the t-SNE of the pre-softmax activations of a network trained for semantic segmentation of the\n Zurich dataset, leaving out one class during training. \nDensity trees were trained on bootstrap samples of all classes but the unseen one. \n\nConfidence of each data point in the test set, the probability is calculated as the average Gaussian likelihood to come from the leaf node clusters.\n\n![Probas](Figures/Zurich/GIF/probas.png)\n\nDarker points represent regions of lower certainty and crosses represent activations of unseen classes.\n \n\n## 📖 Usage of the `DensityForest` class:\n#### Fitting a Density Forest\nSuppose you have your training data `X_train` and test data `X_test`, in `[N, D]` with `N` data points in `D` dimensions:\n\n```python\nfrom density_forest.density_forest import DensityForest\n\nclf_df = DensityForest(**params)         # create new class instance, put hyperparameters here\nclf_df.fit(X_train)                      # fit to a training set\nconf = clf_df.decision_function(X_test)  # get confidence values for test set\noutliers = clf_df.predict(X_test)        # predict whether a point is an outlier (-1 for outliers, 1 for inliers)\n```\nHyperparameters are documented in the docstring. To find the optimal hyperparameters, consider the section below.\n\n#### Finding Hyperparameters\nTo find the optimal hyperparameters, use the `ParameterSearch` from `helpers.cross_validator`, which allows CV, and hyperparameter search.\n\n```python\nfrom helpers.cross_validator import ParameterSearch\n\n# define hyperparameters to test\ntuned_params = [{'max_depth':[2, 3, 4], 'n_trees': [10, 20]}] # optionally add non-default arguments as single-element arrays\ndefault_params = [{'verbose':0, ...}]  # other default parameters \n# do parameter search\nps = ParameterSearch(DensityForest, tuned_parameters, X_train, X_train_all, y_true_tr, f_scorer, n_iter=2, verbosity=0, n_jobs=1, default_params=default_params)\nps.fit()\n\n# get model with the best parameters, as above\nclf_df = DensityForest(**ps.best_params, **default_params)  # create new class instance with best hyperparameters\n...  # continue as above\n```\nCheck the docstrings for more detailed documentation af the `ParameterSearch` class.\n\n\n## 🗂 File Structure\n\n### 👾 Code\nAll libraries for density forests, helper libraries for semantic segmentation and for baselines. \n#### `density_forest/density_forest/`\nPackage for implementation of Decision Trees, Random Forests, Density Trees and Density Forests\n- `create_data.py`: functions for generating labelled and unlabelled data\n- `decision_tree.py`: data structure for decision tree nodes\n- `decision_tree_create.py`: functions for generating decision trees\n- `decision_tree_traverse.py`: functions for traversing a decision tree and predicting labels\n- `density_forest.py`: functions for creating density forests\n- `density_tree.py`: data struture for density tree nodes\n- `density_tree_create.py`: functions for generating a density tree\n- `density_tree_traverse.py`: functions for descending a density tree and retrieving its cluster parameters\n- `helper.py`: various helper functions\n- `random_forests.py`: functions for creating random forests\n\n#### `density_forest/helpers`: \nGeneral helpers library for semantic segmentation\n- `data_augment.py`: custom data augmentation methods applied to both the image and the ground truth\n- `data_loader.py`: PyTorch data loader for Zurich dataset\n- `helpers.py`: functions for importing, cropping, padding images and other related image tranformations\n- `parameter_search.py`: functions for finding optimal hyperparameters for Density Forest, OC-SVM and GMM (explained above)\n- `plots.py`:  Generic plotter functions for labelled and unlabelled 2D and 3D plots, used for t-SNE and PCA plots\n\n#### `density_forest/baselines`:\nHelper functions for confidence estimation baselines MSR, margin, entropy and MC-Dropout\n\n#### `Zurich Land Cover/keras_helpers`\nHelper functions for Keras\n- `helpers.py`: get activations\n- `callbacks.py`: callbacks to be evaluated after each epoch\n- `unet.py`: UNET model for training of network on Zurich dataset\n\n### 🗾 Visualizations\n#### `density_forest/`: \nVisualizations of basic decision tree and density tree\n- `Decision Forest.ipynb`: Decision Trees and Random Forest on randomly generated labelled data\n- `Density Forest.ipynb`: Density Trees on randomly generated unlabelled data\n\n#### `MNIST/`:\n- `MNIST Novelty Detection.ipynb`: Training of a CNN leaving out one class, baselines and DF for novelty detection\n- `MNIST Error Detection.ipynb`: Training of a CNN, baselines and DF for error detection\n\n#### `Zurich/`\n- `Zurich Dataset Novelty Detection.ipynb`: Training of CNN, baselines and DF for novelty detection\n- `Zurich Dataset Error Detection.ipynb`: Training of CNN, baselines and DF for error detection\n\n## 🎓 Supervisors:\n- Prof. Devis Tuia, University of Wageningen\n- Diego Marcos González, University of Wageningen\n- Prof. François Golay, EPFL\n\nCyril Wendl, 2018")
('https://raw.githubusercontent.com/sakshi30/tensorflow/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/examples/udacity/5_word2vec.ipynb', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_transp.png"><br><br>\n</div>\n-----------------\n\n| **`Linux CPU`** | **`Linux GPU`** | **`Mac OS CPU`** | **`Windows CPU`** | **`Android`** |\n|-----------------|---------------------|------------------|-------------------|---------------|\n| [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-cpu)](https://ci.tensorflow.org/job/tensorflow-master-cpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-linux-gpu)](https://ci.tensorflow.org/job/tensorflow-master-linux-gpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-mac)](https://ci.tensorflow.org/job/tensorflow-master-mac) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-win-cmake-py)](https://ci.tensorflow.org/job/tensorflow-master-win-cmake-py) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-android)](https://ci.tensorflow.org/job/tensorflow-master-android) |\n\n**TensorFlow** is an open source software library for numerical computation using\ndata flow graphs.  Nodes in the graph represent mathematical operations, while\nthe graph edges represent the multidimensional data arrays (tensors) that flow\nbetween them.  This flexible architecture lets you deploy computation to one\nor more CPUs or GPUs in a desktop, server, or mobile device without rewriting\ncode.  TensorFlow also includes TensorBoard, a data visualization toolkit.\n\nTensorFlow was originally developed by researchers and engineers\nworking on the Google Brain team within Google\'s Machine Intelligence research\norganization for the purposes of conducting machine learning and deep neural\nnetworks research.  The system is general enough to be applicable in a wide\nvariety of other domains, as well.\n\n**If you\'d like to contribute to TensorFlow, be sure to review the [contribution\nguidelines](CONTRIBUTING.md).**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, but please see\n[Community](tensorflow/docs_src/about/index.md#community) for general questions\nand discussion.**\n\n## Installation\n*See [Installing TensorFlow](https://www.tensorflow.org/install/) for instructions on how to install our release binaries or how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n* Linux CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/))\n* Linux GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/))\n* Mac CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/))\n* Mac GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/))\n* Windows CPU-only: [Python 3.5 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/))\n* Windows GPU: [Python 3.5 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=gpu,OS=windows/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=gpu,OS=windows/))\n* Android: [demo APK](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow_demo.apk), [native libs](http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/native/)\n([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-android/))\n\n#### *Try your first TensorFlow program*\n```shell\n$ python\n```\n```python\n>>> import tensorflow as tf\n>>> hello = tf.constant(\'Hello, TensorFlow!\')\n>>> sess = tf.Session()\n>>> sess.run(hello)\nHello, TensorFlow!\n>>> a = tf.constant(10)\n>>> b = tf.constant(32)\n>>> sess.run(a+b)\n42\n>>>\n```\n\n## For more information\n\n* [TensorFlow website](http://tensorflow.org)\n* [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf)\n* [TensorFlow Model Zoo](https://github.com/tensorflow/models)\n* [TensorFlow MOOC on Udacity](https://www.udacity.com/course/deep-learning--ud730)\n\nThe TensorFlow community has created amazing things with TensorFlow, please see the [resources section of tensorflow.org](https://www.tensorflow.org/versions/master/resources#community) for an incomplete list.')
('https://raw.githubusercontent.com/harshjmodi/Cognitive-Computing/0ed16a39fd6528be015b6742dccee1f24cb047d2/Final%20Project_Team%201/SRGAN.ipynb', None)
('https://raw.githubusercontent.com/zweili/ann_miniproject2/fd9bd682ca7ebf86f40f03b5c8806fbdde6fb4c3/miniproject2_ChatBot.ipynb', '# ann_miniproject2')
('https://raw.githubusercontent.com/ucaiado/Customer_Segments/e6bd7f64f3253eea9e63504bcb4853d762468bc1/customer_segments.ipynb', 'Creating Customer Segments\n--------------------------\n\n\nThis project is part of the [Machine Learning Engineer Nanodegree](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009) program, from Udacity. You can check my report <a href="https://www.dropbox.com/s/xgqklu9t1c8ayaq/creating-customer-segments.pdf?dl=0" target="_blank">here</a> and the notebook with most of the codes used in this project <a href="https://nbviewer.jupyter.org/github/ucaiado/Customer_Segments/blob/master/customer_segments.ipynb" target="_blank">here</a>. I will use unsupervised learning techniques on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. The TEX file was produced with help of [Overleaf](https://www.overleaf.com/read/hqymhncpkrsq).\n\n\n<b>Sources:</b>\n\n- [Original Code](https://github.com/udacity/machine-learning/tree/master/projects/creating_customer_segments)\n- [Project Rubric](https://review.udacity.com/#!/projects/5422789357/rubric)\n- [The Dataset](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers)')
('https://raw.githubusercontent.com/davisgrubin/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/42e8c484d24de0fbeb540a1a1ca70e6d6a710ac0/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC3.ipynb', '# [Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n#### *Using Python and PyMC*\n\n\nThe Bayesian method is the natural approach to inference, yet it is hidden from readers behind chapters of slow, mathematical analysis. The typical text on Bayesian inference involves two to three chapters on probability theory, then enters what Bayesian inference is. Unfortunately, due to mathematical intractability of most Bayesian models, the reader is only shown simple, artificial examples. This can leave the user with a *so-what* feeling about Bayesian inference. In fact, this was the author\'s own prior opinion.\n\nAfter some recent success of Bayesian methods in machine-learning competitions, I decided to investigate the subject again. Even with my mathematical background, it took me three straight-days of reading examples and trying to put the pieces together to understand the methods. There was simply not enough literature bridging theory to practice. The problem with my misunderstanding was the disconnect between Bayesian mathematics and probabilistic programming. That being said, I suffered then so the reader would not have to now. This book attempts to bridge the gap.\n\nIf Bayesian inference is the destination, then mathematical analysis is a particular path towards it. On the other hand, computing power is cheap enough that we can afford to take an alternate route via probabilistic programming. The latter path is much more useful, as it denies the necessity of mathematical intervention at each step, that is, we remove often-intractable mathematical analysis as a prerequisite to Bayesian inference. Simply put, this latter computational path proceeds via small intermediate jumps from beginning to end, where as the first path proceeds by enormous leaps, often landing far away from our target. Furthermore, without a strong mathematical background, the analysis required by the first path cannot even take place.\n\n*Bayesian Methods for Hackers* is designed as an introduction to Bayesian inference from a computational/understanding-first, and mathematics-second, point of view. Of course as an introductory book, we can only leave it at that: an introductory book. For the mathematically trained, they may cure the curiosity this text generates with other texts designed with mathematical analysis in mind. For the enthusiast with less mathematical background, or one who is not interested in the mathematics but simply the practice of Bayesian methods, this text should be sufficient and entertaining.\n\nThe choice of PyMC as the probabilistic programming language is two-fold. As of this writing, there is currently no central resource for examples and explanations in the PyMC universe. The official documentation assumes prior knowledge of Bayesian inference and probabilistic programming. We hope this book encourages users at every level to look at PyMC. Secondly, with recent core developments and popularity of the scientific stack in Python, PyMC is likely to become a core component soon enough.\n\nPyMC does have dependencies to run, namely NumPy and (optionally) SciPy. To not limit the user, the examples in this book will rely only on PyMC, NumPy, SciPy and Matplotlib.\n\n\nPrinted Version by Addison-Wesley\n------\n<div style="float: right; margin-left: 30px;"><img title="Bayesian Methods for Hackersg"style="float: right;margin-left: 30px;" src="http://www-fp.pearsonhighered.com/assets/hip/images/bigcovers/0133902838.jpg" align=right height = 200 /></div>\n\n**Bayesian Methods for Hackers is now available as a printed book!** You can pick up a copy on [Amazon](http://www.amazon.com/Bayesian-Methods-Hackers-Probabilistic-Addison-Wesley/dp/0133902838). What are the differences between the online version and the printed version?\n\n - Additional Chapter on Bayesian A/B testing\n - Updated examples\n - Answers to the end of chapter questions\n - Additional explanation, and rewritten sections to aid the reader. \n\n\nContents\n------\n\nSee the project homepage [here](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) for examples, too.\n\n\nThe below chapters are rendered via the *nbviewer* at\n[nbviewer.jupyter.org/](http://nbviewer.jupyter.org/), and is read-only and rendered in real-time.\nInteractive notebooks + examples can be downloaded by cloning! \n\n### PyMC2\n\n* [**Prologue:**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Prologue/Prologue.ipynb) Why we do it.\n\n* [**Chapter 1: Introduction to Bayesian Methods**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb)\n    Introduction to the philosophy and practice of Bayesian methods and answering the question, "What is probabilistic programming?" Examples include:\n    - Inferring human behaviour changes from text message rates\n    \n* [**Chapter 2: A little more on PyMC**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC2.ipynb)\n    We explore modeling Bayesian problems using Python\'s PyMC library through examples. How do we create Bayesian models? Examples include:\n    - Detecting the frequency of cheating students, while avoiding liars\n    - Calculating probabilities of the Challenger space-shuttle disaster\n    \n* [**Chapter 3: Opening the Black Box of MCMC**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC2.ipynb)\n    We discuss how MCMC operates and diagnostic tools. Examples include:\n    - Bayesian clustering with mixture models\n    \n* [**Chapter 4: The Greatest Theorem Never Told**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC2.ipynb)\n    We explore an incredibly useful, and dangerous, theorem: The Law of Large Numbers. Examples include:\n    - Exploring a Kaggle dataset and the pitfalls of naive analysis\n    - How to sort Reddit comments from best to worst (not as easy as you think)\n    \n* [**Chapter 5: Would you rather lose an arm or a leg?**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC2.ipynb)\n    The introduction of loss functions and their (awesome) use in Bayesian methods.  Examples include:\n    - Solving the *Price is Right*\'s Showdown\n    - Optimizing financial predictions\n    - Winning solution to the Kaggle Dark World\'s competition\n    \n* [**Chapter 6: Getting our *prior*-ities straight**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter6_Priorities/Ch6_Priors_PyMC2.ipynb)\n    Probably the most important chapter. We draw on expert opinions to answer questions. Examples include:\n    - Multi-Armed Bandits and the Bayesian Bandit solution.\n    - What is the relationship between data sample size and prior?\n    - Estimating financial unknowns using expert priors\n    \n    We explore useful tips to be objective in analysis as well as common pitfalls of priors. \n\n### PyMC3\n\n* [**Prologue:**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Prologue/Prologue.ipynb) Why we do it.\n\n* [**Chapter 1: Introduction to Bayesian Methods**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Ch1_Introduction_PyMC3.ipynb)\n    Introduction to the philosophy and practice of Bayesian methods and answering the question, "What is probabilistic programming?" Examples include:\n    - Inferring human behaviour changes from text message rates\n    \n* [**Chapter 2: A little more on PyMC**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC3.ipynb)\n    We explore modeling Bayesian problems using Python\'s PyMC library through examples. How do we create Bayesian models? Examples include:\n    - Detecting the frequency of cheating students, while avoiding liars\n    - Calculating probabilities of the Challenger space-shuttle disaster\n    \n* [**Chapter 3: Opening the Black Box of MCMC**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb)\n    We discuss how MCMC operates and diagnostic tools. Examples include:\n    - Bayesian clustering with mixture models\n    \n* [**Chapter 4: The Greatest Theorem Never Told**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC3.ipynb)\n    We explore an incredibly useful, and dangerous, theorem: The Law of Large Numbers. Examples include:\n    - Exploring a Kaggle dataset and the pitfalls of naive analysis\n    - How to sort Reddit comments from best to worst (not as easy as you think)\n    \n* [**Chapter 5: Would you rather lose an arm or a leg?**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC3.ipynb)\n    The introduction of loss functions and their (awesome) use in Bayesian methods.  Examples include:\n    - Solving the *Price is Right*\'s Showdown\n    - Optimizing financial predictions\n    - Winning solution to the Kaggle Dark World\'s competition\n    \n* [**Chapter 6: Getting our *prior*-ities straight**](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter6_Priorities/Ch6_Priors_PyMC3.ipynb)\n    Probably the most important chapter. We draw on expert opinions to answer questions. Examples include:\n    - Multi-Armed Bandits and the Bayesian Bandit solution.\n    - What is the relationship between data sample size and prior?\n    - Estimating financial unknowns using expert priors\n    \n    We explore useful tips to be objective in analysis as well as common pitfalls of priors. \n\n\n\n    \n**More questions about PyMC?**\nPlease post your modeling, convergence, or any other PyMC question on [cross-validated](http://stats.stackexchange.com/), the statistics stack-exchange.\n    \n    \nUsing the book\n-------\n\nThe book can be read in three different ways, starting from most recommended to least recommended: \n\n1. The most recommended option is to clone the repository to download the .ipynb files to your local machine. If you have Jupyter installed, you can view the \nchapters in your browser *plus* edit and run the code provided (and try some practice questions). This is the preferred option to read\nthis book, though it comes with some dependencies. \n    -  Jupyter is a requirement to view the ipynb files. It can be downloaded [here](http://jupyter.org/). Jupyter notebooks can be run by `(your-virtualenv) ~/path/to/the/book/Chapter1_Introduction $ jupyter notebook`\n    -  For Linux users, you should not have a problem installing NumPy, SciPy, Matplotlib and PyMC. For Windows users, check out [pre-compiled versions](http://www.lfd.uci.edu/~gohlke/pythonlibs/) if you have difficulty. \n    -  In the styles/ directory are a number of files (.matplotlirc) that used to make things pretty. These are not only designed for the book, but they offer many improvements over the default settings of matplotlib.\n2. The second, preferred, option is to use the nbviewer.jupyter.org site, which display Jupyter notebooks in the browser ([example](http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb)).\nThe contents are updated synchronously as commits are made to the book. You can use the Contents section above to link to the chapters.\n \n3. PDFs are the least-preferred method to read the book, as PDFs are static and non-interactive. If PDFs are desired, they can be created dynamically using the [nbconvert](https://github.com/jupyter/nbconvert) utility.\n \n\nInstallation and configuration\n------\n\n\nIf you would like to run the Jupyter notebooks locally, (option 1. above), you\'ll need to install the following:\n\n-  Jupyter is a requirement to view the ipynb files. It can be downloaded [here](http://jupyter.org/install.html) \n- Necessary packages are PyMC, NumPy, SciPy and Matplotlib.   \n   -  For Linux/OSX users, you should not have a problem installing the above, [*except for Matplotlib on OSX*](http://www.penandpants.com/2012/02/24/install-python/).\n   -  For Windows users, check out [pre-compiled versions](http://www.lfd.uci.edu/~gohlke/pythonlibs/) if you have difficulty. \n   - also recommended, for data-mining exercises, are [PRAW](https://github.com/praw-dev/praw) and [requests](https://github.com/kennethreitz/requests). \n- New to Python or Jupyter, and help with the namespaces? Check out [this answer](http://stackoverflow.com/questions/12987624/confusion-between-numpy-scipy-matplotlib-and-pylab). \n\n-  In the styles/ directory are a number of files that are customized for the notebook. \nThese are not only designed for the book, but they offer many improvements over the \ndefault settings of matplotlib and the Jupyter notebook. The in notebook style has not been finalized yet.\n\n\n\nDevelopment\n------\n\nThis book has an unusual development design. The content is open-sourced, meaning anyone can be an author. \nAuthors submit content or revisions using the GitHub interface. \n\n### How to contribute\n\n#### What to contribute?\n\n-  The current chapter list is not finalized. If you see something that is missing (MCMC, MAP, Bayesian networks, good prior choices, Potential classes etc.),\nfeel free to start there. \n-  Cleaning up Python code and making code more PyMC-esque\n-  Giving better explanations\n-  Spelling/grammar mistakes\n-  Suggestions\n-  Contributing to the Jupyter notebook styles\n\n\n#### Commiting\n\n-  All commits are welcome, even if they are minor ;)\n-  If you are unfamiliar with Github, you can email me contributions to the email below.\n\nReviews\n------\n*these are satirical, but real*\n\n"No, but it looks good" - [John D. Cook](https://twitter.com/JohnDCook/status/359672133695184896)\n\n"I ... read this book ... I like it!" - [Andrew Gelman](http://www.andrewgelman.com/2013/07/21/bayes-related)\n\n"This book is a godsend, and a direct refutation to that \'hmph! you don\'t know maths, piss off!\' school of thought...\nThe publishing model is so unusual. Not only is it open source but it relies on pull requests from anyone in order to progress the book. This is ingenious and heartening" - [excited Reddit user](http://www.reddit.com/r/Python/comments/1alnal/probabilistic_programming_and_bayesian_methods/)\n\n\n\nContributions and Thanks\n-----\n\n\nThanks to all our contributing authors, including (in chronological order):\n\nAuthors | | | |\n--- | --- | --- | ---\n[Cameron Davidson-Pilon](http://www.camdp.com) |  [Stef Gibson](http://stefgibson.com) | [Vincent Ohprecio](http://bigsnarf.wordpress.com/) |[Lars Buitinck](https://github.com/larsman)\n[Paul Magwene](http://github.com/pmagwene) |  [Matthias Bussonnier](https://github.com/Carreau) | [Jens Rantil](https://github.com/JensRantil) |  [y-p](https://github.com/y-p)\n[Ethan Brown](http://www.etano.net/) |  [Jonathan Whitmore](http://jonathanwhitmore.com/) | [Mattia Rigotti](https://github.com/matrig) |  [Colby Lemon](https://github.com/colibius)\n[Gustav W Delius](https://github.com/gustavdelius) |  [Matthew Conlen](http://www.mathisonian.com/)  | [Jim Radford](https://github.com/radford) |  [Vannessa Sabino](http://baniverso.com/)\n[Thomas Bratt](https://github.com/thomasbratt) |  [Nisan Haramati](https://github.com/nisanharamati) |  [Robert Grant](https://github.com/bgrant) | [Matthew Wampler-Doty](https://github.com/xcthulhu)\n[Yaroslav Halchenko](https://github.com/yarikoptic) |  [Alex Garel](https://github.com/alexgarel) | [Oleksandr Lysenko](https://twitter.com/sash_ko) |  [liori](https://github.com/liori)\n[ducky427](https://github.com/ducky427) |  [Pablo de Oliveira Castro](https://github.com/pablooliveira) | [sergeyfogelson](https://github.com/sergeyfogelson) |  [Mattia Rigotti](http://neurotheory.columbia.edu/~mrigotti/)\n[Matt Bauman](https://github.com/mbauman) | [Andrew Duberstein](http://www.andrewduberstein.com/) | [Carsten Brandt](http://cebe.cc/) |  [Bob Jansen](http://web2docx.com)\n [ugurthemaster](https://github.com/ugurthemaster)   | [William Scott](https://github.com/williamscott)   |  [Min RK](http://twitter.com/minrk)  |  [Bulwersator](https://github.com/Bulwersator)\n  [elpres](https://github.com/elpres)  |  [Augusto Hack](https://github.com/hackaugusto)  | [Michael Feldmann](https://github.com/michaf)   | [Youki](https://github.com/Youki)\n   [Jens Rantil](http://jensrantil.github.io) |  [Kyle Meyer](http://kyleam.com)  |  [Eric Martin](http://ericmart.in)  | [Inconditus](https://github.com/Inconditus)\n [Kleptine](https://github.com/Kleptine)   |  [Stuart Layton](https://github.com/slayton)  |  [Antonino Ingargiola](https://github.com/tritemio)  |  [vsl9](https://github.com/vsl9)\n  [Tom Christie](https://github.com/tom-christie)  |  [bclow](https://github.com/bclow)  |  [Simon Potter](http://sjp.co.nz/)  | [Garth Snyder](https://github.com/GarthSnyder)\n [Daniel Beauchamp](http://twitter.com/pushmatrix)  |  [Philipp Singer](http://www.philippsinger.info)  | [gbenmartin](https://github.com/gbenmartin) | [Peadar Coyle](https://twitter.com/Springcoil)\n\nWe would like to thank the Python community for building an amazing architecture. We would like to thank the \nstatistics community for building an amazing architecture. \n\nSimilarly, the book is only possible because of the [PyMC](http://github.com/pymc-devs/pymc) library. A big thanks to the core devs of PyMC: Chris Fonnesbeck, Anand Patil, David Huard and John Salvatier.\n\nOne final thanks. This book was generated by Jupyter Notebook, a wonderful tool for developing in Python. We thank the IPython/Jupyter \ncommunity for developing the Notebook interface. All Jupyter notebook files are available for download on the GitHub repository. \n\n\n\n#### Contact\nContact the main author, Cam Davidson-Pilon at cam.davidson.pilon@gmail.com or [@cmrndp](https://twitter.com/cmrn_dp)\n\n\n![Imgur](http://i.imgur.com/Zb79QZb.png)')
('https://raw.githubusercontent.com/learn-co-students/introduction-to-derivatives-lab-data-science-alpha/d408024682b768e5ea2680829872e5611ba40562/index.ipynb', '# Derivatives of Linear Functions Lab\n\n### Introduction: Start here\n\nIn this lab, we will practice our knowledge of derivatives. Remember that our key formula for derivatives, is \n$f\'(x) = \\frac{\\Delta y}{\\Delta x} =  \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$.  So in driving towards this formula, we will do the following: \n\n1. Learn how to represent linear and nonlinear functions in code.  \n2. Then because our calculation of a derivative relies on seeing the output at an initial value and the output at that value plus delta x, we need an `output_at` function.  \n3. Then we will be able to code the $\\Delta f$ function that sees the change in output between the initial x and that initial x plus the $\\Delta x$ \n4. Finally, we will calculate the derivative at a given x value, `derivative_at`. \n\n### Learning objectives \n\nFor this first section, you should be able to answer all of the question with an understanding of our definition of a derivative:\n\n1.  Our intuitive explanation that a derivative is the instantaneous rate of change of a function\n2.  Our mathematical definition is that \n\n$f\'(x) = \\frac{\\Delta y}{\\Delta x} =  \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$\n\n### Let\'s begin: Starting with functions\n\n#### 1. Representing Functions\n\nWe are about to learn to take the derivative of a function in code.  But before doing so, we need to learn how to express any kind of function in code.  This way when we finally write our functions for calculating the derivative, we can use them with both linear and nonlinear functions.\n\nFor example, we want to write the function $f(x) = 2x^2 + 4x - 10 $ in a way that allows us to easily determine the exponent of each term.\n\nThis is our technique: write the formula as a list of tuples.  \n\n> A tuple is a list whose elements cannot be reassigned.  But everything else, for our purposes, is the same.  \n```python\ntuple = (7, 3)\ntuple[0] # 7\ntuple[1] # 3\n```\n\n> We get a TyperError if we try to reassign the tuple\'s elements.\n```python\ntuple[0] = 7\n# TypeError: \'tuple\' object does not support item assignment\n```\n\nTake the following function as an example: \n\n$$f(x) = 4x^2 + 4x - 10 $$\n\nHere it is as a list of tuples:\n\n\n```python\nfour_x_squared_plus_four_x_minus_ten = [(4, 2), (4, 1), (-10, 0)]\n```\n\nSo each tuple in the list represents a different term in the function.  The first element of the tuple is the term\'s constant and the second element of the tuple is the term\'s exponent.  Thus $4x^2$ translates to `(4, 2)` and  $-10$ translates to `(-10, 0)` because $-10$ is the same as $-10*x^0$.  \n> We\'ll refer to this list of tuples as "list of terms", or `list_of_terms`.\n\nOk, so give this a shot. Write $ f(x) = 4x^3 + 11x^2 $ as a list of terms.  Assign it to the variable `four_x_cubed_plus_eleven_x_squared`.\n\n\n```python\nfour_x_cubed_plus_eleven_x_squared = [(4, 3), (11, 2)]\n```\n\n#### 2. Evaluating a function at a specific point \n\nNow that we can represent a function in code, let\'s write a Python function called `term_output` that can evaluate what a single term equals at a value of $x$.  \n\n* For example, when $x = 2$, the term $3x^2 = 3*2^2 = 12 $.  \n* So we represent $3x^2$ in code as `(3, 2)`, and: \n* `term_output((3, 2), 2)` should return 12\n\n\n\n```python\ndef term_output(term, input_value):\n    pass\n```\n\n\n```python\nterm_output((3, 2), 2) # 12\n```\n\n> **Hint:** To raise a number to an exponent in python, like 3^2 use the double star, as in:\n```python\n3**2 # 9 \n```\n\nNow write a function called `output_at`, when passed a `list_of_terms` and a value of $x$, calculates the value of the function at that value.  \n> * For example, we\'ll use `output_at` to calculate $f(x) = 3x^2 - 11$.  \n> * Then `output_at([(3, 2), (-11, 0)], 2)` should return $f(2) = 3*2^2 - 11 = 1$\n\n\n```python\ndef output_at(list_of_terms, x_value):\n    pass\n```\n\n\n```python\nthree_x_squared_minus_eleven = [(3, 2), (-11, 0)]\noutput_at(three_x_squared_minus_eleven, 2) # 1 \noutput_at(three_x_squared_minus_eleven, 3) # 16\n```\n\nNow we can use our `output_at` function to display our function graphically.  We simply declare a list of `x_values` and then calculate `output_at` for each of the `x_values`.\n\n\n```python\nimport plotly\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import plot, trace_values\n\nx_values = list(range(-30, 30, 1))\ny_values = list(map(lambda x: output_at(three_x_squared_minus_eleven, x),x_values))\n\nthree_x_squared_minus_eleven_trace  = trace_values(x_values, y_values, mode = \'lines\')\nplot([three_x_squared_minus_eleven_trace], {\'title\': \'3x^2 - 11\'})\n```\n\n### Moving to derivatives of linear functions\n\nLet\'s start with a function, $f(x) = 4x + 15$.  We represent the function as the following:\n\n\n```python\nfour_x_plus_fifteen = [(4, 1), (15, 0)]\n```\n\nWe can plot the function by calculating outputs at a range of x values.  Note that we use our `output_at` function to calculate the output at each individual x value.\n\n\n```python\nimport plotly\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import plot, trace_values, build_layout\n\nx_values = list(range(0, 6))\n# layout = build_layout(y_axis = {\'range\': [0, 35]})\n\n\nfour_x_plus_fifteen_values = list(map(lambda x: output_at(four_x_plus_fifteen, x),x_values))\nfour_x_plus_fifteen_trace = trace_values(x_values, four_x_plus_fifteen_values, mode = \'lines\')\nplot([four_x_plus_fifteen_trace])\n```\n\nOk, time for what we are here for, derivatives.  Remember that the derivative is the instantaneous rate of change of a function, and is expressed as:\n\n$$ f\'(x) = \\frac{\\Delta f}{\\Delta x}  = \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}  $$ \n\n#### Writing a function for $\\Delta f$\n\nWe can see from the formula above that  $\\Delta f = f(x + \\Delta x ) - f(x) $.  Write a function called `delta_f` that, given a `list_of_terms`, an `x_value`, and a value $\\Delta x $, returns the change in the output over that period.\n> **Hint** Don\'t forget about the `output_at` function.  The `output_at` function takes a list of terms and an $x$ value and returns the corresponding output.  So really **`output_at` is equivalent to $f(x)$**, provided a function and a value of x.\n\n\n```python\nfour_x_plus_fifteen = [(4, 1), (15, 0)]\n```\n\n\n```python\ndef delta_f(list_of_terms, x_value, delta_x):\n    pass\n```\n\n\n```python\ndelta_f(four_x_plus_fifteen, 2, 1) # 4\n```\n\nSo for $f(x) = 4x + 15$, when x = 2, and $\\Delta x = 1$, $\\Delta f$ is 4.  \n\n#### Plotting our function, delta f, and delta x  \n\nLet\'s show $\\Delta f$ and $\\Delta x$ graphically.\n\n\n```python\ndef delta_f_trace(list_of_terms, x_value, delta_x):\n    initial_f_value = output_at(list_of_terms, x_value)\n    delta_f_value = delta_f(list_of_terms, x_value, delta_x)\n    if initial_f_value and delta_f_value:\n        trace =  trace_values(x_values=[x_value + delta_x, x_value + delta_x], \n                              y_values=[initial_f_value, initial_f_value + delta_f_value], mode = \'lines\',\n                              name = \'delta f = \' + str(delta_x))\n        return trace\n```\n\n\n```python\ntrace_delta_f_four_x_plus_fifteen = delta_f_trace(four_x_plus_fifteen, 2, 1)\n```\n\nLet\'s add another function that shows the delta x.\n\n\n```python\ndef delta_x_trace(list_of_terms, x_value, delta_x):\n    initial_f_value = output_at(list_of_terms, x_value)\n    if initial_f_value:\n        trace = trace_values(x_values=[x_value, x_value + delta_x],\n                            y_values=[initial_f_value, initial_f_value], mode = \'lines\', \n                            name = \'delta x = \' + str(delta_x))\n        return trace\n```\n\n\n```python\nfrom graph import plot, trace_values\n\ntrace_delta_x_four_x_plus_fifteen = delta_x_trace(four_x_plus_fifteen, 2, 1)\nif four_x_plus_fifteen_trace and trace_delta_f_four_x_plus_fifteen and trace_delta_x_four_x_plus_fifteen:\n    plot([four_x_plus_fifteen_trace, trace_delta_f_four_x_plus_fifteen, trace_delta_x_four_x_plus_fifteen], {\'title\': \'4x + 15\'})\n```\n\n#### Calculating the derivative\n\nWrite a function, `derivative_at` that calculates $\\frac{\\Delta f}{\\Delta x}$ when given a `list_of_terms`, an `x_value` for the value of $(x)$ the derivative is evaluated at, and `delta_x`, which represents $\\Delta x$.  \n\nLet\'s try this for $f(x) = 4x + 15 $.  Round the result to three decimal places.\n\n\n```python\ndef derivative_of(list_of_terms, x_value, delta_x):\n    pass\n```\n\n\n```python\nderivative_of(four_x_plus_fifteen, 3, 2) # 4.0\n```\n\n### We do: Building more plots\n\nOk, now that we have written a Python function that allows us to plot our list of terms, we can write a function that called `derivative_trace` that shows the rate of change, or slope, for the function between initial x and initial x plus delta x. We\'ll walk you through this one.  \n\n\n```python\ndef derivative_trace(list_of_terms, x_value, line_length = 4, delta_x = .01):\n    derivative_at = derivative_of(list_of_terms, x_value, delta_x)\n    y = output_at(list_of_terms, x_value)\n    if derivative_at and y:\n        x_minus = x_value - line_length/2\n        x_plus = x_value + line_length/2\n        y_minus = y - derivative_at * line_length/2\n        y_plus = y + derivative_at * line_length/2\n        return trace_values([x_minus, x_value, x_plus],[y_minus, y, y_plus], name = "f\' (x) = " + str(derivative_at), mode = \'lines\')\n```\n\n> Our `derivative_trace` function takes as arguments `list_of_terms`, `x_value`, which is where our line should be tangent to our function, `line_length` as the length of our tangent line, and `delta_x` which is our $\\Delta x$.\n\n> The return value of `derivative_trace` is a dictionary that represents tangent line at that values of $x$.  It uses the `derivative_of` function you wrote above to calculate the slope of the tangent line.  Once the slope of the tangent is calculated, we stretch out this tangent line by the `line_length` provided.  The beginning x value is just the midpoint minus the `line_length/2` and the ending $x$ value is midpoint plus the `line_length/2`.  Then we calculate our $y$ endpoints by starting at the $y$ along the function, and having them ending at `line_length/2*slope` in either direction. \n\n\n```python\ntangent_line_four_x_plus_fifteen = derivative_trace(four_x_plus_fifteen, 2, line_length = 4, delta_x = .01)\ntangent_line_four_x_plus_fifteen\n```\n\nNow we provide a function that simply returns all three of these traces.\n\n\n```python\ndef delta_traces(list_of_terms, x_value, line_length = 4, delta_x = .01):\n    tangent = derivative_trace(list_of_terms, x_value, line_length, delta_x)\n    delta_f_line = delta_f_trace(list_of_terms, x_value, delta_x)\n    delta_x_line = delta_x_trace(list_of_terms, x_value, delta_x)\n    return [tangent, delta_f_line, delta_x_line]\n```\n\nBelow we can plot our trace of the function as well \n\n\n```python\ndelta_x = 1\n\n# derivative_traces(list_of_terms, x_value, line_length = 4, delta_x = .01)\n\nthree_x_plus_tangents = delta_traces(four_x_plus_fifteen, 2, line_length= 2*1, delta_x = delta_x)\n\n# only plot the list of traces, if three_x_plus_tangents, does not look like [None, None, None]\nif list(filter(None.__ne__, three_x_plus_tangents)):\n    plot([four_x_plus_fifteen_trace, *three_x_plus_tangents])\n```\n\nSo that function highlights the rate of change is moving at precisely the point x = 2.  Sometimes it is useful to see how the derivative is changing across all x values.  With linear functions we know that our function is always changing by the same rate, and therefore the rate of change is constant.  Let\'s write functions that allow us to see the function, and the derivative side by side.\n\n\n```python\nfrom graph import make_subplots, trace_values, plot_figure\n\ndef function_values_trace(list_of_terms, x_values):\n    function_values = list(map(lambda x: output_at(list_of_terms, x),x_values))\n    return trace_values(x_values, function_values, mode = \'lines\')\n    \ndef derivative_values_trace(list_of_terms, x_values, delta_x):\n    derivative_values = list(map(lambda x: derivative_of(list_of_terms, x, delta_x), x_values))\n    return trace_values(x_values, derivative_values, mode = \'lines\')\n\ndef function_and_derivative_trace(list_of_terms, x_values, delta_x):\n    traced_function = function_values_trace(list_of_terms, x_values)\n    traced_derivative = derivative_values_trace(list_of_terms, x_values, delta_x)\n    return make_subplots([traced_function], [traced_derivative])\n\nfour_x_plus_fifteen_function_and_derivative = function_and_derivative_trace(four_x_plus_fifteen, list(range(0, 7)), 1)\n\nplot_figure(four_x_plus_fifteen_function_and_derivative)\n```\n\n### Summary\n\nIn this section, we coded out our function for calculating and plotting the derivative.  We started with seeing how we can represent different types of functions.  Then we moved onto writing the `output_at` function which evaluates a provided function at a value of x.  We calculated `delta_f` by subtracting the output at initial x value from the output at that initial x plus delta x.  After calculating `delta_f`, we moved onto our `derivative_at` function, which simply divided `delta_f` from `delta_x`.  \n\nIn the final section, we introduced some new functions, `delta_f_trace` and `delta_x_trace` that plot our deltas on the graph.  Then we introduced the `derivative_trace` function that shows the rate of change, or slope, for the function between initial x and initial x plus delta x.')
('https://raw.githubusercontent.com/davide-belli/machine-learning-2-labs-hws/3453076bd80220dc42a87874a2c45c5f83a32af4/lab2/lab2.ipynb', '# Labs and Homeworks for Machine Learning course, MSc AI @ UvA 2018/2019.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n  \nSolutions and implementation from [Davide Belli](https://github.com/davide-belli) and [Gabriele Cesa](https://github.com/Gabri95).\n---\n\n## Lab 1: Independent Component Analysis\n#### Topics:\n- ICA on mixtures of audio files\n\n## Lab 2: Inference in Graphical Models\n#### Topics:\n- The sum-product algorithm\n- The max-sum algorithm\n- Medical Graph\n\n## Lab 3: Expectation Maximization and Variational Autoencoder\n#### Topics:\n- Expectation Maximization\n- Variational Auto-Encoder\n---\n\n## Homework 1: Exponential Families\n#### Topics: \n- Probabilities, \n- MLE and MAP solutions\n- Expectation, mean and covariance\n- Exponential families and conjugate priors\n\n## Homework 2: Information Theory and Graphical Models\n#### Topics: \n- Mutual Information, KL-divergence, entropy\n- Directed Graphs, Bayesian Networks, Markov Blankets\n\n## Homework 3: ICA and Markov Chains\n#### Topics:\n- Conditional entropy and MI\n- ICA\n- Markov Chains and d-separation\n\n## Homework 4: Message passing\n#### Topics:\n- Factor Graphs of BN\n- Sum-Product algorithm\n\n## Homework 5: Generative Models\n#### Topics:\n- Gaussian Mixture Models\n- EM algorithm\n- Mixtures of Bernoulli\n\n## Homework 6: Sampling, Variational EM\n#### Topics:\n- Rejection, Importance, Independence, Gibbs sampling\n- Variational EM on Mixtures of multivariate Bernoulli\n- Random walks\n\n## Homework 7: LDS and Causal Networks\n#### Topics:\n- VEM on Linear Dynamical Systems\n- Causal Bayesian Networks and Simpsons\'s Paradox\n- Structural Causal Models and Truncated Factorization\n\n---\n\n## Copyright\n\nCopyright © 2019 Davide Belli.\n\n<p align=“justify”>\nThis project is distributed under the <a href="LICENSE">MIT license</a>.  \nPlease follow the <a href="http://student.uva.nl/en/content/az/plagiarism-and-fraud/plagiarism-and-fraud.html">UvA regulations governing Fraud and Plagiarism</a> in case you are a student.\n</p>')
('https://raw.githubusercontent.com/learn-co-students/evaluating-regression-lines-lab-nyc-ds-career-012819/9e013eae6441e00972ac4b024b1839f9aef01c63/index.ipynb', '# Evaluating Regression Lines Lab\n\n### Introduction\n\nIn the previous lesson, we learned to evaluate how well a regression line estimated our actual data.  In this lab, we\'ll turn these formulas into code.  In doing so, we\'ll build lots of useful functions for both calculating and displaying our errors for a given regression line and dataset.\n\n> In moving through this lab, we\'ll access to the functions that we previously built out to plot our data, available in the [graph](https://github.com/learn-co-curriculum/evaluating-regression-lines-lab/blob/master/graph.py) here.\n\n### Determining Quality\n\nIn the file, `movie_data.py` you will find movie data written as a python list of dictionaries, with each dictionary representing a movie.  The movies are derived from the first 30 entries from the dataset containing 538 movies [provided here](https://raw.githubusercontent.com/fivethirtyeight/data/master/bechdel/movies.csv).\n\n\n```python\nfrom movie_data import movies \nlen(movies)\n```\n\n> Press shift + enter\n\n\n```python\nmovies[0]\n```\n\n\n```python\nmovies[0][\'budget\']/1000000\n```\n\nThe numbers are in millions, so we will simplify things by dividing everything by a million\n\n\n```python\nscaled_movies = list(map(lambda movie: {\'title\': movie[\'title\'], \'budget\': round(movie[\'budget\']/1000000, 0), \'domgross\': round(movie[\'domgross\']/1000000, 0)}, movies))\nscaled_movies[0]\n```\n\nNote that, like in previous lessons, the budget is our explanatory value and the revenue is our dependent variable.  Here revenue is represented as the key `domgross`.  \n\n#### Plotting our data\n\nLet\'s write the code to plot this data set.\n\nAs a first task, convert the budget values of our `scaled_movies` to `x_values`, and convert the domgross values of the `scaled_movies` to `y_values`.\n\n\n```python\nx_values = None\ny_values = None\n```\n\n\n```python\nx_values and x_values[0] # 13.0\n```\n\n\n```python\ny_values and y_values[0] # 26.0\n```\n\nAssign a variable called `titles` equal to the titles of the movies.\n\n\n```python\ntitles = None\n```\n\n\n```python\ntitles and titles[0]\n```\n\nGreat! Now we have the data necessary to make a trace of our data.\n\n\n```python\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, plot\n\nmovies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n\nplot([movies_trace])\n```\n\n#### Plotting a regression line\n\nNow let\'s add a regression line to make a prediction of output (revenue) based on an input (the budget).  We\'ll use the following regression formula:\n\n* $\\hat{y} = m x + b$, with $m = 1.7$, and $b = 10$. \n\n\n* $\\hat{y} = 1.7x + 10$\n\nWrite a function called `regression_formula` that calculates our $\\hat{y}$ for any provided value of $x$. \n\n\n```python\ndef regression_formula(x):\n    pass\n```\n\nCheck to see that the regression formula generates the correct outputs.\n\n\n```python\nregression_formula(100) # 180.0\nregression_formula(250) # 435.0\n```\n\nLet\'s plot the data as well as the regression line to get a sense of what we are looking at.\n\n\n```python\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace])\n```\n\n### Calculating errors of a regression Line\n\nNow that we have our regression formula, we can move towards calculating the error. We provide a function called `y_actual` that given a data set of `x_values` and `y_values`, finds the actual y value, provided a value of `x`.\n\n\n\n\n```python\ndef y_actual(x, x_values, y_values):\n    combined_values = list(zip(x_values, y_values))\n    point_at_x = list(filter(lambda point: point[0] == x,combined_values))[0]\n    return point_at_x[1]\n```\n\n\n```python\nx_values and y_values and y_actual(13, x_values, y_values) # 26.0\n```\n\nWrite a function called `error`, that given a list of `x_values`, and a list of `y_values`, the values `m` and `b` of a regression line, and a value of `x`, returns the error at that x value.  Remember ${\\varepsilon_i} =  y_i - \\hat{y}_i$.  \n\n\n```python\ndef error(x_values, y_values, m, b, x):\n    pass\n```\n\n\n```python\nerror(x_values, y_values, 1.7, 10, 13) # -6.099999999999994\n```\n\nNow that we have a formula to calculate our errors, write a function called `error_line_trace` that returns a trace of an error at a given point.  So for a given movie budget, it will display the difference between the regression line and the actual movie revenue.\n\n![](./error-line.png)\n\nOk, so the function `error_line_trace` takes our dataset of `x_values` as the first argument and `y_values` as the second argument.  It also takes in values of $m$ and $b$ as the next two arguments to represent the regression line we will calculate errors from. Finally, the last argument is the value $x$ it is drawing an error for.\n\nThe return value is a dictionary that represents a trace, and looks like the following:\n\n```python\n{\'marker\': {\'color\': \'red\'},\n \'mode\': \'lines\',\n \'name\': \'error at 120\',\n \'x\': [120, 120],\n \'y\': [93.0, 214.0]}\n\n```\n\nThe trace represents the error line above. The data in `x` and `y` represent the starting point and ending point of the error line. Note that the x value is the same for the starting and ending point, just as it is for each vertical line. It\'s just the y values that differ - representing the actual value and the expected value. The mode of the trace equals `\'lines\'`.\n\n\n```python\ndef error_line_trace(x_values, y_values, m, b, x):\n    pass\n```\n\n\n```python\nerror_at_120m = error_line_trace(x_values, y_values, 1.7, 10, 120)\n\n# {\'marker\': {\'color\': \'red\'},\n#  \'mode\': \'lines\',\n#  \'name\': \'error at 120\',\n#  \'x\': [120, 120],\n#  \'y\': [93.0, 214.0]}\nerror_at_120m\n```\n\nWe just ran the our function to draw a trace of the error for the movie Elysium.  Let\'s see how it looks.\n\n\n```python\nscaled_movies[17]\n```\n\n\n```python\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom graph import trace_values, m_b_trace, plot\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, error_at_120m])\n```\n\nFrom there, we can write a function called `error_line_traces`, that takes in a list of `x_values` as an argument, `y_values` as an argument, and returns a list of traces for every x value provided.\n\n\n```python\ndef error_line_traces(x_values, y_values, m, b):\n    pass\n```\n\n\n```python\nerrors_for_regression = error_line_traces(x_values, y_values, 1.7, 10)\n```\n\n\n```python\nerrors_for_regression and len(errors_for_regression) # 30\n```\n\n\n```python\nerrors_for_regression and errors_for_regression[-1]\n\n# {\'x\': [200.0, 200.0],\n#  \'y\': [409.0, 350.0],\n#  \'mode\': \'lines\',\n#  \'marker\': {\'color\': \'red\'},\n#  \'name\': \'error at 200.0\'}\n```\n\n\n```python\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom graph import trace_values, m_b_trace, plot\n\nif x_values and y_values:\n    movies_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    regression_trace = m_b_trace(1.7, 10, x_values, name=\'estimated revenue\')\n    plot([movies_trace, regression_trace, *errors_for_regression])\n```\n\n> Don\'t worry about some of the points that don\'t have associated error lines.  It is a complication with Plotly and not our functions.\n\n### Calculating RSS\n\nNow write a function called `squared_error`, that given a value of x, returns the squared error at that x value.\n\n${\\varepsilon_i}^2 =  (y_i - \\hat{y}_i)^2$\n\n\n```python\ndef squared_error(x_values, y_values, m, b, x):\n    pass\n```\n\n\n```python\nx_values and y_values and squared_error(x_values, y_values, 1.7, 10, x_values[0]) # 37.20999999999993\n```\n\nNow write a function that will iterate through the x and y values to create a list of squared errors at each point, $(x_i, y_i)$ of the dataset.\n\n\n```python\ndef squared_errors(x_values, y_values, m, b):\n    pass\n```\n\n\n```python\nx_values and y_values and squared_errors(x_values, y_values, 1.7, 10)\n```\n\nNext, write a function called `residual_sum_squares` that, provided a list of x_values, y_values, and the m and b values of a regression line, returns the sum of the squared error for the movies in our dataset.\n\n\n```python\ndef residual_sum_squares(x_values, y_values, m, b):\n    pass\n```\n\n\n```python\nresidual_sum_squares(x_values, y_values, 1.7, 10) # 327612.2800000001\n```\n\nFinally, write a function called `root_mean_squared_error` that calculates the RMSE for the movies in the dataset, provided the same parameters as RSS.  Remember that `root_mean_squared_error` is a way for us to measure the approximate error per data point.\n\n\n```python\nimport math\ndef root_mean_squared_error(x_values, y_values, m, b):\n    return math.sqrt(residual_sum_squares(x_values, y_values, m, b)/len(x_values))\n```\n\n\n```python\nroot_mean_squared_error(x_values, y_values, 1.7, 10) # 104.50076235766578\n```\n\n#### Some functions for your understanding\n\nNow we\'ll provide a couple functions for you.  Note that we can represent multiple regression lines by a list of m and b values:\n\n\n```python\nregression_lines = [(1.7, 10), (1.9, 20)]\n```\n\nThen we can return a list of the regression lines along with the associated RMSE.\n\n\n```python\ndef root_mean_squared_errors(x_values, y_values, regression_lines):\n    errors = []\n    for regression_line in regression_lines:\n        error = root_mean_squared_error(x_values, y_values, regression_line[0], regression_line[1])\n        errors.append([regression_line[0], regression_line[1], round(error, 0)])\n    return errors\n```\n\nNow let\'s generate the RMSE values for each of these lines.\n\n\n```python\nx_values and y_values and root_mean_squared_errors(x_values, y_values, regression_lines)\n```\n\nNow we\'ll provide a couple functions for you:\n* a function called `trace_rmse`, that builds a bar chart displaying the value of the RMSE.  The return value is a dictionary with keys of `x` and `y`, both which point to lists.  The $x$ key points to a list with one element, a string containing each regression line\'s m and b value.  The $y$ key points to a list of the RMSE values for each corresponding regression line.\n\n\n```python\nimport plotly.graph_objs as go\n\ndef trace_rmse(x_values, y_values, regression_lines):\n    errors = root_mean_squared_errors(x_values, y_values, regression_lines)\n    x_values_bar = list(map(lambda error: \'m: \' + str(error[0]) + \' b: \' + str(error[1]), errors))\n    y_values_bar = list(map(lambda error: error[-1], errors))\n    return dict(\n        x=x_values_bar,\n        y=y_values_bar,\n        type=\'bar\'\n    )\n\n\nx_values and y_values and trace_rmse(x_values, y_values, regression_lines)\n```\n\nOnce this is built, we can create a subplot showing the two regression lines, as well as the related RMSE for each line.\n\n\n```python\nimport plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef regression_and_rss(scatter_trace, regression_traces, rss_calc_trace):\n    fig = tools.make_subplots(rows=1, cols=2)\n    for reg_trace in regression_traces:\n        fig.append_trace(reg_trace, 1, 1)\n    fig.append_trace(scatter_trace, 1, 1)\n    fig.append_trace(rss_calc_trace, 1, 2)\n    iplot(fig)\n```\n\n\n```python\n### add more regression lines here, by adding new elements to the list\nregression_lines = [(1.7, 10), (1, 50)]\n\nif x_values and y_values:\n    regression_traces = list(map(lambda line: m_b_trace(line[0], line[1], x_values, name=\'m:\' + str(line[0]) + \'b: \' + str(line[1])), regression_lines))\n\n    scatter_trace = trace_values(x_values, y_values, text=titles, name=\'movie data\')\n    rmse_calc_trace = trace_rmse(x_values, y_values, regression_lines)\n\n    regression_and_rss(scatter_trace, regression_traces, rmse_calc_trace)\n```\n\nAs we can see above, the second line (m: 1.0, b: 50) has the lower RMSE. We thus can conclude that the second line "fits" our set of movie data better than the first line. Ultimately, our goal will be to choose the regression line with the lowest RSME or RSS. We will learn how to accomplish this goal in the following lessons and labs.')
('https://raw.githubusercontent.com/silvanopessoa/titanic/3cc67babc3f249f83b2507bc09255d5b60dcf380/01%20-%20Análise%20Exploratória%20de%20Dados%20-%20Titanic.ipynb', None)
('https://raw.githubusercontent.com/indiesoul2/MachingLearing-HandOn/b5eaad8c9ffcfbe6df9ddc15107ff838ed960ad9/11_deep_learning.ipynb', '핸즈온 머신러닝 노트북\n==========================\n\n이 깃허브는 [핸즈온 머신러닝(사이킷런과 텐서플로를 활용한 머신러닝, 딥러닝 실무)](http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)에 포함된 예제 코드와 연습문제 해답을 가지고 있습니다:\n\n>(옮긴이)이 깃허브는 사이킷런 0.19.1,0.20.0 텐서플로 1.7, 1.8, 1.9, 1.10, 1.11, 1.12 그리고 OpenAI gym 0.10.5에서 테스트되었습니다.\n\n[![book](http://www.hanbit.co.kr/data/books/B9267655530_l.jpg)](http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)\n\n[주피터](http://jupyter.org/) 노트북은 다음과 같이 사용할 수 있습니다:\n\n* [jupyter.org의 노트북 뷰어](http://nbviewer.jupyter.org/github/rickiepark/handson-ml/blob/master/index.ipynb)\n    * 노트: [github.com의 노트북 뷰어](https://github.com/rickiepark/handson-ml/blob/master/index.ipynb)도 가능하지만 좀 느리고 수식이 제대로 표현되지 않을 수 있습니다.\n* 이 레파지토리를 클론하고 로컬에서 주피터를 실행합니다. 이렇게 하면 코드를 사용해 여러 실험을 할 수 있습니다. 자세한 설치 방법은 아래에 있습니다.\n\n>(옮긴이) 구글의 [Colab](https://colab.research.google.com/github/rickiepark/handson-ml/blob/master/index.ipynb)을 사용하면 로컬에서 주피터를 실행하지 않고도 코드를 실행해 볼 수 있습니다. 변경한 코드는 자신의 구글 드라이브에 저장할 수 있습니다. 만약 변경한 코드를 다시 깃허브에 저장하고 싶다면 이 레파지토리를 포크한 후에 Colab을 사용하세요. 다만 이 깃허브에 있는 노트북을 위한 파이썬 패키지가 Colab에서 모두 제공되지 않을 수 있습니다.\n\n# 설치\n\n먼저 [git](https://git-scm.com/)이 설치되어 있지 않다면 이를 설치해야 합니다.\n\n그다음 터미널을 열고 다음 명령으로 이 레파지토리를 클론합니다.\n\n>(옮긴이) 수정된 내용을 보관하고 싶다면 깃허브에서 포크한 레파지토리를 클론하는 것이 좋습니다\n\n    $ cd $HOME  # 또는 적절한 다른 디렉토리\n    $ git clone https://github.com/rickiepark/handson-ml.git\n    $ cd handson-ml\n\ngit을 설치하고 싶지 않다면, [master.zip](https://github.com/rickiepark/handson-ml/archive/master.zip)을 다운로드한 후 압축을 풀고 디렉토리 이름을 `handson-ml`로 변경한 다음 적절한 작업 디렉토리로 옮기세요.\n\n16장의 강화학습 예제를 위해서는 [OpenAI 짐(gym)](https://gym.openai.com/docs)과 아타리 환경을 설치해야 합니다.\n\n>(옮긴이) 아나콘다가 설치되어 있다면 다음 명령을 사용하여 OpenAI 짐에 필요한 라이브러리를 먼저 시스템에 설치해야 합니다. 리눅스에서는 다음과 같습니다.\n>\n>$ sudo apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev libboost-all-dev libsdl2-dev swig\n>\n>맥OS에서 명령은 다음과 같습니다.\n>\n>$ brew install cmake boost boost-python sdl2 swig wget\n\n파이썬을 잘 알고 파이썬 라이브러리를 설치하는 방법을 알고 있으면 바로 `requirements.txt`에 리스트된 라이브러리를 설치하고 [주피터 시작하기](#starting-jupyter) 섹션으로 가도 됩니다. 자세한 설치 방법이 필요하면 다음을 참고하세요.\n\n## 파이썬과 필수 라이브러리\n\n당연히 파이썬이 필요합니다. 요즘 대부분의 운영체제에는 파이썬 2가 이미 설치되어 있고 때로는 파이썬 3가 설치된 경우도 있습니다. 다음 명령으로 어떤 버전의 파이썬이 설치되어 있는지 확인할 수 있습니다:\n\n    $ python --version   # 파이썬 2\n    $ python3 --version  # 파이썬 3\n\n파이썬 3라면 버전에 상관없지만 3.5버전 이상이 선호됩니다. 파이썬 3가 없다면 설치하는 걸 권장합니다(파이썬 2.6 이상도 작동하지만 곧 지원이 중단될 거라 파이썬 3이 권장됩니다). 파이썬을 설치하는 방법은 몇 가지가 있습니다. 윈도우즈나 맥OS라면 [python.org](https://www.python.org/downloads/)에서 설치 파일을 다운로드 받을 수 있습니다. 맥OS에서는 [맥포트(MacPorts)](https://www.macports.org/)나 [홈브류(Homebrew)](https://brew.sh/)를 사용할 수도 있습니다. 맥OS에서 파이썬 3.6 버전을 사용하고 있다면 다음 명령으로 `certifi` 패키지를 설치해야 합니다. 맥OS의 파이썬 3.6은 SSL 연결을 검증하기 위한 증서를 가지고 있지 않기 때문입니다([스택오버플로우의 질문](https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error)을 참고하세요)):\n\n    $ /Applications/Python\\ 3.6/Install\\ Certificates.command\n\n리눅스에서는 어떻게 해야할지 잘 모를 땐 운영체제의 패키징 도구를 사용합니다. 예를 들어, 데비안이나 우분투에서는 다음과 같이 타이핑합니다:\n\n    $ sudo apt-get update\n    $ sudo apt-get install python3\n\n또 다른 방법은 [아나콘다(Anaconda)](https://www.anaconda.com/downloads) 배포판을 다운로드하고 설치하는 것입니다. 이 배포판에는 파이썬과 많은 과학 라이브러리가 포함되어 있습니다. 파이썬 3 버전을 사용하는 것이 좋습니다.\n\n>(옮긴이) 윈도우즈라면 아나콘다를 사용하는 것이 거의 필수적입니다. 맥OS나 리눅스에서도 가급적 시스템에 설치된 파이썬을 변경하지 않도록 아나콘다 같은 배포판을 따로 설치하여 실험과 개발을 하는 것이 권장됩니다.\n\n아나콘다를 선택한다면 다음 섹션을 참고하세요. 그렇지 않다면 [pip 사용하기](#using-pip) 섹션을 참고하세요.\n\n## 아나콘다 사용하기\n\n>(옮긴이) 콘다 환경을 편리하게 만들어주기 위해 번역서 깃허브에는 `environment.yml` 파일이 포함되어 있습니다. 쉘에서 다음과 같은 명령을 실행하면 `handson-ml` 환경을 만들고 파이썬 3.5 버전과 필요한 라이브러리를 자동으로 설치해 줍니다.\n>\n>$ conda env create -f environment.yml\n>\n>만약 컴퓨터에 GPU가 있다면 environment.yml 파일에 tensorflow를 tensorflow-gpu로 변경해 주세요.\n\n아나콘다를 사용하면 프로젝트 전용의 독립된 파이썬 환경을 만들 수 있습니다. 프로젝트마다 다른 라이브러리와 다른 버전을 설치한 별개의 환경을 유지할 수 있기 때문에 권장되는 방법입니다(가령, 이 깃허브를 위해 독립된 환경을 만듭니다):\n\n    $ conda create -n mlbook python=3.5 anaconda\n    $ source activate mlbook\n\n이 명령은 `mlbook`이라는 이름(이름은 마음대로 바꿀 수 있습니다)으로 깨끗한 파이썬 3.5 환경을 만들고 활성화시킵니다. 이 환경은 아나콘다에 포함된 모든 과학 라이브러리를 포함시킵니다. 여기에는 텐서플로를 제외하고 우리가 필요한 모든 라이브러리가 들어 있습니다. 텐서플로는 다음과 같이 설치합니다:\n\n    $ conda install -n mlbook -c conda-forge tensorflow\n\n이 명령은 `mlbook` 환경에 아나콘다에 등록된 최근 텐서플로 버전(일반적으로 텐서플로의 최신 버전은 아닙니다)을 설치합니다(`conda-forge` 레파지토리에서 다운로드합니다). `mlbook` 환경에 설치하지 않으려면 `-n mlbook` 옵션을 빼면 됩니다.\n\n>(옮긴이) `conda-forge`에 텐서플로의 최신 버전이 다소 늦게 등록됩니다. 따라서 `conda`를 사용하는 것 보다는 `pip`를 사용하여 최신 버전의 텐서플로를 설치하는 것이 좋습니다. `environment.yml` 파일을 사용하여 환경을 만들었다면 자동으로 텐서플로 최신 버전이 설치됩니다.\n\n그다음 선택적으로 주피터 확장팩을 설치할 수 있습니다. 노트북에 테이블을 표시할 때 좋지만 필수적이진 않습니다.\n\n    $ conda install -n mlbook -c conda-forge jupyter_contrib_nbextensions\n\n모든 것이 준비되었습니다! 이제 [주피터 시작하기](#starting-jupyter) 섹션으로 가세요.\n\n## pip 사용하기\n\n아나콘다를 사용하지 않는다면 이 깃허브에 필요한 파이썬 과학 라이브러리를 직접 설치해야 합니다. 특히 넘파이(NumPy), 맷플롯립(Matplotlib), 판다스(Pandas), 주피터(Jupyter) 그리고 텐서플로(TensorFlow) 등입니다. 파이썬 기본 패키징 시스템인 pip나 시스템의 패키징 시스템(가령 우분투의 apt나 맥OS의 맥포트나 홈브류)을 사용할 수 있습니다. pip를 사용하는 장점은 라이브러리와 버전이 다른 독립된 파이썬 환경을 만들기 쉽다는 것입니다(가령 이 깃허브를 위한 전용 환경). 시스템의 패키징 도구를 사용하는 장점은 파이썬 라이브러리와 시스템의 다른 패키지와 충돌할 위험이 낮다는 것입니다. 진행하는 프로젝트가 많다고 가정하고 pip를 사용하여 독립된 환경을 만들겠습니다. 또한 아나콘다와 시스템의 도구의 패키지는 최신 버전에 대응이 조금 느리고 일반적으로 pip 패키지가 가장 빠르게 최신 버전을 제공합니다.\n\npip를 사용해 필요한 라이브러리를 설치하려면 터미널에 직접 명령을 입력해야 합니다. 노트: 만약 파이썬 3가 아니고 파이썬 2를 사용한다면 이후의 모든 명령에서 `pip3`를 `pip`로, `python3`를 `python`으로 바꾸어 주세요.\n\n먼저 최신 버전의 pip가 설치되었는지 확인합니다:\n\n    $ pip3 install --user --upgrade pip\n\n`--user` 옵션은 최신 버전의 pip를 현재 사용자에 대해서만 설치할 것입니다. `--user` 옵션을 빼고 시스템 전역에 걸쳐 설치하려면(즉, 모든 사용자를 위해서) 관리자 권한이 필요합니다(가령, 리눅스에서 `pip3` 대신에 `sudo pip3`를 사용합니다). `--user` 옵션을 사용하는 다음 명령들도 마찬가지입니다.\n\n그다음 독립된 환경을 만들 수도 있습니다. 프로젝트마다 다른 라이브러리와 버전으로 구성된 환경을 만들 수 있으므로 이렇게 하는 것이 좋습니다:\n\n    $ pip3 install --user --upgrade virtualenv\n    $ virtualenv -p `which python3` env\n\n이 명령은 현재 디렉토리에 파이썬 3 버전의 새로운 독립된 환경을 담고 있는 `env`라는 새로운 디렉토리를 만듭니다. 시스템에 파이썬 3의 버전이 여러 개라면 `` `which python3` ``을 적절한 파이썬 경로로 바꾸어 주세요.\n\n이제 이 환경을 활성화시켜야 합니다. 환경을 활성화할 때마다 다음 명령을 실행해야 합니다.\n\n    $ source ./env/bin/activate\n\n윈도우 사용사는 다음 명령을 실행하세요.\n\n    $ .\\env\\Scripts\\activate\n\n다음에 pip를 사용하여 필요한 파이썬 패키지를 설치합니다. virtualenv를 사용하지 않는다면 `--user` 옵션을 사용하세요(또는 시스템 경로에 설치할 수도 있지만 아마도 관리자 권한이 필요할 것 입니다. 가령, 리눅스에서는 `pip3` 대신 `sudo pip3`를 사용합니다).\n\n    $ pip3 install --upgrade -r requirements.txt\n\n좋습니다! 모든 것이 설치되었으니 이제 주피터를 실행해 보죠.\n\n## 주피터 시작하기\n\n주피터 확장을 사용하려면(이 확장은 선택 사항으로 테이블을 미려하게 표현하기 위해 사용합니다) 먼저 관련 자바스크립트와 CSS 파일을 복사해야 합니다:\n\n    $ jupyter contrib nbextension install --user\n\n그런다음 "Table of Contents (2)" 확장을 활성화시킬 수 있습니다:\n\n    $ jupyter nbextension enable toc2/main\n\n좋습니다! 이제 주피터를 실행해 보죠:\n\n    $ jupyter notebook\n\n이 명령은 브라우저를 열고 현재 디렉토리 내용을 주피터의 트리 목록으로 보여줍니다. 브라우저가 자동으로 열리지 않는다면 주소 창에 [localhost:8888](http://localhost:8888/tree)를 입력하고 `index.ipynb` 파일을 클릭하세요.\n\n노트: 주피터 확장을 활성화하고 변경하려면 [http://localhost:8888/nbextensions](http://localhost:8888/nbextensions)를 확인해 보세요.\n\n축하합니다! 이제 머신러닝에 뛰어들 준비를 마쳤습니다!\n\n# 기여자\n\n유용한 피드백을 주거나 이슈를 제기하고 풀 리퀘스트를 보내 준 모든 분들에게 감사드립니다. 특히 `docker` 디렉토리를 만든 Steven Bunkley와 Ziembla에게 감사합니다.\n\n>(옮긴이) 번역 작업을 하면서 깃허브에 포함된 `docker`를 테스트하지는 않았습니다. 윈도우즈, 리눅스, 맥OS에서 텐서플로를 쉽게 설치할 수 있기 때문에 굳이 도커를 사용할 이유는 없는 것 같습니다.')
('https://raw.githubusercontent.com/bharlow058/TensorFlow/0e1716213d3a6d03f65b0d6533d41886f2ec0369/tensorflow/examples/udacity/5_word2vec.ipynb', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_transp.png"><br><br>\n</div>\n\n-----------------\n\n| **`Linux CPU`** | **`Linux GPU`** | **`Mac OS CPU`** | **`Windows CPU`** | **`Android`** |\n|-----------------|---------------------|------------------|-------------------|---------------|\n| [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-cpu)](https://ci.tensorflow.org/job/tensorflow-master-cpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-linux-gpu)](https://ci.tensorflow.org/job/tensorflow-master-linux-gpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-mac)](https://ci.tensorflow.org/job/tensorflow-master-mac) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-win-cmake-py)](https://ci.tensorflow.org/job/tensorflow-master-win-cmake-py) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-android)](https://ci.tensorflow.org/job/tensorflow-master-android) |\n\n**TensorFlow** is an open source software library for numerical computation using\ndata flow graphs.  Nodes in the graph represent mathematical operations, while\nthe graph edges represent the multidimensional data arrays (tensors) that flow\nbetween them.  This flexible architecture lets you deploy computation to one\nor more CPUs or GPUs in a desktop, server, or mobile device without rewriting\ncode.  TensorFlow also includes TensorBoard, a data visualization toolkit.\n\nTensorFlow was originally developed by researchers and engineers\nworking on the Google Brain team within Google\'s Machine Intelligence research\norganization for the purposes of conducting machine learning and deep neural\nnetworks research.  The system is general enough to be applicable in a wide\nvariety of other domains, as well.\n\n**If you\'d like to contribute to TensorFlow, be sure to review the [contribution\nguidelines](CONTRIBUTING.md).**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, but please see\n[Community](tensorflow/docs_src/about/index.md#community) for general questions\nand discussion.**\n\n## Installation\n*See [Installing TensorFlow](https://www.tensorflow.org/install/) for instructions on how to install our release binaries or how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n* Linux CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/))\n* Linux GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.1.0rc1-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/)) / [Python 3.4](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.1.0rc1-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/)) / [Python 3.5](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.1.0rc1-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/))\n* Mac CPU-only: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/))\n* Mac GPU: [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.1.0rc1-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/)) / [Python 3](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.1.0rc1-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/))\n* Windows CPU-only: [Python 3.5 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow-1.1.0rc1-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/))\n* Windows GPU: [Python 3.5 64-bit](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=gpu,OS=windows/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.1.0rc1-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=gpu,OS=windows/))\n* Android: [demo APK](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow_demo.apk), [native libs](http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/native/)\n([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-android/))\n\n#### *Try your first TensorFlow program*\n```shell\n$ python\n```\n```python\n>>> import tensorflow as tf\n>>> hello = tf.constant(\'Hello, TensorFlow!\')\n>>> sess = tf.Session()\n>>> sess.run(hello)\nHello, TensorFlow!\n>>> a = tf.constant(10)\n>>> b = tf.constant(32)\n>>> sess.run(a+b)\n42\n>>>\n```\n\n## For more information\n\n* [TensorFlow website](https://tensorflow.org)\n* [TensorFlow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf)\n* [TensorFlow Model Zoo](https://github.com/tensorflow/models)\n* [TensorFlow MOOC on Udacity](https://www.udacity.com/course/deep-learning--ud730)\n\nThe TensorFlow community has created amazing things with TensorFlow, please see the [resources section of tensorflow.org](https://www.tensorflow.org/about/#community) for an incomplete list.')
('https://raw.githubusercontent.com/iranroman/iranroman.github.io/507f995a8eeada8b2e4f25eb81608b892c72f5dd/algoritmica/index.ipynb', None)
('https://raw.githubusercontent.com/IBMDecisionOptimization/tutorials/730677f141f406a631b25c2fba4bbdb4e5a47b35/jupyter/Beyond_Linear_Programming.ipynb', '# IBM® Decision Optimization Tutorials for Python (DOcplex)\n\nWelcome to the IBM® Decision Optimization Tutorials.\nLicensed under the Apache License v2.0.\n\nWith these tutorials, you will learn the concepts of Mathematical Optimization.\n\nThis library is composed of 2 Jupyter notebooks:\n\n* Linear Programming discovery\n* Beyond Linear Programming\n\nThese notebooks are part of **[Prescriptive Analytics for Python](http://ibmdecisionoptimization.github.io/docplex-doc/)**\n\nThey require either an [installation of CPLEX Optimizers](http://ibmdecisionoptimization.github.io/docplex-doc/getting_started.html) or they can be run on [IBM Cloud Pak for Data as a Service](https://www.ibm.com/products/cloud-pak-for-data/as-a-service/) (Sign up for a [free IBM Cloud account](https://dataplatform.cloud.ibm.com/registration/stepone?context=wdp&apps=all>)\nand you can start using `IBM Cloud Pak for Data as a Service` right away).\n\nCPLEX is available on <i>IBM Cloud Pack for Data</i> and <i>IBM Cloud Pak for Data as a Service</i>:\n- <i>IBM Cloud Pak for Data as a Service</i>: Depends on the runtime used:\n\t- <i>Python 3.x</i> runtime: Community edition\n\t- <i>Python 3.x + DO</i> runtime: full edition\n- <i>Cloud Pack for Data</i>: Community edition is installed by default. Please install the `DO` addon in `Watson Studio Premium` for the full edition\n\nAdditionnaly, you can download  [installation of CPLEX Optimizers](http://ibmdecisionoptimization.github.io/docplex-doc/getting_started.html):\n\n- You can get a free [Community Edition](http://www-01.ibm.com/software/websphere/products/optimization/cplex-studio-community-edition)\n of CPLEX Optimization Studio, with limited solving capabilities in term of problem size.\n\n- Faculty members, research professionals at accredited institutions can get access to an unlimited version of CPLEX through the\n [IBM® Academic Initiative](http://www-304.ibm.com/ibm/university/academic/pub/page/ban_ilog_programming).\n\n\n## License\n\nThis library is delivered under the  Apache License Version 2.0, January 2004 (see LICENSE.txt).')
('https://raw.githubusercontent.com/rosaann/uda/b7457c1fa10f5198a4a8b02612080916ace1be30/smartcab/smartcab.ipynb', '# uda')
('https://raw.githubusercontent.com/nerziego/ML-Proj1/1baaec7cad07065df94e0cad830925d7e53c5af7/projects/customer_segments/customer_segments.ipynb', '# machine-learning\nContent for Udacity\'s Machine Learning curriculum, which includes projects and their descriptions.\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.')
('https://raw.githubusercontent.com/loganyc1934/traffic-sign-classifier/4f8fd3ac1bbcb433ccd46f4bacf9119690d1406f/Traffic_Sign_Classifier.ipynb', '# **Traffic Sign Recognition**\n\nLogan Yang\n---\n\n**Build a Traffic Sign Recognition Project**\n\nThe goals / steps of this project are the following:\n* Load the data set (see below for links to the project data set)\n* Explore, summarize and visualize the data set\n* Design, train and test a model architecture\n* Use the model to make predictions on new images\n* Analyze the softmax probabilities of the new images\n* Summarize the results with a written report\n\n[//]: # (Image References)\n\n[image0]: ./examples/classes.png "Visualization"\n[image1]: ./examples/class_distribution.png "Distribution"\n[image2]: ./examples/normalization.png "Normalization"\n[image3]: ./examples/grayscale.png "Grayscale"\n[image4]: ./examples/random_noise0.png "Translation"\n[image5]: ./examples/random_noise1.png "Rotation"\n[image6]: ./examples/random_noise2.png "Zoom"\n[image7]: ./examples/new_distribution.png "Augmented Data"\n\n[image8]: ./my-signs/00000.png "Traffic Sign 1"\n[image9]: ./my-signs/00003.png "Traffic Sign 2"\n[image10]: ./my-signs/00004.png "Traffic Sign 3"\n[image11]: ./my-signs/00005.png "Traffic Sign 4"\n[image12]: ./my-signs/00007.png "Traffic Sign 5"\n[image13]: ./my-signs/00010.png "Traffic Sign 6"\n[image14]: ./examples/results.png "Traffic Sign 6"\n\n### Data Set Summary & Exploration\n\n#### 1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.\n\n* The size of training set is 34799.\n* The size of the validation set is 4410.\n* The size of test set is 12630.\n* The shape of a traffic sign image is `32, 32, 3`\n* The number of unique classes/labels in the data set is 43.\n\n#### 2. Include an exploratory visualization of the dataset.\n\nHere is an exploratory visualization of the data set. First, all classes are shown with a representative image. Next, a bar chart showing how the classes are distributed. Some of the classes are over-represented and some are under-represented. The idea of data augmentation comes in at this point.\n\n![alt text][image0]\n\n![alt text][image1]\n\n### Design and Test a Model Architecture\n\n#### 1. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc. (OPTIONAL: If you generated additional data for training, describe why you decided to generate additional data, how you generated the data, and provide example images of the additional data. Then describe the characteristics of the augmented training set like number of images in the set, number of images for each class, etc.)\n\nFirst, the images need to be preprocessed. I normalized the images to `[0, 1]` and converted them to grayscale because the significance of color is negligible in this problem.\n\nHere is an example of a traffic sign image before and after normalization.\n\n![alt text][image2]\n\nHere is an example of a traffic sign image before and after grayscaling.\n\n![alt text][image3]\n\nNext, given the imbalance training set, I generated images by translating, rotating or zooming the existing images by a random small amount. In this way, new images can be generated and the key information in the image, aka the sign is still preserved.\n\nHere is an example of an original image and an augmented image:\n\n![alt text][image4]\n![alt text][image5]\n![alt text][image6]\n\nThe difference in distribution between the original data set and the augmented data set is shown below\n\n![alt text][image7]\n\n#### 2. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.\n\nMy final model consisted of the following layers:\n\n| Layer             |     Description                   |\n|:---------------------:|:---------------------------------------------:|\n| Input             | 32x32x1 grayscale image                 |\n| Convolution 5x5       | 1x1 stride, valid padding, outputs 28x28x6  |\n| RELU          |                       |\n| Max pooling         | 2x2 stride,  outputs 14x14x6         |\n| Convolution 5x5     | 1x1 stride, valid padding, outputs 10x10x16    |\n| RELU          |                       |\n| Max pooling         | 2x2 stride,  outputs 5x5x16         |\n| Convolution 5x5     | 1x1 stride, valid padding, outputs 1x1x400    |\n| RELU          |                       |\n| Fully connected   | Flatten the two previous convnets 5x5x16 and 1x1x400, concatenate to size 800, outputs 400  |\n| RELU          |                       |\n| DROPOUT          | Dropout with probability 0.5                      |\n| Fully connected   | outputs 43  |\n| RELU          |                       |\n| Softmax       | Return the logits  |\n|           |                       |\n\n\n#### 3. Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n\nTo train the model, I used `AdamOptimizer` which is better than vanilla gradient descent.\n\n```\nEPOCHS = 20\nBATCH_SIZE = 150\nrate = 0.002\n```\n\n#### 4. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.\n\nMy final model results were:\n* training set accuracy of 99.8%.\n* validation set accuracy of 98.4%.\n* test set accuracy of 94.2%.\n\nAn iterative approach was chosen:\n* The first architecture was the vanilla LeNet5 from the lab. It yielded ~90% test accuracy out-of-the-box, which was not bad. However, both training and validation accuracies are not high, indicating there\'s underfitting. The architecture couldn\'t catch enough details from the dataset, so a deeper / more complex model is needed.\n* The next starting point should be an architecture from the Yann LeCun paper on traffic sign classification. I adapted to its description, added an extra fully connected layer, then managed to get a validation accuracy of ~94%, a significant advantage over the previous model.\n* By printing training accuracy, clearly there was overfitting happening - as the training accuracy went up, validation accuracy went down after it hit 94%. So DROPOUT is added, the validation accuracy went up to ~97%.\n* To fully utilize the training data and have a better training/validation split, `StratifiedShuffleSplit` is used to obtain a validation set that has approximately the same percentage of each class as the training data. I tried splitting a new pair of training and validation set for each epoch, but it turned out that the validation accuracy could go as high as ~99% but the test accuracy didn\'t improve. Theoretically I thought having a new split each time should reduce overfitting, but somehow it wasn\'t the case. Possibly it\'s because I have the generated data in both training and validation, but not in test, so the model fits better for the generated data, but not test data. One approach to find the cause is to do an exploration in the test data and see what\'s different between test and validation.\n\n### Test a Model on New Images\n\n#### 1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n\nHere are five German traffic signs that I found on the web:\n\n![alt text][image8] ![alt text][image9] ![alt text][image10]\n![alt text][image11] ![alt text][image12] ![alt text][image13]\n\nThese images have different resolutions, I transformed them to 32 by 32 but the difference in size and image quality is still a factor that could affect the prediction performance.\n\n#### 2. Discuss the model\'s predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the "Stand Out Suggestions" part of the rubric).\n\nHere are the results of the prediction:\n\n| Image             |     Prediction                    |\n|:---------------------:|:---------------------------------------------:|\n| Vehicles over 3.5 metric tons prohibited         | Vehicles over 3.5 metric tons prohibited                     |\n| Turn right ahead          | Turn right ahead                    |\n| Right-of-way at the next intersection         | Right-of-way at the next intersection                     |\n| Keep right            | Keep right                  |\n| Ahead only     | Ahead only                   |\n| No entry     | No entry                   |\n\n\nThe model was able to correctly guess 6 of the 6 traffic signs, which gives an accuracy of 100%.\n\nCompared to the test accuracy, this is obviously larger, reason being this is a very small sample and there is a chance that a model could guess right 6 out of 6. Say we have a binomial distribution with the success rate of `p`, where `p` can be the test accuracy. There is a chance that `x` can range from `0 - 6` even with a "true" accuracy of `p`. In this case, `x = 6`. It doesn\'t necessarily mean the model is better than what the previous test accuracy suggested. To better evaluate the model\'s performance, a much larger data is needed so that we can confidently check how well the model generalizes in real world.\n\n#### 3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability.\n\nFor the first image, the model is relatively sure that this is a stop sign (probability of 0.6), and the image does contain a stop sign. The top five soft max probabilities were\n\n![alt text][image14]')
('https://raw.githubusercontent.com/sigfried1982/reinforcement-learning/fd13eca60aac3434845ac7125d780e080ce55539/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb', "### Overview\n\nThis repository provides code, exercises and solutions for popular Reinforcement Learning algorithms. These are meant to serve as a learning tool to complement the theoretical materials from\n\n- [Reinforcement Learning: An Introduction (2nd Edition)](http://incompleteideas.net/book/bookdraft2018jan1.pdf)\n- [David Silver's Reinforcement Learning Course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n\nEach folder in corresponds to one or more chapters of the above textbook and/or course. In addition to exercises and solution, each folder also contains a list of learning goals, a brief concept summary, and links to the relevant readings.\n\nAll code is written in Python 3 and uses RL environments from [OpenAI Gym](https://gym.openai.com/). Advanced techniques use [Tensorflow](https://www.tensorflow.org/) for neural network implementations.\n\n\n### Table of Contents\n\n- [Introduction to RL problems & OpenAI Gym](Introduction/)\n- [MDPs and Bellman Equations](MDP/)\n- [Dynamic Programming: Model-Based RL, Policy Iteration and Value Iteration](DP/)\n- [Monte Carlo Model-Free Prediction & Control](MC/)\n- [Temporal Difference Model-Free Prediction & Control](TD/)\n- [Function Approximation](FA/)\n- [Deep Q Learning](DQN/) (WIP)\n- [Policy Gradient Methods](PolicyGradient/) (WIP)\n- Learning and Planning (WIP)\n- Exploration and Exploitation (WIP)\n\n\n### List of Implemented Algorithms\n\n- [Dynamic Programming Policy Evaluation](DP/Policy%20Evaluation%20Solution.ipynb)\n- [Dynamic Programming Policy Iteration](DP/Policy%20Iteration%20Solution.ipynb)\n- [Dynamic Programming Value Iteration](DP/Value%20Iteration%20Solution.ipynb)\n- [Monte Carlo Prediction](MC/MC%20Prediction%20Solution.ipynb)\n- [Monte Carlo Control with Epsilon-Greedy Policies](MC/MC%20Control%20with%20Epsilon-Greedy%20Policies%20Solution.ipynb)\n- [Monte Carlo Off-Policy Control with Importance Sampling](MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb)\n- [SARSA (On Policy TD Learning)](TD/SARSA%20Solution.ipynb)\n- [Q-Learning (Off Policy TD Learning)](TD/Q-Learning%20Solution.ipynb)\n- [Q-Learning with Linear Function Approximation](FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb)\n- [Deep Q-Learning for Atari Games](DQN/Deep%20Q%20Learning%20Solution.ipynb)\n- [Double Deep-Q Learning for Atari Games](DQN/Double%20DQN%20Solution.ipynb)\n- Deep Q-Learning with Prioritized Experience Replay (WIP)\n- [Policy Gradient: REINFORCE with Baseline](PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb)\n- [Policy Gradient: Actor Critic with Baseline](PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb)\n- [Policy Gradient: Actor Critic with Baseline for Continuous Action Spaces](PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n- Deterministic Policy Gradients for Continuous Action Spaces (WIP)\n- Deep Deterministic Policy Gradients (DDPG) (WIP)\n- [Asynchronous Advantage Actor Critic (A3C)](PolicyGradient/a3c)\n\n\n### Resources\n\nTextbooks:\n\n- [Reinforcement Learning: An Introduction (2nd Edition)](http://incompleteideas.net/book/bookdraft2018jan1.pdf)\n\nClasses:\n\n- [David Silver's Reinforcement Learning Course (UCL, 2015)](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n- [CS294 - Deep Reinforcement Learning (Berkeley, Fall 2015)](http://rll.berkeley.edu/deeprlcourse/)\n- [CS 8803 - Reinforcement Learning (Georgia Tech)](https://www.udacity.com/course/reinforcement-learning--ud600)\n- [CS885 - Reinforcement Learning (UWaterloo), Spring 2018](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/)\n\nTalks/Tutorials:\n\n- [Introduction to Reinforcement Learning (Joelle Pineau @ Deep Learning Summer School 2016)](http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/)\n- [Deep Reinforcement Learning (Pieter Abbeel @ Deep Learning Summer School 2016)](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/)\n- [Deep Reinforcement Learning ICML 2016 Tutorial (David Silver)](http://techtalks.tv/talks/deep-reinforcement-learning/62360/)\n- [Tutorial: Introduction to Reinforcement Learning with Function Approximation](https://www.youtube.com/watch?v=ggqnxyjaKe4)\n- [John Schulman - Deep Reinforcement Learning (4 Lectures)](https://www.youtube.com/playlist?list=PLjKEIQlKCTZYN3CYBlj8r58SbNorobqcp)\n- [Deep Reinforcement Learning Slides @ NIPS 2016](http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf)\n\nOther Projects:\n\n- [carpedm20/deep-rl-tensorflow](https://github.com/carpedm20/deep-rl-tensorflow)\n- [matthiasplappert/keras-rl](https://github.com/matthiasplappert/keras-rl)\n\nSelected Papers:\n\n- [Human-Level Control through Deep Reinforcement Learning (2015-02)](http://www.readcube.com/articles/10.1038/nature14236)\n- [Deep Reinforcement Learning with Double Q-learning (2015-09)](http://arxiv.org/abs/1509.06461)\n- [Continuous control with deep reinforcement learning (2015-09)](https://arxiv.org/abs/1509.02971)\n- [Prioritized Experience Replay (2015-11)](http://arxiv.org/abs/1511.05952)\n- [Dueling Network Architectures for Deep Reinforcement Learning (2015-11)](http://arxiv.org/abs/1511.06581)\n- [Asynchronous Methods for Deep Reinforcement Learning (2016-02)](http://arxiv.org/abs/1602.01783)\n- [Deep Reinforcement Learning from Self-Play in Imperfect-Information Games (2016-03)](http://arxiv.org/abs/1603.01121)\n- [Mastering the game of Go with deep neural networks and tree search](https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf)")
('https://raw.githubusercontent.com/oasis-open/cti-python-stix2/f1c1632f3aa916cfa30b0b3625200f01c12dc5ed/docs/guide/taxii.ipynb', None)
('https://raw.githubusercontent.com/LucFrachon/carnd_project1/9b9140794f7f06ed6b347d92f1b0a2ce5eb46fbe/P1.ipynb', '#**Finding Lane Lines on the Road** \n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\n<img src="laneLines_thirdPass.jpg" width="480" alt="Combined Image" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project I detect lane lines in images using Python (3.5) and OpenCV, first using still images, then on movie clips of increasing difficulty (colour of the lines, colour of the road, curves, shadows etc.)\n\n**Step 1:** Getting setup with Python\n\nYou will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies required for this project, makes it easy to install OpenCV, and includes Jupyter Notebook. \n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF="https://www.continuum.io/downloads" target="_blank">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type "help", "copyright", "credits" or "license" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`  \n`> conda install -c menpo opencv3=3.1.0`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  (i.e. did not get an ImportError)\n\n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the "moviepy" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  (i.e. did not get an ImportError)\n\n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF="https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python" target="_blank">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called "P1.ipynb".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  ')
('https://raw.githubusercontent.com/swcarpentry/close-enough-for-scientific-work/ee161b7074f830f69f74820d2b7da6873b57f024/lattice-boltzmann/Main.ipynb', '# Close Enough for Scientific Work\n\nThis repository hosts a collaborative book\nin which scientists show one another how they test their software.\nContributions should be aimed at sophomores in science and engineering,\nsimilar in mathematical complexity to:\n\n*   [Lorena Barba\'s "12 Steps to Navier Stokes"](http://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/)\n*   entries in [the N-body benchmark game](http://benchmarksgame.alioth.debian.org/u32/performance.php?test=nbody)\n*   Software Carpentry\'s [invasion percolation example](http://software-carpentry.org/v4/invperc/index.html)\n\nEach should cover about as much material\nas would normally fit in a one-hour lecture.\nThe format of each entry will vary according to its content,\nbut we expect most will follow this template:\n\n1.  Introduce the problem.\n\n2.  Present a simple solution (100-200 lines of code).\n\n3.  Show how to test that code, explaining:\n    *   what tests have been chosen,\n    *   why they have been chosen,\n    *   why the author chose to implement the tests the way she did, and\n    *   how she decided on the tolerances for those tests.\n\n4.  (Optional) Add a feature, or extend the program in some other way,\n    and show:\n    *   how the tests are extended to handle the change, and\n    *   how the testing pays off.\n\n## Details\n\n1.  Authors will retain the copyright on their work,\n    but all material must be made available under\n    the Creative Commons - Attribution (CC-BY) license,\n    and all software must be made available under the MIT License.\n\n2.  You may use any (widely-used) format and programming language you like ---\n    we will employ a professional editor to handle copy editing and production.\n    However, we ask that you *don\'t*:\n\n    *   explain unit testing, the difference between verification and validation, etc. (we\'ll do that),\n    *   explain how floating point works or that it\'s hard (ditto), or\n    *   use math that even your close colleagues would have to look up.\n\n3.  We would like to have contributions by April 2015,\n    so that we can have the first printed book ready for Fall 2015.\n\n4.  If you would like to contribute, please either:\n\n    *   submit a pull request to this repository that follows the rules described below, or\n    *   [mail Greg Wilson](mailto:gvwilson@software-carpentry.org).\n\n## Layout\n\n> If you are wondering, "Is X allowed?"\n> [mail Greg Wilson](mailto:gvwilson@software-carpentry.org).\n> The answer will almost certainly be "yes",\n> since we would rather have you writing content\n> than worrying about formatting rules.\n\n1.  We will use `gh-pages` as the main branch of our repository\n    rather than `master` so that material is immediately available at\n    [http://swcarpentry.github.io/close-enough-for-scientific-work](http://swcarpentry.github.io/close-enough-for-scientific-work).\n\n2.  Each contribution will be a sub-directory of this repository\n    with a descriptive, hyphenated name,\n    such as `wilson-invasion-percolation`.\n\n3.  That sub-directory must contain a plain text file called `README.txt`\n    with the contributor\'s name\n    and a single paragraph describing the problem to be tackled.\n\n4.  Code, images, and data files may be put in the same sub-directory\n    or in sub-sub-directories as contributors think best.\n\n5.  Finished contributions *must* include a file called `SETUP.txt`\n    that describes how to install any software needed\n    to re-run the code in the chapter.\n\n6.  We strongly prefer vector formats such as SVG for diagrams,\n    plots, etc.\n\n## FAQ\n\n*   *Why not an e-book or a website*?\n\n    We will produce both of those as well.\n\n*   *Why "sized to fit in a one-hour lecture"?*\n\n    To signal that we don\'t want hundred-page contributions,\n    and because we hope people actually will use contributions\n    as lectures in various courses.')
('https://raw.githubusercontent.com/VictorZuanazzi/MachineLearningLab3/3e8303d1f03595c4064eb7f510cdff9158e1b972/12325724_12372773_lab3.ipynb', '# MachineLearningLab3\nSupport vector machines and Unsupervised learning')
('https://raw.githubusercontent.com/zgcanfly/jupyter/425332d0eae643198d2761d14cba08a186962a57/类型注解.ipynb', "# OverVivew \n\t** Python notes **\n## Don't delete the notes, thanks")
('https://raw.githubusercontent.com/zzjf/tensorflow-ios-simple-project/f143790323781e87b45abb96e641de3c9531e617/tensorflow/examples/udacity/6_lstm.ipynb', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_transp.png"><br><br>\n</div>\n\n-----------------\n\n| **`Linux CPU`** | **`Linux GPU`** | **`Mac OS CPU`** | **`Windows CPU`** | **`Android`** |\n|-----------------|---------------------|------------------|-------------------|---------------|\n| [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-cpu)](https://ci.tensorflow.org/job/tensorflow-master-cpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-linux-gpu)](https://ci.tensorflow.org/job/tensorflow-master-linux-gpu) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-mac)](https://ci.tensorflow.org/job/tensorflow-master-mac) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-win-cmake-py)](https://ci.tensorflow.org/job/tensorflow-master-win-cmake-py) | [![Build Status](https://ci.tensorflow.org/buildStatus/icon?job=tensorflow-master-android)](https://ci.tensorflow.org/job/tensorflow-master-android) |\n\n**TensorFlow** is an open source software library for numerical computation using\ndata flow graphs.  The graph nodes represent mathematical operations, while\nthe graph edges represent the multidimensional data arrays (tensors) that flow\nbetween them.  This flexible architecture lets you deploy computation to one\nor more CPUs or GPUs in a desktop, server, or mobile device without rewriting\ncode.  TensorFlow also includes TensorBoard, a data visualization toolkit.\n\nTensorFlow was originally developed by researchers and engineers\nworking on the Google Brain team within Google\'s Machine Intelligence Research\norganization for the purposes of conducting machine learning and deep neural\nnetworks research.  The system is general enough to be applicable in a wide\nvariety of other domains, as well.\n\n**If you want to contribute to TensorFlow, be sure to review the [contribution\nguidelines](CONTRIBUTING.md). This project adheres to TensorFlow\'s\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs. So please see \n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss) for general questions\nand discussion, and please direct specific questions to [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\n## Installation\n*See [Installing TensorFlow](https://www.tensorflow.org/get_started/os_setup.html) for instructions on how to install our release binaries or how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n**Nightly pip packages**\n* We are pleased to announce that TensorFlow now offers nightly pip packages\nunder the [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-gpu](https://pypi.python.org/pypi/tf-nightly-gpu) project on pypi.\nSimply run `pip install tf-nightly` or `pip install tf-nightly-gpu` in a clean\nenvironment to install the nightly TensorFlow build. We support CPU and GPU\npackages on Linux, Mac, and Windows.\n\n\n**Individual whl files**\n* Linux CPU-only: [Python 2](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly-1.head-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/)) / [Python 3.4](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly-1.head-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/)) / [Python 3.5](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly-1.head-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=cpu-slave/))\n* Linux GPU: [Python 2](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/42/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp27-none-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/)) / [Python 3.4](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp34-cp34m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/)) / [Python 3.5](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp35-cp35m-linux_x86_64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/))\n* Mac CPU-only: [Python 2](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-mac/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly-1.head-py2-none-any.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-mac/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/)) / [Python 3](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-mac/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly-1.head-py3-none-any.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-mac/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/))\n* Windows CPU-only: [Python 3.5 64-bit](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly-1.head-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows,PY=35/)) / [Python 3.6 64-bit](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly-1.head-cp36-cp36m-win_amd64.whl) ([build history](http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows,PY=36/))\n* Windows GPU: [Python 3.5 64-bit](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly_gpu-1.head-cp35-cp35m-win_amd64.whl) ([build history](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/)) / [Python 3.6 64-bit](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly_gpu-1.head-cp36-cp36m-win_amd64.whl) ([build history](http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=36/))\n* Android: [demo APK](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow_demo.apk), [native libs](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/native/)\n([build history](https://ci.tensorflow.org/view/Nightly/job/nightly-android/))\n\n#### *Try your first TensorFlow program*\n```shell\n$ python\n```\n```python\n>>> import tensorflow as tf\n>>> hello = tf.constant(\'Hello, TensorFlow!\')\n>>> sess = tf.Session()\n>>> sess.run(hello)\n\'Hello, TensorFlow!\'\n>>> a = tf.constant(10)\n>>> b = tf.constant(32)\n>>> sess.run(a + b)\n42\n>>> sess.close()\n```\n\n## For more information\n\n* [TensorFlow Website](https://www.tensorflow.org)\n* [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n* [TensorFlow Model Zoo](https://github.com/tensorflow/models)\n* [TensorFlow MOOC on Udacity](https://www.udacity.com/course/deep-learning--ud730)\n* [TensorFlow Course at Stanford](https://web.stanford.edu/class/cs20si)\n\nLearn more about the TensorFlow community at the [community page of tensorflow.org](https://www.tensorflow.org/community) for a few ways to participate.\n\n## License\n\n[Apache License 2.0](LICENSE)')
('https://raw.githubusercontent.com/Aakash2206/TwitterAPI_WeRateDogs_Cleaning_Data/31f262c4dd572922a4f1b0e5cadd368467fefb02/wrangle_act.ipynb', '# TwitterAPI_WeRateDogs_Cleaning_Data\nWrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning is required for "Wow!"-worthy analyses and visualizations.\n\n## Software Packages Required\nThe following packages (i.e. libraries) need to be installed.\n- pandas\n- numpy\n- requests\n- tweepy\n- json\n\n## Data\n### Enhanced Twitter Archive\nThe WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet\'s text, which I used to extract rating, dog name, and dog "stage" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive "enhanced."\n\n### Additional Data via the Twitter API\nBack to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter\'s API. Well, "anyone" who has access to data for the 3000 most recent tweets, at least. But we, because we have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+. And guess what? We\'re going to query Twitter\'s API to gather this valuable data.\n\n### Image Predictions File\nEvery image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n\n### Key Points\nKey points to keep in mind when data wrangling for this project:\n\n- We only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n\n- Fully assessing and cleaning the entire dataset requires exceptional effort so only a subset of its issues (eight (8) quality issues and two (2) tidiness issues at minimum) need to be assessed and cleaned.\n\n- Cleaning includes merging individual pieces of data according to the rules of tidy data.\n\n- The fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs.\n\n- We do not need to gather the tweets beyond August 1st, 2017. We can, but note that we won\'t be able to gather the image predictions for these tweets since we don\'t have access to the algorithm used.\n\n## Project Details\nWe will perform the following tasks in this project:\n\nData wrangling, which consists of:\n\n- Gathering data\n\n- Assessing data\n\n- Cleaning data\n\nStoring, analyzing, and visualizing our wrangled data\n\nReporting on 1) data wrangling efforts and 2) data analyses and visualizations')
('https://raw.githubusercontent.com/beaku/fastai/13a76a919ded63e6db38a82e9cd498f49b531216/docs_src/index.ipynb', '[![Build Status](https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai)](https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1)\n[![pypi fastai version](https://img.shields.io/pypi/v/fastai.svg)](https://pypi.python.org/pypi/fastai)\n[![Conda fastai version](https://img.shields.io/conda/v/fastai/fastai.svg)](https://anaconda.org/fastai/fastai)\n\n[![Anaconda-Server Badge](https://anaconda.org/fastai/fastai/badges/platforms.svg)](https://anaconda.org/fastai/fastai)\n[![fastai python compatibility](https://img.shields.io/pypi/pyversions/fastai.svg)](https://pypi.python.org/pypi/fastai)\n[![fastai license](https://img.shields.io/pypi/l/fastai.svg)](https://pypi.python.org/pypi/fastai)\n\n# fastai\n\nThe fastai library simplifies training fast and accurate neural nets using modern best practices. See the [fastai website](https://docs.fast.ai) to get started. The library is based on research into deep learning best practices undertaken at [fast.ai](http://www.fast.ai), and includes \\"out of the box\\" support for [`vision`](https://docs.fast.ai/vision.html#vision), [`text`](https://docs.fast.ai/text.html#text), [`tabular`](https://docs.fast.ai/tabular.html#tabular), and [`collab`](https://docs.fast.ai/collab.html#collab) (collaborative filtering) models. For brief examples, see the [examples](https://github.com/fastai/fastai/tree/master/examples) folder; detailed examples are provided in the full [documentation](https://docs.fast.ai/). For instance, here\'s how to train an MNIST model using [resnet18](https://arxiv.org/abs/1512.03385) (from the [vision example](https://github.com/fastai/fastai/blob/master/examples/vision.ipynb)):\n\n```python\nfrom fastai.vision import *\npath = untar_data(MNIST_PATH)\ndata = image_data_from_folder(path)\nlearn = cnn_learner(data, models.resnet18, metrics=accuracy)\nlearn.fit(1)\n```\n\n## Note for [course.fast.ai](http://course.fast.ai) students\n\nThis document is written for `fastai v1`, which we use for the current version the [course.fast.ai](http://course.fast.ai) deep learning courses. If you\'re following along with a course at [course18.fast.ai](http://course18.fast.ai) (i.e. the machine learning course, which isn\'t updated for v1) you need to use `fastai 0.7`;  please follow the installation instructions [here](https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652).\n\n## Installation\n\n**NB:** *fastai v1 currently supports Linux only, and requires **PyTorch v1** and **Python 3.6** or later. Windows support is at an experimental stage: it should work fine but it\'s much slower and less well tested. Since Macs don\'t currently have good Nvidia GPU support, we do not currently prioritize Mac development.*\n\n`fastai-1.x` can be installed with either `conda` or `pip` package managers and also from source. At the moment you can\'t just run *install*, since you first need to get the correct `pytorch` version installed - thus to get `fastai-1.x` installed choose one of the installation recipes below using your favorite python package manager. Note that **PyTorch v1** and **Python 3.6** are the minimal version requirements.\n\nIt\'s highly recommended you install `fastai` and its dependencies in a virtual environment ([`conda`](https://conda.io/docs/user-guide/tasks/manage-environments.html) or others), so that you don\'t interfere with system-wide python packages. It\'s not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for `fastai`.\n\nStarting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But [you can install the cpu build too](https://docs.fast.ai/install.html#cpu-build).\n\nIf you experience installation problems, please read about [installation issues](https://github.com/fastai/fastai/blob/master/README.md#installation-issues).\n\nIf you are planning on using `fastai` in the jupyter notebook environment, make sure to also install the corresponding [packages](https://docs.fast.ai/install.html#jupyter-notebook-dependencies).\n\nMore advanced installation issues, such as installing only partial dependencies are covered in a dedicated [installation doc](https://docs.fast.ai/install.html).\n\n### Conda Install\n\n```bash\nconda install -c pytorch -c fastai fastai\n```\n\nThis will install the `pytorch` build with the latest `cudatoolkit` version. If you need a higher or lower `CUDA XX` build (e.g. CUDA 9.0), following the instructions [here](https://pytorch.org/get-started/locally/), to install the desired `pytorch` build.\n\nNote that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):\n\n```bash\nconda uninstall --force jpeg libtiff -y\nconda install -c conda-forge libjpeg-turbo\nCC="cc -mavx2" pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd\n```\nIf you only care about faster JPEG decompression, it can be `pillow` or `pillow-simd` in the last command above, the latter speeds up other image processing operations. For the full story see [Pillow-SIMD](https://docs.fast.ai/performance.html#faster-image-processing).\n\n### PyPI Install\n\n```bash\npip install fastai\n```\n\nBy default pip will install the latest `pytorch` with the latest `cudatoolkit`. If your hardware doesn\'t support the latest `cudatoolkit`, follow the instructions [here](https://pytorch.org/get-started/locally/), to install a `pytorch` build that fits your hardware.\n\n### Bug Fix Install\n\nIf a bug fix was made in git and you can\'t wait till a new release is made, you can install the bleeding edge version of `fastai` with:\n\n```\npip install git+https://github.com/fastai/fastai.git\n```\n\n### Developer Install\n\nThe following instructions will result in a [pip editable install](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs), so that you can `git pull` at any time and your environment will automatically get the updates:\n\n```bash\ngit clone https://github.com/fastai/fastai\ncd fastai\ntools/run-after-git-clone\npip install -e ".[dev]"\n```\n\nNext, you can test that the build works by starting the jupyter notebook:\n\n```bash\njupyter notebook\n```\nand executing an example notebook. For example load `examples/tabular.ipynb` and run it.\n\nPlease refer to [CONTRIBUTING.md](https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md) and [Notes For Developers](https://docs.fast.ai/dev/develop.html) for more details on how to contribute to the `fastai` project.\n\n\n\n\n### Building From Source\n\nIf for any reason you can\'t use the prepackaged packages and have to build from source, this section is for you.\n\n1. To build `pytorch` from source follow the [complete instructions](https://github.com/pytorch/pytorch#from-source). Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into `pytorch`.\n\n2. Next, you will also need to build `torchvision` from source:\n\n   ```bash\n   git clone https://github.com/pytorch/vision\n   cd vision\n   python setup.py install\n   ```\n\n3. When both `pytorch` and `torchvision` are installed, first test that you can load each of these libraries:\n\n   ```bash\n   import torch\n   import torchvision\n   ```\n\n   to validate that they were installed correctly\n\n   Finally, proceed with `fastai` installation as normal, either through prepackaged pip or conda builds or installing from source ("the developer install") as explained in the sections above.\n\n\n\n## Installation Issues\n\nIf the installation process fails, first make sure [your system is supported](https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported). And if the problem is still not addressed, please refer to the [troubleshooting document](https://docs.fast.ai/troubleshoot.html).\n\nIf you encounter installation problems with conda, make sure you have the latest `conda` client (`conda install` will do an update too):\n```bash\nconda install conda\n```\n\n### Is My System Supported?\n\n1. Python: You need to have python 3.6 or higher\n\n2. CPU or GPU\n\n   The `pytorch` binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don\'t have to install system-wide NVIDIA\'s CUDA and related libraries if you don\'t need them for something else. If you have them installed already it doesn\'t matter which NVIDIA\'s CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use `pytorch` build with CUDA 10.0 libraries without any problem, since the `pytorch` binary package is self-contained.\n\n   The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running `nvidia-smi`. While it\'s possible that this application is not available on your system, it\'s very likely that if it doesn\'t work, then you don\'t have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.\n\n3. Operating System:\n\n   Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.\n\n   As of this moment pytorch.org\'s 1.0 version supports:\n\n    | Platform | GPU    | CPU    |\n    |----------|--------|--------|\n    | linux    | binary | binary |\n    | mac      | source | binary |\n    | windows  | binary | binary |\n\n   Legend: `binary` = can be installed directly, `source` = needs to be built from source.\n\n   If there is no `pytorch` preview conda or pip package available for your system, you may still be able to [build it from source](https://pytorch.org/get-started/locally/).\n\n4. How do you know which pytorch cuda version build to choose?\n\n   It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built `pytorch` releases:\n\n    | CUDA Toolkit | NVIDIA (Linux x86_64) |\n    |--------------|-----------------------|\n    | CUDA 10.0    | >= 410.00             |\n    | CUDA 9.0     | >= 384.81             |\n    | CUDA 8.0     | >= 367.48             |\n\n   So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.\n\n   You can find a complete table with all variations [here](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html).\n\n   If you use NVIDIA driver 410+, you most likely want to install the `cudatoolkit=10.0` pytorch variant, via:\n   ```bash\n   conda install -c pytorch pytorch cudatoolkit=10.0\n   ```\n   or if you need a lower version, use one of:\n   ```bash\n   conda install -c pytorch pytorch cudatoolkit=8.0\n   conda install -c pytorch pytorch cudatoolkit=9.0\n   ```\n   For other options refer to the complete list of [the available pytorch variants](https://pytorch.org/get-started/locally/).\n\n## Updates\n\nIn order to update your environment, simply install `fastai` in exactly the same way you did the initial installation.\n\nTop level files `environment.yml` and `environment-cpu.yml` belong to the old fastai (0.7). `conda env update` is no longer the way to update your `fastai-1.x` environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under `old/`.\n\n## Contribution guidelines\n\nIf you want to contribute to `fastai`, be sure to review the [contribution guidelines](https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md). This project adheres to fastai\'s [code of conduct](https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md). By participating, you are expected to uphold this code.\n\nWe use GitHub issues for tracking requests and bugs, so please see [fastai forum](https://forums.fast.ai/) for general questions and discussion.\n\nThe fastai project strives to abide by generally accepted best practices in open-source software development:\n\n## History\n\nA detailed history of changes can be found [here](https://github.com/fastai/fastai/blob/master/CHANGES.md).\n\n## Copyright\n\nCopyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project\'s files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.')
('https://raw.githubusercontent.com/RobFirth/pycoco/1c9be662b6dd67f8be5423568bb3ecbff2979492/notebooks/CoCo_SN2007uy_Test.ipynb', '# **`pycoco`**\n___\n\n## v0.9.17\n___\n[![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](http://www.astropy.org/) [![Build Status](https://travis-ci.org/RobFirth/pycoco.svg?branch=master)](https://travis-ci.org/RobFirth/pycoco)[![DOI](https://zenodo.org/badge/74136059.svg)](https://zenodo.org/badge/latestdoi/74136059)\n___\nThis is the development repo for the python frontend for the core-collapse SNe template code \'CoCo\':  https://github.com/UoS-SNe/CoCo\n(my fork is currently https://github.com/RobFirth/CoCo).\n\nCoCo was originally started by Natasha Karpenka, and is currently being updated and maintained by S. Prajs (https://github.com/SzymonPrajs).\n\nA paper, Firth et. al. 2017, is currently in prep.\n___\n\n * Extending spectra with "donor" spectra\n\n * Implemented a raft of changes that make it easier to interact with the spectral fits and the final templates\n\n * fixed a particularly nasty bug (https://github.com/RobFirth/pycoco/issues/28) that was screwing up specphot (via\n filter resampling)\n\n * Updated notebook tarball - (again 17/12/17)\n\n * Now available on PyPi and via `pip` - package available to install as `pycocosn`\n \n * Extending templates now possible with Black Body spectrum, flat and linear\n\n * calling CoCo (or reproduced CoCo functions) now more straightforward\n\n * Added more test cases to test_pycoco\n\n * Travis-CI implemented and Master and dev are passing. Fixed bug with ENVIRONS.\n\n * Dependancies now handled in setup.py\n\n * Calling all parts of CoCo from pycoco now operational\n\n * Migrated to better, and clearer code structure and sub-modules\n\n * Mangling now done within python, rather than C++ in CoCo\n\n * File I/O and interaction with `CoCo LCfit` output now operational - 11/01/17\n\n * Usage of a `SN` class now solid\n\n * Implementing calls to `CoCo LCfit` now\n\n * Calls to `CoCo specfit` now implemented\n\n * Mangling now stable\n\n * Adding tools for calculating magnitude offsets to make it easier to import new data\n\n * regeneration of filter list file now possible\n\n * added colours and bandpasses for LSST filters\n\n * Less dependence on environment variables\n\n * installation through `setup.py` now possible\n\n * better handing of CoCo sim outputs\n\n * SN position and mu now stored in infofile `./testdata/info/info.dat`\n\n * dark sky calculations and integration with LSST Throughputs now done\n\n * Improved stability\n\n * input from `astropy tables` now more straightforward - better integration with `coco.simulate`\n\n * can now batch fit light curves and spectra from within python\n___\n\nTo install:\n\n```\npip install pycocosn\n\n```\n\nTo install from source:\n\n```\ngit clone https://github.com/RobFirth/verbose-enigma.git\n```\n\nthen:\n\n```\ncd verbose-enigma\npython setup.py install --user\n```\n\n(The --user argument only installs current user only, omitting flag will install for all users on the system if there are appropriate permissions)\n\n_NOTE: make sure that the python used to install is the one that you will use with pycoco_\n___\n\nIdeally set the following environment variables:\n\n`COCO_ROOT_DIR` (my default is `~/Code/CoCo/`)\n`PYCOCO_FILTER_DIR`(my default is `~/Code/CoCo/data/filters/`)\n`PYCOCO_DATA_DIR` (my default is `~/Code/CoCo/data/`)\n`SFD_DIR` (my default is `~/data/Dust/sfddata-master/`; see below)\n`LSST_THROUGHPUTS` (my default is `${HOME}/projects/LSST/throughputs`)\n`LSST_THROUGHPUTS_BASELINE` (my default is `${LSST_THROUGHPUTS}/baseline`)\n\nalso `pycoco/` and `CoCo` need to be in your path and pythonpath, i.e.:\n\n ```\n setenv PATH /Users/berto/Code/pycoco/:$PATH\n\n setenv PYTHONPATH "/Users/berto/Code/pycoco/:$PYTHONPATH\n ```\n\n___\n\n# Requirements\n## python packages\n\n* matplotlib\n* numpy\n* scipy\n* astropy\n* lmfit\n* sfdmap (https://github.com/kbarbary/sfdmap)\n\n# additionally\n\n* lsst throughputs (https://github.com/lsst/throughputs; `$LSST_THROUGHPUTS` points to this directory)\n___\n\n\nfor `sfdmap`, the environment variable `SFD_DIR` needs to point at the path to the parent directory of the appropriate dust map files. See the installation instructions here: https://github.com/kbarbary/sfdmap\n___  \n\n\n## Known Problems -\n\nIf using in an environment (i.e. through (ana)conda) on Mac and you see the following:\n\n```\n~/anaconda3/lib/python3.6/site-packages/pycocosn-0.9.6-py3.6.egg/pycoco/__init__.py in <module>()\n     24 from . import extinction\n     25 from . import colours\n---> 26 from . import utils\n     27 from . import errors\n     28 from . import kcorr\n\n~/anaconda3/lib/python3.6/site-packages/pycocosn-0.9.6-py3.6.egg/pycoco/utils.py in <module>()\n     12 import warnings\n     13\n---> 14 import matplotlib.pyplot as plt\n     15 from astropy import units as u\n     16 from astropy.table import Table, Column\n\n~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py in <module>()\n    111 ## Global ##\n    112\n--> 113 _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()\n    114\n    115 _IP_REGISTERED = None\n\n~/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py in pylab_setup(name)\n     58     # imports. 0 means only perform absolute imports.\n     59     backend_mod = __import__(backend_name, globals(), locals(),\n---> 60                              [backend_name], 0)\n     61\n     62     # Things we pull in from all backends\n\n~/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py in <module>()\n     17\n     18 import matplotlib\n---> 19 from matplotlib.backends import _macosx\n     20\n     21 from .backend_agg import RendererAgg, FigureCanvasAgg\n\nRuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of \'python\' with \'pythonw\'. See \'Working with Matplotlib on OSX\' in the Matplotlib FAQ for more information.\n```\nyou need to swap the default backend.\n\nIf you have installed the pip matplotlib, there is a directory in your root called ``~/.matplotlib`.\n\nCreate a file ``~/.matplotlib/matplotlibrc` there and add the following:\n`backend: TkAgg`')
('https://raw.githubusercontent.com/sandeeppaulraj/artificial-intelligence/1af2e6f2908f5f898b8373703a72a41cd0548ec9/Projects/4_HMM%20Tagger/HMM%20Tagger.ipynb', '# Artificial Intelligence Nanodegree Program Resources\n\n## Classroom Exercises\n\n### 1. Constraint Satisfaction Problems\nIn this exercise you will explore Constraint Satisfaction Problems in a Jupyter notebook and use a CSP solver to solve a variety of problems.\n\nRead more [here](/Exercises/1_Constraint%20Satisfaction)\n\n\n### 2. Classical Search for PacMan (only in classroom)\n\n**Please DO NOT publish your work on this exercise.**\n\nIn this exercise you will teach Pac-Man to search his world to complete the following tasks:\n* find a single obstacle\n* find multiple obstacles\n* find the fastest way to eat all the food in the map\n\n\n### 3. Local Search Optimization\n\nIn this exercise, you\'ll implement several local search algorithms and test them on the [Traveling Salesman Problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) (TSP) between a few dozen US state capitals.\n\n\n## Projects\n\n### 1. Sudoku Solver\nIn this project, you will extend the Sudoku-solving agent developed in the classroom lectures to solve diagonal Sudoku puzzles and implement a new constraint strategy called "naked twins". A diagonal Sudoku puzzle is identical to traditional Sudoku puzzles with the added constraint that the boxes on the two main diagonals of the board must also contain the digits 1-9 in each cell (just like the rows, columns, and 3x3 blocks).\n\nRead more [here](/Projects/1_Sudoku)\n\n\n### 2. Classical Planning\nThis project is split between implementation and analysis. First you will combine symbolic logic and classical search to implement an agent that performs progression search to solve planning problems. Then you will experiment with different search algorithms and heuristics, and use the results to answer questions about designing planning systems.\n\nRead more [here](/Projects/2_Classical%20Planning)\n\n\n### 3. Game Playing\nIn this project you will choose an experiment with adversarial game-playing techniques like minimax, Monte Carlo tree search, opening books, and more. Your goal will be to build and evaluate the performance of your agent in a finite deterministic two player game of perfect information called Isolation.\n\nRead more [here](/Projects/3_Adversarial%20Search)\n\n\n### 4. Part of Speech Tagger\n\nIn this notebook, you\'ll use the Pomegranate library to build a hidden Markov model for part of speech tagging with a universal tagset. Hidden Markov models have been able to achieve >96% tag accuracy with larger tagsets on realistic text corpora. Hidden Markov models have also been used for speech recognition and speech generation, machine translation, gene recognition for bioinformatics, and human gesture recognition for computer vision, and more.\n\nRead more [here](/Projects/4_HMM%20Tagger)')
('https://raw.githubusercontent.com/asnatm/NLPHW2/cc0216c19c6fa883114f1ca3f1b65c768f04c7ac/plot_out_of_core_classification.ipynb', '# NLPHW2')
('https://raw.githubusercontent.com/learn-co-students/python-lists-lab-data-science-intro-000/f10b03762354d09ec0ee34fbb31abff7761b3360/index.ipynb', '# Lists Lab \n\n### Introduction\n\nOk, so now that we have a sense of how to read and alter a list in Python, let\'s put this knowledge to use. \n\n### Objectives\n\n* Practice reading one and multiple elements from lists\n* Practice altering data in lists\n* Practice adding elements and removing elements from lists\n\n### Our initial data structure \n\nIn the previous lesson, we had a list of top travel cities.\n\n\n```python\ntop_travel_cities = [\'Solta\', \'Greenville\', \'Buenos Aires\', \'Los Cabos\', \'Walla Walla Valley\', \'Marakesh\', \'Albuquerque\', \'Archipelago Sea\', \'Iguazu Falls\', \'Salina Island\', \'Toronto\', \'Pyeongchang\']\n```\n\n> Remember to press shift+enter to run each gray block of code (including the one above).  Otherwise, the variables will not be defined.\n\nIn this lesson we will work with a list of associated countries corresponding to each of the top travel cities.\n\n\n```python\ncountries = [\'Croatia\',\n \'USA\',\n \'Argentina\',\n \'Mexico\',\n \'USA\',\n \'Morocco\',\n \'New Mexico\',\n \'Finland\',\n \'Argentina\',\n \'Italy\',\n \'Canada\',\n \'South Korea\']\n```\n\n> Run the code in the cell above by pressing shift + enter.\n\nOk, so the list of countries associated with each city has been assigned to the variable `countries`.  Now we will work with reading and manipulating this list.\n\n### Accessing elements from lists\n\nFirst, set the variable `italy` to be equal to the third to last element from `countries`.  \n>**Note:** If you see an **error** stating that `countries` is undefined, it means you must press shift+enter in the second gray box where `countries` variable is assigned.\n\n\n```python\nitaly = None # \'Italy\'\nitaly\n```\n\n> We assign the varible `italy` equal to `None`, but you should change the word `None` to code that uses the `countries` list to assign `italy` to `\'Italy\'`.  We wrote the variable `italy` a second time, so that you can see what it equals when you run the code block.  Currently, nothing is displayed below as it equals `None`, but when it\'s correct it will match the string which is commented out, `\'Italy\'`.\n\n\n```python\nitaly # \'Italy\'\n```\n\nNow access the fourth element and set it equal to the variable `mexico`.\n\n\n```python\nmexico = None\nmexico\n```\n\nNotice that the second through fifth elements are all in a row and all in the Western Hemisphere.  Assign that subset of elements to a variable called `kindof_neighbors`.\n\n\n```python\nkindof_neighbors = None\nkindof_neighbors\n```\n\n### Changing Elements\n\nOk, now let\'s add a couple of countries onto this list.  At the end of the list, add the country \'Malta\'.\n\n\n```python\nNone # add code here\n```\n\nThen add the country \'Thailand\'.\n\n\n```python\nNone # add code here\n```\n\nNow your list of countries should look like the following.\n\n\n```python\ncountries \n# [\'Croatia\', \'USA\', \'Argentina\', \'Mexico\', \'USA\', \'Morocco\', \'New Mexico\', \'Finland\', \n# \'Argentina\', \'Italy\',  \'Canada\', \'South Korea\',  \'Malta\',  \'Thailand\']\n```\n\nYou may have noticed that "New Mexico" is included in our list of countries.  That doesn\'t seem right.  Let\'s change \'New Mexico\' to \'USA\'.\n\n\n```python\ncountries = None # add code here\n```\n\n\n```python\ncountries \n# [\'Croatia\', \'USA\', \'Argentina\', \'Mexico\', \'USA\', \'Morocco\', \'USA\', \'Finland\', \n# \'Argentina\', \'Italy\',  \'Canada\', \'South Korea\',  \'Malta\',  \'Thailand\']\n```\n\nFinally, let\'s remove Thailand from the list.  No good reason, we\'re acting on whimsy.\n\n\n```python\ncountries = [\'Croatia\',\n \'USA\',\n \'Argentina\',\n \'Mexico\',\n \'USA\',\n \'Morocco\',\n \'USA\',\n \'Finland\',\n \'Argentina\',\n \'Italy\',\n \'Canada\',\n \'South Korea\', \n \'Malta\', \n \'Thailand\']\ncountries.pop() # \'Thailand\'\ncountries\n# [\'Croatia\', \'USA\', \'Argentina\', \'Mexico\', \'USA\', \'Morocco\', \'USA\', \'Finland\',  \'Argentina\', \'Italy\', \'Canada\', \'South Korea\',  \'Malta\']\n```\n\n### Exploring Lists with Methods\n\nOk, now we notice that some countries are mentioned more than once.  Let\'s see how many repeat countries are on this list.  \n\nFirst, use the `set` and `list` functions to return a unique list of countries.  Set this list equal to the variable `unique_countries`.\n\n\n```python\nunique_countries = None\n```\n\n\n```python\nunique_countries  # [\'Croatia\', \'Argentina\', \'Canada\', \'Mexico\', \'Italy\', \n#\'South Korea\', \'USA\', \'Morocco\', \'Malta\', \'Finland\'] Note: order of countries may be different\n```\n\nNow the number of repeat countries should be the number of countries minus the number of unique countries.  So use the `len` function on both `unique_countries` and `countries` to calculate this and assign the result to the variable `num_of_repeats`.\n\n\n```python\nnum_of_repeats = None\nnum_of_repeats # 3\n```\n\n### Summary\n\nIn this lesson, we had some practice with working with lists in Python.  We saw how to add and remove elements from a list, as well as select specific elements.  Finally, we saw how to use a different data structure to calculate the number of unique elements in the list.')
('https://raw.githubusercontent.com/shonxg/Selfdriving/bb83eb807ee6046aac93a737113a6a9e341b778b/traffic-signs/Traffic_Signs_Recognition.ipynb', None)
('https://raw.githubusercontent.com/maneesh-chouksey/rnn-sequence-generator/dade5f20806f04b6d49d7bf7d62d6fef33fbb525/RNN_project.ipynb', '# Recurrent Neural Networks course project: time series prediction and text generation')
('https://raw.githubusercontent.com/lealldiogo/CarND-Traffic-Sign-Classifier-Project/e629044c0b68fd6992a0ff3e582bb2cd63a0acc3/Traffic_Sign_Classifier.ipynb', '## Project: Build a Traffic Sign Recognition Program\n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\nOverview\n---\nIn this project, you will use what you\'ve learned about deep neural networks and convolutional neural networks to classify traffic signs. You will train and validate a model so it can classify traffic sign images using the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset). After the model is trained, you will then try out your model on images of German traffic signs that you find on the web.\n\nWe have included an Ipython notebook that contains further instructions \nand starter code. Be sure to download the [Ipython notebook](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb). \n\nWe also want you to create a detailed writeup of the project. Check out the [writeup template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document.\n\nTo meet specifications, the project will require submitting three files: \n* the Ipython notebook with the code\n* the code exported as an html file\n* a writeup report either as a markdown or pdf file \n\nCreating a Great Writeup\n---\nA great writeup should include the [rubric points](https://review.udacity.com/#!/rubrics/481/view) as well as your description of how you addressed each point.  You should include a detailed description of the code used in each step (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples.  \n\nAll that said, please be concise!  We\'re not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). \n\nYou\'re not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup.\n\nThe Project\n---\nThe goals / steps of this project are the following:\n* Load the data set\n* Explore, summarize and visualize the data set\n* Design, train and test a model architecture\n* Use the model to make predictions on new images\n* Analyze the softmax probabilities of the new images\n* Summarize the results with a written report\n\n### Dependencies\nThis lab requires:\n\n* [CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit)\n\nThe lab environment can be created with CarND Term1 Starter Kit. Click [here](https://github.com/udacity/CarND-Term1-Starter-Kit/blob/master/README.md) for the details.\n\n### Dataset and Repository\n\n1. Download the data set. The classroom has a link to the data set in the "Project Instructions" content. This is a pickled dataset in which we\'ve already resized the images to 32x32. It contains a training, validation and test set.\n2. Clone the project, which contains the Ipython notebook and the writeup template.\n```sh\ngit clone https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project\ncd CarND-Traffic-Sign-Classifier-Project\njupyter notebook Traffic_Sign_Classifier.ipynb\n```\n\n### Requirements for Submission\nFollow the instructions in the `Traffic_Sign_Classifier.ipynb` notebook and write the project report using the writeup template as a guide, `writeup_template.md`. Submit the project code and writeup document.')
('https://raw.githubusercontent.com/learn-co-students/plotting-data-readme-dc-ds-career-012919/0720bd85bfba60858c4f78162d636a902a0ee330/index.ipynb', "# Visualizing Data\n\n### Learning Objectives\n\n* Get a sense of how to use a graphing library, like Plotly, to answer questions about how to visualize our data\n* Understand how our dictionary and list data structures can represent graphical information\n\n### Introduction\n\nWe have spent the last few sections introducing ourselves to various data structures in Python.  In this section, we will see how we can use these same data structures to display our data with the help of a library, Plotly.\n\n### Working with Plotly\n\nThere are various Python visualization tools we can use to display our data. In this lesson, we will be using Plotly, as it produces nice looking graphs and is easy to work with.  \n\nWe can easily download the `plotly` library with the use of `pip`.  \n\n> Pip is a package management system that allows us to easily download and install libraries written in Python.  If you are working on Learn, we have already installed pip for you.  We will not walk through installing pip here, however you can find instructions on installing pip [for Mac](http://softwaretester.info/install-and-upgrade-pip-on-mac-os-x/) or [for Windows](https://www.youtube.com/results?search_query=instally+pip+windows) online.  Also, if you are familiar with working with a terminal and have `easy_install`, you can run `sudo easy_install pip` from the terminal.\n\nTo install a package with pip, the general pattern is to run `pip install` followed by the name of the package.  We generally do this from a terminal (whatever that is), but you can also install packages directly from Jupyter, like so:\n\n>**Note:** *You can ignore any messages below that say requirement already satisfied.*\n\n\n```python\n!pip install plotly==3.3.0\n```\n\n> Remember to use shift + enter to run the line above.\n\nNow we have `plotly` on our computer.  The next step is to get it into this notebook.  We do so with the following two lines.\n\n\n```python\nimport plotly\n\nplotly.offline.init_notebook_mode(connected=True)\n# use offline mode to avoid initial registration\n```\n\nIn the code above, we bring in the `plotly` library by using the keyword `import` followed by our library name, `plotly`.  Then we call the method `plotly.offline.init_notebook_mode(connected=True)` so that we do not have to connect plotly to a registered account online.  If you are wondering how to we know all of this, you simply ask Google.\n\n![](./plotly-no-account.png)\n\nOk, now let's use plotly to build our first graph.\n\nFirst, we create a new dictionary and assign it to `trace0`. Then we set `x` key that points to a list of $x$ values.  Similarly, we create a `y` key with a value of a list of $y$ values.  \n\n\n```python\ntrace0 = {'type': 'bar', 'x': ['jack', 'jill', 'sandy'], 'y': [8, 11, 10]}\ntrace0\n```\n\nNow we plot our graph by calling the `plotly.offline.iplot` method and passing through a list of traces to `iplot` method.\n\n\n```python\ntrace0 = {'type': 'bar', 'x': ['jack', 'jill', 'sandy'], 'y': [8, 11, 10]}\n\n\nplotly.offline.iplot([trace0])\n```\n\nIt may be confusing understanding what a trace is, and how it is different from a plot. The easiest way to explain it is maybe to show how the `iplot` method takes in a list of two traces, instead of just one.\n\n\n```python\ntrace0 = {'type': 'bar', 'x': ['jack', 'jill', 'sandy', 'blaise'], 'y': [8, 11, 8, 13, 6, 4]}\ntrace1 = {'type': 'bar', 'x': ['jack', 'jill', 'sandy', 'gob'], 'y': [4, 12, 3, 14, 8, 1]}\n\n\nplotly.offline.iplot([trace0, trace1])\n```\n\nAs we can see, each trace is an associated collection of data, and a plot can display more than one trace.\n\n### Summary\n\nIn this section, we saw how to use data visualizations to better understand the data.\n\nTo display the data with `plotly` we need to do a couple of things.  First, we installed plotly by going to our terminal and running `pip install plotly`.  Then to use the library, we import the `plotly` library into our notebook.  Once the library is loaded in our notebook, it's time to use it.  We create a new dictionary with keys of $x$ and $y$, with each key pointing to an array of the $x$ or $y$ values of our points.  We can pass through a list of traces, with each trace representing associated data.")
('https://raw.githubusercontent.com/leandroohf/LDA/7d31fe2501a41bebdf809341569a1210877da102/intro_fishers_lda.ipynb', None)
('https://raw.githubusercontent.com/jonad/TrafficSignDetection/024e063f4b776c2489d2a0f7edcaa092961e8cca/Traffic_Sign_Classifier.ipynb', '## Project: Build a Traffic Sign Recognition Program\n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\n![Network](./network_img/model_archittecture.png)\n![Network](./network_img/loss_accuracy.png)')
('https://raw.githubusercontent.com/STILLSHI/MLND/3357071b1dbee879a88a034dbb34fd5f3c2b3685/projects/smartcab/smartcab.ipynb', '# machine-learning\nContent for Udacity\'s Machine Learning curriculum, which includes projects and their descriptions.\n\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.')
('https://raw.githubusercontent.com/broadinstitute/wot/ca5e94f05699997b01cf5ae13383f9810f0613f6/notebooks/Notebook-2-compute-transport-maps.ipynb', 'Read the [documentation](http://broadinstitute.github.io/wot).')
('https://raw.githubusercontent.com/VictorZuanazzi/MachineLearningLab2/7e2d5938d6fe101eb4d3ed897a7409b1e644a8ed/12325724_12372773_lab2.ipynb', '# MachineLearningLab2\n# Lab 2: Classification \n### Machine Learning 1, November 2018  \nNotes on implementation:  \n* You should write your code and answers in this IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact your teaching assistant.\n* Please write your answers right below the questions. \n* Among the first lines of your notebook should be "%pylab inline". This imports all required modules, and your plots will appear inline. \n* Use the provided test cells to check if your answers are correct\n* **Make sure your output and plots are correct before handing in your assignment with Kernel -> Restart &amp; Run All**  \n* **If possible, all your implementations should be vectorized and rely on loops as little as possible. Therefore for some questions, we give you a maximum number of loops that are necessary for an efficient implementation. This number refers to the loops in this particular function and does not count the ones in functions that are called from the function. You should not go above this number for the maximum number of points.**')
('https://raw.githubusercontent.com/lancerdancer/CarND-TensorFlow-Lab/0be8887bd01cb878dcb6f640995e2f43b2e92abc/lab.ipynb', None)
('https://raw.githubusercontent.com/Alezzo1592/AA/276d7196b68cd5d1e332ce938a01aa69770622e4/TP1/TP1.ipynb', '# AA')
('https://raw.githubusercontent.com/marthtz/CarND-T1P1-LaneLines/484b3430995ff58d10f61799d0c0a346a049c34a/P1.ipynb', '#**Finding Lane Lines on the Road** \n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\n<img src="laneLines_thirdPass.jpg" width="480" alt="Combined Image" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means "Open-Source Computer Vision", which is a package that has many useful tools for analyzing images.  \n\n## If you have already installed the [CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit/blob/master/README.md) you should be good to go!   If not, you can install the starter kit or follow the install instructions below to get started on this project. ##\n\n**Step 1:** Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if you\'re getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF="https://www.continuum.io/downloads" target="_blank">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type "help", "copyright", "credits" or "license" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`  \n`> conda install -c menpo opencv3=3.1.0`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  (i.e. did not get an ImportError)\n\n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the "moviepy" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  (i.e. did not get an ImportError)\n\n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF="https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python" target="_blank">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called "P1.ipynb".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  ')
('https://raw.githubusercontent.com/vdabravolski/machine-learning/77066bae4079993d35bb8d8e0265b2015e5bef21/projects/finding_donors/finding_donors.ipynb', '# machine-learning\nContent for Udacity\'s Machine Learning curriculum, which includes projects and their descriptions.\n\n<<<<<<< HEAD\nPlease use the `projects` directory for English-language content. Use the `projects_cn` directory for Chinese-language content (for international students).\n\n=======\n>>>>>>> 489bd273bbfad03bd880e00b356660c530f56b85\n<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. Please refer to [Udacity Terms of Service](https://www.udacity.com/legal) for further information.')
('https://raw.githubusercontent.com/trixxupthesleeave/DriveSim/e82b7e13ca9ab4c4635bf636976c0fca328b2b5c/SupportVector.py.ipynb', '# DriveSim\nPlaying around with ML tools a bit. ')
('https://raw.githubusercontent.com/sabbirahm3d/linkin-park-lyrical-analysis/0268616d2661bbb2d1bf4a6fa95c56f6926efcd5/notebooks/linkin-park-analysis.ipynb', '# Linkin Park Lyrical Analysis\n\n[Link](http://sabbirsphere.com/linkin-park-lyrical-analysis/)\n\nSome exploratory analysis on Linkin Park lyrics. This analysis is an attempt to uncover the underlying emotions behind their songs by considering the sentiment score and other features.')
('https://raw.githubusercontent.com/thenielda/Principles-of-Machine-Learning-R/dd3424b987a8d049ceb3c041f55022ffda2f9f1a/Module4/IntroductionToRegression.ipynb', '# Principles-of-Machine-Learning-R\nPrinciples of Machine Learning R')
('https://raw.githubusercontent.com/liyaoran/Fastai/cc90b850fa0559552d62c2cd45086b013203fea1/docs_src/callbacks.ipynb', '[![Build Status](https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai)](https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1)\n[![pypi fastai version](https://img.shields.io/pypi/v/fastai.svg)](https://pypi.python.org/pypi/fastai)\n[![Conda fastai version](https://img.shields.io/conda/v/fastai/fastai.svg)](https://anaconda.org/fastai/fastai)\n\n[![Anaconda-Server Badge](https://anaconda.org/fastai/fastai/badges/platforms.svg)](https://anaconda.org/fastai/fastai)\n[![fastai python compatibility](https://img.shields.io/pypi/pyversions/fastai.svg)](https://pypi.python.org/pypi/fastai)\n[![fastai license](https://img.shields.io/pypi/l/fastai.svg)](https://pypi.python.org/pypi/fastai)\n\n# fastai\n\nThe fastai library simplifies training fast and accurate neural nets using modern best practices. See the [fastai website](https://docs.fast.ai) to get started. The library is based on research into deep learning best practices undertaken at [fast.ai](http://www.fast.ai), and includes \\"out of the box\\" support for [`vision`](https://docs.fast.ai/vision.html#vision), [`text`](https://docs.fast.ai/text.html#text), [`tabular`](https://docs.fast.ai/tabular.html#tabular), and [`collab`](https://docs.fast.ai/collab.html#collab) (collaborative filtering) models. For brief examples, see the [examples](https://github.com/fastai/fastai/tree/master/examples) folder; detailed examples are provided in the full [documentation](https://docs.fast.ai/). For instance, here\'s how to train an MNIST model using [resnet18](https://arxiv.org/abs/1512.03385) (from the [vision example](https://github.com/fastai/fastai/blob/master/examples/vision.ipynb)):\n\n```python\nuntar_data(MNIST_PATH)\ndata = image_data_from_folder(MNIST_PATH)\nlearn = cnn_learner(data, tvm.resnet18, metrics=accuracy)\nlearn.fit(1)\n```\n\n## Note for [course.fast.ai](http://course.fast.ai) students\n\nThis document is written for `fastai v1`, which we use for the current, third version of part 1 of the [course.fast.ai](http://course.fast.ai) deep learning course. If you\'re following along with a course at [course18.fast.ai](http://course18.fast.ai)&mdash;that is, part 2 of the deep learning course, or the machine learning course (which aren\'t yet updated for v1)&mdash;you need to use `fastai 0.7`;  please follow the installation instructions [here](https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652).\n\n*Note: If you want to dive deep into fastai, Jeremy Howard, its lead developer, will be showing internals and advanced features in [Deep Learning Part II](https://www.usfca.edu/data-institute/certificates/deep-learning-part-two) at the University of San Francisco from March 18th, 2018.*\n\n## Installation\n\n**NB:** *fastai v1 currently supports Linux only, and requires **PyTorch v1** and **Python 3.6** or later. Windows support is at an experimental stage: it should work fine but we haven\'t thoroughly tested it. Since Macs don\'t currently have good Nvidia GPU support, we do not currently prioritize Mac development.*\n\n`fastai-1.x` can be installed with either `conda` or `pip` package managers and also from source. At the moment you can\'t just run *install*, since you first need to get the correct `pytorch` version installed - thus to get `fastai-1.x` installed choose one of the installation recipes below using your favorite python package manager. Note that **PyTorch v1** and **Python 3.6** are the minimal version requirements.\n\nIt\'s highly recommended you install `fastai` and its dependencies in a virtual environment ([`conda`](https://conda.io/docs/user-guide/tasks/manage-environments.html) or others), so that you don\'t interfere with system-wide python packages. It\'s not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for `fastai`.\n\nStarting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But [you can install the cpu build too](https://docs.fast.ai/install.html#cpu-build).\n\nIf you experience installation problems, please read about [installation issues](https://github.com/fastai/fastai/blob/master/README.md#installation-issues).\n\nIf you are planning on using `fastai` in the jupyter notebook environment, make sure to also install the corresponding [packages](https://docs.fast.ai/install.html#jupyter-notebook-dependencies).\n\nMore advanced installation issues, such as installing only partial dependencies are covered in a dedicated [installation doc](https://docs.fast.ai/install.html).\n\n### Conda Install\n\n```bash\nconda install -c pytorch -c fastai fastai\n```\n\nThis will install the `pytorch` build with the latest `cudatoolkit` version. If you need a higher or lower `CUDA XX` build (e.g. CUDA 9.0), following the instructions [here](https://pytorch.org/get-started/locally/), to install the desired `pytorch` build.\n\nNote that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):\n\n```bash\nconda uninstall --force jpeg libtiff -y\nconda install -c conda-forge libjpeg-turbo\nCC="cc -mavx2" pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd\n```\nIf you only care about faster JPEG decompression, it can be `pillow` or `pillow-simd` in the last command above, the latter speeds up other image processing operations. For the full story see [Pillow-SIMD](https://docs.fast.ai/performance.html#faster-image-processing).\n\n### PyPI Install\n\n```bash\npip install fastai\n```\n\nBy default pip will install the latest `pytorch` with the latest `cudatoolkit`. If your hardware doesn\'t support the latest `cudatoolkit`, follow the instructions [here](https://pytorch.org/get-started/locally/), to install a `pytorch` build that fits your hardware.\n\n### Bug Fix Install\n\nIf a bug fix was made in git and you can\'t wait till a new release is made, you can install the bleeding edge version of `fastai` with:\n\n```\npip install git+https://github.com/fastai/fastai.git\n```\n\n### Developer Install\n\nThe following instructions will result in a [pip editable install](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs), so that you can `git pull` at any time and your environment will automatically get the updates:\n\n```bash\ngit clone https://github.com/fastai/fastai\ncd fastai\ntools/run-after-git-clone\npip install -e ".[dev]"\n```\n\nNext, you can test that the build works by starting the jupyter notebook:\n\n```bash\njupyter notebook\n```\nand executing an example notebook. For example load `examples/tabular.ipynb` and run it.\n\nPlease refer to [CONTRIBUTING.md](https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md) and [Notes For Developers](https://docs.fast.ai/dev/develop.html) for more details on how to contribute to the `fastai` project.\n\n\n\n\n### Building From Source\n\nIf for any reason you can\'t use the prepackaged packages and have to build from source, this section is for you.\n\n1. To build `pytorch` from source follow the [complete instructions](https://github.com/pytorch/pytorch#from-source). Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into `pytorch`.\n\n2. Next, you will also need to build `torchvision` from source:\n\n   ```bash\n   git clone https://github.com/pytorch/vision\n   cd vision\n   python setup.py install\n   ```\n\n3. When both `pytorch` and `torchvision` are installed, first test that you can load each of these libraries:\n\n   ```bash\n   import torch\n   import torchvision\n   ```\n\n   to validate that they were installed correctly\n\n   Finally, proceed with `fastai` installation as normal, either through prepackaged pip or conda builds or installing from source ("the developer install") as explained in the sections above.\n\n\n\n## Installation Issues\n\nIf the installation process fails, first make sure [your system is supported](https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported). And if the problem is still not addressed, please refer to the [troubleshooting document](https://docs.fast.ai/troubleshoot.html).\n\nIf you encounter installation problems with conda, make sure you have the latest `conda` client (`conda install` will do an update too):\n```bash\nconda install conda\n```\n\n### Is My System Supported?\n\n1. Python: You need to have python 3.6 or higher\n\n2. CPU or GPU\n\n   The `pytorch` binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don\'t have to install system-wide NVIDIA\'s CUDA and related libraries if you don\'t need them for something else. If you have them installed already it doesn\'t matter which NVIDIA\'s CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use `pytorch` build with CUDA 10.0 libraries without any problem, since the `pytorch` binary package is self-contained.\n\n   The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running `nvidia-smi`. While it\'s possible that this application is not available on your system, it\'s very likely that if it doesn\'t work, then you don\'t have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.\n\n3. Operating System:\n\n   Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.\n\n   As of this moment pytorch.org\'s 1.0 version supports:\n\n    | Platform | GPU    | CPU    |\n    |----------|--------|--------|\n    | linux    | binary | binary |\n    | mac      | source | binary |\n    | windows  | binary | binary |\n\n   Legend: `binary` = can be installed directly, `source` = needs to be built from source.\n\n   If there is no `pytorch` preview conda or pip package available for your system, you may still be able to [build it from source](https://pytorch.org/get-started/locally/).\n\n4. How do you know which pytorch cuda version build to choose?\n\n   It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built `pytorch` releases:\n\n    | CUDA Toolkit | NVIDIA (Linux x86_64) |\n    |--------------|-----------------------|\n    | CUDA 10.0    | >= 410.00             |\n    | CUDA 9.0     | >= 384.81             |\n    | CUDA 8.0     | >= 367.48             |\n\n   So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.\n\n   You can find a complete table with all variations [here](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html).\n\n   If you use NVIDIA driver 410+, you most likely want to install the `cudatoolkit=10.0` pytorch variant, via:\n   ```bash\n   conda install -c pytorch pytorch cudatoolkit=10.0\n   ```\n   or if you need a lower version, use one of:\n   ```bash\n   conda install -c pytorch pytorch cudatoolkit=8.0\n   conda install -c pytorch pytorch cudatoolkit=9.0\n   ```\n   For other options refer to the complete list of [the available pytorch variants](https://pytorch.org/get-started/locally/).\n\n## Updates\n\nIn order to update your environment, simply install `fastai` in exactly the same way you did the initial installation.\n\nTop level files `environment.yml` and `environment-cpu.yml` belong to the old fastai (0.7). `conda env update` is no longer the way to update your `fastai-1.x` environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under `old/`.\n\n## Contribution guidelines\n\nIf you want to contribute to `fastai`, be sure to review the [contribution guidelines](https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md). This project adheres to fastai\'s [code of conduct](https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md). By participating, you are expected to uphold this code.\n\nWe use GitHub issues for tracking requests and bugs, so please see [fastai forum](https://forums.fast.ai/) for general questions and discussion.\n\nThe fastai project strives to abide by generally accepted best practices in open-source software development:\n\n## History\n\nA detailed history of changes can be found [here](https://github.com/fastai/fastai/blob/master/CHANGES.md).\n\n## Copyright\n\nCopyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project\'s files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.')
('https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/9e3d2f6ed023d937587cf2ef2ecfbf7afc3d8054/06-Multivariate-Kalman-Filters.ipynb', '# [Kalman and Bayesian Filters in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)\n\n\nIntroductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Jupyter Notebook so that you can run and modify the code in your browser. What better way to learn?\n\n\n**"Kalman and Bayesian Filters in Python" looks amazing! ... your book is just what I needed** - Allen Downey, Professor and O\'Reilly author.\n\n**Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.** - Sam Rodkey, SpaceX\n\nStart reading online now by clicking the binder or Azure badge below:\n\n\n[![Binder](http://mybinder.org/badge.svg)](https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master)\n\n\n![alt tag](https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif)\n\nWhat are Kalman and Bayesian Filters?\n-----\n\nSensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.\n\nIn simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.\n\nThis book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.\n\nIf I asked you the heading of my car at this moment you would have no idea. You\'d prefer a number between 1° and 360° degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243°. In 2 seconds my car could not turn very far, so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.\n\nThe world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.\n\nThere is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.\n\nSay we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we\'d be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We\'d further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.\n\nThe Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.\n\nMotivation\n-----\n\nThe motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I\'m a software engineer that spent almost two decades in the avionics field, and so I have always been \'bumping elbows\' with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew\'s excellent *Kalman Filtering*. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester\'s worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. "But what does that *mean?*" was my repeated thought.\n\nHowever, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.\n\nAs I began to understand the math and theory more difficulties present themselves. A book or paper\'s author makes some statement of fact and presents a graph as proof.  Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder "is this true if R=0?"  Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid \'cheating\' by the student in the classroom.\n\nFrom my point of view none of this is necessary. Certainly if you are designing a Kalman filter for an aircraft or missile you must thoroughly master all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.\n\nI wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you\'ll need it. This book is for the hobbyist, the curious, and the working engineer that needs to filter or smooth data.\n\nThis book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter\'s value, and click \'Run\'. A new plot or printed output will appear in the book.\n\nThis book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.\n\nThis book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.\n\nFinally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey [here](http://www.greenteapress.com/). It\'s time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and MathJax to create the book.\n\n\n## Reading Online\n\nThe book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.\n\n### binder\n\nbinder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.\n\n[![Binder](http://mybinder.org/badge.svg)](https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master)\n\n\n### nbviewer\n\nThe website http://nbviewer.org provides a Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use [*this nbviewer link*](http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb) to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.\n\nnbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.\n\n\n### GitHub\n\nGitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.\n\n\nPDF Version\n-----\n\nA PDF version of the book is available [here]https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&resourcekey=0-41olC9ht9xE3wQe2zHZ45A)\n\n\nThe PDF will usually lag behind what is in github as I don\'t update it for every minor check in.\n\n\n## Downloading and Running the Book\n\nHowever, this book is intended to be interactive and I recommend using it in that form. It\'s a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder "what happens if". Try it and see!\n\nThe book and supporting software can be downloaded from GitHub by running this command on  the command line:\n\n    git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git\n    pip install filterpy\n\nInstructions for installation of the IPython ecosystem can be found in the Installation appendix, found [here](http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb).\n\nOnce the software is installed you can navigate to the installation directory and run Jupyter notebook with the command line instruction\n\n    jupyter notebook\n\nThis will open a browser window showing the contents of the base directory. The book is organized into chapters, each contained within one IPython Notebook (these notebook files have a .ipynb file extension). For example, to read Chapter 2, click on the file *02-Discrete-Bayes.ipynb*. Sometimes there are supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look. You can find these notebooks in the folder named *Supporting_Notebooks*.\n\nThis is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.\n\n\nCompanion Software\n-----\n\n[![Latest Version](http://img.shields.io/pypi/v/filterpy.svg)](http://pypi.python.org/pypi/filterpy)\n\nI wrote an open source Bayesian filtering Python library called **FilterPy**. I have made the project available on PyPi, the Python Package Index.  To install from PyPi, at the command line issue the command\n\n    pip install filterpy\n\nIf you do not have pip, you may follow the instructions here: https://pip.pypa.io/en/latest/installing.html.\n\nAll of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available [here](https://github.com/rlabbe/filterpy). You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more.  It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.\n\n\nFilterPy is hosted on github at (https://github.com/rlabbe/filterpy).  If you want the bleeding edge release you will want to grab a copy from github, and follow your Python installation\'s instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.\n\nAlternative Way of Running the Book in Conda environment\n----\nIf you have conda or miniconda installed, you can create an environment by\n\n    conda env update -f environment.yml\n\nand use\n\n    conda activate kf_bf\n\nand\n\n    conda deactivate kf_bf\n\nto activate and deactivate the environment.\n\n\nIssues or Questions\n------\n\nIf you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don\'t view it as a way to report bugs only. Alternatively I\'ve created a gitter room for more informal discussion. [![Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\nLicense\n-----\n<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Kalman and Bayesian Filters in Python</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" property="cc:attributionName" rel="cc:attributionURL">Roger R. Labbe</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.\n\nAll software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:\n\nThe MIT License (MIT)\n\nCopyright (c) 2015 Roger R. Labbe Jr\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nContact\n-----\n\nrlabbejr at gmail.com')
