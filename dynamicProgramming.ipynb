{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-states MDP\n",
    "\n",
    "* $P(s_0 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_1 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_0 | s_0, a_1) = 0$\n",
    "* $P(s_1 | s_0, a_1) = 1,  r = 10$\n",
    "* $P(s_1 | s_0, a_2) = 0$\n",
    "* $P(s_1 | s_1, a_2) = 1,  r = -1$\n",
    "\n",
    "\n",
    "* $\\gamma = 0.95$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros((2,2,3)) # P(s'|s,a) ... our model of the environment\n",
    "P[0,0,0] = 0.5\n",
    "P[1,0,0] = 0.5\n",
    "P[1,0,1] = 1.\n",
    "P[1,1,2] = 1.\n",
    "\n",
    "R = np.zeros((2,3))  # R(s,a) ... the reward funciton \n",
    "R[0,0] = 5\n",
    "R[0,1] = 10\n",
    "R[1,2] = -1\n",
    "\n",
    "states = [0, 1]\n",
    "actions = [[0, 1], [2]]\n",
    "next_states = [0, 1]\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, states, actions, next_states, epsilon=1e-4):\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = np.zeros((2,), dtype=np.float) # V(s) ... our value function estimate for PI\n",
    "    PI = np.zeros((2), dtype=np.int)     # PI(s) ... our greedy policy\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    \n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges) \n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        while delta > epsilon:\n",
    "\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                for n in next_states:\n",
    "\n",
    "                    # Bellman's update rule\n",
    "                    a = int(PI[s])\n",
    "                    V_kplus1[s] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "\n",
    "                    # Keeps biggest difference seen so far\n",
    "                    delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = V_kplus1\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = {0: {0: 0,   # state0, action0\n",
    "                 1: 0},  # state0, action1\n",
    "             1: {2: 0}}  # state1, action2\n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        for s in states:\n",
    "            \n",
    "            for a in actions[s]:\n",
    "                for n in next_states:\n",
    "                    \n",
    "                    # Policy Improvement rule\n",
    "                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "                    \n",
    "            PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "                    \n",
    "            if old_PI[s] != PI[s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy found in 2 iterations, where each policy evaluation lasted for k = [2, 2]\n",
      "V =  [ 38.82335938  -2.95      ]\n",
      "PI =  [0 2]\n"
     ]
    }
   ],
   "source": [
    "V_k, all_k, PI = policy_iteration(P, R, states, actions, next_states)\n",
    "\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(all_k), all_k))\n",
    "print(\"V = \", V_k)\n",
    "print(\"PI = \", PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modified_policy_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld MDP\n",
    "\n",
    "(Image taken from Sutton's slides on DP)\n",
    "![grid world](gridworld.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "def create_gridworld(world_size):\n",
    "    \"\"\"\n",
    "    world_size: height and width of the squared-shape gridworld\n",
    "    return\n",
    "        actions: list of str, possible actions\n",
    "        states: list of coordinate tuples representing all non-terminal states\n",
    "        nextState: list of list of dict, index 3 times to return the next state coordinate tuple\n",
    "    \"\"\"\n",
    "\n",
    "    # left, up, right, down\n",
    "    actions = ['L', 'U', 'R', 'D']\n",
    "\n",
    "    # Next\n",
    "    nextState = []\n",
    "    for i in range(0, WORLD_SIZE):\n",
    "        nextState.append([])\n",
    "        for j in range(0, WORLD_SIZE):\n",
    "            # Creates a dictionnary that\n",
    "            next = dict()\n",
    "            if i == 0:\n",
    "                next['U'] = (i, j)\n",
    "            else:\n",
    "                next['U'] = (i - 1, j)\n",
    "\n",
    "            if i == WORLD_SIZE - 1:\n",
    "                next['D'] = (i, j)\n",
    "            else:\n",
    "                next['D'] = (i + 1, j)\n",
    "\n",
    "            if j == 0:\n",
    "                next['L'] = (i, j)\n",
    "            else:\n",
    "                next['L'] = (i, j - 1)\n",
    "\n",
    "            if j == WORLD_SIZE - 1:\n",
    "                next['R'] = (i, j)\n",
    "            else:\n",
    "                next['R'] = (i, j + 1)\n",
    "\n",
    "            nextState[i].append(next)\n",
    "\n",
    "    states = []\n",
    "    for i in range(0, WORLD_SIZE):\n",
    "        for j in range(0, WORLD_SIZE):\n",
    "            if (i == 0 and j == 0) or (i == WORLD_SIZE - 1 and j == WORLD_SIZE - 1):\n",
    "                continue\n",
    "            else:\n",
    "                states.append((i, j))\n",
    "                \n",
    "    return actions, states, nextState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Policy : 218 iterations\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "WORLD_SIZE = 4\n",
    "REWARD = -1.0\n",
    "EPSILON = 1e-4\n",
    "ACTION_PROBS = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "def policy_evaluation():\n",
    "    # Initializes the gridworld, our value function estimate and iteration index k\n",
    "    actions, states, nextState = create_gridworld(WORLD_SIZE)\n",
    "    \n",
    "    k = 0\n",
    "    V_k = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    \n",
    "    sum_of_diffs = EPSILON + 1 # arbitrary initialization just to get into the while loop\n",
    "\n",
    "    # Policy evaluation iterates util the value function converges\n",
    "    while sum_of_diffs > EPSILON:\n",
    "\n",
    "        # Initialize our new estimation of the value function\n",
    "        V_kplus1 = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "\n",
    "        for i, j in states:\n",
    "            for action, prob in zip(actions, ACTION_PROBS):\n",
    "                newPosition = nextState[i][j][action]\n",
    "\n",
    "                # Bellman update rule\n",
    "                V_kplus1[i, j] += prob * (REWARD + V_k[newPosition[0], newPosition[1]])\n",
    "\n",
    "        # Computes the sum of differences between last and new estimates\n",
    "        sum_of_diffs = np.sum(np.abs(V_kplus1 - V_k))\n",
    "\n",
    "        # Updates our current estimate\n",
    "        V_k = V_kplus1\n",
    "        k += 1\n",
    "\n",
    "    print('Random Policy : {} iterations'.format(k))\n",
    "    print(np.round(V_k))\n",
    "\n",
    "policy_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}