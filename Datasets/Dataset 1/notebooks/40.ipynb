{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<!--NOTEBOOK_INFORMATION-->\n", "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/LogoOpenclassrooms.png\">\n", "<font size=\"4\">\n", "    \n", "Cette \u00e9tude a \u00e9t\u00e9 r\u00e9alis\u00e9e dans le cadre du 6\u00e8me projet de ma formation Datascientist dispens\u00e9e en MOOC par \n", "\n", "<font color='blus'>Openclassrooms / \u00e9coles Centrale-Sup\u00e9lec</font>.\n", "\n", "\n", "\n", "\n", "<p></p><p></p><p></p>\n", "\n", "\n", "\n", "\n", "\n", "**Le probl\u00e8me pos\u00e9 :**\n", "\n", "# <font color='blus'>Indexation d'images</font>\n", "\n", "Vous \u00eates b\u00e9n\u00e9vole pour l'association de protection des animaux de votre quartier. C'est d'ailleurs ainsi que vous avez trouv\u00e9 votre compagnon id\u00e9al, Snooky. Du coup, vous vous demandez ce que vous pouvez faire en retour pour aider l'association.\n", "\n", "Vous apprenez, en discutant avec un b\u00e9n\u00e9vole, que leur base de donn\u00e9es de pensionnaires commence \u00e0 s'agrandir et qu'ils n'ont pas toujours le temps de r\u00e9f\u00e9rencer les images des animaux qu'ils ont accumul\u00e9es depuis plusieurs ann\u00e9es. Ils aimeraient donc r\u00e9aliser un index de l\u2019ensemble de la base de donn\u00e9es d\u2019images qu\u2019ils poss\u00e8dent, pour classer les chiens par races.\n", "\n", "**<font color='blus'>Les donn\u00e9es</font>**\n", "\n", "Les b\u00e9n\u00e9voles de l'association n'ont pas eu le temps de r\u00e9unir les diff\u00e9rentes images des pensionnaires dispers\u00e9es sur leurs disques durs. Pas de probl\u00e8me, vous d\u00e9velopperez un algorithme en utilisant le Stanford Dogs Dataset pour entra\u00eener votre algorithme.\n", "\n", "**<font color='blus'>Votre mission</font>**\n", "\n", "En tant que Data Scientist, l'association vous demande de r\u00e9aliser un algorithme de d\u00e9tection de la race du chien sur une photo, afin d'acc\u00e9l\u00e9rer leur travail d\u2019indexation.\n", "\n", "**<font color='blus'>Contraintes</font>**\n", "\n", "Lors de ce projet, vous mettrez en \u0153uvre deux approches.\n", "\n", "* Une approche classique : il s\u2019agit de pre-processer des images avec des techniques sp\u00e9cifiques (e.g.whitening, equalisation, filtre lin\u00e9aire/laplacien/gaussien, \u00e9ventuellement modifier la taille des images), puis d\u2019extraire des features (e.g. texture, corners, edges et SIFT detector). Il faut ensuite r\u00e9duire les dimensions, soit par des approches classiques (e.g. PCA, k-means) soit avec une approche par histogrammes et dictionary learning (bag-of-words appliqu\u00e9 aux images), puis appliquer des algorithmes de classification standards.\n", "\n", "\n", "\n", "* Lors de l\u2019analyse exploratoire, vous regarderez si les features extraites et utilis\u00e9es en classification sont prometteuses en utilisant des m\u00e9thodes de r\u00e9duction de dimension pour visualiser le dataset en 2D. Cela vous permettra d\u2019affiner votre intuition sur les diff\u00e9rents traitements possibles, sans que cela ne se substitue \u00e0 des mesures de performances rigoureuses.\n", "\n", "\n", "\n", "* Une approche s\u2019appuyant sur l\u2019\u00e9tat de l\u2019art et l\u2019utilisation de CNN (r\u00e9seaux de neurones convolutionnels). Compte tenu de la taille et de la complexit\u00e9 du dataset, et de la puissance de calcul \u00e0 votre disposition, il est tr\u00e8s difficile d\u2019obtenir de bonnes performances (pour \u00e7a, essayez MNIST). Aussi, est-il recommand\u00e9 d\u2019utiliser le transfer learning, c\u2019est-\u00e0-dire utiliser un r\u00e9seau d\u00e9j\u00e0 entra\u00een\u00e9, et le modifier pour r\u00e9pondre \u00e0 votre probl\u00e8me. Une premi\u00e8re chose obligatoire est de r\u00e9-entra\u00eener les derni\u00e8res couches pour pr\u00e9dire les classes qui vous int\u00e9ressent seulement. Il est \u00e9galement possible d\u2019adapter la structure (supprimer certaines couches par exemple) ou de r\u00e9-entra\u00eener le mod\u00e8le avec un tr\u00e8s faible learning rate pour ajuster les poids \u00e0 votre probl\u00e8me (plus long) et optimiser les performances."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Using TensorFlow backend.\n", "/home/bangui/.local/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n", "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n", "  \"found relative to the 'datapath' directory.\".format(key))\n"]}], "source": ["from IPython.display import display, clear_output\n", "\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "import P7_DataBreed\n", "import p5_util\n", "\n", "%load_ext autoreload\n", "%autoreload 2\n", "%reload_ext autoreload\n", "\n", "from  sklearn import model_selection\n", "import numpy as np\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# <font color='blus'>1. Data is loaded</font>"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["p5_util.object_load : fileName= ./data/arr_keras_X_y_train_test.dump\n"]}], "source": ["import p5_util\n", "filename='./data/arr_keras_X_y_train_test.dump'\n", "(X_train, X_test, y_train, y_test) = p5_util.object_load(filename)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": ["((414, 224, 224, 3), (47, 224, 224, 3))"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["X_train.shape, X_test.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# <font color='blus'>2. Breeds classification using CNN VGG16 network</font>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Building VGG16 Keras layers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1st convolution layer is added "]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["224 224 3\n"]}], "source": ["from keras.models import Sequential\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.layers import Flatten, Dense, Dropout\n", "\n", "VGG16Seq = Sequential()  # Cr\u00e9ation d'un r\u00e9seau de neurones vide \n", "\n", "# Ajout de la premi\u00e8re couche de convolution, suivie d'une couche ReLU\n", "w=X_train.shape[1]\n", "h=X_train.shape[2]\n", "c=X_train.shape[3]\n", "print(w,h,c)\n", "VGG16Seq.add(Conv2D(64, (3, 3), input_shape=(w, h, c), padding='same', activation='relu'))\n", "VGG16Seq.add(Conv2D(64, (3, 3), activation='relu'))\n", "VGG16Seq.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2nd convolution layer is added "]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# Ajout de la deuxi\u00e8me couche de convolution, suivie  d'une couche ReLU\n", "VGG16Seq.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n", "VGG16Seq.add(Conv2D(64, (3, 3), activation='relu'))\n", "VGG16Seq.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 3th convolution layer is added "]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# Ajout de la deuxi\u00e8me couche de convolution, suivie  d'une couche ReLU\n", "VGG16Seq.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n", "VGG16Seq.add(Conv2D(64, (3, 3), activation='relu'))\n", "VGG16Seq.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Pooling layer is added "]}, {"cell_type": "raw", "metadata": {}, "source": ["# Ajout de la premi\u00e8re couche de pooling\n", "my_VGG16.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of labels for training process : 414\n"]}], "source": ["nb_labels = y_train.shape[0]\n", "print(\"Number of labels for training process : \"+str(nb_labels))\n", "VGG16Seq.add(Flatten())  # Conversion des matrices 3D en vecteur 1D"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1st Dense layer is added"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["VGG16Seq.add(Dense(nb_labels, activation='relu'))\n", "VGG16Seq.add(Dropout(0.5))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2nd Dense layer is added"]}, {"cell_type": "raw", "metadata": {}, "source": ["# Ajout de la deuxi\u00e8me couche fully-connected, suivie d'une couche ReLU\n", "VGG16Seq.add(Dense(nb_labels, activation='relu'))\n", "VGG16Seq.add(Dropout(0.5))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 3th Dense layer is added"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "# Ajout de la derni\u00e8re couche fully-connected qui permet de classifier\n", "VGG16Seq.add(Dense(nb_labels, activation='softmax'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### VGG16 compilation"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["from keras import optimizers\n", "sgd = optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n", "rmsprop = optimizers.RMSprop(lr=1e-4)\n", "\n", "#my_VGG16.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n", "#my_VGG16.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n", "VGG16Seq.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Keras Sequential model is dumped"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["_________________________________________________________________\n", "Layer (type)                 Output Shape              Param #   \n", "=================================================================\n", "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n", "_________________________________________________________________\n", "conv2d_2 (Conv2D)            (None, 222, 222, 64)      36928     \n", "_________________________________________________________________\n", "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 64)      0         \n", "_________________________________________________________________\n", "conv2d_3 (Conv2D)            (None, 111, 111, 64)      36928     \n", "_________________________________________________________________\n", "conv2d_4 (Conv2D)            (None, 109, 109, 64)      36928     \n", "_________________________________________________________________\n", "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n", "_________________________________________________________________\n", "conv2d_5 (Conv2D)            (None, 54, 54, 64)        36928     \n", "_________________________________________________________________\n", "conv2d_6 (Conv2D)            (None, 52, 52, 64)        36928     \n", "_________________________________________________________________\n", "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n", "_________________________________________________________________\n", "flatten_1 (Flatten)          (None, 43264)             0         \n", "_________________________________________________________________\n", "dense_1 (Dense)              (None, 414)               17911710  \n", "_________________________________________________________________\n", "dropout_1 (Dropout)          (None, 414)               0         \n", "_________________________________________________________________\n", "dense_2 (Dense)              (None, 414)               171810    \n", "=================================================================\n", "Total params: 18,269,952\n", "Trainable params: 18,269,952\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["import p5_util\n", "p5_util.object_dump(VGG16Seq,'./data/VGG16Seq_sgd.dump')\n", "\n", "VGG16Seq.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Result is picked and saved into a file"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["import p5_util\n", "filename = './data/dict_cls_cnn_built.dump'\n", "dict_cls_cnn_built = {'CNN Built':0.75}\n", "p5_util.object_dump(dict_cls_cnn_built,filename)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Loading VGG166 Sequential model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.display import display, clear_output\n", "\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "import P7_DataBreed\n", "import p5_util\n", "\n", "%load_ext autoreload\n", "%autoreload 2\n", "%reload_ext autoreload\n", "\n", "from  sklearn import model_selection\n", "import numpy as np\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["import p5_util\n", "if True:\n", "    VGG16Seq = p5_util.object_load('./data/VGG16Seq_sgd.dump')\n", "    VGG16Seq.summary()\n", "\n", "import p5_util\n", "import P7_DataBreed\n", "if False :\n", "    oP7_DataBreed = p5_util.object_load('./data/oP7_DataBreed.dump')\n", "    oP7_DataBreed.show()\n", "    oP7_DataBreed.breed_show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Test for images classification"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Data is loaded form dumped file"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["p5_util.object_load : fileName= ./data/arr_keras_X_y_train_test.dump\n", "(414, 224, 224, 3) (47, 224, 224, 3) (414, 3) (47, 3)\n"]}], "source": ["import p5_util\n", "filename='./data/arr_keras_X_y_train_test.dump'\n", "(arr_X_train,arr_X_test, arr_y_train, arr_y_test) = p5_util.object_load(filename)\n", "print(arr_X_train.shape,arr_X_test.shape,arr_y_train.shape,arr_y_test.shape)\n", "\n", "#### Data normalization\n", "\n", "arr_X_train = arr_X_train.astype('float32')\n", "arr_X_test = arr_X_test.astype('float32')\n", "\n", "# Scale the data to lie between 0 to 1\n", "arr_X_train /= 255\n", "arr_X_test /= 255"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Model training"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(414, 224, 224, 3)\n"]}, {"ename": "ValueError", "evalue": "Error when checking target: expected dense_2 to have shape (1,) but got array with shape (3,)", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", "\u001b[0;32m<ipython-input-15-14c9832b3847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_X_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16Seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (1,) but got array with shape (3,)"]}], "source": ["print(arr_X_train.shape)\n", "history = VGG16Seq.fit(arr_X_train, arr_y_train, epochs=50, batch_size=90,verbose=1,validation_data=(arr_X_test, arr_y_test))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["[test_loss, test_acc] = VGG16Seq.evaluate(arr_X_test, arr_y_test)\n", "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Plot the Loss Curves\n", "import matplotlib.pyplot as plt\n", "\n", "plt.figure(figsize=[8,6])\n", "plt.plot(history.history['loss'],'r',linewidth=1.0)\n", "plt.plot(history.history['val_loss'],'b',linewidth=1.0)\n", "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n", "plt.xlabel('Epochs ',fontsize=16)\n", "plt.ylabel('Loss',fontsize=16)\n", "plt.title('Loss Curves ',fontsize=16)\n", " \n", "#Plot the Accuracy Curves\n", "plt.figure(figsize=[8,6])\n", "plt.plot(history.history['acc'],'r',linewidth=1.0)\n", "plt.plot(history.history['val_acc'],'b',linewidth=1.0)\n", "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n", "plt.xlabel('Epochs ',fontsize=16)\n", "plt.ylabel('Accuracy',fontsize=16)\n", "plt.title('Accuracy Curves',fontsize=16)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import p5_util\n", "is_score_dumped=True\n", "if is_score_dumped is True :\n", "    filename = './data/dict_cls_score.dump'\n", "    dict_cls_score = p5_util.object_load(filename)\n", "else:\n", "    dict_cls_score = dict()\n", "dict_classifier = dict()\n", "\n", "dict_cls_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = VGG16Seq.predict(arr_X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["filename= './data/result_VGG16Seq.dump'\n", "p5_util.object_dump(y_pred,filename)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Measure of accuracy\n", "\n", "Result is stored into `arr_result` where :\n", "* rows : number of observations from test dataset\n", "* columns : probabibilty for each observation from train dataset.\n", "\n", "For each row from `arr_result`, greater probability value is searched for. \n", "<br>\n", "The result of search provides index of label into `list_index`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import p5_util\n", "filename= './data/result_vgg16.dump'\n", "p5_util.object_load(y_pred,filename)"]}, {"cell_type": "raw", "metadata": {}, "source": ["# List of indexes greater probabilities: greater values for each row from arr_result.\n", "list_index = [np.where(y_pred[i] == max(y_pred[i]))[0][0] for i in range(0, len(y_pred))]\n", "\n", "#print(arr_result)\n", "print(len(arr_y_train))\n", "print(list_index)\n", "list_result = [arr_y_train[list_index[i]][0] for i in range(0,len(list_index))]\n", "\n", "score_keras = len(set(list_result).intersection(arr_y_test.flatten()))/len(list(arr_y_test))\n", "print(\"*** INFO : Result accuracy = {0:1.1F}%\".format(score_keras*100))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(loss, score_vgg16_seq) = VGG16Seq.evaluate(arr_X_test, arr_y_test, verbose=True)\n", "\n", "score_vgg16_seq"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import p5_util\n", "import p6_util_plot\n", "is_dumped=True\n", "if is_dumped is True:\n", "    filename = './data/dict_cls_score.dump'\n", "    dict_cls_score = p5_util.object_load(filename)\n", "else :\n", "    pass\n", "\n", "dict_cls_score['VGG16 Seq'] = score_vgg16_seq\n", "dict_benchmark_result = dict_cls_score.copy()\n", "\n", "df_result = pd.DataFrame.from_dict( dict_benchmark_result, orient='index')\n", "df_result.reset_index(inplace=True)\n", "df_result.rename(columns={'index':'Classifier',0:'Score'}, inplace=True)\n", "df_result\n", "nb_images = oP7_DataBreed._sampling_breed_count*oP7_DataBreed._sampling_image_per_breed_count\n", "nb_images = oP7_DataBreed.df_pil_image_kpdesc.shape[0]\n", "if oP7_DataBreed.is_kp_filtered :\n", "    title = \"Benchmark classifiers accuracy / GMM clustering / \"+str(nb_images)+\" filtered splitted images / \"+str(oP7_DataBreed.sampling_breed_count)+\" breeds\"\n", "else :\n", "    title = \"Benchmark classifiers accuracy / GMM clustering / \"+str(nb_images)+\" splitted images / \"+str(oP7_DataBreed.sampling_breed_count)+\" breeds\"\n", "\n", "p6_util_plot.ser_item_occurency_plot(df_result.Classifier, df_result.Score*100, item_count=None, title=title,\\\n", "                                    p_reverse=False,p_x_title='Classifiers', p_y_title='Accuracy')\n", "\n", "#### Classifier API\n", "p5_util.object_dump(dict_cls_score,filename)\n", "\n", "dict_cls_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["oP7_DataBreed.show()\n", "oP7_DataBreed.show_breed_name()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2}