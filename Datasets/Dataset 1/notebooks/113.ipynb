{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["##\u6a21\u5757\u5bfc\u5165"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": false}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "import os.path\n", "import access_data \n", "import preprocessing\n", "import setting\n", "import feature\n", "import sys\n", "import time\n", "\n", "pd.set_option('display.max_columns' ,1000)\n", "pd.set_option('display.max_rows',60)\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#\u6e05\u7406 + \u5206\u8bcd + label encode+\u8ba1\u7b97\u8bcd\u9891"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["INFO:root:finished cleaning symbol http[0-9a-zA-Z?:=._@%/\\-#&\\+|]+\n", "INFO:root:finished cleaning symbol //@\n", "INFO:root:finished cleaning symbol @\n", "INFO:root:finished cleaning symbol #\n", "INFO:root:finished cleaning symbol \u3010\n", "INFO:root:finished cleaning symbol \u300a\n", "INFO:root:finished cleaning symbol \\[\n", "INFO:root:Start to segment\n", "Building prefix dict from C:\\Anaconda\\lib\\site-packages\\jieba\\dict.txt ...\n", "DEBUG:jieba:Building prefix dict from C:\\Anaconda\\lib\\site-packages\\jieba\\dict.txt ...\n", "Loading model from cache c:\\users\\admini~1\\appdata\\local\\temp\\jieba.cache\n", "DEBUG:jieba:Loading model from cache c:\\users\\admini~1\\appdata\\local\\temp\\jieba.cache\n", "Loading model cost 0.955 seconds.\n", "DEBUG:jieba:Loading model cost 0.955 seconds.\n", "Prefix dict has been built succesfully.\n", "DEBUG:jieba:Prefix dict has been built succesfully.\n", "INFO:root:have segmented 0\n", "INFO:root:have segmented 100000\n", "INFO:root:Start to segment\n", "INFO:root:have segmented 0\n", "INFO:root:have segmented 100000\n", "INFO:root:have segmented 200000\n", "INFO:root:have segmented 300000\n", "INFO:root:have segmented 400000\n", "INFO:root:have segmented 500000\n", "INFO:root:have segmented 600000\n", "INFO:root:have segmented 700000\n", "INFO:root:have segmented 800000\n", "INFO:root:have segmented 900000\n", "INFO:root:have segmented 1000000\n", "INFO:root:have segmented 1100000\n", "INFO:root:have segmented 1200000\n", "INFO:root:have segmented 1300000\n", "INFO:root:have segmented 1400000\n", "INFO:root:have segmented 1500000\n", "INFO:root:have segmented 1600000\n", "INFO:root:have segmented 1700000\n", "INFO:root:have segmented 1800000\n", "INFO:root:have segmented 1900000\n", "INFO:root:have segmented 2000000\n", "INFO:root:have segmented 2100000\n", "INFO:root:have segmented 2200000\n", "INFO:root:have segmented 2300000\n", "INFO:root:have segmented 2400000\n", "INFO:root:have segmented 2500000\n", "INFO:root:have segmented 2600000\n", "INFO:root:have segmented 2700000\n", "INFO:root:have segmented 2800000\n"]}], "source": ["train_corpus ,test_corpus = preprocessing.clean_corpus()\n", "test_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\n", "train_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n", "\n", "train_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\n", "test_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n", "\n", "train_uid,test_uid = preprocessing.encode_label()\n", "\n", "preprocessing.bag_of_word(train_corpus['corpus'].values,test_corpus['corpus'].values,min_df=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##\u4e3b\u9898\u6a21\u578b"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%run run_lda.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["  ##\u6784\u5efa\u7528\u6237\u57fa\u672c\u7279\u5f81 "]}, {"cell_type": "code", "execution_count": 8, "metadata": {"collapsed": false}, "outputs": [], "source": ["train_uid = access_data.load_processed_data('uid&pid_train')\n", "test_uid  = access_data.load_processed_data('uid&pid_test')\n", "\n", "train ,test = access_data.load_raw_data()\n", "\n", "train_corpus = access_data.load_processed_data('cleaned&segment_train')\n", "test_corpus  = access_data.load_processed_data('cleaned&segment_test')                                               \n", "train = pd.concat([train_uid,train,train_corpus],axis=1)\n", "test  = pd.concat([test_uid,test,test_corpus],axis=1)\n", "\n", "train.drop([0,1],axis=1,inplace=True)\n", "test.drop([0,1],axis=1,inplace=True)\n", "\n", "train.columns = ['pid','uid','time','share','comment','zan','raw_corpus','clean&segment','\u94fe\u63a5','//@','@','#','\u3010','\u300a','\\[']\n", "test.columns = ['pid','uid','time','raw_corpus','clean&segment','\u94fe\u63a5','//@','@','#','\u3010','\u300a','\\[']\n", "train['uid'] = train['uid'].astype(np.uint16)\n", "test['uid']  = test['uid'].astype(np.uint16)\n", "l = ['\u94fe\u63a5','//@','@','#','\u3010','\u300a','\\[']\n", "\n", "for string in l :\n", "    train[string] = train[string].astype(np.int8)\n", "    test[string]  = test[string].astype(np.int8)\n", "\n", "#\u5728training set\u548ctest set\u4e2d\u548c\u7528\u6237\u53d1\u9001\u5fae\u535a\u7684\u603b\u6570\u91cf\n", "tot = pd.concat([pd.DataFrame(train['uid']),pd.DataFrame(test['uid'])])\n", "c = pd.DataFrame(tot['uid'].value_counts())\n", "c.columns = ['tot_counts']\n", "train = train.merge(c,left_on='uid',right_index=True,how='left')\n", "test  = test.merge(c,left_on='uid',right_index=True,how='left')\n", "\n", "# \u7528\u6237\u51fa\u73b0\u5728\u8bad\u7ec3\u96c6\u7684\u6b21\u6570\n", "c = pd.DataFrame(train['uid'].value_counts())\n", "c.columns = ['train_counts']\n", "train = train.merge(c,left_on='uid',right_index=True,how='left')\n", "test  = test.merge(c,left_on='uid',right_index=True,how='left')\n", "\n", "test.fillna(-1,inplace=True)\n", "train['tot_counts'] = train['tot_counts'].astype(np.int32)\n", "train['train_counts'] = train['train_counts'].astype(np.int32)\n", "\n", "test['tot_counts'] = test['tot_counts'].astype(np.int32)\n", "test['train_counts'] = test['train_counts'].astype(np.int32)\n", "\n", "addr1 = setting.raw_data_dir + 'basic_train'\n", "addr2 = setting.raw_data_dir + 'basic_test'\n", "\n", "lda_result = np.load('processed_data/lda_result_version3.npy')\n", "lda_result = pd.DataFrame(lda_result,columns=['topic_%d' %i for i in range(0,25)])\n", "\n", "for string in ['topic_%d' %i for i in range(0,25)]:\n", "    train[string] = lda_result.loc[:train.shape[0]-1,string].values\n", "    test[string] = lda_result.loc[train.shape[0]:,string].values\n", "\n", "#train.to_pickle(addr1)\n", "#test.to_pickle(addr2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##\u7528\u6237\u7279\u5f81 + \u65f6\u95f4\u7279\u5f81  + \u6587\u672c\u7279\u5f81"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": false}, "outputs": [], "source": ["#train_basic = pd.read_pickle('raw_data/basic_train')\n", "#test_basic  = pd.read_pickle('raw_data/basic_test')\n", "#train_basic = train_basic.loc[1626750:]\n", "#\u8ba1\u7b97\u60c5\u611f\u6781\u6027\n", "begin_time= time.time()\n", "train_sentiment,test_sentiment = feature.sentiment_feature(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "#\u8ba1\u7b97\u4e00\u5468\u5185\u51fa\u73b0\u5fae\u535a\u7684\u6570\u91cf\n", "begin_time= time.time()\n", "train_seven_days ,test_seven_days = feature.find_seven_days(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "#lda\u7279\u5f81\n", "begin_time = time.time()\n", "train_lda_feature,test_lda_feature = feature.lda_feature(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "#\u7528\u6237\u7279\u5f81\n", "begin_time= time.time()\n", "train_user,test_user = feature.user_basic_feature(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "#\u6587\u672c\u7279\u5f81\n", "begin_time= time.time()\n", "train_content,test_content = feature.content_basic_feature(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "#\u65f6\u95f4\u7279\u5f81\n", "begin_time= time.time()\n", "train_time,test_time = feature.time_feature(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "#\u5173\u952e\u8bcd\u7279\u5f81\n", "begin_time = time.time()\n", "train_keyword,test_keyword = feature.key_word_feature(train_basic,test_basic)\n", "end_time = time.time()\n", "print end_time - begin_time"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["train_basic.drop(['uid','pid','time','share','comment','zan','raw_corpus','clean&segment'],axis=1,inplace=True)\n", "test_basic.drop(['uid','pid','time','raw_corpus','clean&segment'],axis=1,inplace=True)\n", "train_user = pd.concat([train_user,train_basic],axis=1)\n", "test_user  = pd.concat([test_user,test_basic],axis=1)\n", "\n", "train = train_user.merge(train_content,how='left',left_on='pid',right_index=True)\n", "test  = test_user.merge(test_content,how='left',left_on='pid',right_index=True)\n", "\n", "train = train.merge(train_time,how='left',left_on='pid',right_index=True)\n", "test  = test.merge(test_time,how='left',left_on='pid',right_index=True)\n", "\n", "train = pd.concat([train,train_keyword],axis=1)\n", "test  = pd.concat([test,test_keyword],axis=1)\n", "\n", "train = train.merge(train_lda_feature,how='left',left_on='pid',right_index=True)\n", "test  = test.merge(test_lda_feature,how='left',left_on='pid',right_index=True)\n", "\n", "train = train.merge(train_sentiment,how='left',left_on='pid',right_index=True)\n", "test  = test.merge(test_sentiment,how='left',left_on='pid',right_index=True)\n", "\n", "train = train.merge(train_seven_days,how='left',left_on='pid',right_index=True)\n", "test  = test.merge(test_seven_days,how='left',left_on='pid',right_index=True)\n", "\n", "begin_time = time.time()\n", "result,gmm = feature.clustering_feature(train,test)\n", "end_time = time.time()\n", "print end_time - begin_time\n", "\n", "train = train.merge(result,how='left',left_on='uid',right_index=True)\n", "test  = test.merge(result,how='left',left_on='uid',right_index=True)"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["##\u6574\u7406\u683c\u5f0f"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [], "source": ["#train.drop(['sentiment','seven_days'],axis=1,inplace=True)\n", "#test.drop(['sentiment','seven_days'],axis=1,inplace=True)\n", "result_test = []\n", "result_train = []\n", "tot = 0\n", "for string in ['share','comment','zan','content_len','\u94fe\u63a5','//@','@','#','\u3010','\u300a','\\[']:\n", "    temp = []\n", "    for i in test[string+'_histogram']:\n", "        if isinstance(i,int):\n", "            temp.append(np.zeros(shape=8))\n", "            tot +=1\n", "        else:\n", "            temp.append(i[0])\n", "    result_test.append(np.asarray(temp))\n", "    temp = []\n", "    for i in train[string+'_histogram']:\n", "        temp.append(i[0])\n", "    result_train.append(np.asarray(temp))\n", "    \n", "    train.drop(string+'_histogram',axis=1,inplace=True)\n", "    test.drop(string+'_histogram',axis=1,inplace=True)\n", "train.drop(['pid','uid'],inplace=True,axis = 1)\n", "test.drop(['pid','uid'],inplace=True,axis = 1)\n", "\n", "train_y = train[['share','comment','zan']].values\n", "train.drop(['share','comme\u00b7nt','zan'],axis = 1,inplace=True)\n", "train_x = train.values\n", "test_x  = test.values\n", "for i in result_train:\n", "    train_x = np.c_[train_x,i]\n", "for i in result_test:\n", "    test_x = np.c_[test_x,i]\n", "np.save('processed_data/train3_np',train_x)\n", "np.save('processed_data/test3_np',test_x)\n", "np.save('processed_data/target3_np',train_y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##\u8bad\u7ec3\u6a21\u578b"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%run learning.py"]}], "metadata": {"kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "version": "2.7.10"}}, "nbformat": 4, "nbformat_minor": 0}