# read in refactoring observations
read.csv("4Refactoring_Observations.csv")
# read in refactoring observations
refactoring_observations <- read.csv("4Refactoring_Observations.csv")
View(refactoring_observations)
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
View(all_commits_in_sample)
View(refactoring_observations)
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <- merge(refactoring_observations, all_commits_in_sample, by = "commit_hash", all.x = TRUE)
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <- merge(refactoring_observations, all_commits_in_sample, by.x = "commit_hash", by.y = "commit", all.x = TRUE)
View(merged_data)
count(merged_data)
library(dplyr)
count(merged_data)
count(merged_data, commit)
count(merged_data, commit_hash)
count(merged_data, observation_id)
arrange(count(merged_data, observation_id), desc(n))
merged_data$observation_id == "277"
filter(merged_data, observation_id == "277")
# there's some extras, by inspection I assumed the actual data set removed these "duplicates"
merged_data <- distinct(merged_data)
as.Date(as.POSIXct(1555790142,origin="1970-01-01"))
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "cfs", "ec", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
library(dplyr)
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "cfs", "ec", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
count(merged_data, refactor_code)
count(merged_data)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
count(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "cfs", "ec", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
count(merged_data, refactor_code)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "cfs", "ec", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
View(merged_data)
count(merged_data)
count(merged_data$notebook)
count(unique(merged_data$notebook))
unique(merged_data$notebook)
repo
# export merged data
# Function to create the string URL
create_url <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "/", notebook)
}
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url = create_url(repo, commit_hash, notebook))
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(url) {
# Generate unique filename based on URL
filename <- gsub("[^[:alnum:]]", "_", url)
output_file <- paste0(filename, ".html")
# Run nbdime-web command
command <- paste("nbdime-web", url, "--output", output_file)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(pair) {
# Extract URLs from the pair
url1 <- pair[1]
url2 <- pair[2]
# Generate unique filename based on URLs
filename1 <- gsub("[^[:alnum:]]", "_", url1)
filename2 <- gsub("[^[:alnum:]]", "_", url2)
output_file <- paste0(filename1, "_vs_", filename2, ".html")
# Run nbdime-web command
command <- paste("nbdime-web", url1, url2, "--output", output_file)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
pair <- c("https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202/13_convolutional_neural_networks.ipynb",
"https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202~1/13_convolutional_neural_networks.ipynb")
runNbdimeWeb(pair)
# Extract URLs from the pair
url1 <- pair[1]
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(pair) {
# Extract URLs from the pair
url1 <- pair[1]
url2 <- pair[2]
# Generate unique filename based on URLs
filename1 <- gsub("[^[:alnum:]]", "_", url1)
filename2 <- gsub("[^[:alnum:]]", "_", url2)
output_file <- paste0(filename1, "_vs_", filename2, ".html")
# Run nbdime-web command
command <- paste("nbdiff-web", url1, url2, "--output", output_file)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
pair <- c("https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202/13_convolutional_neural_networks.ipynb",
"https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202~1/13_convolutional_neural_networks.ipynb")
runNbdimeWeb(pair)
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(pair) {
# Extract URLs from the pair
url1 <- pair[1]
url2 <- pair[2]
print(url1)
# Generate unique filename based on URLs
filename1 <- gsub("[^[:alnum:]]", "_", url1)
filename2 <- gsub("[^[:alnum:]]", "_", url2)
output_file <- paste0(filename1, "_vs_", filename2, ".html")
# Run nbdime-web command
command <- paste("nbdiff-web", url1, url2, "--output", output_file)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
pair <- c("https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202/13_convolutional_neural_networks.ipynb",
"https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202~1/13_convolutional_neural_networks.ipynb")
runNbdimeWeb(pair)
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(pair) {
# Extract URLs from the pair
url1 <- pair[1]
url2 <- pair[2]
# Generate unique filename based on URLs
filename1 <- gsub("[^[:alnum:]]", "_", url1)
filename2 <- gsub("[^[:alnum:]]", "_", url2)
output_file <- paste0(filename1, "_vs_", filename2, ".html")
# Run nbdime-web command
command <- paste("nbdiff-web", url1, url2, "--output", output_file)
print(command)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
pair <- c("https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202/13_convolutional_neural_networks.ipynb",
"https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202~1/13_convolutional_neural_networks.ipynb")
runNbdimeWeb(pair)
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(pair) {
# Extract URLs from the pair
url1 <- pair[1]
url2 <- pair[2]
# Generate unique filename based on URLs
filename1 <- gsub("[^[:alnum:]]", "_", url1)
filename2 <- gsub("[^[:alnum:]]", "_", url2)
output_file <- paste0(filename1, "_vs_", filename2, ".html")
# Run nbdime-web command
command <- paste("nbdiff-web", url1, url2)
print(command)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
pair <- c("https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202/13_convolutional_neural_networks.ipynb",
"https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202~1/13_convolutional_neural_networks.ipynb")
runNbdimeWeb(pair)
# Function to run nbdime-web and export HTML
runNbdimeWeb <- function(pair) {
# Extract URLs from the pair
url1 <- pair[1]
url2 <- pair[2]
# Generate unique filename based on URLs
filename1 <- gsub("[^[:alnum:]]", "_", url1)
filename2 <- gsub("[^[:alnum:]]", "_", url2)
output_file <- paste0(filename1, "_vs_", filename2, ".html")
# Run nbdime-web command
command <- paste("nbdiff-web", url1, url2)
print(command)
system(command)
# Print output file path
cat("Exported:", output_file, "\n")
}
pair <- c("https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202~1/13_convolutional_neural_networks.ipynb",
"https://raw.githubusercontent.com/188xuhe/handson-ml/cc1f5e323c984c9494860f0dc808e2265cda2202/13_convolutional_neural_networks.ipynb")
runNbdimeWeb(pair)
runNbdimeWeb(pair)
View(merged_data)
View(merged_data)
View(merged_data)
View(merged_data)
View(merged_data)
merged_data <- c(merged_data$commit_hash, merged_data$refactor_code, merged_data$repo, merged_data$notebook, merged_data$url)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "cfs", "ec", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
c(merged_data$commit_hash, merged_data$refactor_code, merged_data$repo, merged_data$notebook, merged_data$url)
data.frame(merged_data$commit_hash, merged_data$refactor_code, merged_data$repo, merged_data$notebook, merged_data$url)
merged_data <- subset(merged_data, select c=('commit_hash', 'refactor_code', 'repo', 'notebook', 'url'))
merged_data <- subset(merged_data, select = c('commit_hash', 'refactor_code', 'repo', 'notebook', 'url'))
View(merged_data)
# export merged data
# Function to create the string URL
create_url <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "/", notebook)
}
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url = create_url(repo, commit_hash, notebook))
# keep columns
merged_data <- subset(merged_data, select = c('commit_hash', 'refactor_code', 'repo', 'notebook', 'url'))
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
# export merged data
# Function to create the string URL
create_url <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "/", notebook)
}
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url = create_url(repo, commit_hash, notebook))
# keep columns
merged_data <- subset(merged_data, select = c('commit_hash', 'refactor_code', 'repo', 'notebook', 'url'))
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
View(merged_data)
View(merged_data)
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
# export merged data
# Function to create the string URL
create_url <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "/", notebook)
}
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url = create_url(repo, commit_hash, notebook))
# keep columns
merged_data <- subset(merged_data, select = c('commit_hash', 'refactor_code', 'repo', 'notebook', 'url'))
View(merged_data)
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
# export merged data
# Function to create the string URL
create_url_before <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "~1/", notebook)
}
create_url_after <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "/", notebook)
}
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url_before = create_url_before(repo, commit_hash, notebook))
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url_before = create_url_after(repo, commit_hash, notebook))
# keep columns
merged_data <- subset(merged_data, select = c('commit_hash', 'refactor_code', 'repo', 'notebook', 'url'))
library(dplyr)
# read in refactoring observations
refactoring_observations <-
read.csv("4Refactoring_Observations.csv")
all_commits_in_sample <- read.csv("6All_Commits_in_Sample.csv")
# join the data sets based on commit_hash and keep only those in refactoring_observations
merged_data <-
merge(
refactoring_observations,
all_commits_in_sample,
by.x = "commit_hash",
by.y = "commit",
all.x = TRUE
)
# there's some extras, by inspection I assumed the actual data set removed these "duplicates" as multiple things happened in one?
merged_data <- distinct(merged_data)
# filter the merged data frame to only keep those rows where refactor_code matches what i want to keep
keeps <-
c("efun", "ek", "ev", "t1c", "t2c", "t3c", "rs")
merged_data <-
merged_data %>%
filter(refactor_code %in% keeps)
# export merged data
# Function to create the string URL
create_url_before <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "~1/", notebook)
}
create_url_after <- function(repo, commit_hash, notebook) {
paste0("https://raw.githubusercontent.com/", repo, "/", commit_hash, "/", notebook)
}
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url_before = create_url_before(repo, commit_hash, notebook))
# Create a new column with the URL strings
merged_data <- merged_data %>%
mutate(url_after = create_url_after(repo, commit_hash, notebook))
# keep columns
merged_data <- subset(merged_data, select = c('commit_hash', 'refactor_code', 'repo', 'notebook', 'url_before', 'url_after'))
View(merged_data)
write.csv(merged_data, "dataset1.csv")
