{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import openai\n",
    "\n",
    "sys.path.append('../../Tool')\n",
    "from notebook_processing import *\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from utils import num_tokens_from_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\"\n",
    "\n",
    "def gpt_wrapper(msgs):\n",
    "    while True:\n",
    "        try:\n",
    "            completions = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature = 1,\n",
    "                messages= msgs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if 'maximum context length' in str(e):\n",
    "                print('...Error.. too long...aborting...' + str(e))\n",
    "                return None\n",
    "            else:\n",
    "                print('...Error.. trying again...' + str(e))\n",
    "        else:\n",
    "            break\n",
    "    return completions.choices[0].finish_reason, completions.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_main = \"\"\"\"Write an introductory cell in markdown for this Jupyter Notebook under the following headings:\n",
    "- Title\n",
    "- Background (2-4 sentences briefly introducing the wider topic)\n",
    "- Goals (2-4 sentences specifically relating the wider topic to the purpose of the project)\n",
    "- Structure (concise bulleted list of the organization of the notebook's cells)\n",
    "\n",
    "Provided is 'Purpose' which provides background information to the project's purpose and 'Notebook' which includes all of the notebook's cells. Use these in your response.\"\"\"\n",
    "\n",
    "def introduction_prompt(notebook: Notebook):\n",
    "    cell_contents = notebook.get_all_cells(include_outputs=False)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": introduction_main},\n",
    "        {\"role\": \"user\", \"content\": f\"Purpose:\\n{notebook.purpose}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Notebook:\\n{cell_contents}\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"# Title\"},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "summarize_cell = f\"\"\"Summarize this code cell under the following headings in markdown:\n",
    "- Explanation (2-4 sentences summarizing what the code is doing)\n",
    "- Reasoning (2-4 sentences linking the code to the project's purpose under a specific section in the introduction)\n",
    "Provided below is the code cell itself as well as the introduction to the Jupyter Notebook\"\"\"\n",
    "\n",
    "summarize_cell_plus_output = f\"\"\"Summarize this code cell under the following headings in markdown:\n",
    "- Explanation (2-4 sentences summarizing what the code is doing)\n",
    "- Reasoning (2-4 sentences linking the code to the project's purpose under a specific section in the introduction)\n",
    "- Output (2-4 sentences interpreting the meaning of the output and linking this to the project)\n",
    "Provided below is the code cell itself, its output, as well as the introduction to the Jupyter Notebook. Use these in your response.\"\"\"\n",
    "\n",
    "def summarize_cell_prompt(notebook: Notebook, introduction: str, cell_id: int):\n",
    "    cell = notebook.get_single_cell(cell_id, include_outputs=True)\n",
    "    cell_src = cell['src']\n",
    "    cell_output = cell['outputs']\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": summarize_cell_plus_output if cell_output else summarize_cell},\n",
    "        {\"role\": \"user\", \"content\": f\"Introduction:\\n{introduction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Code cell:\\n```python\\n{cell_src}\\n```\"}\n",
    "    ]\n",
    "    if cell_output:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Output:\\n```{cell_output}```\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": \"## Explanation\"})\n",
    "    return messages\n",
    "\n",
    "conclusion_main = \"\"\"Write a conclusion in markdown for this Jupyter Notebook in past tense under the following headings:\n",
    "- Summary (2-4 sentences very briefly reiterating the purpose and goals of the notebook and explaining what was done in the notebook)\n",
    "- Interpretation (2-4 sentences interpreting key results, findings, or outputs and linking these to the project)\n",
    "\n",
    "Provided is 'Introduction' which contains the introduction of the notebook and 'Notebook' which contains all cells and any associated outputs. Use these in your response.\"\"\"\n",
    "\n",
    "def conclusion_prompt(notebook: Notebook, introduction: str):\n",
    "    cells = notebook.get_all_cells(include_outputs=False, include_markdown=True)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": conclusion_main},\n",
    "        {\"role\": \"user\", \"content\": f\"Introduction:\\n{introduction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Notebook:\\n{cells}\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"## Summary\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import notebooks\n",
    "titanic = Notebook(file_path='titanic.ipynb') # https://www.kaggle.com/code/zxarifi/titanic-competition-with-logistic-regression\n",
    "house = Notebook(file_path='house.ipynb') # https://www.kaggle.com/code/ddjhala/house-price-prediction-using-ml-models\n",
    "\n",
    "titanic.purpose = \"This notebook is from the popular Kaggle competition, Titanic Survival Prediction. The competition involves using machine learning to predict which passengers survived the Titanic shipwreck.\"\n",
    "house.purpose = \"This notebook is from the Kaggle House Price Prediction competition. The competition involves using regression techniques and creative feature engineering to predict residential home sale prices in Ames, Iowa, based on a dataset with 79 variables.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic num cells: 37\n",
      "titanic num md cells: 4\n",
      "titanic num code cells: 33\n",
      "titanic average cell length: 167.8108108108108\n",
      "titanic average md cell length: 147.75\n",
      "titanic average code cell length: 170.24242424242425\n",
      "----------------------------------------\n",
      "house num cells: 29\n",
      "house num md cells: 8\n",
      "house num code cells: 21\n",
      "house average cell length: 239.72413793103448\n",
      "house average md cell length: 160.375\n",
      "house average code cell length: 269.95238095238096\n"
     ]
    }
   ],
   "source": [
    "# basic stats\n",
    "print(f\"titanic num cells: {titanic.num_cells}\")\n",
    "print(f\"titanic num md cells: {titanic.get_number_of_md_cells()}\")\n",
    "print(f\"titanic num code cells: {titanic.get_number_of_code_cells()}\")\n",
    "print(f\"titanic average cell length: {titanic.get_average_cell_length()}\")\n",
    "print(f\"titanic average md cell length: {titanic.get_average_md_cell_length()}\")\n",
    "print(f\"titanic average code cell length: {titanic.get_average_code_cell_length()}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"house num cells: {house.num_cells}\")\n",
    "print(f\"house num md cells: {house.get_number_of_md_cells()}\")\n",
    "print(f\"house num code cells: {house.get_number_of_code_cells()}\")\n",
    "print(f\"house average cell length: {house.get_average_cell_length()}\")\n",
    "print(f\"house average md cell length: {house.get_average_md_cell_length()}\")\n",
    "print(f\"house average code cell length: {house.get_average_code_cell_length()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '\"Write an introductory cell in markdown for this Jupyter Notebook under the following headings:\\n- Title\\n- Background (2-4 sentences briefly introducing the wider topic)\\n- Goals (2-4 sentences specifically relating the wider topic to the purpose of the project)\\n- Structure (concise bulleted list of the organization of the notebook\\'s cells)\\n\\nProvided is \\'Purpose\\' which provides background information to the project\\'s purpose and \\'Notebook\\' which includes all of the notebook\\'s cells. Use these in your response.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Purpose:\\nThis notebook is from the popular Kaggle competition, Titanic Survival Prediction. The competition involves using machine learning to predict which passengers survived the Titanic shipwreck.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Notebook:\\n[{\\'id\\': 0, \\'type\\': \\'markdown\\', \\'src\\': \\'# Titanic competition with Logistic Regression\\\\n\\\\n\\\\n## Objectives\\\\n\\\\n*   Clean Dataset\\\\n*   Train a Model to predict survive Vs Unsurvive \\\\n\\'}, {\\'id\\': 1, \\'type\\': \\'code\\', \\'src\\': \\'import pandas as pd\\\\nimport pylab as pl\\\\nimport numpy as np\\\\nfrom sklearn import preprocessing\\\\n%matplotlib inline \\\\nimport matplotlib.pyplot as plt\\'}, {\\'id\\': 2, \\'type\\': \\'code\\', \\'src\\': \\'df_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\", index_col=\\\\\\'PassengerId\\\\\\')\\'}, {\\'id\\': 3, \\'type\\': \\'markdown\\', \\'src\\': \\'### Become Famaliar with data \\'}, {\\'id\\': 4, \\'type\\': \\'code\\', \\'src\\': \\'df_train.info()\\'}, {\\'id\\': 5, \\'type\\': \\'code\\', \\'src\\': \\'df_train.describe()\\'}, {\\'id\\': 6, \\'type\\': \\'code\\', \\'src\\': \\'df_train.isnull().sum()\\'}, {\\'id\\': 7, \\'type\\': \\'markdown\\', \\'src\\': \\'## Data Cleaning\\'}, {\\'id\\': 8, \\'type\\': \\'code\\', \\'src\\': \"\\'\\'\\'\\\\n    Let\\'s fill Age with mean of Age columns and Embarked with mode of it (to better filling Embarked I think it is good first \\\\n    we splite our dataset to Survived an Unsurvides and fill embarked on mode of relative)\\\\n\\'\\'\\'\\\\ndf_train[\\'Age\\'].fillna(df_train[\\'Age\\'].mean(), inplace=True)\\\\ndf_train[\\'Embarked\\'].fillna(df_train[\\'Embarked\\'].mode()[0], inplace=True)\\\\n\"}, {\\'id\\': 9, \\'type\\': \\'code\\', \\'src\\': \\'import random\\\\ndef fill_cabin(cabin):\\\\n    cabins = []\\\\n    for c in list(df_train[\\\\\\'Cabin\\\\\\'].mode()):\\\\n        cabins += c.split(\" \")\\\\n    cabin = random.choice(cabins)\\\\n    return cabin\\\\nmask = df_train[\\\\\\'Cabin\\\\\\'].isnull()\\\\ndf_train.loc[mask, \\\\\\'Cabin\\\\\\'] = df_train.loc[mask, \\\\\\'Cabin\\\\\\'].apply(fill_cabin)\\'}, {\\'id\\': 10, \\'type\\': \\'code\\', \\'src\\': \\'df_train.isnull().sum()\\'}, {\\'id\\': 11, \\'type\\': \\'code\\', \\'src\\': \\'cabin_mapping = { \\\\\\'G6\\\\\\': 1700, \\\\\\'T\\\\\\': 1800}\\\\ndef get_dicemal(cabin):\\\\n    hex_number = cabin.split(\" \")[0]\\\\n    if hex_number in cabin_mapping:\\\\n        return cabin_mapping.get(hex_number)\\\\n    return int(hex_number, 16)\\\\n\\\\ndf_train[\\\\\\'Cabin\\\\\\'] = df_train[\\\\\\'Cabin\\\\\\'].apply(get_dicemal)\\'}, {\\'id\\': 12, \\'type\\': \\'code\\', \\'src\\': \"# let\\'s change Embarked characters to a number {C: 0, Q:1, S:2}\\\\nfrom sklearn.preprocessing import LabelEncoder\\\\nembark_label_encoder = LabelEncoder()\\\\ndf_train[\\'Embarked\\']= embark_label_encoder.fit_transform(df_train[\\'Embarked\\'])\\\\ndf_train[\\'Embarked\\'].unique()\"}, {\\'id\\': 13, \\'type\\': \\'code\\', \\'src\\': \"# Delete Ticket and name of Passenger\\\\ndf_train.drop(columns=[\\'Ticket\\', \\'Name\\'], inplace=True)\"}, {\\'id\\': 14, \\'type\\': \\'code\\', \\'src\\': \"# change sex to number {male:1, female:0}\\\\nsex_label_encoder = LabelEncoder()\\\\ndf_train[\\'Sex\\'] = sex_label_encoder.fit_transform(df_train[\\'Sex\\'])\\\\ndf_train[\\'Sex\\'].unique()\"}, {\\'id\\': 15, \\'type\\': \\'code\\', \\'src\\': \\'# As we can see our dataset looks good to a machile learning model(it\\\\\\'s features are all numric)\\\\n# Let\\\\\\'s splite the dataset features to independent(X) and dependent(Y)\\\\nimport numpy as np\\\\nX = np.asarray(df_train[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]])\\\\nY = np.asarray(df_train[\\\\\\'Survived\\\\\\'])\\'}, {\\'id\\': 16, \\'type\\': \\'code\\', \\'src\\': \\'from sklearn.preprocessing import StandardScaler\\\\nscaler = StandardScaler()\\'}, {\\'id\\': 17, \\'type\\': \\'code\\', \\'src\\': \\'X = scaler.fit(X).transform(X)\\'}, {\\'id\\': 18, \\'type\\': \\'code\\', \\'src\\': \"from sklearn.linear_model import LogisticRegression\\\\nLR = LogisticRegression(C=0.1, solver=\\'liblinear\\') #solver can be {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\\\\nLR.fit(X, Y)\"}, {\\'id\\': 19, \\'type\\': \\'code\\', \\'src\\': \\'Y_pred = LR.predict(X)\\'}, {\\'id\\': 20, \\'type\\': \\'code\\', \\'src\\': \"# Let\\'s try the jaccard index for accuracy evaluation. we can define jaccard as the size of the intersection divided by \\\\n# the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly match with \\\\n# the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\\\\nfrom sklearn.metrics import jaccard_score\\\\njaccard_score(Y, Y_pred, pos_label=0)\"}, {\\'id\\': 21, \\'type\\': \\'code\\', \\'src\\': \\'from sklearn.metrics import classification_report, confusion_matrix\\\\nimport itertools\\\\ndef plot_confusion_matrix(cm, classes,\\\\n                          normalize=False,\\\\n                          title=\\\\\\'Confusion matrix\\\\\\',\\\\n                          cmap=plt.cm.Blues):\\\\n    \"\"\"\\\\n    This function prints and plots the confusion matrix.\\\\n    Normalization can be applied by setting `normalize=True`.\\\\n    \"\"\"\\\\n    if normalize:\\\\n        cm = cm.astype(\\\\\\'float\\\\\\') / cm.sum(axis=1)[:, np.newaxis]\\\\n        print(\"Normalized confusion matrix\")\\\\n    else:\\\\n        print(\\\\\\'Confusion matrix, without normalization\\\\\\')\\\\n\\\\n    print(cm)\\\\n\\\\n    plt.imshow(cm, interpolation=\\\\\\'nearest\\\\\\', cmap=cmap)\\\\n    plt.title(title)\\\\n    plt.colorbar()\\\\n    tick_marks = np.arange(len(classes))\\\\n    plt.xticks(tick_marks, classes, rotation=45)\\\\n    plt.yticks(tick_marks, classes)\\\\n\\\\n    fmt = \\\\\\'.2f\\\\\\' if normalize else \\\\\\'d\\\\\\'\\\\n    thresh = cm.max() / 2.\\\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\\\n        plt.text(j, i, format(cm[i, j], fmt),\\\\n                 horizontalalignment=\"center\",\\\\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\\\\n\\\\n    plt.tight_layout()\\\\n    plt.ylabel(\\\\\\'True label\\\\\\')\\\\n    plt.xlabel(\\\\\\'Predicted label\\\\\\')\\\\nprint(confusion_matrix(Y, Y_pred, labels=[1,0]))\\'}, {\\'id\\': 22, \\'type\\': \\'code\\', \\'src\\': \"cnf_matrix = confusion_matrix(Y, Y_pred, labels=[1,0])\\\\nnp.set_printoptions(precision=2)\\\\n\\\\nplt.figure()\\\\nplot_confusion_matrix(cnf_matrix, classes=[\\'Survive=1\\',\\'Survive=0\\'],normalize= True,  title=\\'Confusion matrix\\')\"}, {\\'id\\': 23, \\'type\\': \\'code\\', \\'src\\': \\'print (classification_report(Y, Y_pred))\\'}, {\\'id\\': 24, \\'type\\': \\'markdown\\', \\'src\\': \\'**Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\\\\n\\\\n**Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\\\\n\\\\nSo, we can calculate the precision and recall of each class.\\\\n\\\\n**F1 score:** Now we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\\'}, {\\'id\\': 25, \\'type\\': \\'code\\', \\'src\\': \\'from sklearn.metrics import log_loss\\\\nlog_loss(Y, Y_pred)\\'}, {\\'id\\': 26, \\'type\\': \\'code\\', \\'src\\': \\'df_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\", index_col=\\\\\\'PassengerId\\\\\\')\\'}, {\\'id\\': 27, \\'type\\': \\'code\\', \\'src\\': \\'df_test[\\\\\\'Age\\\\\\'].fillna(df_test[\\\\\\'Age\\\\\\'].mean(), inplace=True)\\\\n\\\\ndef fill_cabin(cabin):\\\\n    cabins = []\\\\n    for c in list(df_test[\\\\\\'Cabin\\\\\\'].mode()):\\\\n        cabins += c.split(\" \")\\\\n    cabin = random.choice(cabins)\\\\n    return cabin\\\\n\\\\ndef get_dicemal(cabin):\\\\n    hex_number = cabin.split(\" \")[0]\\\\n    if hex_number in cabin_mapping:\\\\n        return cabin_mapping.get(hex_number)\\\\n    return int(hex_number, 16)\\\\n\\\\n\\\\nmask = df_test[\\\\\\'Cabin\\\\\\'].isnull()\\\\ndf_test.loc[mask, \\\\\\'Cabin\\\\\\'] = df_test.loc[mask, \\\\\\'Cabin\\\\\\'].apply(fill_cabin)\\\\n\\\\ndf_test[\\\\\\'Cabin\\\\\\'] = df_test[\\\\\\'Cabin\\\\\\'].apply(get_dicemal)\\\\n\\\\n\\'}, {\\'id\\': 28, \\'type\\': \\'code\\', \\'src\\': \"df_test[\\'Embarked\\']= embark_label_encoder.fit_transform(df_test[\\'Embarked\\'])\\\\n\\\\n\\\\nsex_label_encoder = LabelEncoder()\\\\ndf_test[\\'Sex\\'] = sex_label_encoder.fit_transform(df_test[\\'Sex\\'])\\\\n\\\\n\"}, {\\'id\\': 29, \\'type\\': \\'code\\', \\'src\\': \"df_test.drop(columns=[\\'Ticket\\', \\'Name\\'], inplace=True)\"}, {\\'id\\': 30, \\'type\\': \\'code\\', \\'src\\': \"df_test[\\'Fare\\'].fillna(35.627188489208635, inplace=True)\"}, {\\'id\\': 31, \\'type\\': \\'code\\', \\'src\\': \\'X = np.asarray(df_test[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]])\\\\nX = scaler.fit(X).transform(X)\\'}, {\\'id\\': 32, \\'type\\': \\'code\\', \\'src\\': \\'Y_pred = LR.predict(X)\\'}, {\\'id\\': 33, \\'type\\': \\'code\\', \\'src\\': \"pred = pd.DataFrame(Y_pred, columns = [\\'Survived\\'])\"}, {\\'id\\': 34, \\'type\\': \\'code\\', \\'src\\': \"ids = pd.DataFrame(df_test.index, columns = [\\'PassengerId\\'])\"}, {\\'id\\': 35, \\'type\\': \\'code\\', \\'src\\': \\'sub = pd.concat([ids, pred], axis=1)\\'}, {\\'id\\': 36, \\'type\\': \\'code\\', \\'src\\': \"sub.to_csv(\\'submission.csv\\', index=False)\"}]'},\n",
       " {'role': 'assistant', 'content': '# Title'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_intro_prompt = introduction_prompt(titanic)\n",
    "titanic_intro_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2943"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(str(titanic_intro_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_intro_gpt = gpt_wrapper(titanic_intro_prompt)\n",
    "with open('titanic_intro.txt', 'r') as f:\n",
    "    titanic_intro_gpt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Title\n",
      "Titanic Survival Prediction using Logistic Regression\n",
      "\n",
      "# Background\n",
      "The Titanic Survival Prediction competition on Kaggle involves using machine learning techniques to predict which passengers survived the Titanic shipwreck. The dataset includes information such as the passenger's class, gender, age, number of siblings/spouses aboard, number of parents/children aboard, fare, cabin, and port of embarkation.\n",
      "\n",
      "# Goals\n",
      "The goal of this project is to clean the dataset, train a Logistic Regression model, and predict the survival of passengers on the Titanic based on their characteristics.\n",
      "\n",
      "# Structure\n",
      "The notebook is organized into the following cells:\n",
      "- Data Cleaning: Various steps to clean the dataset, such as filling missing values for age and embarked, handling cabin data, and encoding categorical variables.\n",
      "- Model Training: Scaling the features, fitting the Logistic Regression model to the training data, and predicting the survival of passengers.\n",
      "- Evaluation: Calculating the Jaccard index for accuracy evaluation, plotting the confusion matrix, and calculating precision, recall, and F1 score for each class.\n",
      "- Submission: Preparing the predicted survival data for submission to Kaggle.\n"
     ]
    }
   ],
   "source": [
    "print(titanic_intro_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic cell explanation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Summarize this code cell under the following headings in markdown:\\n- Explanation (2-4 sentences summarizing what the code is doing)\\n- Reasoning (2-4 sentences linking the code to the project's purpose under a specific section in the introduction)\\n- Output (2-4 sentences interpreting the meaning of the output and linking this to the project)\\nProvided below is the code cell itself, its output, as well as the introduction to the Jupyter Notebook. Use these in your response.\"},\n",
       " {'role': 'user',\n",
       "  'content': \"Introduction:\\n# Title\\nTitanic Survival Prediction using Logistic Regression\\n\\n# Background\\nThe Titanic Survival Prediction competition on Kaggle involves using machine learning techniques to predict which passengers survived the Titanic shipwreck. The dataset includes information such as the passenger's class, gender, age, number of siblings/spouses aboard, number of parents/children aboard, fare, cabin, and port of embarkation.\\n\\n# Goals\\nThe goal of this project is to clean the dataset, train a Logistic Regression model, and predict the survival of passengers on the Titanic based on their characteristics.\\n\\n# Structure\\nThe notebook is organized into the following cells:\\n- Data Cleaning: Various steps to clean the dataset, such as filling missing values for age and embarked, handling cabin data, and encoding categorical variables.\\n- Model Training: Scaling the features, fitting the Logistic Regression model to the training data, and predicting the survival of passengers.\\n- Evaluation: Calculating the Jaccard index for accuracy evaluation, plotting the confusion matrix, and calculating precision, recall, and F1 score for each class.\\n- Submission: Preparing the predicted survival data for submission to Kaggle.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Code cell:\\n```python\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        print(\\'Confusion matrix, without normalization\\')\\n\\n    print(cm)\\n\\n    plt.imshow(cm, interpolation=\\'nearest\\', cmap=cmap)\\n    plt.title(title)\\n    plt.colorbar()\\n    tick_marks = np.arange(len(classes))\\n    plt.xticks(tick_marks, classes, rotation=45)\\n    plt.yticks(tick_marks, classes)\\n\\n    fmt = \\'.2f\\' if normalize else \\'d\\'\\n    thresh = cm.max() / 2.\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\n        plt.text(j, i, format(cm[i, j], fmt),\\n                 horizontalalignment=\"center\",\\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\\n\\n    plt.tight_layout()\\n    plt.ylabel(\\'True label\\')\\n    plt.xlabel(\\'Predicted label\\')\\nprint(confusion_matrix(Y, Y_pred, labels=[1,0]))\\n```'},\n",
       " {'role': 'user', 'content': \"Output:\\n```['[[243  99]\\\\n [ 80 469]]\\\\n']```\"},\n",
       " {'role': 'assistant', 'content': '## Explanation'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_cell_prompt(titanic, titanic_intro_gpt, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Explanation\n",
      "The code cell defines a function called `plot_confusion_matrix` that takes in a confusion matrix, a list of classes, and some optional parameters. This function is used to print and plot the confusion matrix, which is a table used to evaluate the performance of a classification model. It shows the counts of true positive, true negative, false positive, and false negative predictions made by the model.\n",
      "\n",
      "## Reasoning\n",
      "The `plot_confusion_matrix` function is essential for evaluating the performance of the Logistic Regression model trained in the previous cell. By visualizing the confusion matrix, we can see how well the model predicts the survival of passengers on the Titanic based on their characteristics. This information is critical for understanding the accuracy, precision, recall, and F1 score of the model, which are used for further evaluation and comparison with alternative models.\n",
      "\n",
      "## Output\n",
      "The output of the code cell is the confusion matrix itself, represented as a 2D array. In this case, the confusion matrix is:\n",
      "```\n",
      "[[243  99]\n",
      " [ 80 469]]\n",
      "```\n",
      "This means that the model correctly predicted 243 passengers as survivors and 469 passengers as non-survivors. However, it incorrectly predicted 99 passengers as non-survivors when they actually survived and 80 passengers as survivors when they did not survive. This information can be used to evaluate the model's performance and identify any patterns or biases in its predictions.\n"
     ]
    }
   ],
   "source": [
    "# titanic_cell_reason, titanic_cell_result = gpt_wrapper(summarize_cell_prompt(titanic, titanic_intro_gpt, 21))\n",
    "with open('titanic_cell21_explanation.txt', 'r') as f:\n",
    "    titanic_cell_result = f.read()\n",
    "# print(reason)\n",
    "print(titanic_cell_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic conclusion text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Write a conclusion in markdown for this Jupyter Notebook in past tense under the following headings:\\n- Summary (2-4 sentences very briefly reiterating the purpose and goals of the notebook and explaining what was done in the notebook)\\n- Interpretation (2-4 sentences interpreting key results, findings, or outputs and linking these to the project)\\n\\nProvided is 'Introduction' which contains the introduction of the notebook and 'Notebook' which contains all cells and any associated outputs. Use these in your response.\"},\n",
       " {'role': 'user',\n",
       "  'content': \"Introduction:\\n# Title\\nTitanic Survival Prediction using Logistic Regression\\n\\n# Background\\nThe Titanic Survival Prediction competition on Kaggle involves using machine learning techniques to predict which passengers survived the Titanic shipwreck. The dataset includes information such as the passenger's class, gender, age, number of siblings/spouses aboard, number of parents/children aboard, fare, cabin, and port of embarkation.\\n\\n# Goals\\nThe goal of this project is to clean the dataset, train a Logistic Regression model, and predict the survival of passengers on the Titanic based on their characteristics.\\n\\n# Structure\\nThe notebook is organized into the following cells:\\n- Data Cleaning: Various steps to clean the dataset, such as filling missing values for age and embarked, handling cabin data, and encoding categorical variables.\\n- Model Training: Scaling the features, fitting the Logistic Regression model to the training data, and predicting the survival of passengers.\\n- Evaluation: Calculating the Jaccard index for accuracy evaluation, plotting the confusion matrix, and calculating precision, recall, and F1 score for each class.\\n- Submission: Preparing the predicted survival data for submission to Kaggle.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Notebook:\\n[{\\'id\\': 0, \\'type\\': \\'markdown\\', \\'src\\': \\'# Titanic competition with Logistic Regression\\\\n\\\\n\\\\n## Objectives\\\\n\\\\n*   Clean Dataset\\\\n*   Train a Model to predict survive Vs Unsurvive \\\\n\\'}, {\\'id\\': 1, \\'type\\': \\'code\\', \\'src\\': \\'import pandas as pd\\\\nimport pylab as pl\\\\nimport numpy as np\\\\nfrom sklearn import preprocessing\\\\n%matplotlib inline \\\\nimport matplotlib.pyplot as plt\\'}, {\\'id\\': 2, \\'type\\': \\'code\\', \\'src\\': \\'df_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\", index_col=\\\\\\'PassengerId\\\\\\')\\'}, {\\'id\\': 3, \\'type\\': \\'markdown\\', \\'src\\': \\'### Become Famaliar with data \\'}, {\\'id\\': 4, \\'type\\': \\'code\\', \\'src\\': \\'df_train.info()\\'}, {\\'id\\': 5, \\'type\\': \\'code\\', \\'src\\': \\'df_train.describe()\\'}, {\\'id\\': 6, \\'type\\': \\'code\\', \\'src\\': \\'df_train.isnull().sum()\\'}, {\\'id\\': 7, \\'type\\': \\'markdown\\', \\'src\\': \\'## Data Cleaning\\'}, {\\'id\\': 8, \\'type\\': \\'code\\', \\'src\\': \"\\'\\'\\'\\\\n    Let\\'s fill Age with mean of Age columns and Embarked with mode of it (to better filling Embarked I think it is good first \\\\n    we splite our dataset to Survived an Unsurvides and fill embarked on mode of relative)\\\\n\\'\\'\\'\\\\ndf_train[\\'Age\\'].fillna(df_train[\\'Age\\'].mean(), inplace=True)\\\\ndf_train[\\'Embarked\\'].fillna(df_train[\\'Embarked\\'].mode()[0], inplace=True)\\\\n\"}, {\\'id\\': 9, \\'type\\': \\'code\\', \\'src\\': \\'import random\\\\ndef fill_cabin(cabin):\\\\n    cabins = []\\\\n    for c in list(df_train[\\\\\\'Cabin\\\\\\'].mode()):\\\\n        cabins += c.split(\" \")\\\\n    cabin = random.choice(cabins)\\\\n    return cabin\\\\nmask = df_train[\\\\\\'Cabin\\\\\\'].isnull()\\\\ndf_train.loc[mask, \\\\\\'Cabin\\\\\\'] = df_train.loc[mask, \\\\\\'Cabin\\\\\\'].apply(fill_cabin)\\'}, {\\'id\\': 10, \\'type\\': \\'code\\', \\'src\\': \\'df_train.isnull().sum()\\'}, {\\'id\\': 11, \\'type\\': \\'code\\', \\'src\\': \\'cabin_mapping = { \\\\\\'G6\\\\\\': 1700, \\\\\\'T\\\\\\': 1800}\\\\ndef get_dicemal(cabin):\\\\n    hex_number = cabin.split(\" \")[0]\\\\n    if hex_number in cabin_mapping:\\\\n        return cabin_mapping.get(hex_number)\\\\n    return int(hex_number, 16)\\\\n\\\\ndf_train[\\\\\\'Cabin\\\\\\'] = df_train[\\\\\\'Cabin\\\\\\'].apply(get_dicemal)\\'}, {\\'id\\': 12, \\'type\\': \\'code\\', \\'src\\': \"# let\\'s change Embarked characters to a number {C: 0, Q:1, S:2}\\\\nfrom sklearn.preprocessing import LabelEncoder\\\\nembark_label_encoder = LabelEncoder()\\\\ndf_train[\\'Embarked\\']= embark_label_encoder.fit_transform(df_train[\\'Embarked\\'])\\\\ndf_train[\\'Embarked\\'].unique()\"}, {\\'id\\': 13, \\'type\\': \\'code\\', \\'src\\': \"# Delete Ticket and name of Passenger\\\\ndf_train.drop(columns=[\\'Ticket\\', \\'Name\\'], inplace=True)\"}, {\\'id\\': 14, \\'type\\': \\'code\\', \\'src\\': \"# change sex to number {male:1, female:0}\\\\nsex_label_encoder = LabelEncoder()\\\\ndf_train[\\'Sex\\'] = sex_label_encoder.fit_transform(df_train[\\'Sex\\'])\\\\ndf_train[\\'Sex\\'].unique()\"}, {\\'id\\': 15, \\'type\\': \\'code\\', \\'src\\': \\'# As we can see our dataset looks good to a machile learning model(it\\\\\\'s features are all numric)\\\\n# Let\\\\\\'s splite the dataset features to independent(X) and dependent(Y)\\\\nimport numpy as np\\\\nX = np.asarray(df_train[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]])\\\\nY = np.asarray(df_train[\\\\\\'Survived\\\\\\'])\\'}, {\\'id\\': 16, \\'type\\': \\'code\\', \\'src\\': \\'from sklearn.preprocessing import StandardScaler\\\\nscaler = StandardScaler()\\'}, {\\'id\\': 17, \\'type\\': \\'code\\', \\'src\\': \\'X = scaler.fit(X).transform(X)\\'}, {\\'id\\': 18, \\'type\\': \\'code\\', \\'src\\': \"from sklearn.linear_model import LogisticRegression\\\\nLR = LogisticRegression(C=0.1, solver=\\'liblinear\\') #solver can be {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\\\\nLR.fit(X, Y)\"}, {\\'id\\': 19, \\'type\\': \\'code\\', \\'src\\': \\'Y_pred = LR.predict(X)\\'}, {\\'id\\': 20, \\'type\\': \\'code\\', \\'src\\': \"# Let\\'s try the jaccard index for accuracy evaluation. we can define jaccard as the size of the intersection divided by \\\\n# the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly match with \\\\n# the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\\\\nfrom sklearn.metrics import jaccard_score\\\\njaccard_score(Y, Y_pred, pos_label=0)\"}, {\\'id\\': 21, \\'type\\': \\'code\\', \\'src\\': \\'from sklearn.metrics import classification_report, confusion_matrix\\\\nimport itertools\\\\ndef plot_confusion_matrix(cm, classes,\\\\n                          normalize=False,\\\\n                          title=\\\\\\'Confusion matrix\\\\\\',\\\\n                          cmap=plt.cm.Blues):\\\\n    \"\"\"\\\\n    This function prints and plots the confusion matrix.\\\\n    Normalization can be applied by setting `normalize=True`.\\\\n    \"\"\"\\\\n    if normalize:\\\\n        cm = cm.astype(\\\\\\'float\\\\\\') / cm.sum(axis=1)[:, np.newaxis]\\\\n        print(\"Normalized confusion matrix\")\\\\n    else:\\\\n        print(\\\\\\'Confusion matrix, without normalization\\\\\\')\\\\n\\\\n    print(cm)\\\\n\\\\n    plt.imshow(cm, interpolation=\\\\\\'nearest\\\\\\', cmap=cmap)\\\\n    plt.title(title)\\\\n    plt.colorbar()\\\\n    tick_marks = np.arange(len(classes))\\\\n    plt.xticks(tick_marks, classes, rotation=45)\\\\n    plt.yticks(tick_marks, classes)\\\\n\\\\n    fmt = \\\\\\'.2f\\\\\\' if normalize else \\\\\\'d\\\\\\'\\\\n    thresh = cm.max() / 2.\\\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\\\n        plt.text(j, i, format(cm[i, j], fmt),\\\\n                 horizontalalignment=\"center\",\\\\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\\\\n\\\\n    plt.tight_layout()\\\\n    plt.ylabel(\\\\\\'True label\\\\\\')\\\\n    plt.xlabel(\\\\\\'Predicted label\\\\\\')\\\\nprint(confusion_matrix(Y, Y_pred, labels=[1,0]))\\'}, {\\'id\\': 22, \\'type\\': \\'code\\', \\'src\\': \"cnf_matrix = confusion_matrix(Y, Y_pred, labels=[1,0])\\\\nnp.set_printoptions(precision=2)\\\\n\\\\nplt.figure()\\\\nplot_confusion_matrix(cnf_matrix, classes=[\\'Survive=1\\',\\'Survive=0\\'],normalize= True,  title=\\'Confusion matrix\\')\"}, {\\'id\\': 23, \\'type\\': \\'code\\', \\'src\\': \\'print (classification_report(Y, Y_pred))\\'}, {\\'id\\': 24, \\'type\\': \\'markdown\\', \\'src\\': \\'**Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\\\\n\\\\n**Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\\\\n\\\\nSo, we can calculate the precision and recall of each class.\\\\n\\\\n**F1 score:** Now we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\\'}, {\\'id\\': 25, \\'type\\': \\'code\\', \\'src\\': \\'from sklearn.metrics import log_loss\\\\nlog_loss(Y, Y_pred)\\'}, {\\'id\\': 26, \\'type\\': \\'code\\', \\'src\\': \\'df_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\", index_col=\\\\\\'PassengerId\\\\\\')\\'}, {\\'id\\': 27, \\'type\\': \\'code\\', \\'src\\': \\'df_test[\\\\\\'Age\\\\\\'].fillna(df_test[\\\\\\'Age\\\\\\'].mean(), inplace=True)\\\\n\\\\ndef fill_cabin(cabin):\\\\n    cabins = []\\\\n    for c in list(df_test[\\\\\\'Cabin\\\\\\'].mode()):\\\\n        cabins += c.split(\" \")\\\\n    cabin = random.choice(cabins)\\\\n    return cabin\\\\n\\\\ndef get_dicemal(cabin):\\\\n    hex_number = cabin.split(\" \")[0]\\\\n    if hex_number in cabin_mapping:\\\\n        return cabin_mapping.get(hex_number)\\\\n    return int(hex_number, 16)\\\\n\\\\n\\\\nmask = df_test[\\\\\\'Cabin\\\\\\'].isnull()\\\\ndf_test.loc[mask, \\\\\\'Cabin\\\\\\'] = df_test.loc[mask, \\\\\\'Cabin\\\\\\'].apply(fill_cabin)\\\\n\\\\ndf_test[\\\\\\'Cabin\\\\\\'] = df_test[\\\\\\'Cabin\\\\\\'].apply(get_dicemal)\\\\n\\\\n\\'}, {\\'id\\': 28, \\'type\\': \\'code\\', \\'src\\': \"df_test[\\'Embarked\\']= embark_label_encoder.fit_transform(df_test[\\'Embarked\\'])\\\\n\\\\n\\\\nsex_label_encoder = LabelEncoder()\\\\ndf_test[\\'Sex\\'] = sex_label_encoder.fit_transform(df_test[\\'Sex\\'])\\\\n\\\\n\"}, {\\'id\\': 29, \\'type\\': \\'code\\', \\'src\\': \"df_test.drop(columns=[\\'Ticket\\', \\'Name\\'], inplace=True)\"}, {\\'id\\': 30, \\'type\\': \\'code\\', \\'src\\': \"df_test[\\'Fare\\'].fillna(35.627188489208635, inplace=True)\"}, {\\'id\\': 31, \\'type\\': \\'code\\', \\'src\\': \\'X = np.asarray(df_test[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]])\\\\nX = scaler.fit(X).transform(X)\\'}, {\\'id\\': 32, \\'type\\': \\'code\\', \\'src\\': \\'Y_pred = LR.predict(X)\\'}, {\\'id\\': 33, \\'type\\': \\'code\\', \\'src\\': \"pred = pd.DataFrame(Y_pred, columns = [\\'Survived\\'])\"}, {\\'id\\': 34, \\'type\\': \\'code\\', \\'src\\': \"ids = pd.DataFrame(df_test.index, columns = [\\'PassengerId\\'])\"}, {\\'id\\': 35, \\'type\\': \\'code\\', \\'src\\': \\'sub = pd.concat([ids, pred], axis=1)\\'}, {\\'id\\': 36, \\'type\\': \\'code\\', \\'src\\': \"sub.to_csv(\\'submission.csv\\', index=False)\"}]'},\n",
       " {'role': 'assistant', 'content': '## Summary'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_conclusion_prompt = conclusion_prompt(titanic, titanic_intro_gpt)\n",
    "titanic_conclusion_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3143"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(str(titanic_conclusion_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    }
   ],
   "source": [
    "titanic_conclusion_reason, titanic_conclusion_result = gpt_wrapper(titanic_conclusion_prompt)\n",
    "print(titanic_conclusion_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(titanic_conclusion_result)\n",
    "with open('titanic_conclusion.txt', 'r') as f:\n",
    "    titanic_cell_result = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Summary\n",
      "The purpose of this notebook was to clean the Titanic survival prediction dataset, train a Logistic Regression model, and predict the survival of passengers based on their characteristics. The notebook is structured into different sections, including data cleaning, model training, evaluation, and submission.\n",
      "\n",
      "## Interpretation\n",
      "The Logistic Regression model was trained using the cleaned dataset, and the predictions were made on the same dataset. The Jaccard index was used for accuracy evaluation, and the confusion matrix was plotted to visualize the performance of the model. Precision, recall, F1 score, and log loss were also calculated to assess the model's performance. Finally, the model was used to predict survival on a test dataset, and the results were prepared for submission to Kaggle.\n"
     ]
    }
   ],
   "source": [
    "print(titanic_cell_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '\"Write an introductory cell in markdown for this Jupyter Notebook under the following headings:\\n- Title\\n- Background (2-4 sentences briefly introducing the wider topic)\\n- Goals (2-4 sentences specifically relating the wider topic to the purpose of the project)\\n- Structure (concise bulleted list of the organization of the notebook\\'s cells)\\n\\nProvided is \\'Purpose\\' which provides background information to the project\\'s purpose and \\'Notebook\\' which includes all of the notebook\\'s cells. Use these in your response.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Purpose:\\nThis notebook is from the Kaggle House Price Prediction competition. The competition involves using regression techniques and creative feature engineering to predict residential home sale prices in Ames, Iowa, based on a dataset with 79 variables.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Notebook:\\n[{\\'id\\': 0, \\'type\\': \\'markdown\\', \\'src\\': \\'# **House Price Prediction Using Random Forest Regression**\\'}, {\\'id\\': 1, \\'type\\': \\'markdown\\', \\'src\\': \\'This notebook is about data visualization, pre-processing the data and predicting the price of house.\\'}, {\\'id\\': 2, \\'type\\': \\'markdown\\', \\'src\\': \\'# Reading the dataset using Pandas\\\\nThe training and testing dataset are read using read_csv function of Pandas library.\\'}, {\\'id\\': 3, \\'type\\': \\'code\\', \\'src\\': \"import pandas as pd\\\\ntrain = pd.read_csv(\\'/kaggle/input/house-prices-advanced-regression-techniques/train.csv\\')\\\\ntest = pd.read_csv(\\'/kaggle/input/house-prices-advanced-regression-techniques/test.csv\\')\\\\ny = train[\\'SalePrice\\']\\\\ntrain = train.drop([\\'SalePrice\\'], axis=1)\\\\ndf = pd.concat([train, test])\\\\ndf = df.drop([\\'Id\\'], axis=1)\\\\ndf.describe()\"}, {\\'id\\': 4, \\'type\\': \\'markdown\\', \\'src\\': \\'# Checking the Null values\\\\nThe dataset contains NaN values which needs to be filled properly.\\'}, {\\'id\\': 5, \\'type\\': \\'code\\', \\'src\\': \"df[\\'Alley\\'] = df[\\'Alley\\'].fillna(\\'No alley access\\')\\\\ndf[\\'BsmtQual\\'] = df[\\'BsmtQual\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtCond\\'] = df[\\'BsmtCond\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtExposure\\'] = df[\\'BsmtExposure\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtFinType1\\'] = df[\\'BsmtFinType1\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtFinType2\\'] = df[\\'BsmtFinType2\\'].fillna(\\'No Basement\\')\\\\ndf[\\'FireplaceQu\\'] = df[\\'FireplaceQu\\'].fillna(\\'FireplaceQu\\')\\\\ndf[\\'GarageType\\'] = df[\\'GarageType\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'GarageFinish\\'] = df[\\'GarageFinish\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'GarageQual\\'] = df[\\'GarageQual\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'GarageCond\\'] = df[\\'GarageCond\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'PoolQC\\'] = df[\\'PoolQC\\'].fillna(\\'No Pool\\')\\\\ndf[\\'Fence\\'] = df[\\'Fence\\'].fillna(\\'No Fence\\')\\\\ndf\"}, {\\'id\\': 6, \\'type\\': \\'markdown\\', \\'src\\': \\'# Handling missing values with taking new feature\\\\nThere are some features where many data are missing. In order to tackle that a new column corresponding to that column is made where the data in the new column is True if it is Null in that column else it is False. It will keep track that the data was missing.\\'}, {\\'id\\': 7, \\'type\\': \\'code\\', \\'src\\': \"df = df.drop([\\'MiscFeature\\'], axis=1)\\\\nmissing_cols = []\\\\nfor x in df:\\\\n    if df[x].isnull().sum()>400:\\\\n        missing_cols.append(x)\\\\nfor x in missing_cols:\\\\n    df[x+\\'_missing\\'] = df[x].isnull()\\\\ndf\"}, {\\'id\\': 8, \\'type\\': \\'code\\', \\'src\\': \\'missing_cols\\'}, {\\'id\\': 9, \\'type\\': \\'markdown\\', \\'src\\': \\'# Filling the Null/ Missing Values\\\\nThe columns of the dataset still has missing values. These values are filled using the Mode and Mean of data. For the categorical data, the missing values are filled with the Mode of the data for that particular feature and for continuous data, it is filled with the mean value.\\'}, {\\'id\\': 10, \\'type\\': \\'code\\', \\'src\\': \"for x in df:\\\\n    if any(df[x].isnull()):\\\\n        if df[x].dtype == \\'object\\' or df[x].dtype == \\'bool\\':\\\\n            df[x] = df[x].fillna(df[x].mode()[0])\\\\n        else:\\\\n            df[x] = df[x].fillna(df[x].mean())\\\\ndf\"}, {\\'id\\': 11, \\'type\\': \\'code\\', \\'src\\': \\'df.describe()\\'}, {\\'id\\': 12, \\'type\\': \\'markdown\\', \\'src\\': \\'# Categorical data to numeric data\\\\nThe Ordinal encoding is a type of data encoding where the categorical variables are represented as  numerical values.\\'}, {\\'id\\': 13, \\'type\\': \\'code\\', \\'src\\': \"from sklearn.preprocessing import OrdinalEncoder\\\\nencoder = OrdinalEncoder()\\\\nfor x in df:\\\\n    if df[x].dtype == \\'object\\' or df[x].dtype == \\'bool\\':\\\\n        column_values = df[x].values.reshape(-1, 1)\\\\n        transformed_values = encoder.fit_transform(column_values)\\\\n        df[x] = transformed_values\\\\ndf\"}, {\\'id\\': 14, \\'type\\': \\'code\\', \\'src\\': \"testing = df.iloc[len(train):, :]\\\\ntraining = df.iloc[:len(train), :]\\\\ntraining[\\'SalePrice\\'] = y\\\\ntraining.head()\"}, {\\'id\\': 15, \\'type\\': \\'markdown\\', \\'src\\': \\'# Data Visualization\\\\nThe correlation matrix is computed for the training data and created a heatmap to visualize the correlation matrix.\\'}, {\\'id\\': 16, \\'type\\': \\'code\\', \\'src\\': \\'import seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\ncorr = training.corr()\\\\nplt.subplots(figsize=(13,10))\\\\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\\'}, {\\'id\\': 17, \\'type\\': \\'code\\', \\'src\\': \"import matplotlib.pyplot as plt\\\\n# Plot the histogram\\\\nplt.hist(training[\\'SalePrice\\'], bins=100, edgecolor=\\'black\\')  # \\'bins\\' controls the number of bins in the histogram\\\\nplt.xlabel(\\'SalePrice\\')\\\\nplt.ylabel(\\'Frequency\\')\\\\nplt.title(\\'SalePrice Histogram\\')\\\\nplt.show()\"}, {\\'id\\': 18, \\'type\\': \\'code\\', \\'src\\': \\'import numpy as np\\\\nimport scipy\\\\n\\\\ndata_mean = np.mean(y)\\\\ndata_median = np.median(y)\\\\ndata_mode = np.argmax(np.bincount(y))\\\\ndata_std = np.std(y)\\\\ndata_range = np.ptp(y)\\\\ndata_skewness = scipy.stats.skew(y)\\\\ndata_kurtosis = scipy.stats.kurtosis(y)\\\\n\\\\nprint(\"Mean:\", data_mean)\\\\nprint(\"Median:\", data_median)\\\\nprint(\"Mode:\", data_mode)\\\\nprint(\"Standard Deviation:\", data_std)\\\\nprint(\"Data Range:\", data_range)\\\\nprint(\"Skewness:\", data_skewness)\\\\nprint(\"Kurtosis:\", data_kurtosis)\\'}, {\\'id\\': 19, \\'type\\': \\'code\\', \\'src\\': \\'import pandas as pd\\\\n\\\\ncorrelation_matrix = training.corr()\\\\noutput_correlations = correlation_matrix[\\\\\\'SalePrice\\\\\\']\\\\n\\\\ntop_10_correlated = output_correlations.abs().sort_values(ascending=False)[1:11]\\\\ntop_10_correlated_columns = top_10_correlated.index\\\\ntop_10_correlation_values = output_correlations[top_10_correlated_columns]\\\\n\\\\nprint(\"Top 10 Correlated Columns with sale price:\")\\\\nprint(top_10_correlation_values)\\'}, {\\'id\\': 20, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nsns.boxplot(x=\\'OverallQual\\', y=\\'SalePrice\\', data=training[[\\'OverallQual\\', \\'SalePrice\\']])\\\\nplt.title(\\'Box Plot of OverallQual vs. SalePrice\\')\\\\nplt.xlabel(\\'Overall Quality\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.show()\"}, {\\'id\\': 21, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nplt.scatter(training[\\'GrLivArea\\'], training[\\'SalePrice\\'], c=\\'blue\\', alpha=0.7)\\\\nplt.xlabel(\\'GrLivArea\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.title(\\'Scatter Plot: GrLivArea vs. Sale Price\\')\\\\nplt.grid(True)\\\\nplt.show()\"}, {\\'id\\': 22, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nsns.boxplot(x=\\'GarageCars\\', y=\\'SalePrice\\', data=training[[\\'GarageCars\\', \\'SalePrice\\']])\\\\nplt.title(\\'Box Plot of GarageCars vs. SalePrice\\')\\\\nplt.xlabel(\\'GarageCars\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.show()\"}, {\\'id\\': 23, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nsns.boxplot(x=\\'ExterQual\\', y=\\'SalePrice\\', data=training[[\\'ExterQual\\', \\'SalePrice\\']])\\\\nplt.title(\\'Box Plot of ExterQual vs. SalePrice\\')\\\\nplt.xlabel(\\'ExterQual\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.show()\"}, {\\'id\\': 24, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nplt.scatter(training[\\'GarageArea\\'], training[\\'SalePrice\\'], c=\\'blue\\', alpha=0.7)\\\\nplt.xlabel(\\'GarageArea\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.title(\\'Scatter Plot: GarageArea vs. Sale Price\\')\\\\nplt.grid(True)\\\\nplt.show()\"}, {\\'id\\': 25, \\'type\\': \\'code\\', \\'src\\': \\'training.iloc[:, :80]\\'}, {\\'id\\': 26, \\'type\\': \\'code\\', \\'src\\': \"from sklearn.model_selection import train_test_split\\\\nxtrain, xtest, ytrain, ytest = train_test_split(np.array(training.iloc[:, :79]),training[\\'SalePrice\\'], test_size=0.2, random_state=42)\\\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\"}, {\\'id\\': 27, \\'type\\': \\'code\\', \\'src\\': \\'# XGBoost Model\\\\nimport xgboost as xgb\\\\nxgbModel = xgb.XGBRegressor(n_estimators=3500, learning_rate=0.05, random_state=42)\\\\nxgbModel.fit(xtrain, ytrain)\\\\nxgbpred = xgbModel.predict(xtest)\\\\nmse = mean_squared_error(ytest, xgbpred)\\\\nrmse = np.sqrt(mse)\\\\nr2 = r2_score(ytest, xgbpred)\\\\nprint(\\\\\\'eXtreme Boost Regression Model\\\\\\')\\\\nprint(\"Mean Squared Error (MSE):\", mse)\\\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\\\nprint(\"R-squared (R2) score:\", r2)\\'}, {\\'id\\': 28, \\'type\\': \\'code\\', \\'src\\': \"# Model Selection: xgboost\\\\nypred = xgbModel.predict(np.array(testing))\\\\nsubmission = pd.DataFrame()\\\\nsubmission[\\'Id\\'] = test[\\'Id\\']\\\\nsubmission[\\'SalePrice\\'] = ypred.astype(int)\\\\nsubmission.to_csv(\\'/kaggle/working/Submission.csv\\', index=False)\\\\nsubmission\"}]'},\n",
       " {'role': 'assistant', 'content': '# Title'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_intro_prompt = introduction_prompt(house)\n",
    "house_intro_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(str(house_intro_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    }
   ],
   "source": [
    "# house_intro_reason, house_intro_result = gpt_wrapper(house_intro_prompt)\n",
    "# print(house_intro_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# House Price Prediction Using Random Forest Regression\n",
      "\n",
      "# Background\n",
      "This Jupyter Notebook is based on the Kaggle House Price Prediction competition. The competition involves using regression techniques and creative feature engineering to predict residential home sale prices in Ames, Iowa, based on a dataset with 79 variables.\n",
      "\n",
      "# Goals\n",
      "The purpose of this project is to explore the dataset, preprocess the data, and build a random forest regression model to predict house prices. The ultimate goal is to accurately predict house prices based on the given features and improve upon the baseline models.\n",
      "\n",
      "# Structure\n",
      "This notebook is organized in the following manner:\n",
      "- Reading the dataset using Pandas\n",
      "- Checking the Null values\n",
      "- Handling missing values by creating new features\n",
      "- Filling the Null/Missing Values\n",
      "- Converting categorical data to numeric data using Ordinal encoding\n",
      "- Data Visualization using correlation matrix and histograms\n",
      "- Analysis of highly correlated features with the SalePrice\n",
      "- Splitting the data into training and testing sets\n",
      "- Building the XGBoost regression model\n",
      "- Making predictions and generating the submission file\n"
     ]
    }
   ],
   "source": [
    "with open('house_intro.txt', 'r') as f:\n",
    "    house_intro_result = f.read()\n",
    "print(house_intro_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation cell house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Summarize this code cell under the following headings in markdown:\\n- Explanation (2-4 sentences summarizing what the code is doing)\\n- Reasoning (2-4 sentences linking the code to the project's purpose under a specific section in the introduction)\\n- Output (2-4 sentences interpreting the meaning of the output and linking this to the project)\\nProvided below is the code cell itself, its output, as well as the introduction to the Jupyter Notebook. Use these in your response.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Introduction:\\n# House Price Prediction Using Random Forest Regression\\n\\n# Background\\nThis Jupyter Notebook is based on the Kaggle House Price Prediction competition. The competition involves using regression techniques and creative feature engineering to predict residential home sale prices in Ames, Iowa, based on a dataset with 79 variables.\\n\\n# Goals\\nThe purpose of this project is to explore the dataset, preprocess the data, and build a random forest regression model to predict house prices. The ultimate goal is to accurately predict house prices based on the given features and improve upon the baseline models.\\n\\n# Structure\\nThis notebook is organized in the following manner:\\n- Reading the dataset using Pandas\\n- Checking the Null values\\n- Handling missing values by creating new features\\n- Filling the Null/Missing Values\\n- Converting categorical data to numeric data using Ordinal encoding\\n- Data Visualization using correlation matrix and histograms\\n- Analysis of highly correlated features with the SalePrice\\n- Splitting the data into training and testing sets\\n- Building the XGBoost regression model\\n- Making predictions and generating the submission file'},\n",
       " {'role': 'user',\n",
       "  'content': 'Code cell:\\n```python\\n# XGBoost Model\\nimport xgboost as xgb\\nxgbModel = xgb.XGBRegressor(n_estimators=3500, learning_rate=0.05, random_state=42)\\nxgbModel.fit(xtrain, ytrain)\\nxgbpred = xgbModel.predict(xtest)\\nmse = mean_squared_error(ytest, xgbpred)\\nrmse = np.sqrt(mse)\\nr2 = r2_score(ytest, xgbpred)\\nprint(\\'eXtreme Boost Regression Model\\')\\nprint(\"Mean Squared Error (MSE):\", mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\nprint(\"R-squared (R2) score:\", r2)\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': \"Output:\\n```['eXtreme Boost Regression Model\\\\nMean Squared Error (MSE): 650040704.2765023\\\\nRoot Mean Squared Error (RMSE): 25495.895832006026\\\\nR-squared (R2) score: 0.915252534674808\\\\n']```\"},\n",
       " {'role': 'assistant', 'content': '## Explanation'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_cell_explanation_prompt = summarize_cell_prompt(house, house_intro_result, 27)\n",
    "house_cell_explanation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(str(house_cell_explanation_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    }
   ],
   "source": [
    "house_cell_explanation_reason, house_cell_explanation_result = gpt_wrapper(house_cell_explanation_prompt)\n",
    "print(house_cell_explanation_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code cell is fitting an XGBoost regression model to the training data and using it to make predictions on the test data. The model is then evaluating the performance by calculating the mean squared error (MSE), root mean squared error (RMSE), and R-squared (R2) score.\n",
      "\n",
      "## Reasoning\n",
      "\n",
      "This code cell is directly linked to the project's purpose of building a random forest regression model to predict house prices. XGBoost is a popular machine learning algorithm that can handle both regression and classification problems. By fitting an XGBoost regression model to the training data and evaluating its performance on the test data, the code is an essential component of building and improving upon the baseline models.\n",
      "\n",
      "## Output\n",
      "\n",
      "The output of this code cell displays the evaluation metrics of the XGBoost regression model. The mean squared error (MSE) measures the average squared difference between the predicted house prices and the actual house prices. The root mean squared error (RMSE) is the square root of the MSE, providing a more interpretable measure of the error. The R-squared (R2) score represents the proportion of the variance in the target variable (house prices) that can be explained by the predictor variables. A higher RMSE and lower R2 score indicate poorer model performance, while a lower RMSE and higher R2 score indicate better performance. In this case, the XGBoost regression model has achieved an RMSE of 25495.90 and an R2 score of 0.915, indicating relatively good performance in predicting house prices.\n"
     ]
    }
   ],
   "source": [
    "print(house_cell_explanation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion cell house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Write a conclusion in markdown for this Jupyter Notebook in past tense under the following headings:\\n- Summary (2-4 sentences very briefly reiterating the purpose and goals of the notebook and explaining what was done in the notebook)\\n- Interpretation (2-4 sentences interpreting key results, findings, or outputs and linking these to the project)\\n\\nProvided is 'Introduction' which contains the introduction of the notebook and 'Notebook' which contains all cells and any associated outputs. Use these in your response.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Introduction:\\n# House Price Prediction Using Random Forest Regression\\n\\n# Background\\nThis Jupyter Notebook is based on the Kaggle House Price Prediction competition. The competition involves using regression techniques and creative feature engineering to predict residential home sale prices in Ames, Iowa, based on a dataset with 79 variables.\\n\\n# Goals\\nThe purpose of this project is to explore the dataset, preprocess the data, and build a random forest regression model to predict house prices. The ultimate goal is to accurately predict house prices based on the given features and improve upon the baseline models.\\n\\n# Structure\\nThis notebook is organized in the following manner:\\n- Reading the dataset using Pandas\\n- Checking the Null values\\n- Handling missing values by creating new features\\n- Filling the Null/Missing Values\\n- Converting categorical data to numeric data using Ordinal encoding\\n- Data Visualization using correlation matrix and histograms\\n- Analysis of highly correlated features with the SalePrice\\n- Splitting the data into training and testing sets\\n- Building the XGBoost regression model\\n- Making predictions and generating the submission file'},\n",
       " {'role': 'user',\n",
       "  'content': 'Notebook:\\n[{\\'id\\': 0, \\'type\\': \\'markdown\\', \\'src\\': \\'# **House Price Prediction Using Random Forest Regression**\\'}, {\\'id\\': 1, \\'type\\': \\'markdown\\', \\'src\\': \\'This notebook is about data visualization, pre-processing the data and predicting the price of house.\\'}, {\\'id\\': 2, \\'type\\': \\'markdown\\', \\'src\\': \\'# Reading the dataset using Pandas\\\\nThe training and testing dataset are read using read_csv function of Pandas library.\\'}, {\\'id\\': 3, \\'type\\': \\'code\\', \\'src\\': \"import pandas as pd\\\\ntrain = pd.read_csv(\\'/kaggle/input/house-prices-advanced-regression-techniques/train.csv\\')\\\\ntest = pd.read_csv(\\'/kaggle/input/house-prices-advanced-regression-techniques/test.csv\\')\\\\ny = train[\\'SalePrice\\']\\\\ntrain = train.drop([\\'SalePrice\\'], axis=1)\\\\ndf = pd.concat([train, test])\\\\ndf = df.drop([\\'Id\\'], axis=1)\\\\ndf.describe()\"}, {\\'id\\': 4, \\'type\\': \\'markdown\\', \\'src\\': \\'# Checking the Null values\\\\nThe dataset contains NaN values which needs to be filled properly.\\'}, {\\'id\\': 5, \\'type\\': \\'code\\', \\'src\\': \"df[\\'Alley\\'] = df[\\'Alley\\'].fillna(\\'No alley access\\')\\\\ndf[\\'BsmtQual\\'] = df[\\'BsmtQual\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtCond\\'] = df[\\'BsmtCond\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtExposure\\'] = df[\\'BsmtExposure\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtFinType1\\'] = df[\\'BsmtFinType1\\'].fillna(\\'No Basement\\')\\\\ndf[\\'BsmtFinType2\\'] = df[\\'BsmtFinType2\\'].fillna(\\'No Basement\\')\\\\ndf[\\'FireplaceQu\\'] = df[\\'FireplaceQu\\'].fillna(\\'FireplaceQu\\')\\\\ndf[\\'GarageType\\'] = df[\\'GarageType\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'GarageFinish\\'] = df[\\'GarageFinish\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'GarageQual\\'] = df[\\'GarageQual\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'GarageCond\\'] = df[\\'GarageCond\\'].fillna(\\'No Garbage\\')\\\\ndf[\\'PoolQC\\'] = df[\\'PoolQC\\'].fillna(\\'No Pool\\')\\\\ndf[\\'Fence\\'] = df[\\'Fence\\'].fillna(\\'No Fence\\')\\\\ndf\"}, {\\'id\\': 6, \\'type\\': \\'markdown\\', \\'src\\': \\'# Handling missing values with taking new feature\\\\nThere are some features where many data are missing. In order to tackle that a new column corresponding to that column is made where the data in the new column is True if it is Null in that column else it is False. It will keep track that the data was missing.\\'}, {\\'id\\': 7, \\'type\\': \\'code\\', \\'src\\': \"df = df.drop([\\'MiscFeature\\'], axis=1)\\\\nmissing_cols = []\\\\nfor x in df:\\\\n    if df[x].isnull().sum()>400:\\\\n        missing_cols.append(x)\\\\nfor x in missing_cols:\\\\n    df[x+\\'_missing\\'] = df[x].isnull()\\\\ndf\"}, {\\'id\\': 8, \\'type\\': \\'code\\', \\'src\\': \\'missing_cols\\'}, {\\'id\\': 9, \\'type\\': \\'markdown\\', \\'src\\': \\'# Filling the Null/ Missing Values\\\\nThe columns of the dataset still has missing values. These values are filled using the Mode and Mean of data. For the categorical data, the missing values are filled with the Mode of the data for that particular feature and for continuous data, it is filled with the mean value.\\'}, {\\'id\\': 10, \\'type\\': \\'code\\', \\'src\\': \"for x in df:\\\\n    if any(df[x].isnull()):\\\\n        if df[x].dtype == \\'object\\' or df[x].dtype == \\'bool\\':\\\\n            df[x] = df[x].fillna(df[x].mode()[0])\\\\n        else:\\\\n            df[x] = df[x].fillna(df[x].mean())\\\\ndf\"}, {\\'id\\': 11, \\'type\\': \\'code\\', \\'src\\': \\'df.describe()\\'}, {\\'id\\': 12, \\'type\\': \\'markdown\\', \\'src\\': \\'# Categorical data to numeric data\\\\nThe Ordinal encoding is a type of data encoding where the categorical variables are represented as  numerical values.\\'}, {\\'id\\': 13, \\'type\\': \\'code\\', \\'src\\': \"from sklearn.preprocessing import OrdinalEncoder\\\\nencoder = OrdinalEncoder()\\\\nfor x in df:\\\\n    if df[x].dtype == \\'object\\' or df[x].dtype == \\'bool\\':\\\\n        column_values = df[x].values.reshape(-1, 1)\\\\n        transformed_values = encoder.fit_transform(column_values)\\\\n        df[x] = transformed_values\\\\ndf\"}, {\\'id\\': 14, \\'type\\': \\'code\\', \\'src\\': \"testing = df.iloc[len(train):, :]\\\\ntraining = df.iloc[:len(train), :]\\\\ntraining[\\'SalePrice\\'] = y\\\\ntraining.head()\"}, {\\'id\\': 15, \\'type\\': \\'markdown\\', \\'src\\': \\'# Data Visualization\\\\nThe correlation matrix is computed for the training data and created a heatmap to visualize the correlation matrix.\\'}, {\\'id\\': 16, \\'type\\': \\'code\\', \\'src\\': \\'import seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\ncorr = training.corr()\\\\nplt.subplots(figsize=(13,10))\\\\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\\'}, {\\'id\\': 17, \\'type\\': \\'code\\', \\'src\\': \"import matplotlib.pyplot as plt\\\\n# Plot the histogram\\\\nplt.hist(training[\\'SalePrice\\'], bins=100, edgecolor=\\'black\\')  # \\'bins\\' controls the number of bins in the histogram\\\\nplt.xlabel(\\'SalePrice\\')\\\\nplt.ylabel(\\'Frequency\\')\\\\nplt.title(\\'SalePrice Histogram\\')\\\\nplt.show()\"}, {\\'id\\': 18, \\'type\\': \\'code\\', \\'src\\': \\'import numpy as np\\\\nimport scipy\\\\n\\\\ndata_mean = np.mean(y)\\\\ndata_median = np.median(y)\\\\ndata_mode = np.argmax(np.bincount(y))\\\\ndata_std = np.std(y)\\\\ndata_range = np.ptp(y)\\\\ndata_skewness = scipy.stats.skew(y)\\\\ndata_kurtosis = scipy.stats.kurtosis(y)\\\\n\\\\nprint(\"Mean:\", data_mean)\\\\nprint(\"Median:\", data_median)\\\\nprint(\"Mode:\", data_mode)\\\\nprint(\"Standard Deviation:\", data_std)\\\\nprint(\"Data Range:\", data_range)\\\\nprint(\"Skewness:\", data_skewness)\\\\nprint(\"Kurtosis:\", data_kurtosis)\\'}, {\\'id\\': 19, \\'type\\': \\'code\\', \\'src\\': \\'import pandas as pd\\\\n\\\\ncorrelation_matrix = training.corr()\\\\noutput_correlations = correlation_matrix[\\\\\\'SalePrice\\\\\\']\\\\n\\\\ntop_10_correlated = output_correlations.abs().sort_values(ascending=False)[1:11]\\\\ntop_10_correlated_columns = top_10_correlated.index\\\\ntop_10_correlation_values = output_correlations[top_10_correlated_columns]\\\\n\\\\nprint(\"Top 10 Correlated Columns with sale price:\")\\\\nprint(top_10_correlation_values)\\'}, {\\'id\\': 20, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nsns.boxplot(x=\\'OverallQual\\', y=\\'SalePrice\\', data=training[[\\'OverallQual\\', \\'SalePrice\\']])\\\\nplt.title(\\'Box Plot of OverallQual vs. SalePrice\\')\\\\nplt.xlabel(\\'Overall Quality\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.show()\"}, {\\'id\\': 21, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nplt.scatter(training[\\'GrLivArea\\'], training[\\'SalePrice\\'], c=\\'blue\\', alpha=0.7)\\\\nplt.xlabel(\\'GrLivArea\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.title(\\'Scatter Plot: GrLivArea vs. Sale Price\\')\\\\nplt.grid(True)\\\\nplt.show()\"}, {\\'id\\': 22, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nsns.boxplot(x=\\'GarageCars\\', y=\\'SalePrice\\', data=training[[\\'GarageCars\\', \\'SalePrice\\']])\\\\nplt.title(\\'Box Plot of GarageCars vs. SalePrice\\')\\\\nplt.xlabel(\\'GarageCars\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.show()\"}, {\\'id\\': 23, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nsns.boxplot(x=\\'ExterQual\\', y=\\'SalePrice\\', data=training[[\\'ExterQual\\', \\'SalePrice\\']])\\\\nplt.title(\\'Box Plot of ExterQual vs. SalePrice\\')\\\\nplt.xlabel(\\'ExterQual\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.show()\"}, {\\'id\\': 24, \\'type\\': \\'code\\', \\'src\\': \"# Box plot for highly correlated data with output data i.e. SalePrice\\\\nplt.figure(figsize=(8, 6))\\\\nplt.scatter(training[\\'GarageArea\\'], training[\\'SalePrice\\'], c=\\'blue\\', alpha=0.7)\\\\nplt.xlabel(\\'GarageArea\\')\\\\nplt.ylabel(\\'Sale Price\\')\\\\nplt.title(\\'Scatter Plot: GarageArea vs. Sale Price\\')\\\\nplt.grid(True)\\\\nplt.show()\"}, {\\'id\\': 25, \\'type\\': \\'code\\', \\'src\\': \\'training.iloc[:, :80]\\'}, {\\'id\\': 26, \\'type\\': \\'code\\', \\'src\\': \"from sklearn.model_selection import train_test_split\\\\nxtrain, xtest, ytrain, ytest = train_test_split(np.array(training.iloc[:, :79]),training[\\'SalePrice\\'], test_size=0.2, random_state=42)\\\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\"}, {\\'id\\': 27, \\'type\\': \\'code\\', \\'src\\': \\'# XGBoost Model\\\\nimport xgboost as xgb\\\\nxgbModel = xgb.XGBRegressor(n_estimators=3500, learning_rate=0.05, random_state=42)\\\\nxgbModel.fit(xtrain, ytrain)\\\\nxgbpred = xgbModel.predict(xtest)\\\\nmse = mean_squared_error(ytest, xgbpred)\\\\nrmse = np.sqrt(mse)\\\\nr2 = r2_score(ytest, xgbpred)\\\\nprint(\\\\\\'eXtreme Boost Regression Model\\\\\\')\\\\nprint(\"Mean Squared Error (MSE):\", mse)\\\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\\\nprint(\"R-squared (R2) score:\", r2)\\'}, {\\'id\\': 28, \\'type\\': \\'code\\', \\'src\\': \"# Model Selection: xgboost\\\\nypred = xgbModel.predict(np.array(testing))\\\\nsubmission = pd.DataFrame()\\\\nsubmission[\\'Id\\'] = test[\\'Id\\']\\\\nsubmission[\\'SalePrice\\'] = ypred.astype(int)\\\\nsubmission.to_csv(\\'/kaggle/working/Submission.csv\\', index=False)\\\\nsubmission\"}]'},\n",
       " {'role': 'assistant', 'content': '## Summary'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_conclusion_prompt = conclusion_prompt(house, house_intro_result)\n",
    "house_conclusion_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3185"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(str(house_conclusion_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# house_conclusion_reason, house_conclusion_result = gpt_wrapper(house_conclusion_prompt)\n",
    "# print(house_conclusion_reason)\n",
    "with open('house_conclusion.txt', 'r') as f:\n",
    "    house_conclusion_result = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Summary\n",
      "This Jupyter Notebook aimed to predict house prices using random forest regression. The dataset was read using Pandas and preprocessed by handling null values and filling missing values. Categorical data was encoded using ordinal encoding. Data visualization techniques such as correlation matrix and histograms were used to analyze the data. Highly correlated features with the sale price were identified and analyzed. The data was then split into training and testing sets, and an XGBoost regression model was built and used to make predictions.\n",
      "\n",
      "## Interpretation\n",
      "The analysis of the dataset and the XGBoost regression model provided valuable insights into predicting house prices. The correlation matrix and histograms helped identify the key features that have a significant influence on the sale price. The highly correlated features, such as OverallQual, GrLivArea, GarageCars, ExterQual, and GarageArea, were found to have a strong impact on the sale price. The XGBoost regression model achieved good performance, as evidenced by the low mean squared error (MSE) and root mean squared error (RMSE) values, as well as a high R-squared (R2) score. This indicates that the model is able to accurately predict house prices based on the given features. Therefore, this notebook successfully achieved its goal of predicting house prices and improving upon the baseline models.\n"
     ]
    }
   ],
   "source": [
    "print(house_conclusion_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
